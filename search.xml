<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[EFK-5 ES集群开启用户认证]]></title>
    <url>%2F2020%2F04%2F14%2FEFK-5%EF%BC%9AES%E9%9B%86%E7%BE%A4%E5%BC%80%E5%90%AF%E7%94%A8%E6%88%B7%E8%AE%A4%E8%AF%81%20%2F</url>
    <content type="text"><![CDATA[转载自:https://mp.weixin.qq.com/s?__biz=MzUyNzk0NTI4MQ==&amp;mid=2247483826&amp;idx=1&amp;sn=583e9a526050682ae060f601eced917b&amp;chksm=fa769a9ccd01138a8740171769d1149a5df706ab3523eb0cbb5697293bda6e46954641d49f99&amp;mpshare=1&amp;scene=1&amp;srcid=01245nkmqHupjQF7csKql1i5&amp;sharer_sharetime=1579865464558&amp;sharer_shareid=6ec87ec9a11a0c18d61cde7663a9ef87#rd基于ES内置及自定义用户实现kibana和filebeat的认证关闭服务先关闭所有ElasticSearch、kibana、filebeat进程elasticsearch-修改elasticsearch.yml配置按以上表格对应的实例新增conf目录下elasticsearch.yml配置参数123456789# 在所有实例上加上以下配置# 开启本地用户xpack.security.enabled: true# xpack的版本xpack.license.self_generated.type: basicelasticsearch-开启服务开启所有ES服务1sudo -u elasticsearch ./bin/elasticsearchelasticsearch-建立本地内置用户本地内置elastic、apmsystem、kibana、logstashsystem、beatssystem、remotemonitoring_user用户1234567891011121314151617# 在其中一台master节点操作# interactive 自定密码 auto自动生密码sudo -u elasticsearch ./bin/elasticsearch-setup-passwords interactive# 输入elastic密码# 输入apm_system密码# 输入kibana密码# 输入logstash_system密码# 输入beats_system密码# 输入remote_monitoring_user密码测试内部用户通过base64将elastic用户进行加密，格式为“elastic:elastic的密码“123# 例如以下格式curl -H &quot;Authorization: Basic ZWxhc3RpYzplbGFzdGkxMjM0NTY3OA==&quot; &quot;http://192.168.1.31:9200/_cat/nodes?v&quot;如果不通过Basic访问或base64加密错误会报以下错误:“status”: 401kibana-创建私钥库在192.168.1.21创建私钥库12345678910111213cd /opt/kibana/# 创建密钥库sudo -u kibana ./bin/kibana-keystore create# 连接ES用户名，这里输入kibanasudo -u kibana ./bin/kibana-keystore add elasticsearch.username# 连接ES密码，这里输入刚刚设置kibana的密码sudo -u kibana ./bin/kibana-keystore add elasticsearch.password在192.168.1.21确认私钥库1sudo -u kibana ./bin/kibana-keystore list启动服务1sudo -u kibana /opt/kibana/bin/kibana -c /opt/kibana/config/kibana.ymlfilebeat-服务器上创建密钥库在192.168.1.11创建filebeat密钥库123456789cd /opt/filebeat/#创建密钥库./filebeat keystore create#创建test-filebeat用户私钥./filebeat keystore add test-filebeat确认filebeat密钥库1./filebeat keystore listfilebeat-配置filebeat.yml配置filebeat.yml123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293# 文件输入filebeat.inputs: # 文件输入类型 - type: log # 开启加载 enabled: true # 文件位置 paths: - /var/log/nginx/access.log # 自定义参数 fields: type: nginx_access # 类型是nginx_access,和上面fields.type是一致的# 输出至elasticsearchoutput.elasticsearch: # 连接ES集群的用户名 username: test-filebeat # 连接ES集群的密码 password: &quot;$&#123;test-filebeat密码&#125;&quot; # elasticsearch集群 hosts: [&quot;http://192.168.1.31:9200&quot;, &quot;http://192.168.1.32:9200&quot;, &quot;http://192.168.1.33:9200&quot;] # 索引配置 indices: # 索引名 - index: &quot;nginx_access_%&#123;+yyy.MM&#125;&quot; # 当类型是nginx_access时使用此索引 when.equals: fields.type: &quot;nginx_access&quot;# 关闭自带模板setup.template.enabled: false# 开启日志记录logging.to_files: true# 日志等级logging.level: info# 日志文件logging.files: # 日志位置 path: /opt/logs/filebeat/ # 日志名字 name: filebeat # 日志轮转期限，必须要2~1024 keepfiles: 7 # 日志轮转权限 permissions: 0600启动filebeat1/opt/filebeat/filebeat -e -c /opt/filebeat/filebeat.yml -d &quot;publish&quot;测试写入一条数据1curl -I &quot;http://192.168.1.11&quot;在kibana中查看附录kibana角色权限相关文档链接1https://www.elastic.co/guide/en/elasticsearch/reference/7.3/security-privileges.html#privileges-list-clusterbase64加密解密网站链接https://tool.oschina.net/encrypt?type=3]]></content>
      <categories>
        <category>elk</category>
      </categories>
      <tags>
        <tag>elk</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[EFK-4 ElasticSearch集群TLS加密通讯]]></title>
    <url>%2F2020%2F04%2F14%2FEFK-4%EF%BC%9AElasticSearch%E9%9B%86%E7%BE%A4TLS%E5%8A%A0%E5%AF%86%E9%80%9A%E8%AE%AF%20%2F</url>
    <content type="text"><![CDATA[转载自:https://mp.weixin.qq.com/s?__biz=MzUyNzk0NTI4MQ==&amp;mid=2247483822&amp;idx=1&amp;sn=6813b22eb5bd3a727a56e0fb5ba3f7fb&amp;chksm=fa769a80cd011396cb6717124ebb9fb17bbff2f9d1fbcae50578cb2959225055cce0268d3633&amp;mpshare=1&amp;scene=1&amp;srcid=1205igKg8cJK4Owayo9UNt4Q&amp;sharer_sharetime=1575553256278&amp;sharer_shareid=6ec87ec9a11a0c18d61cde7663a9ef87#rd基于TLS实现ElasticSearch集群加密通讯，为ES集群创建CA、CERT证书，实现ElasticSearch集群之间数据通过TLS进行双向加密交互。Step1. 关闭服务首先，需要停止所有ElasticSearch、kibana、filebeat服务，待证书配置完成后再启动Step2. 创建CA证书找任一一台ElasticSearch节点服务器操作即可123cd /opt/elasticsearch/# --days: 表示有效期多久sudo -u elasticsearch ./bin/elasticsearch-certutil ca --days 3660务必将生成的CA证书，传到安全地方永久存储，因为后期若需要新增ES节点，还会用到该证书请将elastic-stack-ca.p12证书传到所有ES实例服务器上Step3. 创建CERT证书按上面表格进入相对应的目录创建CERT证书1234567891011# 在ES目录中建立证书目录及给予elasticsearch权限mkdir -p config/certs;chown elasticsearch.elasticsearch config/certs -R# 每一个实例一个证书# --ca CA证书的文件名，必选参数# --dns 服务器名，多服务器名用逗号隔开，可选参数# --ip 服务器IP，多IP用逗号隔开，可选参数# --out 输出到哪里，可选参数# --days 有效期多久，可选参数sudo -u elasticsearch ./bin/elasticsearch-certutil cert --ca elastic-stack-ca.p12 --ip $&#123;本机IP&#125;,127.0.0.1 --out config/certs/cert.p12 --days 3660# 例如elasticsearch-master-1（192.168.1.31）执行命令：sudo -u elasticsearch ./bin/elasticsearch-certutil cert --ca elastic-stack-ca.p12 --ip 192.168.1.31,127.0.0.1 --out config/certs/cert.p12 --days 3660如果想批量生成CERT证书，请自行查阅附录链接，不过批量生成有时会碰到生成的证书不可用，因此建议一台一台生成Step4. 创建密钥库按上面表格进入相对应的目录创建密钥库1234567# 每一个实例都要操作# 创建密钥库sudo -u elasticsearch ./bin/elasticsearch-keystore create# PKCS＃12文件的密码sudo -u elasticsearch ./bin/elasticsearch-keystore add xpack.security.transport.ssl.keystore.secure_password# 信任库的密码sudo -u elasticsearch ./bin/elasticsearch-keystore add xpack.security.transport.ssl.truststore.secure_password确认keystore、truststore已录入至密钥库1sudo -u elasticsearch ./bin/elasticsearch-keystore listStep5. 删除CA证书由于上面创建的elastic-stack-ca.p12含有私钥，因此为了安全，建议将该文件删除（请务必提前备份好，因为后期增加节点还会用到）按上面表格进入相对应的目录删除CA证书1rm -f elastic-stack-ca.p12Step6. 修改elasticsearch.yml配置按上面表格对应的实例配置conf目录下elasticsearch.yml123456789# 在所有实例上加上以下配置# 开启transport.ssl认证xpack.security.transport.ssl.enabled: true# xpack认证方式 full为主机或IP认证及证书认证，certificates为证书认证，不对主机和IP认证，默认为fullxpack.security.transport.ssl.verification_mode: full# xpack包含私钥和证书的PKCS＃12文件的路径xpack.security.transport.ssl.keystore.path: certs/cert.p12# xpack包含要信任的证书的PKCS＃12文件的路径xpack.security.transport.ssl.truststore.path: certs/cert.p12Step7. 启动服务12345678# 开启所有ES实例sudo -u elasticsearch ./bin/elasticsearch# 开启filebeat/opt/filebeat/filebeat -e -c /opt/filebeat/filebeat.yml -d &quot;publish&quot;# 开启kibanasudo -u kibana /opt/kibana/bin/kibana -c /opt/kibana/config/kibana.yml附. 参考文档12https://www.elastic.co/guide/en/elasticsearch/reference/current/configuring-tls.htmlhttps://www.elastic.co/guide/en/elasticsearch/reference/7.3/certutil.html]]></content>
      <categories>
        <category>elk</category>
      </categories>
      <tags>
        <tag>elk</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[EFK-3 ES多实例部署]]></title>
    <url>%2F2020%2F04%2F14%2FEFK-3%EF%BC%9A%20ES%E5%A4%9A%E5%AE%9E%E4%BE%8B%E9%83%A8%E7%BD%B2%20%2F</url>
    <content type="text"><![CDATA[转载自:https://mp.weixin.qq.com/s?__biz=MzUyNzk0NTI4MQ==&amp;mid=2247483816&amp;idx=1&amp;sn=bfaf70613bcb775ccf5d40c2871a05a8&amp;chksm=fa769a86cd011390f22ff178071a580a8f17791e57166dfc8463984a5613c11875ef2ebb2ad7&amp;mpshare=1&amp;scene=1&amp;srcid=11253n8AXjLegAeaoHiCssEs&amp;sharer_sharetime=1574686178097&amp;sharer_shareid=6ec87ec9a11a0c18d61cde7663a9ef87#rd基于ElasticSearch多实例架构，实现资源合理分配、冷热数据分离。ES多实例部署，将不同热度的数据存在不同的磁盘上，实现了数据冷热分离、资源合理分配。在一个集群中部署多个ES实例，来实现资源合理分配。例如data服务器存在SSD与SAS硬盘，可以将热数据存放到SSD，而冷数据存放到SAS，实现数据冷热分离。192.168.1.51 elasticsearch-data部署双实例索引迁移（此步不能忽略）：将192.168.1.51上的索引放到其它2台data节点上1234curl -X PUT &quot;192.168.1.31:9200/*/_settings?pretty&quot; -H &apos;Content-Type: application/json&apos; -d&apos;&#123; &quot;index.routing.allocation.include._ip&quot;: &quot;192.168.1.52,192.168.1.53&quot;&#125;&apos;确认当前索引存储位置确认所有索引不在192.168.1.51节点上1curl &quot;http://192.168.1.31:9200/_cat/shards?h=n&quot;停掉192.168.1.51的进程，修改目录结构及配置：请自行按SSD和SAS硬盘挂载好数据盘1234567891011121314151617181920212223# 安装包下载和部署请参考第一篇《EFK-1: 快速指南》cd /opt/software/tar -zxvf elasticsearch-7.3.2-linux-x86_64.tar.gzmv /opt/elasticsearch /opt/elasticsearch-SASmv elasticsearch-7.3.2 /opt/mv /opt/elasticsearch-7.3.2 /opt/elasticsearch-SSDchown elasticsearch.elasticsearch /opt/elasticsearch-* -Rrm -rf /data/SAS/*chown elasticsearch.elasticsearch /data/* -Rmkdir -p /opt/logs/elasticsearch-SASmkdir -p /opt/logs/elasticsearch-SSDchown elasticsearch.elasticsearch /opt/logs/* -R123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475# SAS实例/opt/elasticsearch-SAS/config/elasticsearch.yml配置 cluster.name: my-application node.name: 192.168.1.51-SAS path.data: /data/SAS path.logs: /opt/logs/elasticsearch-SAS network.host: 192.168.1.51 http.port: 9200 transport.port: 9300 # discovery.seed_hosts和cluster.initial_master_nodes 一定要带上端口号，不然会走http.port和transport.port端口 discovery.seed_hosts: [&quot;192.168.1.31:9300&quot;,&quot;192.168.1.32:9300&quot;,&quot;192.168.1.33:9300&quot;] cluster.initial_master_nodes: [&quot;192.168.1.31:9300&quot;,&quot;192.168.1.32:9300&quot;,&quot;192.168.1.33:9300&quot;] http.cors.enabled: true http.cors.allow-origin: &quot;*&quot; node.master: false node.ingest: false node.data: true # 本机只允行启2个实例 node.max_local_storage_nodes: 2# SSD实例/opt/elasticsearch-SSD/config/elasticsearch.yml配置 cluster.name: my-application node.name: 192.168.1.51-SSD path.data: /data/SSD path.logs: /opt/logs/elasticsearch-SSD network.host: 192.168.1.51 http.port: 9201 transport.port: 9301 # discovery.seed_hosts和cluster.initial_master_nodes 一定要带上端口号，不然会走http.port和transport.port端口 discovery.seed_hosts: [&quot;192.168.1.31:9300&quot;,&quot;192.168.1.32:9300&quot;,&quot;192.168.1.33:9300&quot;] cluster.initial_master_nodes: [&quot;192.168.1.31:9300&quot;,&quot;192.168.1.32:9300&quot;,&quot;192.168.1.33:9300&quot;] http.cors.enabled: true http.cors.allow-origin: &quot;*&quot; node.master: false node.ingest: false node.data: true # 本机只允行启2个实例 node.max_local_storage_nodes: 2SAS实例和SSD实例启动方式123sudo -u elasticsearch /opt/elasticsearch-SAS/bin/elasticsearchsudo -u elasticsearch /opt/elasticsearch-SSD/bin/elasticsearch确认SAS和SSD已启2实例1curl &quot;http://192.168.1.31:9200/_cat/nodes?v&quot;192.168.1.52 elasticsearch-data部署双实例索引迁移（此步不能忽略）：将192.168.1.52上的索引放到其它2台data节点上1234curl -X PUT &quot;192.168.1.31:9200/*/_settings?pretty&quot; -H &apos;Content-Type: application/json&apos; -d&apos;&#123; &quot;index.routing.allocation.include._ip&quot;: &quot;192.168.1.51,192.168.1.53&quot;&#125;&apos;确认当前索引存储位置确认所有索引不在192.168.1.52节点上1curl &quot;http://192.168.1.31:9200/_cat/shards?h=n&quot;停掉192.168.1.52的进程，修改目录结构及配置：请自行按SSD和SAS硬盘挂载好数据盘1234567891011121314151617181920212223# 安装包下载和部署请参考第一篇《EFK-1: 快速指南》cd /opt/software/tar -zxvf elasticsearch-7.3.2-linux-x86_64.tar.gzmv /opt/elasticsearch /opt/elasticsearch-SASmv elasticsearch-7.3.2 /opt/mv /opt/elasticsearch-7.3.2 /opt/elasticsearch-SSDchown elasticsearch.elasticsearch /opt/elasticsearch-* -Rrm -rf /data/SAS/*chown elasticsearch.elasticsearch /data/* -Rmkdir -p /opt/logs/elasticsearch-SASmkdir -p /opt/logs/elasticsearch-SSDchown elasticsearch.elasticsearch /opt/logs/* -R123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475# SAS实例/opt/elasticsearch-SAS/config/elasticsearch.yml配置 cluster.name: my-application node.name: 192.168.1.52-SAS path.data: /data/SAS path.logs: /opt/logs/elasticsearch-SAS network.host: 192.168.1.52 http.port: 9200 transport.port: 9300 # discovery.seed_hosts和cluster.initial_master_nodes 一定要带上端口号，不然会走http.port和transport.port端口 discovery.seed_hosts: [&quot;192.168.1.31:9300&quot;,&quot;192.168.1.32:9300&quot;,&quot;192.168.1.33:9300&quot;] cluster.initial_master_nodes: [&quot;192.168.1.31:9300&quot;,&quot;192.168.1.32:9300&quot;,&quot;192.168.1.33:9300&quot;] http.cors.enabled: true http.cors.allow-origin: &quot;*&quot; node.master: false node.ingest: false node.data: true # 本机只允行启2个实例 node.max_local_storage_nodes: 2# SSD实例/opt/elasticsearch-SSD/config/elasticsearch.yml配置 cluster.name: my-application node.name: 192.168.1.52-SSD path.data: /data/SSD path.logs: /opt/logs/elasticsearch-SSD network.host: 192.168.1.52 http.port: 9201 transport.port: 9301 # discovery.seed_hosts和cluster.initial_master_nodes 一定要带上端口号，不然会走http.port和transport.port端口 discovery.seed_hosts: [&quot;192.168.1.31:9300&quot;,&quot;192.168.1.32:9300&quot;,&quot;192.168.1.33:9300&quot;] cluster.initial_master_nodes: [&quot;192.168.1.31:9300&quot;,&quot;192.168.1.32:9300&quot;,&quot;192.168.1.33:9300&quot;] http.cors.enabled: true http.cors.allow-origin: &quot;*&quot; node.master: false node.ingest: false node.data: true # 本机只允行启2个实例 node.max_local_storage_nodes: 2SAS实例和SSD实例启动方式123sudo -u elasticsearch /opt/elasticsearch-SAS/bin/elasticsearchsudo -u elasticsearch /opt/elasticsearch-SSD/bin/elasticsearch确认SAS和SSD已启2实例1curl &quot;http://192.168.1.31:9200/_cat/nodes?v&quot;192.168.1.53 elasticsearch-data部署双实例索引迁移（此步不能忽略）：一定要做这步，将192.168.1.53上的索引放到其它2台data节点上1234curl -X PUT &quot;192.168.1.31:9200/*/_settings?pretty&quot; -H &apos;Content-Type: application/json&apos; -d&apos;&#123; &quot;index.routing.allocation.include._ip&quot;: &quot;192.168.1.51,192.168.1.52&quot;&#125;&apos;确认当前索引存储位置确认所有索引不在192.168.1.52节点上1curl &quot;http://192.168.1.31:9200/_cat/shards?h=n&quot;停掉192.168.1.53的进程，修改目录结构及配置：请自行按SSD和SAS硬盘挂载好数据盘1234567891011121314151617181920212223# 安装包下载和部署请参考第一篇《EFK-1: 快速指南》cd /opt/software/tar -zxvf elasticsearch-7.3.2-linux-x86_64.tar.gzmv /opt/elasticsearch /opt/elasticsearch-SASmv elasticsearch-7.3.2 /opt/mv /opt/elasticsearch-7.3.2 /opt/elasticsearch-SSDchown elasticsearch.elasticsearch /opt/elasticsearch-* -Rrm -rf /data/SAS/*chown elasticsearch.elasticsearch /data/* -Rmkdir -p /opt/logs/elasticsearch-SASmkdir -p /opt/logs/elasticsearch-SSDchown elasticsearch.elasticsearch /opt/logs/* -R123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475# SAS实例/opt/elasticsearch-SAS/config/elasticsearch.yml配置 cluster.name: my-application node.name: 192.168.1.53-SAS path.data: /data/SAS path.logs: /opt/logs/elasticsearch-SAS network.host: 192.168.1.53 http.port: 9200 transport.port: 9300 # discovery.seed_hosts和cluster.initial_master_nodes 一定要带上端口号，不然会走http.port和transport.port端口 discovery.seed_hosts: [&quot;192.168.1.31:9300&quot;,&quot;192.168.1.32:9300&quot;,&quot;192.168.1.33:9300&quot;] cluster.initial_master_nodes: [&quot;192.168.1.31:9300&quot;,&quot;192.168.1.32:9300&quot;,&quot;192.168.1.33:9300&quot;] http.cors.enabled: true http.cors.allow-origin: &quot;*&quot; node.master: false node.ingest: false node.data: true # 本机只允行启2个实例 node.max_local_storage_nodes: 2# SSD实例/opt/elasticsearch-SSD/config/elasticsearch.yml配置 cluster.name: my-application node.name: 192.168.1.53-SSD path.data: /data/SSD path.logs: /opt/logs/elasticsearch-SSD network.host: 192.168.1.53 http.port: 9201 transport.port: 9301 # discovery.seed_hosts和cluster.initial_master_nodes 一定要带上端口号，不然会走http.port和transport.port端口 discovery.seed_hosts: [&quot;192.168.1.31:9300&quot;,&quot;192.168.1.32:9300&quot;,&quot;192.168.1.33:9300&quot;] cluster.initial_master_nodes: [&quot;192.168.1.31:9300&quot;,&quot;192.168.1.32:9300&quot;,&quot;192.168.1.33:9300&quot;] http.cors.enabled: true http.cors.allow-origin: &quot;*&quot; node.master: false node.ingest: false node.data: true # 本机只允行启2个实例 node.max_local_storage_nodes: 2SAS实例和SSD实例启动方式123sudo -u elasticsearch /opt/elasticsearch-SAS/bin/elasticsearchsudo -u elasticsearch /opt/elasticsearch-SSD/bin/elasticsearch确认SAS和SSD已启2实例1curl &quot;http://192.168.1.31:9200/_cat/nodes?v&quot;测试将所有索引移到SSD硬盘上1234567891011121314151617# 下面的参数会在后面的文章讲解，此处照抄即可curl -X PUT &quot;192.168.1.31:9200/*/_settings?pretty&quot; -H &apos;Content-Type: application/json&apos; -d&apos;&#123; &quot;index.routing.allocation.include._host_ip&quot;: &quot;&quot;, &quot;index.routing.allocation.include._host&quot;: &quot;&quot;, &quot;index.routing.allocation.include._name&quot;: &quot;&quot;, &quot;index.routing.allocation.include._ip&quot;: &quot;&quot;, &quot;index.routing.allocation.require._name&quot;: &quot;*-SSD&quot;&#125;&apos;确认所有索引全在SSD硬盘上1curl &quot;http://192.168.1.31:9200/_cat/shards?h=n&quot;将nginx9月份的日志索引迁移到SAS硬盘上1234curl -X PUT &quot;192.168.1.31:9200/nginx_*_2019.09/_settings?pretty&quot; -H &apos;Content-Type: application/json&apos; -d&apos;&#123; &quot;index.routing.allocation.require._name&quot;: &quot;*-SAS&quot;&#125;&apos;确认nginx9月份的日志索引迁移到SAS硬盘上1curl &quot;http://192.168.1.31:9200/_cat/shards&quot;]]></content>
      <categories>
        <category>elk</category>
      </categories>
      <tags>
        <tag>elk</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[EFK-2 ElasticSearch高性能高可用架构]]></title>
    <url>%2F2020%2F04%2F14%2FEFK-2%EF%BC%9AElasticSearch%E9%AB%98%E6%80%A7%E8%83%BD%E9%AB%98%E5%8F%AF%E7%94%A8%E6%9E%B6%E6%9E%84%2F</url>
    <content type="text"><![CDATA[转载自:https://mp.weixin.qq.com/s?__biz=MzUyNzk0NTI4MQ==&amp;mid=2247483811&amp;idx=1&amp;sn=a413dea65f8f64abb24d82feea55db5b&amp;chksm=fa769a8dcd01139b1da8794914e10989c6a39a99971d8013e9d3b26766b80d5833e2fbaf0ab8&amp;mpshare=1&amp;scene=1&amp;srcid=1125tjbylqn3EdoMtaX2p73J&amp;sharer_sharetime=1574686271229&amp;sharer_shareid=6ec87ec9a11a0c18d61cde7663a9ef87#rd阐述了EFK的data/ingest/master角色的用途及分别部署三节点，在实现性能最大化的同时保障高可用elasticsearch-data安装3台均执行相同的安装步骤12345678910111213141516171819202122tar -zxvf elasticsearch-7.3.2-linux-x86_64.tar.gzmv elasticsearch-7.3.2 /opt/elasticsearchuseradd elasticsearch -d /opt/elasticsearch -s /sbin/nologinmkdir -p /opt/logs/elasticsearchchown elasticsearch.elasticsearch /opt/elasticsearch -Rchown elasticsearch.elasticsearch /opt/logs/elasticsearch -R# 数据盘需要elasticsearch写权限chown elasticsearch.elasticsearch /data/SAS -R# 限制一个进程可以拥有的VMA(虚拟内存区域)的数量要超过262144，不然elasticsearch会报max virtual memory areas vm.max_map_count [65535] is too low, increase to at least [262144]echo &quot;vm.max_map_count = 655350&quot; &gt;&gt; /etc/sysctl.confsysctl -pelasticsearch-data配置123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104# 192.168.1.51 /opt/elasticsearch/config/elasticsearch.yml cluster.name: my-application node.name: 192.168.1.51 # 数据盘位置，如果有多个硬盘位置，用&quot;,&quot;隔开 path.data: /data/SAS path.logs: /opt/logs/elasticsearch network.host: 192.168.1.51 discovery.seed_hosts: [&quot;192.168.1.31&quot;,&quot;192.168.1.32&quot;,&quot;192.168.1.33&quot;] cluster.initial_master_nodes: [&quot;192.168.1.31&quot;,&quot;192.168.1.32&quot;,&quot;192.168.1.33&quot;] http.cors.enabled: true http.cors.allow-origin: &quot;*&quot; # 关闭master功能 node.master: false # 关闭ingest功能 node.ingest: false # 开启data功能 node.data: true# 192.168.1.52 /opt/elasticsearch/config/elasticsearch.yml cluster.name: my-application node.name: 192.168.1.52 # 数据盘位置，如果有多个硬盘位置，用&quot;,&quot;隔开 path.data: /data/SAS path.logs: /opt/logs/elasticsearch network.host: 192.168.1.52 discovery.seed_hosts: [&quot;192.168.1.31&quot;,&quot;192.168.1.32&quot;,&quot;192.168.1.33&quot;] cluster.initial_master_nodes: [&quot;192.168.1.31&quot;,&quot;192.168.1.32&quot;,&quot;192.168.1.33&quot;] http.cors.enabled: true http.cors.allow-origin: &quot;*&quot; # 关闭master功能 node.master: false # 关闭ingest功能 node.ingest: false # 开启data功能 node.data: true# 192.168.1.53 /opt/elasticsearch/config/elasticsearch.yml cluster.name: my-application node.name: 192.168.1.53 # 数据盘位置，如果有多个硬盘位置，用&quot;,&quot;隔开 path.data: /data/SAS path.logs: /opt/logs/elasticsearch network.host: 192.168.1.53 discovery.seed_hosts: [&quot;192.168.1.31&quot;,&quot;192.168.1.32&quot;,&quot;192.168.1.33&quot;] cluster.initial_master_nodes: [&quot;192.168.1.31&quot;,&quot;192.168.1.32&quot;,&quot;192.168.1.33&quot;] http.cors.enabled: true http.cors.allow-origin: &quot;*&quot; # 关闭master功能 node.master: false # 关闭ingest功能 node.ingest: false # 开启data功能 node.data: trueelasticsearch-data启动1sudo -u elasticsearch /opt/elasticsearch/bin/elasticsearchelasticsearch集群状态1curl &quot;http://192.168.1.31:9200/_cat/health?v&quot;elasticsearch-data状态1curl &quot;http://192.168.1.31:9200/_cat/nodes?v&quot;elasticsearch-data参数说明12345678910111213status: green # 集群健康状态node.total: 6 # 有6台机子组成集群node.data: 6 # 有6个节点的存储node.role: d # 只拥有data角色node.role: i # 只拥有ingest角色node.role: m # 只拥有master角色node.role: mid # 拥master、ingest、data角色elasticsearch-ingest新增三台ingest节点加入集群，同时关闭master和data功能elasticsearch-ingest安装3台es均执行相同的安装步骤123456789101112131415161718tar -zxvf elasticsearch-7.3.2-linux-x86_64.tar.gzmv elasticsearch-7.3.2 /opt/elasticsearchuseradd elasticsearch -d /opt/elasticsearch -s /sbin/nologinmkdir -p /opt/logs/elasticsearchchown elasticsearch.elasticsearch /opt/elasticsearch -Rchown elasticsearch.elasticsearch /opt/logs/elasticsearch -R# 限制一个进程可以拥有的VMA(虚拟内存区域)的数量要超过262144，不然elasticsearch会报max virtual memory areas vm.max_map_count [65535] is too low, increase to at least [262144]echo &quot;vm.max_map_count = 655350&quot; &gt;&gt; /etc/sysctl.confsysctl -pelasticsearch-ingest配置1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192# 192.168.1.41 /opt/elasticsearch/config/elasticsearch.yml cluster.name: my-application node.name: 192.168.1.41 path.logs: /opt/logs/elasticsearch network.host: 192.168.1.41 discovery.seed_hosts: [&quot;192.168.1.31&quot;,&quot;192.168.1.32&quot;,&quot;192.168.1.33&quot;] cluster.initial_master_nodes: [&quot;192.168.1.31&quot;,&quot;192.168.1.32&quot;,&quot;192.168.1.33&quot;] http.cors.enabled: true http.cors.allow-origin: &quot;*&quot; # 关闭master功能 node.master: false # 开启ingest功能 node.ingest: true # 关闭data功能 node.data: false# 192.168.1.42 /opt/elasticsearch/config/elasticsearch.yml cluster.name: my-application node.name: 192.168.1.42 path.logs: /opt/logs/elasticsearch network.host: 192.168.1.42 discovery.seed_hosts: [&quot;192.168.1.31&quot;,&quot;192.168.1.32&quot;,&quot;192.168.1.33&quot;] cluster.initial_master_nodes: [&quot;192.168.1.31&quot;,&quot;192.168.1.32&quot;,&quot;192.168.1.33&quot;] http.cors.enabled: true http.cors.allow-origin: &quot;*&quot; # 关闭master功能 node.master: false # 开启ingest功能 node.ingest: true # 关闭data功能 node.data: false# 192.168.1.43 /opt/elasticsearch/config/elasticsearch.yml cluster.name: my-application node.name: 192.168.1.43 path.logs: /opt/logs/elasticsearch network.host: 192.168.1.43 discovery.seed_hosts: [&quot;192.168.1.31&quot;,&quot;192.168.1.32&quot;,&quot;192.168.1.33&quot;] cluster.initial_master_nodes: [&quot;192.168.1.31&quot;,&quot;192.168.1.32&quot;,&quot;192.168.1.33&quot;] http.cors.enabled: true http.cors.allow-origin: &quot;*&quot; # 关闭master功能 node.master: false # 开启ingest功能 node.ingest: true # 关闭data功能 node.data: falseelasticsearch-ingest启动1sudo -u elasticsearch /opt/elasticsearch/bin/elasticsearchelasticsearch集群状态1curl &quot;http://192.168.1.31:9200/_cat/health?v&quot;elasticsearch-ingest状态1curl &quot;http://192.168.1.31:9200/_cat/nodes?v&quot;elasticsearch-ingest参数说明12345678910111213141516171819 status: green # 集群健康状态 node.total: 9 # 有9台机子组成集群 node.data: 6 # 有6个节点的存储 node.role: d # 只拥有data角色 node.role: i # 只拥有ingest角色 node.role: m # 只拥有master角色 node.role: mid # 拥master、ingest、data角色``` # elasticsearch-master首先，将上一篇《EFK-1》中部署的3台es（192.168.1.31、192.168.1.32、192.168.1.33）改成只有master的功能， 因此需要先将这3台上的索引数据迁移到本次所做的data节点中## 索引迁移一定要做这步，将之前的索引放到data节点上curl -X PUT &quot;192.168.1.31:9200/*/_settings?pretty&quot; -H &apos;Content-Type: application/json&apos; -d&apos; { &quot;index.routing.allocation.include._ip&quot;: &quot;192.168.1.51,192.168.1.52,192.168.1.53&quot; }&apos; 12## 确认当前索引存储位置确认所有索引不在192.168.1.31、192.168.1.32、192.168.1.33节点上curl &quot;http://192.168.1.31:9200/_cat/shards?h=n&quot; 12## elasticsearch-master配置注意事项：修改配置，重启进程，需要一台一台执行，要确保第一台成功后，再执行下一台。192.168.1.31 /opt/elasticsearch/config/elasticsearch.ymlcluster.name: my-application node.name: 192.168.1.31 path.logs: /opt/logs/elasticsearch network.host: 192.168.1.31 discovery.seed_hosts: [&quot;192.168.1.31&quot;,&quot;192.168.1.32&quot;,&quot;192.168.1.33&quot;] cluster.initial_master_nodes: [&quot;192.168.1.31&quot;,&quot;192.168.1.32&quot;,&quot;192.168.1.33&quot;] http.cors.enabled: true http.cors.allow-origin: &quot;*&quot; #开启master功能 node.master: true #关闭ingest功能 node.ingest: false #关闭data功能 node.data: false 192.168.1.32 /opt/elasticsearch/config/elasticsearch.ymlcluster.name: my-application node.name: 192.168.1.32 path.logs: /opt/logs/elasticsearch network.host: 192.168.1.32 discovery.seed_hosts: [&quot;192.168.1.31&quot;,&quot;192.168.1.32&quot;,&quot;192.168.1.33&quot;] cluster.initial_master_nodes: [&quot;192.168.1.31&quot;,&quot;192.168.1.32&quot;,&quot;192.168.1.33&quot;] http.cors.enabled: true http.cors.allow-origin: &quot;*&quot; #开启master功能 node.master: true #关闭ingest功能 node.ingest: false #关闭data功能 node.data: false 192.168.1.33 /opt/elasticsearch/config/elasticsearch.ymlcluster.name: my-application node.name: 192.168.1.33 path.logs: /opt/logs/elasticsearch network.host: 192.168.1.33 discovery.seed_hosts: [&quot;192.168.1.31&quot;,&quot;192.168.1.32&quot;,&quot;192.168.1.33&quot;] cluster.initial_master_nodes: [&quot;192.168.1.31&quot;,&quot;192.168.1.32&quot;,&quot;192.168.1.33&quot;] http.cors.enabled: true http.cors.allow-origin: &quot;*&quot; #开启master功能 node.master: true #关闭ingest功能 node.ingest: false #关闭data功能 node.data: false 1## elasticsearch集群状态curl &quot;http://192.168.1.31:9200/_cat/health?v&quot; 1## elasticsearch-master状态curl &quot;http://192.168.1.31:9200/_cat/nodes?v&quot; `至此，当node.role里所有服务器都不再出现“mid”，则表示一切顺利完成。]]></content>
      <categories>
        <category>elk</category>
      </categories>
      <tags>
        <tag>elk</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[EFK-1 快速指南]]></title>
    <url>%2F2020%2F04%2F14%2FEFK-1%EF%BC%9A%E5%BF%AB%E9%80%9F%E6%8C%87%E5%8D%97%2F</url>
    <content type="text"><![CDATA[转载自:https://mp.weixin.qq.com/s?__biz=MzUyNzk0NTI4MQ==&amp;mid=2247483801&amp;idx=1&amp;sn=11fee5756c8770688238624802ac51ea&amp;chksm=fa769ab7cd0113a1ad19241290abe374b857227eebe989b3ba6b671b1eca855d380b76eeedde&amp;mpshare=1&amp;scene=1&amp;srcid=1125q5BPyFOD05H2trj4UdOf&amp;sharer_sharetime=1574686325386&amp;sharer_shareid=6ec87ec9a11a0c18d61cde7663a9ef87#阐述了EFK的安装部署，其中ES的架构为三节点，即master、ingest、data角色同时部署在三台服务器上。elasticsearch安装：3台es均执行相同的安装步骤12345678910111213141516171819202122mkdir -p /opt/software &amp;&amp; cd /opt/softwarewget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.3.2-linux-x86_64.tar.gztar -zxvf elasticsearch-7.3.2-linux-x86_64.tar.gzmv elasticsearch-7.3.2 /opt/elasticsearchuseradd elasticsearch -d /opt/elasticsearch -s /sbin/nologinmkdir -p /opt/logs/elasticsearchchown elasticsearch.elasticsearch /opt/elasticsearch -Rchown elasticsearch.elasticsearch /opt/logs/elasticsearch -R# 限制一个进程可以拥有的VMA(虚拟内存区域)的数量要超过262144，不然elasticsearch会报max virtual memory areas vm.max_map_count [65535] is too low, increase to at least [262144]echo &quot;vm.max_map_count = 655350&quot; &gt;&gt; /etc/sysctl.confsysctl -pfilebeat安装123456789mkdir -p /opt/software &amp;&amp; cd /opt/softwarewget https://artifacts.elastic.co/downloads/beats/filebeat/filebeat-7.3.2-linux-x86_64.tar.gzmkdir -p /opt/logs/filebeat/tar -zxvf filebeat-7.3.2-linux-x86_64.tar.gzmv filebeat-7.3.2-linux-x86_64 /opt/filebeatkibana安装1234567891011mkdir -p /opt/software &amp;&amp; cd /opt/softwarewget https://artifacts.elastic.co/downloads/kibana/kibana-7.3.2-linux-x86_64.tar.gztar -zxvf kibana-7.3.2-linux-x86_64.tar.gzmv kibana-7.3.2-linux-x86_64 /opt/kibanauseradd kibana -d /opt/kibana -s /sbin/nologinchown kibana.kibana /opt/kibana -Rnginx安装12345# 只在192.168.1.11安装yum install -y nginx/usr/sbin/nginx -c /etc/nginx/nginx.confelasticsearch配置123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155# 192.168.1.31 /opt/elasticsearch/config/elasticsearch.yml # 集群名字 cluster.name: my-application # 节点名字 node.name: 192.168.1.31 # 日志位置 path.logs: /opt/logs/elasticsearch # 本节点访问IP network.host: 192.168.1.31 # 本节点访问 http.port: 9200 # 节点运输端口 transport.port: 9300 # 集群中其他主机的列表 discovery.seed_hosts: [&quot;192.168.1.31&quot;, &quot;192.168.1.32&quot;, &quot;192.168.1.33&quot;] # 首次启动全新的Elasticsearch集群时，在第一次选举中便对其票数进行计数的master节点的集合 cluster.initial_master_nodes: [&quot;192.168.1.31&quot;, &quot;192.168.1.32&quot;, &quot;192.168.1.33&quot;] # 启用跨域资源共享 http.cors.enabled: true http.cors.allow-origin: &quot;*&quot; # 只要有2台数据或主节点已加入集群，就可以恢复 gateway.recover_after_nodes: 2# 192.168.1.32 /opt/elasticsearch/config/elasticsearch.yml # 集群名字 cluster.name: my-application # 节点名字 node.name: 192.168.1.32 # 日志位置 path.logs: /opt/logs/elasticsearch # 本节点访问IP network.host: 192.168.1.32 # 本节点访问 http.port: 9200 # 节点运输端口 transport.port: 9300 # 集群中其他主机的列表 discovery.seed_hosts: [&quot;192.168.1.31&quot;, &quot;192.168.1.32&quot;, &quot;192.168.1.33&quot;] # 首次启动全新的Elasticsearch集群时，在第一次选举中便对其票数进行计数的master节点的集合 cluster.initial_master_nodes: [&quot;192.168.1.31&quot;, &quot;192.168.1.32&quot;, &quot;192.168.1.33&quot;] # 启用跨域资源共享 http.cors.enabled: true http.cors.allow-origin: &quot;*&quot; # 只要有2台数据或主节点已加入集群，就可以恢复 gateway.recover_after_nodes: 2# 192.168.1.33 /opt/elasticsearch/config/elasticsearch.yml # 集群名字 cluster.name: my-application # 节点名字 node.name: 192.168.1.33 # 日志位置 path.logs: /opt/logs/elasticsearch # 本节点访问IP network.host: 192.168.1.33 # 本节点访问 http.port: 9200 # 节点运输端口 transport.port: 9300 # 集群中其他主机的列表 discovery.seed_hosts: [&quot;192.168.1.31&quot;, &quot;192.168.1.32&quot;, &quot;192.168.1.33&quot;] # 首次启动全新的Elasticsearch集群时，在第一次选举中便对其票数进行计数的master节点的集合 cluster.initial_master_nodes: [&quot;192.168.1.31&quot;, &quot;192.168.1.32&quot;, &quot;192.168.1.33&quot;] # 启用跨域资源共享 http.cors.enabled: true http.cors.allow-origin: &quot;*&quot; # 只要有2台数据或主节点已加入集群，就可以恢复 gateway.recover_after_nodes: 2filebeat配置1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586# 192.168.1.11 /opt/filebeat/filebeat.yml # 文件输入 filebeat.inputs: # 文件输入类型 - type: log # 开启加载 enabled: true # 文件位置 paths: - /var/log/nginx/access.log # 自定义参数 fields: type: nginx_access # 类型是nginx_access,和上面fields.type是一致的 # 输出至elasticsearch output.elasticsearch: # elasticsearch集群 hosts: [&quot;http://192.168.1.31:9200&quot;, &quot;http://192.168.1.32:9200&quot;, &quot;http://192.168.1.33:9200&quot;] # 索引配置 indices: # 索引名 - index: &quot;nginx_access_%&#123;+yyy.MM&#125;&quot; # 当类型是nginx_access时使用此索引 when.equals: fields.type: &quot;nginx_access&quot; # 关闭自带模板 setup.template.enabled: false # 开启日志记录 logging.to_files: true # 日志等级 logging.level: info # 日志文件 logging.files: # 日志位置 path: /opt/logs/filebeat/ # 日志名字 name: filebeat # 日志轮转期限，必须要2~1024 keepfiles: 7 # 日志轮转权限 permissions: 0600kibana配置1234567891011121314151617181920212223# 192.168.1.21 /opt/kibana/config/kibana.yml # 本节点访问端口 server.port: 5601 # 本节点IP server.host: &quot;192.168.1.21&quot; # 本节点名字 server.name: &quot;192.168.1.21&quot; # elasticsearch集群IP elasticsearch.hosts: [&quot;http://192.168.1.31:9200&quot;, &quot;http://192.168.1.32:9200&quot;, &quot;http://192.168.1.33:9200&quot;]启动服务12345678910111213# elasticsearch启动（3台es均启动）sudo -u elasticsearch /opt/elasticsearch/bin/elasticsearch# filebeat启动/opt/filebeat/filebeat -e -c /opt/filebeat/filebeat.yml -d &quot;publish&quot;# kibana启动sudo -u kibana /opt/kibana/bin/kibana -c /opt/kibana/config/kibana.yml]]></content>
      <categories>
        <category>elk</category>
      </categories>
      <tags>
        <tag>elk</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基于ELK Nginx日志分析]]></title>
    <url>%2F2020%2F04%2F14%2F%E5%9F%BA%E4%BA%8EELK%20Nginx%E6%97%A5%E5%BF%97%E5%88%86%E6%9E%90%20%2F</url>
    <content type="text"><![CDATA[配置Nginx 日志Nginx 默认的access 日志为log格式，需要logstash 进行正则匹配和清洗处理，从而极大的增加了logstash的压力 所以我们Nginx 的日志修改为json 格式 。Nginx access 日志和 Nginx error 日志12345678910111213141516171819202122http &#123; include /etc/nginx/mime.types; default_type application/octet-stream; log_format json &apos;&#123;&quot;@timestamp&quot;:&quot;$time_iso8601&quot;,&apos; &apos;&quot;server_addr&quot;:&quot;$server_addr&quot;,&apos; &apos;&quot;hostname&quot;:&quot;$hostname&quot;,&apos; &apos;&quot;remote_add&quot;:&quot;$remote_addr&quot;,&apos; &apos;&quot;request_method&quot;:&quot;$request_method&quot;,&apos; &apos;&quot;scheme&quot;:&quot;$scheme&quot;,&apos; &apos;&quot;server_name&quot;:&quot;$server_name&quot;,&apos; &apos;&quot;http_referer&quot;:&quot;$http_referer&quot;,&apos; &apos;&quot;request_uri&quot;:&quot;$request_uri&quot;,&apos; &apos;&quot;args&quot;:&quot;$args&quot;,&apos; &apos;&quot;body_bytes_sent&quot;:$body_bytes_sent,&apos; &apos;&quot;status&quot;: $status,&apos; &apos;&quot;request_time&quot;:$request_time,&apos; &apos;&quot;upstream_response_time&quot;:&quot;$upstream_response_time&quot;,&apos; &apos;&quot;upstream_addr&quot;:&quot;$upstream_addr&quot;,&apos; &apos;&quot;http_user_agent&quot;:&quot;$http_user_agent&quot;,&apos; &apos;&quot;https&quot;:&quot;$https&quot;&apos; &apos;&#125;&apos;; access_log /var/log/nginx/access.log json;针对不同的虚拟主机配置Nginx日志1234access_log /var/log/nginx/80.access.log json;error_log /var/log/nginx/80.error.log error;access_log /var/log/nginx/8001.access.log json;error_log /var/log/nginx/8001.error.log error;Nginx error_log 类型1[ debug | info | notice | warn | error | crit ]例如：error_log /var/log/nginx/8001.error.log crit;解释：日志文件存储在/var/log/nginx/8001.error.log 文件中，错误类型为 crit ，也就是记录最少错误信息（debug最详细 crit最少）；filebeat 配置针对.access.log 和 .error.log 的日志进行不同的标签封装12345678910111213141516171819202122232425[root@elk-node1 nginx]# egrep -v &quot;*#|^$&quot; /etc/filebeat/filebeat.yml filebeat.inputs:- type: log enabled: true paths: - /var/log/nginx/*.access.log tags: [&quot;nginx.access&quot;]- type: log paths: - /var/log/nginx/*.error.log tags: [&quot;nginx.error&quot;]filebeat.config.modules: path: $&#123;path.config&#125;/modules.d/*.yml reload.enabled: falsesetup.template.settings: index.number_of_shards: 3setup.kibana:output.logstash: hosts: [&quot;192.168.99.186:6044&quot;]processors: - add_host_metadata: ~ - add_cloud_metadata: ~``` # logstash 配置## 查看logstash 安装已经安装插件/usr/share/logstash/bin/logstash-plugin list[root@elk-node2 ~]#/usr/share/logstash/bin/logstash-plugin list |grep geoiplogstash-filter-geoip/usr/share/logstash/bin/logstash-plugin install logstash-filter-geoip12## Nginx 日志清洗规则[root@elk-node2 ~]# cat /etc/logstash/conf.d/nginx.confinput {beats {port =&gt; 6044}}filter {if “nginx.access” in [tags] {json {source =&gt; “message”remove_field =&gt; “message”}date {match =&gt; [“timestamp” , “dd/MMM/YYYY:HH:mm:ss Z” ]}useragent {target =&gt; “agent”source =&gt; “http_user_agent”}geoip { #target =&gt; &quot;geoip&quot; source =&gt; &quot;remote_add&quot; fields =&gt; [&quot;city_name&quot;, &quot;country_code2&quot;, &quot;country_name&quot;, &quot;region_name&quot;,&quot;longitude&quot;,&quot;latitude&quot;,&quot;ip&quot;] add_field =&gt; [&quot;[geoip][coordinates]&quot;,&quot;%{[geoip][longitude]}&quot;] add_field =&gt; [&quot;[geoip][coordinates]&quot;,&quot;%{[geoip][latitude]}&quot;] } mutate { convert =&gt; [&quot;[geoip][coordinates]&quot;,&quot;float&quot;] } } else if “nginx.error” in [tags] {mutate {remove_field =&gt; [“@timestamp”]}grok {match =&gt; {“message” =&gt; “(?%{YEAR}[./-]%{MONTHNUM}[./-]%{MONTHDAY}[- ]%{TIME}) [%{LOGLEVEL:severity}] %{POSINT:pid}#%{NUMBER}: %{GREEDYDATA:errormessage}(?:, client: (?&lt;real_ip&gt;%{IP}|%{HOSTNAME}))(?:, server: %{IPORHOST:domain}?)(?:, request: %{QS:request})?(?:, upstream: (?\”%{URI}\”|%{QS}))?(?:, host: %{QS:request_host})?(?:, referrer: \”%{URI:referrer}\”)?”}}date {match =&gt; [“datetime”, “yyyy/MM/dd HH:mm:ss”]target =&gt; “@timestamp”}mutate {remove_field =&gt; [“message”]}}}output{stdout{codec =&gt; rubydebug}if &quot;nginx.access&quot; in [tags]{ elasticsearch{ index =&gt; &quot;logstash-nginx.access-%{+YYYY.MM.dd}&quot; hosts =&gt; [&quot;192.168.99.186:9200&quot;] } } else if &quot;nginx.error&quot; in [tags]{ elasticsearch { index =&gt; &quot;nginx.error-%{+YYYY.MM.dd}&quot; hosts =&gt; [&quot;192.168.99.186:9200&quot;] } } }12345678910111213141516171819202122232425262728293031323334353637注意：source 可以是任意处理后的字段，需要注意的是 IP 必须是公网 IP，否则logstash 的返回的geoip字段为空## Logstash解析Logstash 分为 Input、Output、Filter、Codec 等多种plugins。- Input：数据的输入源也支持多种插件，如elk官网的beats、file、graphite、http、kafka、redis、exec等等。- Output：数据的输出目的也支持多种插件，如本文的elasticsearch，当然这可能也是最常用的一种输出。以及exec、stdout终端、graphite、http、zabbix、nagios、redmine等等。- Filter：使用过滤器根据日志事件的特征，对数据事件进行处理过滤后，在输出。支持grok、date、geoip、mutate、ruby、json、kv、csv、checksum、dns、drop、xml等等。- Codec：编码插件，改变事件数据的表示方式，它可以作为对输入或输出运行该过滤。和其它产品结合，如rubydebug、graphite、fluent、nmap等等。## 配置文件的含义inputfilebeat 传入filtergrok：数据结构化转换工具match：匹配条件格式geoip：该过滤器从geoip中匹配ip字段，显示该ip的地理位置source：ip来源字段 target：指定插入的logstash字段目标存储为geoip add_field: 增加的字段，坐标经度 add_field: 增加的字段，坐标纬度mutate：数据的修改、删除、类型转换 convert：将坐标转为float类型 replace：替换一个字段 remove_field：移除message 的内容，因为数据已经过滤了一份，这里不必在用到该字段了，不然会相当于存两份 date: 时间处理，该插件很实用，主要是用你日志文件中事件的事件来对timestamp进行转换 match：匹配到timestamp字段后，修改格式为`dd/MMM/yyyy:HH:mm:ss Z` mutate：数据修改 remove_field：移除timestamp字段。 outputelasticsearch：输出到es中host：es的主机ip＋端口或者es 的FQDN＋端口index：为日志创建索引logstash-nginx-access-*，这里也就是kibana那里添加索引时的名称# Kibana 配置注意：默认配置中Kibana的访问日志会记录在/var/log/message 中，使用logging.quiet参数关闭日志[root@elk-node1 nginx]# egrep -v “*#|^$” /etc/kibana/kibana.ymlserver.port: 5601server.host: “192.168.99.185”elasticsearch.hosts: [“http://192.168.99.185:9200&quot;]kibana.index: “.kibana”logging.quiet: truei18n.locale: “zh-CN”tilemap.url: ‘http://webrd02.is.autonavi.com/appmaptile?lang=zh_cn&amp;size=1&amp;scale=1&amp;style=7&amp;x={x}&amp;y={y}&amp;z={z}&#39;`配置“tilemap.url:”参数使Kibana使用高德地图]]></content>
      <categories>
        <category>elk</category>
        <category>nginx</category>
      </categories>
      <tags>
        <tag>elk</tag>
        <tag>nginx</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux日志切割方法[Logrotate、python、shell实现方式]]]></title>
    <url>%2F2020%2F04%2F10%2FLinux%E6%97%A5%E5%BF%97%E5%88%87%E5%89%B2%E6%96%B9%E6%B3%95%5BLogrotate%E3%80%81python%E3%80%81shell%E5%AE%9E%E7%8E%B0%E6%96%B9%E5%BC%8F%5D%2F</url>
    <content type="text"><![CDATA[Linux日志切割方法[Logrotate、python、shell实现方式]​ 对于Linux系统安全来说，日志文件是极其重要的工具。不知为何，我发现很多运维同学的服务器上都运行着一些诸如每天切分Nginx日志之类的cron脚本，大家似乎遗忘了Logrotate，争相发明自己的轮子，这真是让人沮丧啊！就好比明明身边躺着现成的性感美女，大家却忙着自娱自乐，罪过！logrotate程序是一个日志文件管理工具。用于分割日志文件，删除旧的日志文件，并创建新的日志文件，起到“转储”作用。可以节省磁盘空间。下面就对logrotate日志轮转操作做一梳理记录。1、什么是轮转？日志轮循（轮转）：日志轮转，切割，备份，归档2、为什么需要轮转？☆ 避免日志过大占满/var/log的文件系统☆ 方便日志查看 ☆ 将丢弃系统中最旧的日志文件，以节省空间 ☆ 日志轮转的程序是logrotate☆ logrotate本身不是系统守护进程，它是通过计划任务crond每天执行3、安装与配置logrotate1yum install logrotate -y3.1、配置文件介绍Linux系统默认安装logrotate工具，它默认的配置文件在：12#/etc/logrotate.conf#/etc/logrotate.d/logrotate.conf 是主要的配置文件，logrotate.d 是一个目录，该目录里的所有文件都会被主动的读入/etc/logrotate.conf中执行。另外，如果 /etc/logrotate.d/ 里面的文件中没有设定一些细节，则会以/etc/logrotate.conf这个文件的设定来作为默认值。logrotate是基于cron来运行的，其脚本是/etc/cron.daily/logrotate，日志轮转是系统自动完成的。实际运行时，Logrotate会调用配置文件/etc/logrotate.conf。可以在/etc/logrotate.d目录里放置自定义好的配置文件，用来覆盖Logrotate的缺省值。123456789[root@huanqiu_web1 ~]# cat /etc/cron.daily/logrotate#!/bin/sh/usr/sbin/logrotate /etc/logrotate.conf &gt;/dev/null 2&gt;&amp;1EXITVALUE=$?if [ $EXITVALUE != 0 ]; then /usr/bin/logger -t logrotate "ALERT exited abnormally with [$EXITVALUE]"fiexit 0注意：如果等不及cron自动执行日志轮转，想手动强制切割日志，需要加 -f 参数；不过正式执行前最好通过Debug选项来验证一下（-d参数），这对调试也很重要！12# /usr/sbin/logrotate -f /etc/logrotate.d/nginx# /usr/sbin/logrotate -d -f /etc/logrotate.d/nginxlogrotate命令格式123456logrotate [OPTION...] &lt;configfile&gt;-d, --debug ：debug模式，测试配置文件是否有错误。-f, --force ：强制转储文件。-m, --mail=command ：压缩日志后，发送日志到指定邮箱。-s, --state=statefile ：使用指定的状态文件。-v, --verbose ：显示转储过程。根据日志切割设置进行操作，并显示详细信息12[root@huanqiu_web1 ~]# /usr/sbin/logrotate -v /etc/logrotate.conf[root@huanqiu_web1 ~]# /usr/sbin/logrotate -v /etc/logrotate.d/php根据日志切割设置进行执行，并显示详细信息,但是不进行具体操作，debug模式12[root@huanqiu_web1 ~]# /usr/sbin/logrotate -d /etc/logrotate.conf[root@huanqiu_web1 ~]# /usr/sbin/logrotate -d /etc/logrotate.d/nginx查看各log文件的具体执行情况1[root@fangfull_web1 ~]# cat /var/lib/logrotate.status3.2、logrotate配置文件123456789101112131415161718192021222324252627282930313233343536373839404142# vim /etc/logrotate.conf1 # see "man logrotate" for details2 # rotate log files weekly3 weekly4 # 以7天为一个周期(每周轮转)syslog子配置文件：5 # keep 4 weeks worth of backlogs6 rotate 4# 一次将存储4个归档日志。对于第五个归档，时间最久的归档将被删除7 8# create new (empty) log files after rotating old ones9 create# 当老的转储文件被归档后,创建一个新的空的转储文件重新记录,权限和原来的转储文件权限一样1011 # use date as a suffix of the rotated file12 dateext# 用日期来做轮转之后的文件的后缀名1314 # uncomment this if you want your log files compressed15 #compress# 指定不压缩转储文件,如需压缩去掉注释就可以了，主要是通过gzip压缩1617 # RPM packages drop log rotation information into this directory18 include /etc/logrotate.d# 加载外部目录1920 # no packages own wtmp and btmp -- we'll rotate them here21 /var/log/wtmp &#123;22 monthly 表示此文件是每月轮转，而不会用到上面的每周轮转23 create 0664 root utmp 轮转之后创建新文件，权限是0664，属于root用户和utmp组24 minsize 1M 文件大于1M，而且周期到了，才会轮转# size 1M 文件大小大于1M立马轮转，不管有没有到周期25 rotate 1 保留1份日志文件，每1个月备份一次日志文件26 &#125;2728 /var/log/btmp &#123;29 missingok 如果日志文件不存在，不报错30 monthly31 create 0600 root utmp32 rotate 133 &#125;3435 # system-specific logs may be also be configured here.3.3、syslog 子配置文件12345678910111213141516171819202122232425[root@yunwei ~]# cat /etc/logrotate.d/syslog//这个子配置文件，没有指定的参数都会以默认方式轮转/var/log/cron/var/log/maillog/var/log/messages/var/log/secure/var/log/spooler&#123;sharedscripts不管有多少个文件待轮转，prerotate 和 postrotate 代码只执行一次postrotate日志轮转常见参数：4、实践：SSH服务日志轮转要求：☆ 每天进行轮转，保留5天的日志文件☆ 日志文件大小大于5M进行轮转，不管是否到轮转周期思路：☆ 将ssh服务的日志单独记录 /var/log/ssh.log☆ 修改logrotate程序的主配置文件或者在/etc/logrotate.d/目录创建一个文件轮转完后执行postrotate 和 endscript 之间的shell代码/bin/kill -HUP `cat /var/run/syslogd.pid 2&gt; /dev/null` 2&gt; /dev/null || true上面这一句话表示轮转后对rsyslog的pid进行刷新（但pid其实不变)endscript&#125; 思考：为什么轮转后需要对rsyslog的pid进行刷新呢？3.4、日志轮转常见参数1234567891011121314151617常用的指令解释，这些指令都可以在man logrotate 中找得到。daily 指定转储周期为每天monthly 指定转储周期为每月weekly &lt;-- 每周轮转一次(monthly)rotate 4 &lt;-- 同一个文件最多轮转4次，4次之后就删除该文件create 0664 root utmp &lt;-- 轮转之后创建新文件，权限是0664，属于root用户和utmp组dateext &lt;-- 用日期来做轮转之后的文件的后缀名compress &lt;-- 用gzip对轮转后的日志进行压缩minsize 30K &lt;-- 文件大于30K，而且周期到了，才会轮转size 30k &lt;-- 文件必须大于30K才会轮转，而且文件只要大于30K就会轮转不管周期是否已到missingok &lt;-- 如果日志文件不存在，不报错notifempty &lt;-- 如果日志文件是空的，不轮转delaycompress &lt;-- 下一次轮转的时候才压缩sharedscripts &lt;-- 不管有多少文件待轮转，prerotate和postrotate 代码只执行一次prerotate &lt;-- 如果符合轮转的条件则在轮转之前执行prerotate和endscript 之间的shell代码postrotate &lt;-- 轮转完后执行postrotate 和 endscript 之间的shell代码3.5、logrotate默认生效以及相关配置进行解释1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283Logrotate是基于CRON来运行的，其脚本是/etc/cron.daily/logrotate，实际运行时，Logrotate会调用配置文件/etc/logrotate.conf。[root@test ~]# cat /etc/cron.daily/logrotate#!/bin/sh/usr/sbin/logrotate /etc/logrotate.confEXITVALUE=$?if [ $EXITVALUE != 0 ]; then /usr/bin/logger -t logrotate "ALERT exited abnormally with [$EXITVALUE]"fiexit 0 Logrotate是基于CRON运行的，所以这个时间是由CRON控制的，具体可以查询CRON的配置文件/etc/anacrontab（老版本的文件是/etc/crontab）[root@test ~]# cat /etc/anacrontab# /etc/anacrontab: configuration file for anacron # See anacron(8) and anacrontab(5) for details. SHELL=/bin/shPATH=/sbin:/bin:/usr/sbin:/usr/binMAILTO=root# the maximal random delay added to the base delay of the jobsRANDOM_DELAY=45 //这个是随机的延迟时间，表示最大45分钟# the jobs will be started during the following hours onlySTART_HOURS_RANGE=3-22 //这个是开始时间 #period in days delay in minutes job-identifier command1 5 cron.daily nice run-parts /etc/cron.daily7 25 cron.weekly nice run-parts /etc/cron.weekly@monthly 45 cron.monthly nice run-parts /etc/cron.monthly 第一个是Recurrence period第二个是延迟时间所以cron.daily会在3:22+(5,45)这个时间段执行，/etc/cron.daily是个文件夹 通过默认/etc/anacrontab文件配置，会发现logrotate自动切割日志文件的默认时间是凌晨3点多。 =====================================================================================现在需要将切割时间调整到每天的晚上12点，即每天切割的日志是前一天的0-24点之间的内容。操作如下：[root@kevin ~]# mv /etc/anacrontab /etc/anacrontab.bak //取消日志自动轮转的设置 [root@G6-bs02 logrotate.d]# cat nstc_nohup.out/data/nstc/nohup.out &#123;rotate 30dateextdailycopytruncatecompressnotifemptymissingok&#125; [root@G6-bs02 logrotate.d]# cat syslog/var/log/cron/var/log/maillog/var/log/messages/var/log/secure/var/log/history&#123; sharedscripts compress rotate 30 daily dateext postrotate /bin/kill -HUP `cat /var/run/syslogd.pid 2&gt; /dev/null` 2&gt; /dev/null || true endscript&#125; 结合crontab进行自定义的定时轮转操作[root@kevin ~]# crontab -l#log logrotate59 23 * * * /usr/sbin/logrotate -f /etc/logrotate.d/syslog &gt;/dev/null 2&gt;&amp;159 23 * * * /usr/sbin/logrotate -f /etc/logrotate.d/nstc_nohup.out &gt;/dev/null 2&gt;&amp;1 [root@G6-bs02 ~]# ll /data/nstc/nohup.out*-rw------- 1 app app 33218 1月 25 09:43 /data/nstc/nohup.out-rw------- 1 app app 67678 1月 25 23:59 /data/nstc/nohup.out-20180125.gz4、相关案例4.1、logrotate实现Nginx日志切割123456789101112131415[root@master-server ~]# vim /etc/logrotate.d/nginx/usr/local/nginx/logs/*.log &#123;dailyrotate 7missingoknotifemptydateextsharedscriptspostrotate if [ -f /usr/local/nginx/logs/nginx.pid ]; then kill -USR1 `cat /usr/local/nginx/logs/nginx.pid` fiendscript&#125;kill -USR1 指的是Nginx平滑重启4.2、shell脚本实现Nginx日志切割1234567891011121314151617181920212223[root@bastion-IDC ~]# vim /usr/local/sbin/logrotate-nginx.sh#!/bin/bash#创建转储日志压缩存放目录mkdir -p /data/nginx_logs/days#手工对nginx日志进行切割转换/usr/sbin/logrotate -vf /etc/logrotate.d/nginx#当前时间time=$(date -d "yesterday" +"%Y-%m-%d")#进入转储日志存放目录cd /data/nginx_logs/days#对目录中的转储日志文件的文件名进行统一转换for i in $(ls ./ | grep "^\(.*\)\.[[:digit:]]$")domv $&#123;i&#125; ./$(echo $&#123;i&#125;|sed -n 's/^\(.*\)\.\([[:digit:]]\)$/\1/p')-$(echo $time)done#对转储的日志文件进行压缩存放，并删除原有转储的日志文件，只保存压缩后的日志文件。以节约存储空间for i in $(ls ./ | grep "^\(.*\)\-\([[:digit:]-]\+\)$")dotar jcvf $&#123;i&#125;.bz2 ./$&#123;i&#125;rm -rf ./$&#123;i&#125;done#只保留最近7天的压缩转储日志文件find /data/nginx_logs/days/* -name "*.bz2" -mtime 7 -type f -exec rm -rf &#123;&#125; \;crontab定时执行123[root@bastion-IDC ~# crontab -e#logrotate0 0 * * * /bin/bash -x /usr/local/sbin/logrotate-nginx.sh &gt; /dev/null 2&gt;&amp;1手动执行脚本123[root@bastion-IDC ~]# /bin/bash -x /usr/local/sbin/logrotate-nginx.sh[root@bastion-IDC ~]# cd /data/nginx_logs/days[root@bastion-IDC days]# lshuantest.access_log-2017-01-18.bz24.3、PHP日志切割123456789101112131415161718192021222324252627282930[root@huanqiu_web1 ~]# cat /etc/logrotate.d/php/Data/logs/php/*log &#123; daily rotate 365 missingok notifempty compress dateext sharedscripts postrotate if [ -f /Data/app/php5.6.26/var/run/php-fpm.pid ]; then kill -USR1 `cat /Data/app/php5.6.26/var/run/php-fpm.pid` fi endscript postrotate /bin/chmod 644 /Data/logs/php/*gz endscript&#125; [root@huanqiu_web1 ~]# ll /Data/app/php5.6.26/var/run/php-fpm.pid-rw-r--r-- 1 root root 4 Dec 28 17:03 /Data/app/php5.6.26/var/run/php-fpm.pid [root@huanqiu_web1 ~]# cd /Data/logs/php[root@huanqiu_web1 php]# lltotal 25676-rw-r--r-- 1 root root 0 Jun 1 2016 error.log-rw-r--r-- 1 nobody nobody 182 Aug 30 2015 error.log-20150830.gz-rw-r--r-- 1 nobody nobody 371 Sep 1 2015 error.log-20150901.gz-rw-r--r-- 1 nobody nobody 315 Sep 7 2015 error.log-20150907.gz.........4.4、linux系统日志切割12345678910111213141516171819202122232425262728293031323334353637[root@huanqiu_web1 ~]# cat /etc/logrotate.d/syslog/var/log/cron/var/log/maillog/var/log/messages/var/log/secure/var/log/spooler&#123; sharedscripts postrotate /bin/kill -HUP `cat /var/run/syslogd.pid 2&gt; /dev/null` 2&gt; /dev/null || true endscript&#125; [root@huanqiu_web1 ~]# ll /var/log/messages*-rw------- 1 root root 34248975 Jan 19 18:42 /var/log/messages-rw------- 1 root root 51772994 Dec 25 03:11 /var/log/messages-20161225-rw------- 1 root root 51800210 Jan 1 03:05 /var/log/messages-20170101-rw------- 1 root root 51981366 Jan 8 03:36 /var/log/messages-20170108-rw------- 1 root root 51843025 Jan 15 03:40 /var/log/messages-20170115[root@huanqiu_web1 ~]# ll /var/log/cron*-rw------- 1 root root 2155681 Jan 19 18:43 /var/log/cron-rw------- 1 root root 2932618 Dec 25 03:11 /var/log/cron-20161225-rw------- 1 root root 2939305 Jan 1 03:06 /var/log/cron-20170101-rw------- 1 root root 2951820 Jan 8 03:37 /var/log/cron-20170108-rw------- 1 root root 3203992 Jan 15 03:41 /var/log/cron-20170115[root@huanqiu_web1 ~]# ll /var/log/secure*-rw------- 1 root root 275343 Jan 19 18:36 /var/log/secure-rw------- 1 root root 2111936 Dec 25 03:06 /var/log/secure-20161225-rw------- 1 root root 2772744 Jan 1 02:57 /var/log/secure-20170101-rw------- 1 root root 1115543 Jan 8 03:26 /var/log/secure-20170108-rw------- 1 root root 731599 Jan 15 03:40 /var/log/secure-20170115[root@huanqiu_web1 ~]# ll /var/log/spooler*-rw------- 1 root root 0 Jan 15 03:41 /var/log/spooler-rw------- 1 root root 0 Dec 18 03:21 /var/log/spooler-20161225-rw------- 1 root root 0 Dec 25 03:11 /var/log/spooler-20170101-rw------- 1 root root 0 Jan 1 03:06 /var/log/spooler-20170108-rw------- 1 root root 0 Jan 8 03:37 /var/log/spooler-201701154.5、Tomcat日志切割12345678910111213141516171819202122232425262728293031323334353637[root@huanqiu_web1 ~]# cat /etc/logrotate.d/syslog/var/log/cron/var/log/maillog/var/log/messages/var/log/secure/var/log/spooler&#123; sharedscripts postrotate /bin/kill -HUP `cat /var/run/syslogd.pid 2&gt; /dev/null` 2&gt; /dev/null || true endscript&#125; [root@huanqiu_web1 ~]# ll /var/log/messages*-rw------- 1 root root 34248975 Jan 19 18:42 /var/log/messages-rw------- 1 root root 51772994 Dec 25 03:11 /var/log/messages-20161225-rw------- 1 root root 51800210 Jan 1 03:05 /var/log/messages-20170101-rw------- 1 root root 51981366 Jan 8 03:36 /var/log/messages-20170108-rw------- 1 root root 51843025 Jan 15 03:40 /var/log/messages-20170115[root@huanqiu_web1 ~]# ll /var/log/cron*-rw------- 1 root root 2155681 Jan 19 18:43 /var/log/cron-rw------- 1 root root 2932618 Dec 25 03:11 /var/log/cron-20161225-rw------- 1 root root 2939305 Jan 1 03:06 /var/log/cron-20170101-rw------- 1 root root 2951820 Jan 8 03:37 /var/log/cron-20170108-rw------- 1 root root 3203992 Jan 15 03:41 /var/log/cron-20170115[root@huanqiu_web1 ~]# ll /var/log/secure*-rw------- 1 root root 275343 Jan 19 18:36 /var/log/secure-rw------- 1 root root 2111936 Dec 25 03:06 /var/log/secure-20161225-rw------- 1 root root 2772744 Jan 1 02:57 /var/log/secure-20170101-rw------- 1 root root 1115543 Jan 8 03:26 /var/log/secure-20170108-rw------- 1 root root 731599 Jan 15 03:40 /var/log/secure-20170115[root@huanqiu_web1 ~]# ll /var/log/spooler*-rw------- 1 root root 0 Jan 15 03:41 /var/log/spooler-rw------- 1 root root 0 Dec 18 03:21 /var/log/spooler-20161225-rw------- 1 root root 0 Dec 25 03:11 /var/log/spooler-20170101-rw------- 1 root root 0 Jan 1 03:06 /var/log/spooler-20170108-rw------- 1 root root 0 Jan 8 03:37 /var/log/spooler-201701154.6、使用python脚本进行jumpserver日志切割1234567891011121314151617181920212223242526272829303132333435363738394041[root@test-vm01 mnt]# cat log_rotate.py#!/usr/bin/env python import datetime,os,sys,shutil log_path = '/opt/jumpserver/logs/'log_file = 'jumpserver.log' yesterday = (datetime.datetime.now() - datetime.timedelta(days = 1)) try: os.makedirs(log_path + yesterday.strftime('%Y') + os.sep + yesterday.strftime('%m'))except OSError,e: print print e sys.exit() shutil.move(log_path + log_file,log_path \ + yesterday.strftime('%Y') + os.sep \ + yesterday.strftime('%m') + os.sep \ + log_file + '_' + yesterday.strftime('%Y%m%d') + '.log') os.popen("sudo /opt/jumpserver/service.sh restart") 手动执行这个脚本：[root@test-vm01 mnt]# chmod 755 log_rotate.py[root@test-vm01 mnt]# python log_rotate.py 查看日志切割后的效果：[root@test-vm01 mnt]# ls /opt/jumpserver/logs/2017 jumpserver.log [root@test-vm01 mnt]# ls /opt/jumpserver/logs/2017/09[root@test-vm01 mnt]# ls /opt/jumpserver/logs/2017/09/jumpserver.log_20170916.log 然后做每日的定时切割任务：[root@test-vm01 mnt]# crontab -e30 1 * * * /usr/bin/python /mnt/log_rotate.py &gt; /dev/null 2&gt;&amp;14.7、使用python脚本进行Nginx日志切割12345678910111213141516171819202122232425[root@test-vm01 mnt]# vim log_rotate.py#!/usr/bin/env python import datetime,os,sys,shutil log_path = '/app/nginx/logs/'log_file = 'www_access.log' yesterday = (datetime.datetime.now() - datetime.timedelta(days = 1)) try: os.makedirs(log_path + yesterday.strftime('%Y') + os.sep + yesterday.strftime('%m'))except OSError,e: print print e sys.exit() shutil.move(log_path + log_file,log_path \ + yesterday.strftime('%Y') + os.sep \ + yesterday.strftime('%m') + os.sep \ + log_file + '_' + yesterday.strftime('%Y%m%d') + '.log') os.popen("sudo kill -USR1 `cat /app/nginx/logs/nginx.pid`")5、logrotate无法自动轮询日志的解决办法起始原因：使用logrotate轮询nginx日志，配置好之后，发现nginx日志连续两天没被切割，这是为什么呢？？然后开始检查日志切割的配置文件是否有问题，检查后确定配置文件一切正常。于是怀疑是logrotate预定的cron没执行，查看了cron的日志，发现有一条”Dec 7 04:02:01 www crond[18959]: (root) CMD (run-parts /etc/cron.daily)”的日志，证明cron在04:02分时已经执行/etc/cron.daily目录下的程序。接着查看/etc /cron.daily/logrotate（这是logrotate自动轮转的脚本）的内容：123456789[root@huanqiu_test ~]# cat /etc/cron.daily/logrotate#!/bin/sh/usr/sbin/logrotate /etc/logrotate.conf &gt;/dev/null 2&gt;&amp;1EXITVALUE=$?if [ $EXITVALUE != 0 ]; then /usr/bin/logger -t logrotate "ALERT exited abnormally with [$EXITVALUE]"fiexit 0有发现异常，配置好的日志轮转操作都是由这个脚本完成的，一切运行正常，脚本应该就没问题。 直接执行命令：1[root@huanqiu_test ~]# /usr/sbin/logrotate /etc/logrotate.conf这些系统日志是正常轮询了，但nginx日志却还是没轮询,接着强行启动记录文件维护操作，纵使logrotate指令认为没有需要，应该有可能是logroate认为nginx日志太小，不进行轮询。 故需要强制轮询，即在/etc/cron.daily/logrotate脚本中将 -t 参数替换成 -f 参数123456789[root@huanqiu_test ~]# cat /etc/cron.daily/logrotate#!/bin/sh/usr/sbin/logrotate /etc/logrotate.conf &gt;/dev/null 2&gt;&amp;1EXITVALUE=$?if [ $EXITVALUE != 0 ]; then /usr/bin/logger -f logrotate "ALERT exited abnormally with [$EXITVALUE]"fiexit 0重启下cron服务12[root@huanqiu_test ~]# /etc/init.d/crond restartStopping crond: [ OK ]Starting crond: [ OK ]]]></content>
      <categories>
        <category>shell</category>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>shell</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用 Auditbeat 模块监控 shell 命令]]></title>
    <url>%2F2020%2F04%2F03%2F%E4%BD%BF%E7%94%A8%20Auditbeat%20%E6%A8%A1%E5%9D%97%E7%9B%91%E6%8E%A7%20shell%20%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[使用 Auditbeat 模块监控 shell 命令Auditbeat Audited 模块可以用来监控所有用户在系统上执行的 shell 命令。在终端用户偶尔才会登录的服务器上，通常需要进行监控。该示例是在 CentOS Linux 7.6 上使用 Auditbeat 7.4.2 RPM 软件包和 Elasticsearch Service（ESS）[https://www.elastic.co/products/elasticsearch/service]上的 Elastic Stack ] 7.4.2 部署的。可以参考其中的思路，配置流程等，使用本机自建的ES，不使用Elasticsearch Service（ESS）集群禁用 Auditd系统守护进程 auditd 会影响 Auditbeat Audited 模块的正常使用，所以必须将其禁用。12345# 停止 auditd：service auditd stop# 禁用服务：systemctl disable auditd.service如果您在使用 Auditbeat Auditd 模块的同时也必须要运行 Audited 进程，那么在内核版本为 3.16 或者更高的情况下可以考虑设置 socket_type: multicast 参数。默认值为 unicast。有关此参数的更多信息，请参见文档[https://www.elastic.co/guide/en/beats/auditbeat/master/auditbeat-module-auditd.html#_configuration_options_14]的配置选项部分。配置 AuditbeatAuditbeat 守护进程将事件数据发送到一个 Elasticsearch Service（ESS）集群中。有关更多详细信息，请参见文档Auditbeat[https://www.elastic.co/guide/en/beats/auditbeat/master/configuring-howto-auditbeat.html]中的配置部分。要想获取工作示例，必须配置 Auditbeat 的 cloud.id 和 cloud.auth 参数，详情参见此文档[https://www.elastic.co/guide/en/beats/auditbeat/master/configure-cloud-id.html]。编辑 /etc/auditbeat/auditbeat.yml：12cloud.id: &lt;your_cloud_id&gt;cloud.auth: ingest_user:password如果您要将数据发送到 Elasticsearch 集群（例如本地实例），请参见此文档：[https://www.elastic.co/guide/en/beats/auditbeat/master/configure-cloud-id.html]。Auditbeat 模块规则Audited 模块订阅内核以接收系统事件。定义规则以捕获这些事件，并且使用Linux Auditctl 进程所使用的格式，详情参见此文档：[https://linux.die.net/man/8/auditctl]。12345# cat /etc/auditbeat/audit.rules.d/rules.conf-a exit,always -F arch=b64 -F euid=0 -S execve -k root_acct-a exit,always -F arch=b32 -F euid=0 -S execve -k root_acct-a exit,always -F arch=b64 -F euid&gt;=1000 -S execve -k user_acct-a exit,always -F arch=b32 -F euid&gt;=1000 -S execve -k user_accteuid 是用户的有效ID。0 代表会获取 root 用户和 uid &gt;=1000 或者权限更高的其他用户的所有活动。-k用于为事件分配任意“键”，它将显示在 tags 字段中。它还可以在 Kibana 中用来对事件进行过滤和分类。Auditbeat 设置命令运行Auditbeat 加载索引模板，读取 node pipelines，索引文件周期策略和Kibana 仪表板。auditbeat -e setup如果您不使用ESS，欢迎参考此文档[https://www.elastic.co/guide/en/beats/auditbeat/current/setup-kibana-endpoint.html] 来设置您的 Kibana 端点。开始使用123456789systemctl start auditbeat# 列出启用的规则：auditbeat show auditd-rules-a never,exit -S all -F pid=23617-a always,exit -F arch=b64 -S execve -F euid=root -F key=root_acct-a always,exit -F arch=b32 -S execve -F euid=root -F key=root_acct-a always,exit -F arch=b64 -S execve -F euid&gt;=vagrant -F key=user_acct-a always,exit -F arch=b32 -S execve -F euid&gt;=vagrant -F key=user_acct监控数据当用户执行一些类似于 whoami，ls 以及 lsblk 的 shell 命令时，kibana 中就会发现这些事件。Kibana 会显示出 user.name，process.executable，process.args 和 tags 这些选定的字段。过滤的字段是 user.name: root 和 auditd.data.syscall: execve。每秒刷新一次数据。TTY 审计当系统中发生 TTY 事件时，Auditbeat Audited 模块也可以接收它们。配置system-auth PAM 配置文件以启用 TTY。只有 root 用户的 TTY 事件将被实时记录。其他用户的事件通常会被缓冲直到 exit。TTY 审计会捕获系统内置命令像pwd，test 等。追加以下内容到 /etc/pam.d/system-auth 便可以对所有用户启用审核，关于 pam_tty_audit 的详细信息，参见此文档：[https://linux.die.net/man/8/pam_tty_audit]。session required pam_tty_audit.so enable=*测试12345$ sudo su -Last login: Fri Nov 22 23:43:00 UTC 2019 on pts/0$ helllloooo there!-bash: helllloooo: command not found$ exitKibana 发现思考Auditbeat 还可以做什么：当一个文件在磁盘上更改（创建，更新或删除）时可以发送事件，得益于 file_integrity 模块，详情参考此文档：[https://www.elastic.co/guide/en/beats/auditbeat/current/auditbeat-module-file_integrity.html]。通过 system 模块发送有关系统的指标，详情参考此文档：[https://www.elastic.co/guide/en/beats/auditbeat/current/auditbeat-module-system.html]。该链接还提供了 Auditbeat 的相关文档，详情参考此文档：[https://www.elastic.co/guide/en/beats/auditbeat/current/index.html]。]]></content>
      <categories>
        <category>elk</category>
        <category>Auditbeat</category>
        <category>shell</category>
      </categories>
      <tags>
        <tag>elk</tag>
        <tag>Auditbeat</tag>
        <tag>shell</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch：如何对PDF文件进行搜索]]></title>
    <url>%2F2020%2F04%2F03%2FElasticsearch%EF%BC%9A%E5%A6%82%E4%BD%95%E5%AF%B9PDF%E6%96%87%E4%BB%B6%E8%BF%9B%E8%A1%8C%E6%90%9C%E7%B4%A2%2F</url>
    <content type="text"><![CDATA[Elasticsearch 通常用于字符串，数字，日期等数据类型的检索，但是在 HCM、ERP 和电子商务等应用程序中经常存在对办公文档进行搜索的需求。今天的这篇文章中我们来讲一下如何实现 PDF、DOC、XLS 等办公文件的搜索，本解决方案适用于 Elasticsearch 5.0 以后的版本。实现原理首先把我们的 .pdf 文件进行 Base64 处理，然后上传到 Elasticsearch 中的 ingest node 中进行处理。我们可以通过 Ingest attachment plugin 来使得 Elasticsearch 提取通用格式的文件附件比如 PPT、XLS及PDF。最终，数据进入到 Elasticsearch 的 data node 中以便让我们进行搜索。导入PDF文件到Elasticsearch中准备PDF文件我们可以使用 Word 或其它编辑软件来生产一个 PDF 文件，暂且我们叫这个文件的名字为 sample.pdf，而它的内容非常简单，在 sample.pdf 文件中，我们只有一句话：“I like this useful tool”。安装 Ingest attachment pluginIngest attachment plugin 允许 Elasticsearch 通过使用 Apache 文本提取库 Tika 提取通用格式（例如：PPT，XLS 和 PDF）的文件附件。Apache Tika 工具包可从一千多种不同的文件类型中检测并提取元数据和文本。所有这些文件类型都可以通过一个界面进行解析，从而使 Tika 对搜索引擎索引，内容分析，翻译等有用。需要注意的是，源字段必须是 Base64 编码的二进制，如果不想增加在 Base64 之间来回转换的开销，则可以使用 CBOR 格式而不是 JSON，并将字段指定为字节数组而不是字符串表示形式，这样处理器将跳过 Base64 解码。可以使用插件管理器安装此插件，该插件必须安装在集群中的每个节点上，并且每个节点必须在安装后重新启动。sudo bin/elasticsearch-plugin install ingest-attachment等我们安装好这个插件后，我们可以通过如下的命令来查看该插件是否已经被成功安装好了:./bin/elasticsearch-plugin list创建 attachment pipeline在我们的 ingest node 上创建一个叫做 pdfattachment 的 pipleline：1234567891011PUT _ingest/pipeline/pdfattachment&#123; &quot;description&quot;: &quot;Extract attachment information encoded in Base64 with UTF-8 charset&quot;, &quot;processors&quot;: [ &#123; &quot;attachment&quot;: &#123; &quot;field&quot;: &quot;file&quot; &#125; &#125; ]&#125;转换并上传PDF文件的内容到Elasticsearch中对于 Ingest attachment plugin 来说，它的数据必须是 Base64 的。我们可以在网站Base64 encoder 来进行转换，我们可以直接通过下面的脚本来进行操作：123456789!/bin/bashencodedPdf=`cat sample.pdf | base64`json=&quot;&#123;\&quot;file\&quot;:\&quot;$&#123;encodedPdf&#125;\&quot;&#125;&quot;echo &quot;$json&quot; &gt; json.filecurl -XPOST &apos;http://localhost:9200/pdf-test1/_doc?pipeline=pdfattachment&amp;pretty&apos; -H &apos;Content-Type: application/json&apos; -d @json.file在上面的脚本中，我们针对 sample.pdf 进行 Base64 的转换，并生成一个叫做 json.file 的文件。在最后，我们把这个 json.file 文件的内容通过 curl 指令上传到 Elasticsearch 中，我们可以在 Elasticsearch 中查看一个叫做 pdf-test1 的索引。查看索引并搜索可以通过如下的命令来查询 pdf-test1 索引：GET pdf-test1/_search可以看出来，我们的索引中有一个叫做 content 的字段，它包含了我们的 pdf 文件的内容，这个字段可以同我们进行搜索。在上面我们也看到了一个很大的一个字段 file，它含有我们转换过的 Base64 格式的内容。如果我们不想要这个字段，我们可以通过添加另外一个 remove processor 来除去这个字段：12345678910111213141516PUT _ingest/pipeline/pdfattachment&#123; &quot;description&quot;: &quot;Extract attachment information encoded in Base64 with UTF-8 charset&quot;, &quot;processors&quot;: [ &#123; &quot;attachment&quot;: &#123; &quot;field&quot;: &quot;file&quot; &#125; &#125;, &#123; &quot;remove&quot;: &#123; &quot;field&quot;: &quot;file&quot; &#125; &#125; ]&#125;]]></content>
      <categories>
        <category>elk</category>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>elk</tag>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch索引和查询性能调优的21条建议]]></title>
    <url>%2F2020%2F04%2F03%2FElasticsearch%E7%B4%A2%E5%BC%95%E5%92%8C%E6%9F%A5%E8%AF%A2%E6%80%A7%E8%83%BD%E8%B0%83%E4%BC%98%E7%9A%8421%E6%9D%A1%E5%BB%BA%E8%AE%AE%2F</url>
    <content type="text"><![CDATA[Elasticsearch部署建议1. 选择合理的硬件配置：尽可能使用 SSDElasticsearch 最大的瓶颈往往是磁盘读写性能，尤其是随机读取性能。使用SSD（PCI-E接口SSD卡/SATA接口SSD盘）通常比机械硬盘（SATA盘/SAS盘）查询速度快5~10倍，写入性能提升不明显。对于文档检索类查询性能要求较高的场景，建议考虑 SSD 作为存储，同时按照 1:10 的比例配置内存和硬盘。对于日志分析类查询并发要求较低的场景，可以考虑采用机械硬盘作为存储，同时按照 1:50 的比例配置内存和硬盘。单节点存储数据建议在2TB以内，不要超过5TB，避免查询速度慢、系统不稳定。2. 给JVM配置机器一半的内存，但是不建议超过32G修改 conf/jvm.options 配置，-Xms 和 -Xmx 设置为相同的值，推荐设置为机器内存的一半左右，剩余一半留给操作系统缓存使用。JVM 内存建议不要低于 2G，否则有可能因为内存不足导致 ES 无法正常启动或内存溢出，JVM 建议不要超过 32G，否则 JVM 会禁用内存对象指针压缩技术，造成内存浪费。机器内存大于 64G 内存时，推荐配置 -Xms30g -Xmx30g。JVM 堆内存较大时，内存垃圾回收暂停时间比较长，建议配置 ZGC 或 G1 垃圾回收算法。3. 规模较大的集群配置专有主节点，避免脑裂问题Elasticsearch 主节点负责集群元信息管理、index 的增删操作、节点的加入剔除，定期将最新的集群状态广播至各个节点。在集群规模较大时，建议配置专有主节点只负责集群管理，不存储数据，不承担数据读写压力。12345678910# 专有主节点配置(conf/elasticsearch.yml)：node.master:truenode.data: falsenode.ingest:false# 数据节点配置(conf/elasticsearch.yml)：node.master:falsenode.data:truenode.ingest:trueElasticsearch 默认每个节点既是候选主节点，又是数据节点。最小主节点数量参数 minimum_master_nodes 推荐配置为候选主节点数量一半以上，该配置告诉 Elasticsearch 当没有足够的 master 候选节点的时候，不进行 master 节点选举，等 master 节点足够了才进行选举。例如对于 3 节点集群，最小主节点数量从默认值 1 改为 2。12# 最小主节点数量配置(conf/elasticsearch.yml)：discovery.zen.minimum_master_nodes: 24. Linux操作系统调优关闭交换分区，防止内存置换降低性能。123456789101112131415# 将/etc/fstab 文件中包含swap的行注释掉sed -i &apos;/swap/s/^/#/&apos; /etc/fstabswapoff -a# 单用户可以打开的最大文件数量，可以设置为官方推荐的65536或更大些echo &quot;* - nofile 655360&quot; &gt;&gt; /etc/security/limits.conf# 单用户线程数调大echo &quot;* - nproc 131072&quot; &gt;&gt; /etc/security/limits.conf# 单进程可以使用的最大map内存区域数量echo &quot;vm.max_map_count = 655360&quot; &gt;&gt; /etc/sysctl.conf# 参数修改立即生效sysctl -p索引性能调优建议1. 设置合理的索引分片数和副本数索引分片数建议设置为集群节点的整数倍，初始数据导入时副本数设置为 0，生产环境副本数建议设置为 1（设置 1 个副本，集群任意 1 个节点宕机数据不会丢失；设置更多副本会占用更多存储空间，操作系统缓存命中率会下降，检索性能不一定提升）。单节点索引分片数建议不要超过 3 个，每个索引分片推荐 10-40GB 大小，索引分片数设置后不可以修改，副本数设置后可以修改。Elasticsearch6.X 及之前的版本默认索引分片数为 5、副本数为 1，从 Elasticsearch7.0 开始调整为默认索引分片数为 1、副本数为 1。1234567891011121314151617181920212223242526272829303132333435363738394041424344454647# 索引设置curl -XPUT http://localhost:9200/fulltext001?pretty -H &apos;Content-Type: application/json&apos; -d &apos;&#123; &quot;settings&quot;: &#123; &quot;refresh_interval&quot;: &quot;30s&quot;, &quot;merge.policy.max_merged_segment&quot;: &quot;1000mb&quot;, &quot;translog.durability&quot;: &quot;async&quot;, &quot;translog.flush_threshold_size&quot;: &quot;2gb&quot;, &quot;translog.sync_interval&quot;: &quot;100s&quot;, &quot;index&quot;: &#123; &quot;number_of_shards&quot;: &quot;21&quot;, &quot;number_of_replicas&quot;: &quot;0&quot; &#125; &#125;&#125;&apos;# mapping 设置curl -XPOST http://localhost:9200/fulltext001/doc/_mapping?pretty -H &apos;Content-Type: application/json&apos; -d &apos;&#123; &quot;doc&quot;: &#123; &quot;_all&quot;: &#123; &quot;enabled&quot;: false &#125;, &quot;properties&quot;: &#123; &quot;content&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;analyzer&quot;: &quot;ik_max_word&quot; &#125;, &quot;id&quot;: &#123; &quot;type&quot;: &quot;keyword&quot; &#125; &#125; &#125;&#125;&apos;# 写入数据示例curl -XPUT &apos;http://localhost:9200/fulltext001/doc/1?pretty&apos; -H &apos;Content-Type: application/json&apos; -d &apos;&#123; &quot;id&quot;: &quot;https://www.huxiu.com/article/215169.html&quot;, &quot;content&quot;: &quot;“娃娃机，迷你KTV，VR体验馆，堪称商场三大标配‘神器’。”一家地处商业中心的大型综合体负责人告诉懂懂笔记，在过去的这几个月里，几乎所有的综合体都“标配”了这三种“设备”…&quot;&#125;&apos;# 修改副本数示例curl -XPUT &quot;http://localhost:9200/fulltext001/_settings&quot; -H &apos;Content-Type: application/json&apos; -d &apos;&#123; &quot;number_of_replicas&quot;: 1&#125;&apos;2. 使用批量请求使用批量请求将产生比单文档索引请求好得多的性能。写入数据时调用批量提交接口，推荐每批量提交 5~15MB 数据。例如单条记录 1KB 大小，每批次提交 10000 条左右记录写入性能较优；单条记录 5KB 大小，每批次提交 2000 条左右记录写入性能较优。1234567# 批量请求接口APIcurl -XPOST &quot;http://localhost:9200/_bulk&quot; -H &apos;Content-Type: application/json&apos; -d&apos;&#123; &quot;index&quot; : &#123; &quot;_index&quot; : &quot;test&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;1&quot; &#125; &#125;&#123; &quot;field1&quot; : &quot;value1&quot; &#125;&#123; &quot;delete&quot; : &#123; &quot;_index&quot; : &quot;test&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;2&quot; &#125; &#125;&#123; &quot;create&quot; : &#123; &quot;_index&quot; : &quot;test&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;3&quot; &#125; &#125;&#123; &quot;field1&quot; : &quot;value3&quot; &#125;&#123; &quot;update&quot; : &#123;&quot;_id&quot; : &quot;1&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_index&quot; : &quot;test&quot;&#125; &#125;&#123; &quot;doc&quot; : &#123;&quot;field2&quot; : &quot;value2&quot;&#125; &#125;&apos;3. 通过多进程/线程发送数据单线程批量写入数据往往不能充分利用服务器 CPU 资源，可以尝试调整写入线程数或者在多个客户端上同时向 Elasticsearch 服务器提交写入请求。与批量调整大小请求类似，只有测试才能确定最佳的 worker 数量。可以通过逐渐增加工作任务数量来测试，直到集群上的 I/O 或 CPU 饱和。4. 调大refresh interval在 Elasticsearch 中，写入和打开一个新段的轻量的过程叫做 refresh 。 默认情况下每个分片会每秒自动刷新一次。这就是为什么我们说 Elasticsearch 是近实时搜索: 文档的变化并不是立即对搜索可见，但会在一秒之内变为可见。并不是所有的情况都需要每秒刷新。可能你正在使用 Elasticsearch 索引大量的日志文件，你可能想优化索引速度而不是近实时搜索，可以通过设置 refresh_interval，降低每个索引的刷新频率。1234567# 设置 refresh interval APIcurl -XPUT &quot;http://localhost:9200/index&quot; -H &apos;Content-Type: application/json&apos; -d&apos;&#123; &quot;settings&quot;: &#123; &quot;refresh_interval&quot;: &quot;30s&quot; &#125;&#125;&apos;refresh_interval 可以在已经存在的索引上进行动态更新，在生产环境中，当你正在建立一个大的新索引时，可以先关闭自动刷新，待开始使用该索引时，再把它们调回来。12345curl -XPUT &quot;http://localhost:9200/index/_settings&quot; -H &apos;Content-Type: application/json&apos; -d&apos;&#123; &quot;refresh_interval&quot;: -1 &#125;&apos;curl -XPUT &quot;http://localhost:9200/index/_settings&quot; -H &apos;Content-Type: application/json&apos; -d&apos;&#123; &quot;refresh_interval&quot;: &quot;1s&quot; &#125;&apos;5. 配置事务日志参数事务日志 translog 用于防止节点失败时的数据丢失。它的设计目的是帮助 shard 恢复操作，否则数据可能会从内存 flush 到磁盘时发生意外而丢失。事务日志 translog 的落盘(fsync)是 ES 在后台自动执行的，默认每 5 秒钟提交到磁盘上，或者当 translog 文件大小大于 512MB 提交，或者在每个成功的索引、删除、更新或批量请求时提交。索引创建时，可以调整默认日志刷新间隔 5 秒，例如改为 60 秒，index.translog.sync_interval: “60s”。创建索引后，可以动态调整 translog 参数，”index.translog.durability”:”async” 相当于关闭了 index、bulk 等操作的同步 flush translog 操作，仅使用默认的定时刷新、文件大小阈值刷新的机制。12345678# 动态设置 translog APIcurl -XPUT &quot;http://localhost:9200/index&quot; -H &apos;Content-Type: application/json&apos; -d&apos;&#123; &quot;settings&quot;: &#123; &quot;index.translog.durability&quot;: &quot;async&quot;, &quot;translog.flush_threshold_size&quot;: &quot;2gb&quot; &#125;&#125;&apos;6. 设计mapping配置合适的字段类型Elasticsearch 在写入文档时，如果请求中指定的索引名不存在，会自动创建新索引，并根据文档内容猜测可能的字段类型。但这往往不是最高效的，我们可以根据应用场景来设计合理的字段类型。1234567# 例如写入一条记录curl -XPUT &quot;http://localhost:9200/twitter/doc/1?pretty&quot; -H &apos;Content-Type: application/json&apos; -d&apos;&#123; &quot;user&quot;: &quot;kimchy&quot;, &quot;post_date&quot;: &quot;2009-11-15T13:12:00&quot;, &quot;message&quot;: &quot;Trying out Elasticsearch, so far so good?&quot;&#125;&apos;查询 Elasticsearch 自动创建的索引 mapping，会发现将 post_date 字段自动识别为 date 类型，但是 message 和 user 字段被设置为 text、keyword 冗余字段，造成写入速度降低、占用更多磁盘空间。12345678910111213141516171819202122232425262728293031323334353637&#123; &quot;twitter&quot;: &#123; &quot;mappings&quot;: &#123; &quot;doc&quot;: &#123; &quot;properties&quot;: &#123; &quot;message&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;fields&quot;: &#123; &quot;keyword&quot;: &#123; &quot;type&quot;: &quot;keyword&quot;, &quot;ignore_above&quot;: 256 &#125; &#125; &#125;, &quot;post_date&quot;: &#123; &quot;type&quot;: &quot;date&quot; &#125;, &quot;user&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;fields&quot;: &#123; &quot;keyword&quot;: &#123; &quot;type&quot;: &quot;keyword&quot;, &quot;ignore_above&quot;: 256 &#125; &#125; &#125; &#125; &#125; &#125;, &quot;settings&quot;: &#123; &quot;index&quot;: &#123; &quot;number_of_shards&quot;: &quot;5&quot;, &quot;number_of_replicas&quot;: &quot;1&quot; &#125; &#125; &#125;&#125;根据业务场景设计索引配置合理的分片数、副本数，设置字段类型、分词器。如果不需要合并全部字段，禁用 _all 字段，通过 copy_to 来合并字段。123456789101112131415161718192021222324252627282930curl -XPUT &quot;http://localhost:9200/twitter?pretty&quot; -H &apos;Content-Type: application/json&apos; -d&apos;&#123; &quot;settings&quot;: &#123; &quot;index&quot;: &#123; &quot;number_of_shards&quot;: &quot;20&quot;, &quot;number_of_replicas&quot;: &quot;0&quot; &#125; &#125;&#125;&apos;curl -XPOST &quot;http://localhost:9200/twitter/doc/_mapping?pretty&quot; -H &apos;Content-Type: application/json&apos; -d&apos;&#123; &quot;doc&quot;: &#123; &quot;_all&quot;: &#123; &quot;enabled&quot;: false &#125;, &quot;properties&quot;: &#123; &quot;user&quot;: &#123; &quot;type&quot;: &quot;keyword&quot; &#125;, &quot;post_date&quot;: &#123; &quot;type&quot;: &quot;date&quot; &#125;, &quot;message&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;analyzer&quot;: &quot;cjk&quot; &#125; &#125; &#125;&#125;&apos;查询性能调优建议1. 使用过滤器缓存和分片查询缓存默认情况下，Elasticsearch 的查询会计算返回的每条数据与查询语句的相关度，但对于非全文索引的使用场景，用户并不关心查询结果与查询条件的相关度，只是想精确地查找目标数据。此时，可以通过 filter 来让 Elasticsearch 不计算评分，并且尽可能地缓存 filter 的结果集，供后续包含相同 filter 的查询使用，提高查询效率。1234567891011121314151617181920212223# 普通查询curl -XGET &quot;http://localhost:9200/twitter/_search&quot; -H &apos;Content-Type: application/json&apos; -d&apos;&#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;user&quot;: &quot;kimchy&quot; &#125; &#125;&#125;&apos;# 过滤器(filter)查询curl -XGET &quot;http://localhost:9200/twitter/_search&quot; -H &apos;Content-Type: application/json&apos; -d&apos;&#123; &quot;query&quot;: &#123; &quot;bool&quot;: &#123; &quot;filter&quot;: &#123; &quot;match&quot;: &#123; &quot;user&quot;: &quot;kimchy&quot; &#125; &#125; &#125; &#125;&#125;&apos;分片查询缓存的目的是缓存聚合、提示词结果和命中数（它不会缓存返回的文档，因此，它只在 search_type=count 时起作用）。通过下面的参数我们可以设置分片缓存的大小，默认情况下是 JVM 堆的 1% 大小，当然我们也可以手动设置在 config/elasticsearch.yml 文件里。1indices.requests.cache.size: 1%查看缓存占用内存情况(name 表示节点名, query_cache 表示过滤器缓存，request_cache 表示分片缓存，fielddata 表示字段数据缓存，segments 表示索引段)。1curl -XGET &quot;http://localhost:9200/_cat/nodes?h=name,query_cache.memory_size,request_cache.memory_size,fielddata.memory_size,segments.memory&amp;v&quot;2. 使用路由 routingElasticsearch写入文档时，文档会通过一个公式路由到一个索引中的一个分片上。默认的公式如下：1shard_num = hash(_routing) % num_primary_shards_routing 字段的取值，默认是 _id 字段，可以根据业务场景设置经常查询的字段作为路由字段。例如可以考虑将用户 id、地区作为路由字段，查询时可以过滤不必要的分片，加快查询速度。12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152# 写入时指定路由curl -XPUT &quot;http://localhost:9200/my_index/my_type/1?routing=user1&quot; -H &apos;Content-Type: application/json&apos; -d&apos;&#123; &quot;title&quot;: &quot;This is a document&quot;, &quot;author&quot;: &quot;user1&quot;&#125;&apos;# 查询时不指定路由，需要查询所有分片curl -XGET &quot;http://localhost:9200/my_index/_search&quot; -H &apos;Content-Type: application/json&apos; -d&apos;&#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;title&quot;: &quot;document&quot; &#125; &#125;&#125;&apos;# 返回结果&#123; &quot;took&quot;: 2, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;skipped&quot;: 0, &quot;failed&quot;: 0 &#125; ... ...&#125;# 查询时指定路由，只需要查询1个分片curl -XGET &quot;http://localhost:9200/my_index/_search?routing=user1&quot; -H &apos;Content-Type: application/json&apos; -d&apos;&#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;title&quot;: &quot;document&quot; &#125; &#125;&#125;&apos;# 返回结果&#123; &quot;took&quot;: 1, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 1, &quot;successful&quot;: 1, &quot;skipped&quot;: 0, &quot;failed&quot;: 0 &#125; ... ...&#125;3. 强制合并只读索引，关闭历史数据索引只读索引可以从合并成一个单独的大 segment 中收益，减少索引碎片，减少 JVM 堆常驻内存。强制合并索引操作会耗费大量磁盘 IO，尽量配置在业务低峰期(例如凌晨)执行。历史数据索引如果业务上不再支持查询请求，可以考虑关闭索引，减少 JVM 内存占用。12345# 索引forcemerge APIcurl -XPOST &quot;http://localhost:9200/abc20180923/_forcemerge?max_num_segments=1&quot;# 索引关闭APIcurl -XPOST &quot;http://localhost:9200/abc2017*/_close&quot;4. 配置合适的分词器Elasticsearch 内置了很多分词器，包括 standard、cjk、nGram 等，也可以安装自研/开源分词器。根据业务场景选择合适的分词器，避免全部采用默认 standard 分词器。常用分词器：standard：默认分词，英文按空格切分，中文按照单个汉字切分。cjk：根据二元索引对中日韩文分词，可以保证查全率。nGram：可以将英文按照字母切分，结合ES的短语搜索(match_phrase)使用。IK：比较热门的中文分词，能按照中文语义切分，可以自定义词典。pinyin：可以让用户输入拼音，就能查找到相关的关键词。aliws：阿里巴巴自研分词，支持多种模型和分词算法，词库丰富，分词结果准确，适用于电商等对查准要求高的场景。123456# 分词效果测试APIcurl -XPOST &quot;http://localhost:9200/_analyze&quot; -H &apos;Content-Type: application/json&apos; -d&apos;&#123; &quot;analyzer&quot;: &quot;ik_max_word&quot;, &quot;text&quot;: &quot;南京市长江大桥&quot;&#125;&apos;5. 配置查询聚合节点查询聚合节点可以发送粒子查询请求到其他节点，收集和合并结果，以及响应发出查询的客户端。通过给查询聚合节点配置更高规格的 CPU 和内存，可以加快查询运算速度、提升缓存命中率。1234# 查询聚合节点配置(conf/elasticsearch.yml)：node.master:falsenode.data:falsenode.ingest:false6. 设置查询读取记录条数和字段默认的查询请求通常返回排序后的前 10 条记录，最多一次读取 10000 条记录，通过 from 和 size 参数控制读取记录范围，避免一次读取过多的记录。通过 _source 参数可以控制返回字段信息，尽量避免读取大字段。12345678910111213141516171819202122232425# 查询请求示例curl -XGET http://localhost:9200/fulltext001/_search?pretty -H &apos;Content-Type: application/json&apos; -d &apos;&#123; &quot;from&quot;: 0, &quot;size&quot;: 10, &quot;_source&quot;: &quot;id&quot;, &quot;query&quot;: &#123; &quot;bool&quot;: &#123; &quot;must&quot;: [ &#123; &quot;match&quot;: &#123; &quot;content&quot;: &quot;虎嗅&quot; &#125; &#125; ] &#125; &#125;, &quot;sort&quot;: [ &#123; &quot;id&quot;: &#123; &quot;order&quot;: &quot;asc&quot; &#125; &#125; ]&#125;&apos;7. 设置 teminate_after 查询快速返回如果不需要精确统计查询命中记录条数，可以配 teminate_after 指定每个 shard 最多匹配 N 条记录后返回，设置查询超时时间 timeout。在查询结果中可以通过 “terminated_early” 字段标识是否提前结束查询请求。1234567891011121314151617# teminate_after 查询语法示例curl -XGET &quot;http://localhost:9200/twitter/_search&quot; -H &apos;Content-Type: application/json&apos; -d&apos;&#123; &quot;from&quot;: 0, &quot;size&quot;: 10, &quot;timeout&quot;: &quot;10s&quot;, &quot;terminate_after&quot;: 1000, &quot;query&quot;: &#123; &quot;bool&quot;: &#123; &quot;filter&quot;: &#123; &quot;term&quot;: &#123; &quot;user&quot;: &quot;elastic&quot; &#125; &#125; &#125; &#125;&#125;&apos;8. 避免查询深度翻页Elasticsearch 默认只允许查看排序前 10000 条的结果，当翻页查看排序靠后的记录时，响应耗时一般较长。使用 search_after 方式查询会更轻量级，如果每次只需要返回 10 条结果，则每个 shard 只需要返回 search_after 之后的 10 个结果即可，返回的总数据量只是和 shard 个数以及本次需要的个数有关，和历史已读取的个数无关。1234567891011121314151617181920212223242526# search_after查询语法示例curl -XGET &quot;http://localhost:9200/twitter/_search&quot; -H &apos;Content-Type: application/json&apos; -d&apos;&#123; &quot;size&quot;: 10, &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;message&quot;: &quot;Elasticsearch&quot; &#125; &#125;, &quot;sort&quot;: [ &#123; &quot;_score&quot;: &#123; &quot;order&quot;: &quot;desc&quot; &#125; &#125;, &#123; &quot;_id&quot;: &#123; &quot;order&quot;: &quot;asc&quot; &#125; &#125; ], &quot;search_after&quot;: [ 0.84290016, //上一次response中某个doc的score &quot;1024&quot; //上一次response中某个doc的id ]&#125;&apos;9. 避免前缀模糊匹配Elasticsearch 默认支持通过 ? 正则表达式来做模糊匹配，如果在一个数据量较大规模的索引上执行模糊匹配，尤其是前缀模糊匹配，通常耗时会比较长，甚至可能导致内存溢出。尽量避免在高并发查询请求的生产环境执行这类操作。某客户需要对车牌号进行模糊查询，通过查询请求 “车牌号:A8848*” 查询时，往往导致整个集群负载较高。通过对数据预处理，增加冗余字段 “车牌号.keyword”，并事先将所有车牌号按照1元、2元、3元…7元分词后存储至该字段，字段存储内容示例：沪,A,8,4,沪A,A8,88,84,48,沪A8…沪A88488。通过查询”车牌号.keyword:A8848”即可解决原来的性能问题。10. 避免索引稀疏Elasticsearch6.X 之前的版本默认允许在一个 index 下面创建多个 type，Elasticsearch6.X 版本只允许创建一个 type，Elasticsearch7.X 版本只允许 type 值为 “_doc”。在一个索引下面创建多个字段不一样的 type，或者将几百个字段不一样的索引合并到一个索引中，会导致索引稀疏问题。建议每个索引下只创建一个 type，字段不一样的数据分别独立创建 index，不要合并成一个大索引。每个查询请求根据需要去读取相应的索引，避免查询大索引扫描全部记录，加快查询速度。11. 扩容集群节点个数，升级节点规格通常服务器节点数越多，服务器硬件配置规格越高，Elasticsearch 集群的处理能力越强。]]></content>
      <categories>
        <category>elk</category>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>elk</tag>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Logstash集成GaussDB(高斯DB)数据到Elasticsearch]]></title>
    <url>%2F2020%2F04%2F03%2FLogstash%E9%9B%86%E6%88%90GaussDB(%E9%AB%98%E6%96%AFDB)%E6%95%B0%E6%8D%AE%E5%88%B0Elasticsearch%2F</url>
    <content type="text"><![CDATA[GaussDB 简介GaussDB 数据库分为 GaussDB T 和 GaussDB A，分别面向 OLTP 和 OLAP 的业务用户。GaussDB T 数据库是华为公司全自研的分布式数据库，支持x86和华为鲲鹏硬件架构。基于创新性数据库内核，提供高并发事务实时处理能力、两地三中心金融级高可用能力和分布式高扩展能力。GaussDB A 是一款具备分析及混合负载能力的分布式数据库，支持x86和华为鲲鹏硬件架构，支持行存储与列存储，提供PB级数据分析能力、多模分析能力和实时处理能力，用于数据仓库、数据集市、实时分析、实时决策和混合负载等场景，广泛应用于金融、政府、电信等行业核心系统。Logstash 的 jdbc input plugin参考 Logstash的 Jdbc input plugin 的官方文档，该插件可以通过JDBC接口将任何数据库中的数据导入 Logstash。周期性加载或一次加载，每一行是一个 event，列转成 filed。我们先解读下文档里提到的重要配置项。1234567jdbc_driver_library：JDBC驱动包路径。jdbc_driver_class：JDBC驱动程序类。jdbc_connection_string：JDBC连接串。jdbc_user：数据库用户名。jdbc_password：数据库用户口令。statement_filepath：SQL语句所在文件路径。scheduler：调度计划。以上参数已经支持了周期性加载或一次性加载。如果想按字段的自增列或时间戳来集成数据，还需要以下参数：1234567sql_last_value：这个参数内置在sql语句里。作为条件的变量值。last_run_metadata_path：sql_last_value 上次运行值所在的文件路径。use_column_value：设置为时true时，将定义的 tracking_column 值用作 :sql_last_value。默认false。tracking_column：值设置为将被跟踪的列。tracking_column_type：跟踪列的类型。目前仅支持数字和时间戳。record_last_run：上次运行 sql_last_value 值是否保存到 last_run_metadata_path。默认true。clean_run：是否应保留先前的运行状态。默认false。另外如果想使用预编译语句，语句里用？作为占位符，再增加以下参数：123use_prepared_statements：设置为 true 时，启用预编译语句。prepared_statement_name：预编译语句名称。prepared_statement_bind_values：数组类型，存放绑定值。:sql_last_value 可以作为预定义参数。参考：https://www.elastic.co/guide/en/logstash/7.5/plugins-inputs-jdbc.html对接 GaussDB T按每分钟一次频率的周期性来加载 GaussDB T 的会话信息到 Elasticsearch 中，input 区域的配置如下：1234567891011input &#123; jdbc &#123; jdbc_connection_string =&gt; &quot;jdbc:zenith:@vip:40000&quot; jdbc_user =&gt; &quot;omm&quot; jdbc_password =&gt; &quot;omm_password&quot; jdbc_driver_library =&gt; &quot;/opt/gs/com.huawei.gauss.jdbc.ZenithDriver-GaussDB_100_1.0.1.SPC2.B003.jar&quot; jdbc_driver_class =&gt; &quot;com.huawei.gauss.jdbc.ZenithDriver&quot; statement_filepath =&gt; &quot;/opt/statement_filepath/gs_100_session.sql&quot; schedule =&gt; &quot;*/1 * * * *&quot; &#125;&#125;statement_filepath 路径文件里配置的sql如下：1select * from dv_sessions启动 logstash，可以看到logstash 日志中显示有select * from dv_sessions的信息对接 GaussDB A按字段的时间戳来增量加载数据，注意 GaussDB A 的驱动和 GaussDB T 是不同的。input 区域的配置如下：1234567891011121314151617input &#123; jdbc &#123; jdbc_connection_string =&gt; &quot;jdbc:postgresql://vip:25308/postgres&quot; jdbc_user =&gt; &quot;monitor&quot; jdbc_password =&gt; &quot;monitor_password&quot; jdbc_driver_library =&gt; &quot;/opt/gsdriver/gsjdbc4.jar&quot; jdbc_driver_class =&gt; &quot;org.postgresql.Driver&quot; statement_filepath =&gt; &quot;/opt/statement_filepath/gauss_active_session.sql&quot; schedule =&gt; &quot;*/1 * * * *&quot; record_last_run =&gt; &quot;true&quot; use_column_value =&gt; &quot;true&quot; tracking_column =&gt; &quot;sample_time&quot; tracking_column_type =&gt; &quot;timestamp&quot; clean_run =&gt; &quot;false&quot; last_run_metadata_path =&gt; &quot;/opt/last_run_metadata_path/gauss_last_sample_time&quot; &#125;&#125;statement_filepath 路径文件里配置的sql如下，注意里面的预定义变量 :sql_last_value。1select clustername,coorname,sample_time,datid,datname,pid,usesysid,usename,application_name,abbrev(client_addr) AS client_addr,client_hostname,client_port,backend_start,xact_start,query_start,state_change,waiting,enqueue,state,resource_pool,query_id,query from monitor.ash_pg_stat_activity_r where sample_time &gt; :sql_last_valuelast_run_metadata_path 路径下的文件内容：1--- 2020-02-05 12:10:00.000000000 +08:00启动 logstash，可以看到 logstash 日志，注意 :sql_last_value的地方数据 output 到 Elasticsearchlogstash 的 output 区域的配置如下：1234567891011output &#123; elasticsearch &#123; hosts =&gt; [&quot;https://vip:9200&quot;] index =&gt; &quot;gauss_active_session-%&#123;+YYYY.MM.dd&#125;&quot; document_type =&gt; &quot;gauss_active_session&quot; user =&gt; &quot;elastic&quot; password =&gt; &quot;elastic_password&quot; ssl =&gt; true cacert =&gt; &quot;../es_client-ca.cer&quot; &#125;&#125;登入 kibana 查看，按每分钟增量加载的会话表数据已经集成到了 elasticsearch，后续就可以开始做数据分析和可视化了。]]></content>
      <categories>
        <category>elk</category>
        <category>Logstash</category>
        <category>GaussDB</category>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>elk</tag>
        <tag>Elasticsearch</tag>
        <tag>Logstash</tag>
        <tag>GaussDB</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[详细说明-CentOS7部署FastDFS+nginx模块]]></title>
    <url>%2F2020%2F04%2F02%2F%E8%AF%A6%E7%BB%86%E8%AF%B4%E6%98%8E-CentOS7%E9%83%A8%E7%BD%B2FastDFS%2Bnginx%E6%A8%A1%E5%9D%97%2F</url>
    <content type="text"><![CDATA[软件下载123456# 已经事先把所需软件下载好并上传到/usr/local/src目录了https://github.com/happyfish100/libfastcommon/archive/V1.0.43.tar.gzhttps://github.com/happyfish100/fastdfs-nginx-module/archive/V1.22.tar.gzhttps://github.com/happyfish100/fastdfs/archive/V6.06.tar.gzhttps://github.com/happyfish100/fastdfs-client-java/archive/V1.28.tar.gzhttps://openresty.org/download/openresty-1.15.8.3.tar.gz基础环境设置安装依赖组件12yum -y install gcc gcc-c++ libeventyum -y groupinstall &apos;Development Tools&apos;安装libfastcommon12345678910cd /usr/local/srctar -zxvf libfastcommon-1.0.43.tar.gzcd libfastcommon-1.0.43./make.sh./make.sh install# 检查文件是否存在，确保在/usr/lib路径下有libfastcommon.so和libfdfsclient.soll /usr/lib | grep &quot;libf&quot;lrwxrwxrwx 1 root root 27 Apr 2 10:07 libfastcommon.so -&gt; /usr/lib64/libfastcommon.so-rwxr-xr-x 1 root root 356664 Apr 2 10:15 libfdfsclient.so安装fastdfs1234567891011121314151617181920cd /usr/local/srctar -zxvf fastdfs-6.06.tar.gzcd fastdfs-6.06./make.sh./make.sh install# FastDFS的配置文件默认安装到/etc/fdfs目录下# 安装成功后将fastdfs-6.06/conf下的俩文件拷贝到/etc/fdfs/下cd confcp http.conf mime.types /etc/fdfs/cd /etc/fdfs/[root@bogon fdfs]# lltotal 68-rw-r--r-- 1 root root 1909 Apr 2 10:15 client.conf.sample-rw-r--r-- 1 root root 965 Apr 2 10:16 http.conf-rw-r--r-- 1 root root 31172 Apr 2 10:16 mime.types-rw-r--r-- 1 root root 10246 Apr 2 10:15 storage.conf.sample-rw-r--r-- 1 root root 620 Apr 2 10:15 storage_ids.conf.sample-rw-r--r-- 1 root root 9138 Apr 2 10:15 tracker.conf.samplefdfs_trackerd配置并启动12345678910111213141516171819202122232425262728# 创建tracker工作目录,storage存储目录(选择大磁盘空间)等mkdir -p /opt/&#123;fdfs_tracker,fdfs_storage,fdfs_storage_data&#125;cd /etc/fdfs/cp tracker.conf.sample tracker.confvim tracker.conf disabled = false # 配置tracker.conf这个配置文件是否生效，因为在启动fastdfs服务端进程时需要指定配置文件，所以需要使次配置文件生效。false是生效，true是屏蔽。 bind_addr = # 程序的监听地址，如果不设定则监听所有地址，可以设置本地ip地址 port = 22122 #tracker监听的端口 base_path = /opt/fdfs_tracker # tracker保存data和logs的路径 http.server_port=8080 # http服务端口，保持默认# 启动fdfs_trackerd/usr/bin/fdfs_trackerd /etc/fdfs/tracker.conf start# 查看/opt/fdfs_tracker目录，发现目录下多了data和logs两个目录# 查看端口号，验证启动情况[root@bogon fdfs]# ps -ef | grep fdfsroot 2119 1 0 10:22 ? 00:00:00 /usr/bin/fdfs_trackerd /etc/fdfs/tracker.conf start[root@bogon fdfs]# ss -tulnp | grep 22122tcp LISTEN 0 128 *:22122 *:* users:((&quot;fdfs_trackerd&quot;,pid=2119,fd=5))# 命令行选项Usage: /usr/bin/fdfs_trackerd &lt;config_file&gt; [start|stop|restart]# 设置开机自启动echo &quot;/usr/bin/fdfs_trackerd /etc/fdfs/tracker.conf restart&quot; | tee -a /etc/rc.d/rc.localfdfs_storage配置并启动与tracker不同的是，storage还需要一个目录用来存储数据，所以在上面步骤中另外多建了两个目录fdfs_storage_data,fdfs_storage123456789101112131415161718192021222324252627cd /etc/fdfs/cp storage.conf.sample storage.confvim storage.conf disabled=false # 启用这个配置文件 group_name=group1 #组名，根据实际情况修改，文件链接中会用到 port=23000 #设置storage的端口号，默认是23000，同一个组的storage端口号必须一致 base_path = /opt/fdfs_storage # #设置storage数据文件和日志目录，注意,这个目录最好有大于50G的磁盘空间 store_path_count=1 #存储路径个数，需要和store_path个数匹配 store_path0 = /opt/fdfs_storage_data # 实际保存文件的路径，注意,这个目录最好有大于50G的磁盘空间 tracker_server = 192.168.75.5:22122 # tracker监听地址和端口号，要与tracker.conf文件中设置的保持一致 http.server_port=8888 #设置 http 端口号 # 启动fdfs_storaged/usr/bin/fdfs_storaged /etc/fdfs/storage.conf start# 查看端口号，验证启动情况[root@bogon fdfs]# ps -ef | grep &quot;fdfs_storaged&quot;root 2194 1 7 10:36 ? 00:00:01 /usr/bin/fdfs_storaged /etc/fdfs/storage.conf start[root@bogon fdfs]# ss -tulnp | grep &quot;fdfs&quot;tcp LISTEN 0 128 *:23000 *:* users:((&quot;fdfs_storaged&quot;,pid=2194,fd=5))tcp LISTEN 0 128 *:22122 *:* users:((&quot;fdfs_trackerd&quot;,pid=2119,fd=5))# 命令行选项Usage: /usr/bin/fdfs_trackerd &lt;config_file&gt; [start|stop|restart]# 设置开机自启动echo &quot;/usr/bin/fdfs_storaged /etc/fdfs/storage.conf restart&quot; | tee -a /etc/rc.d/rc.local校验整合要确定一下，storage是否注册到了tracker中去1/usr/bin/fdfs_monitor /etc/fdfs/storage.conf成功后可以看到：ip_addr = 192.168.75.5 ACTIVE使用FastDFS自带工具测试123456cd /etc/fdfs/cp client.conf.sample client.confvim client.conf base_path = /opt/fdfs_tracker # tracker服务器文件路径 tracker_server = 192.168.75.5:22122 #tracker服务器IP地址和端口号 http.tracker_server_port = 8080 # tracker服务器的http端口号,必须和tracker的设置对应起来上传一张图片1.jpg到Centos服务器上的 /tmp 目录下，进行测试，命令如下：123456789101112131415161718192021222324252627282930/usr/bin/fdfs_test /etc/fdfs/client.conf upload /tmp/1.jpgThis is FastDFS client test program v6.06Copyright (C) 2008, Happy Fish / YuQingFastDFS may be copied only under the terms of the GNU GeneralPublic License V3, which may be found in the FastDFS source kit.Please visit the FastDFS Home Page http://www.fastken.com/ for more detail.[2020-04-02 10:47:57] DEBUG - base_path=/opt/fdfs_tracker, connect_timeout=5, network_timeout=60, tracker_server_count=1, anti_steal_token=0, anti_steal_secret_key length=0, use_connection_pool=0, g_connection_pool_max_idle_time=3600s, use_storage_id=0, storage server id count: 0tracker_query_storage_store_list_without_group: server 1. group_name=, ip_addr=192.168.75.5, port=23000group_name=group1, ip_addr=192.168.75.5, port=23000storage_upload_by_filenamegroup_name=group1, remote_filename=M00/00/00/wKhLBV6FUl6AA0eTACSnIi51C7k037.jpgsource ip address: 192.168.75.5file timestamp=2020-04-02 10:47:58file size=2402082file crc32=779422649example file url: http://192.168.75.5:8080/group1/M00/00/00/wKhLBV6FUl6AA0eTACSnIi51C7k037.jpgstorage_upload_slave_by_filenamegroup_name=group1, remote_filename=M00/00/00/wKhLBV6FUl6AA0eTACSnIi51C7k037_big.jpgsource ip address: 192.168.75.5file timestamp=2020-04-02 10:47:58file size=2402082file crc32=779422649example file url: http://192.168.75.5:8080/group1/M00/00/00/wKhLBV6FUl6AA0eTACSnIi51C7k037_big.jpg以上图中的文件地址：http://192.168.75.5:8080/group1/M00/00/00/wKhLBV6FUl6AA0eTACSnIi51C7k037.jpg对应storage服务器上的/opt/fdfs_storage_data/data/00/00/wKhLBV6FUl6AA0eTACSnIi51C7k037.jpg文件;组名：group1磁盘：M00目录：00/00文件名称：wKhLBV6FUl6AA0eTACSnIi51C7k037.jpg注意图片路径中的8080端口,这个是tracker的端口上传的图片会被上传到我们创建的fdfs_storage_data目录下，会有四个图片文件:12345678[root@bogon 00]# pwd/opt/fdfs_storage_data/data/00/00[root@bogon 00]# lltotal 4704-rw-r--r-- 1 root root 2402082 Apr 2 10:47 wKhLBV6FUl6AA0eTACSnIi51C7k037_big.jpg-rw-r--r-- 1 root root 49 Apr 2 10:47 wKhLBV6FUl6AA0eTACSnIi51C7k037_big.jpg-m-rw-r--r-- 1 root root 2402082 Apr 2 10:47 wKhLBV6FUl6AA0eTACSnIi51C7k037.jpg-rw-r--r-- 1 root root 49 Apr 2 10:47 wKhLBV6FUl6AA0eTACSnIi51C7k037.jpg-mdata下有256个1级目录，每级目录下又有256个2级子目录，总共65536个文件，新写的文件会以hash的方式被路由到其中某个子目录下，然后将文件数据直接作为一个本地文件存储到该目录中。FastDFS和nginx结合使用FastDFS通过Tracker服务器,将文件放在Storage服务器存储,但是同组之间的服务器需要复制文件,有延迟的问题.假设Tracker服务器将文件上传到了172.20.132.57,文件ID已经返回客户端,这时,后台会将这个文件复制到172.20.132.57,如果复制没有完成,客户端就用这个ID在172.20.132.57取文件,肯定会出现错误。这个fastdfs-nginx-module可以重定向连接到源服务器取文件,避免客户端由于复制延迟的问题,出现错误。正是这样，FastDFS需要结合nginx，所以取消原来对HTTP的直接支持。在tracker上安装 nginx在每个tracker上安装nginx的主要目的是做负载均衡及实现高可用。如果只有一台tracker服务器可以不配置nginx.一个tracker对应多个storage，通过nginx对storage负载均衡;在storage上安装nginx(openresty)12345678910cd /usr/local/src/tar -zxvf fastdfs-nginx-module-1.22.tar.gzcd fastdfs-nginx-module-1.22/srccp mod_fastdfs.conf /etc/fdfs/vim /etc/fdfs/mod_fastdfs.conf base_path=/opt/fdfs_storage # 与storage.conf配置中的保持一致 tracker_server=192.168.75.5:22122 #tracker服务器的IP地址以及端口号 url_have_group_name = true # url中包含group名称 store_path0=/opt/fdfs_storage_data #与storage.conf中的路径保持一致 group_count = 1 #设置组的个数123456789101112131415161718192021222324252627282930313233343536373839404142434445yum -y install pcre pcre-devel openssl openssl-devel zlib zlib-devel cd /usr/local/srctar -zxvf openresty-1.15.8.3.tar.gzcd openresty-1.15.8.3./configure \ --with-luajit \ --with-http_stub_status_module \ --with-http_ssl_module \ --with-http_realip_module \ --with-http_gzip_static_module \ --add-module=/usr/local/src/fastdfs-nginx-module-1.22/srcgmakegmake install# 修改配置文件vim /usr/local/openresty/nginx/conf/nginx.conf error_log logs/error.log; pid logs/nginx.pid; server&#123; server_name 192.168.75.5; &#125;# 启动/usr/local/openresty/nginx/sbin/nginx -c /usr/local/openresty/nginx/conf/nginx.conf# 浏览器访问，出现openresty欢迎页面# 设置nginx开机启动echo &quot;/usr/local/openresty/nginx/sbin/nginx -c /usr/local/openresty/nginx/conf/nginx.conf&quot; | tee -a /etc/rc.d/rc.local# 再次修改配置文件，加载fastdfs模块vim /usr/local/openresty/nginx/conf/nginx.conf server&#123; location /group1/M00/ &#123; root /opt/fdfs_storage/data; ngx_fastdfs_module; &#125; &#125;# 重载nginx/usr/local/openresty/nginx/sbin/nginx -s reload# 参考上面测试的那一步图片url地址：http://192.168.75.5:8080/group1/M00/00/00/wKhLBV6FUl6AA0eTACSnIi51C7k037.jpg使用nginxf访问的话，实际地址是：http://192.168.75.5/group1/M00/00/00/wKhLBV6FUl6AA0eTACSnIi51C7k037.jpg需要把tracker使用的8080端口去掉，否则无法访问123456789101112131415161718192021222324252627282930313233343536373839404142# 进一步完善nginx配置文件 # 这个server设置的是storage nginx server &#123; listen 9991; server_name localhost; location / &#123; root html; index index.html index.htm; &#125; location ~/group1/M00 &#123; root /opt/fastdfs_storage/data; ngx_fastdfs_module; &#125; location = /50x.html &#123; root html; &#125; &#125; # 若访问不到图片需要配置这个软连接 # ln -s /opt/fastdfs_storage_data/data/ /opt/fastdfs_storage_data/data/M00 # 这个server设置的是tracker nginx upstream fdfs_group1 &#123; server 127.0.0.1:9991; &#125; server &#123; listen 80; server_name localhost; location /group1/M00 &#123; proxy_pass http://fdfs_group1; &#125; error_page 500 502 503 504 /50x.html; location = /50x.html &#123; root html; &#125; &#125;搭建集群集群规划(单tracker,双storage)1234虚拟机 IP 说明tracker 192.168.75.5 tracker 服务器storage01 192.168.75.6 storage01服务器【group1】storage02 192.168.75.7 storage02服务器【group2】软件清单12345fastdfs-6.06.tar.gzfastdfs-client-java-1.28.tar.gzfastdfs-nginx-module-1.22.tar.gzlibfastcommon-1.0.43.tar.gzopenresty-1.15.8.3.tar.gz安装步骤1.tracker服务器12345678# 1. 安装libfastcommon 模块# 2. 编译安装 FastDFS# 3. 修改配置文件tarcker.conf和client.conf(测试上传)# vim /etc/fdfs/tracker.conf store_lookup=0 #采用轮询策略进行存储，0：轮询 1：始终定向到某个group 2：选择存储空间最大的进行存储# 4. 开机启动2.storage服务器123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657# 1. 安装libfastcommon 模块# 2. 编译安装 FastDFS# 3. 修改配置文件storage.conf# storage01 配置# vim /etc/fdfs/storage.confgroup_name=group1base_path=/home/fastdfs_storagestore_path0=/home/fastdfs_storagetracker_server=192.168.75.6:22122http.server_port=8888# storage02 配置# vim /etc/fdfs/storage.confgroup_name=group2base_path=/home/fastdfs_storagestore_path0=/home/fastdfs_storagetracker_server=192.168.75.7:22122http.server_port=8888# 4. 开机启动# 5. 安装nginx和fastdfs-nginx-module模块# storage01 配置：# vim /etc/fdfs/mod_fastdfs.confconnect_timeout=10base_path=/home/fastdfs_storageurl_have_group_name=truestore_path0=/home/fastdfs_storagetracker_server=192.168.75.6:22122group_name=group1# storage02 配置：# vim /etc/fdfs/mod_fastdfs.confconnect_timeout=10base_path=/home/fastdfs_storageurl_have_group_name=truestore_path0=/home/fastdfs_storagetracker_server=192.168.75.7:22122group_name=group2# 6. 复制 FastDFS 安装目录的部分配置文件到 /etc/fdfs 目录cp http.conf mime.types /etc/fdfs/# 7. 配置nginxserver &#123; listen 8888; server_name localhost; location ~/group([0-9])/M00 &#123; ngx_fastdfs_module; &#125; error_page 500 502 503 504 /50x.html; location = /50x.html &#123; root html; &#125; &#125;3.测试12345# vim /etc/fdfs/client.conf base_path=/home/fastdfs_tracker tracker_server=192.168.75.5:22122/usr/bin/fdfs_upload_file /etc/fdfs/client.conf test.jpg4. tracker安装nginx12345678910111213141516171819202122232425262728293031323334http &#123; include mime.types; default_type application/octet-stream; sendfile on; keepalive_timeout 65; #group1 upstream fdfs_group1 &#123; server 192.168.75.6:8888; &#125; #group2 upstream fdfs_group2 &#123; server 192.168.75.7:8888; &#125; server &#123; listen 8000; server_name localhost; location /group1/M00 &#123; proxy_pass http://fdfs_group1; &#125; location /group2/M00 &#123; proxy_pass http://fdfs_group2; &#125; error_page 500 502 503 504 /50x.html; location = /50x.html &#123; root html; &#125; &#125; &#125;]]></content>
      <categories>
        <category>nginx</category>
        <category>FastDFS</category>
      </categories>
      <tags>
        <tag>nginx</tag>
        <tag>FastDFS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CentOS7部署FastDFS+nginx模块]]></title>
    <url>%2F2020%2F04%2F02%2FCentOS7%E9%83%A8%E7%BD%B2FastDFS%2Bnginx%E6%A8%A1%E5%9D%97%2F</url>
    <content type="text"><![CDATA[软件下载123456# 已经事先把所需软件下载好并上传到/usr/local/src目录了https://github.com/happyfish100/libfastcommon/archive/V1.0.43.tar.gzhttps://github.com/happyfish100/fastdfs-nginx-module/archive/V1.22.tar.gzhttps://github.com/happyfish100/fastdfs/archive/V6.06.tar.gzhttps://github.com/happyfish100/fastdfs-client-java/archive/V1.28.tar.gzhttps://openresty.org/download/openresty-1.15.8.3.tar.gz基础环境设置安装依赖组件12yum -y install gcc gcc-c++ libeventyum -y groupinstall &apos;Development Tools&apos;安装libfastcommon1234567891011cd /usr/local/srctar -zxvf libfastcommon-1.0.43.tar.gzcd libfastcommon-1.0.43./make.sh./make.sh install# 检查文件是否存在[root@bogon libfastcommon-1.0.43]# ll /usr/lib64 | grep &quot;libfastcommon.so&quot; -rwxr-xr-x 1 root root 1035264 Apr 2 10:07 libfastcommon.so[root@bogon libfastcommon-1.0.43]# ll /usr/lib | grep &quot;libfastcommon.so&quot; lrwxrwxrwx 1 root root 27 Apr 2 10:07 libfastcommon.so -&gt; /usr/lib64/libfastcommon.so安装fastdfs123456789101112131415161718cd /usr/local/srctar -zxvf fastdfs-6.06.tar.gzcd fastdfs-6.06./make.sh./make.sh install# 安装成功后将解压目录下的conf下的俩文件拷贝到/etc/fdfs/下cd confcp http.conf mime.types /etc/fdfs/cd /etc/fdfs/[root@bogon fdfs]# lltotal 68-rw-r--r-- 1 root root 1909 Apr 2 10:15 client.conf.sample-rw-r--r-- 1 root root 965 Apr 2 10:16 http.conf-rw-r--r-- 1 root root 31172 Apr 2 10:16 mime.types-rw-r--r-- 1 root root 10246 Apr 2 10:15 storage.conf.sample-rw-r--r-- 1 root root 620 Apr 2 10:15 storage_ids.conf.sample-rw-r--r-- 1 root root 9138 Apr 2 10:15 tracker.conf.samplefdfs_trackerd配置并启动1234567891011121314151617181920cd /etc/fdfs/cp tracker.conf.sample tracker.confmkdir -p /opt/&#123;fdfs_tracker,fdfs_storage,fdfs_storage_data&#125;vim tracker.conf base_path = /opt/fdfs_tracker# 启动fdfs_trackerd/usr/bin/fdfs_trackerd /etc/fdfs/tracker.conf start# 查看端口号，验证启动情况[root@bogon fdfs]# ps -ef | grep fdfsroot 2119 1 0 10:22 ? 00:00:00 /usr/bin/fdfs_trackerd /etc/fdfs/tracker.conf start[root@bogon fdfs]# ss -tulnp | grep 22122tcp LISTEN 0 128 *:22122 *:* users:((&quot;fdfs_trackerd&quot;,pid=2119,fd=5))# 命令行选项Usage: /usr/bin/fdfs_trackerd &lt;config_file&gt; [start|stop|restart]# 注意：在/opt/fdfs_data目录下生成两个目录,一个是数据,一个是日志.# 设置开机自启动echo &quot;/usr/bin/fdfs_trackerd /etc/fdfs/tracker.conf restart&quot; | tee -a /etc/rc.d/rc.localfdfs_storaged配置并启动123456789101112131415161718cd /etc/fdfs/cp storage.conf.sample storage.confvim storage.conf base_path = /opt/fdfs_storage # 注意,这个目录最好有大于50G的磁盘空间 store_path0 = /opt/fdfs_storage_data # 若配置这个参数，则该目录为实际保存文件的路径 tracker_server = 192.168.75.5:22122 # 注意：这个参数不能设置127.0.0.1，否则storage注册时会报错：ERROR - file: storage_func.c, line: 1361, conf file &quot;/etc/fdfs/storage.conf&quot;, tracker: &quot;127.0.0.1:22122&quot; is invalid, tracker server ip can&apos;t be 127.0.0.1# 启动fdfs_storaged/usr/bin/fdfs_storaged /etc/fdfs/storage.conf start[root@bogon fdfs]# ps -ef | grep &quot;fdfs_storaged&quot;root 2194 1 7 10:36 ? 00:00:01 /usr/bin/fdfs_storaged /etc/fdfs/storage.conf start[root@bogon fdfs]# ss -tulnp | grep &quot;fdfs&quot;tcp LISTEN 0 128 *:23000 *:* users:((&quot;fdfs_storaged&quot;,pid=2194,fd=5))tcp LISTEN 0 128 *:22122 *:* users:((&quot;fdfs_trackerd&quot;,pid=2119,fd=5))# 命令行选项Usage: /usr/bin/fdfs_trackerd &lt;config_file&gt; [start|stop|restart]# 设置开机自启动echo &quot;/usr/bin/fdfs_storaged /etc/fdfs/storage.conf restart&quot; | tee -a /etc/rc.d/rc.local校验整合要确定一下，storage是否注册到了tracker中去1/usr/bin/fdfs_monitor /etc/fdfs/storage.conf成功后可以看到：ip_addr = 192.168.75.5 ACTIVE使用FastDFS自带工具测试123456cd /etc/fdfs/cp client.conf.sample client.confvim client.conf base_path = /opt/fdfs_tracker #tracker服务器文件路径 tracker_server = 192.168.75.5:22122 #tracker服务器IP地址和端口号 http.tracker_server_port = 8080 # tracker服务器的http端口号,必须和tracker的设置对应起来上传一张图片1.jpg到Centos服务器上的 /tmp 目录下，进行测试，命令如下：123456789101112131415161718192021222324252627282930/usr/bin/fdfs_test /etc/fdfs/client.conf upload /tmp/1.jpgThis is FastDFS client test program v6.06Copyright (C) 2008, Happy Fish / YuQingFastDFS may be copied only under the terms of the GNU GeneralPublic License V3, which may be found in the FastDFS source kit.Please visit the FastDFS Home Page http://www.fastken.com/ for more detail.[2020-04-02 10:47:57] DEBUG - base_path=/opt/fdfs_tracker, connect_timeout=5, network_timeout=60, tracker_server_count=1, anti_steal_token=0, anti_steal_secret_key length=0, use_connection_pool=0, g_connection_pool_max_idle_time=3600s, use_storage_id=0, storage server id count: 0tracker_query_storage_store_list_without_group: server 1. group_name=, ip_addr=192.168.75.5, port=23000group_name=group1, ip_addr=192.168.75.5, port=23000storage_upload_by_filenamegroup_name=group1, remote_filename=M00/00/00/wKhLBV6FUl6AA0eTACSnIi51C7k037.jpgsource ip address: 192.168.75.5file timestamp=2020-04-02 10:47:58file size=2402082file crc32=779422649example file url: http://192.168.75.5:8080/group1/M00/00/00/wKhLBV6FUl6AA0eTACSnIi51C7k037.jpgstorage_upload_slave_by_filenamegroup_name=group1, remote_filename=M00/00/00/wKhLBV6FUl6AA0eTACSnIi51C7k037_big.jpgsource ip address: 192.168.75.5file timestamp=2020-04-02 10:47:58file size=2402082file crc32=779422649example file url: http://192.168.75.5:8080/group1/M00/00/00/wKhLBV6FUl6AA0eTACSnIi51C7k037_big.jpg以上图中的文件地址：http://192.168.75.5:8080/group1/M00/00/00/wKhLBV6FUl6AA0eTACSnIi51C7k037.jpg对应storage服务器上的/opt/fdfs_storage_data/data/00/00/wKhLBV6FUl6AA0eTACSnIi51C7k037.jpg文件;注意图片路径中的8080端口,这个是tracker的端口，但是查看该目录，会有四个图片文件:12345678[root@bogon 00]# pwd/opt/fdfs_storage_data/data/00/00[root@bogon 00]# lltotal 4704-rw-r--r-- 1 root root 2402082 Apr 2 10:47 wKhLBV6FUl6AA0eTACSnIi51C7k037_big.jpg-rw-r--r-- 1 root root 49 Apr 2 10:47 wKhLBV6FUl6AA0eTACSnIi51C7k037_big.jpg-m-rw-r--r-- 1 root root 2402082 Apr 2 10:47 wKhLBV6FUl6AA0eTACSnIi51C7k037.jpg-rw-r--r-- 1 root root 49 Apr 2 10:47 wKhLBV6FUl6AA0eTACSnIi51C7k037.jpg-mFastDFS和nginx结合使用在tracker上安装 nginx在每个tracker上安装nginx的主要目的是做负载均衡及实现高可用。如果只有一台tracker服务器可以不配置nginx.一个tracker对应多个storage，通过nginx对storage负载均衡;在storage上安装nginx(openresty)123456789cd /usr/local/src/tar -zxvf fastdfs-nginx-module-1.22.tar.gzcd fastdfs-nginx-module-1.22/srccp mod_fastdfs.conf /etc/fdfs/vim /etc/fdfs/mod_fastdfs.conf base_path=/opt/fdfs_storage tracker_server=192.168.75.5:22122 url_have_group_name = true #url中包含group名称 store_path0=/opt/fdfs_storage_data #与storage.conf中的路径保持一致12345678910111213141516171819202122232425262728293031323334353637383940yum -y install pcre pcre-devel openssl openssl-develcd /usr/local/srctar -zxvf openresty-1.15.8.3.tar.gzcd openresty-1.15.8.3./configure \ --with-luajit \ --with-http_stub_status_module \ --with-http_ssl_module \ --with-http_realip_module \ --with-http_gzip_static_module \ --add-module=/usr/local/src/fastdfs-nginx-module-1.22/srcgmakegmake install# 启动vim /usr/local/openresty/nginx/conf/nginx.conf error_log logs/error.log; pid logs/nginx.pid; server&#123; server_name 192.168.75.5; &#125;/usr/local/openresty/nginx/sbin/nginx -c /usr/local/openresty/nginx/conf/nginx.conf# 浏览器访问，出现openresty欢迎页面# 设置nginx开机启动echo &quot;/usr/local/openresty/nginx/sbin/nginx -c /usr/local/openresty/nginx/conf/nginx.conf&quot; | tee -a /etc/rc.d/rc.localvim /usr/local/openresty/nginx/conf/nginx.conf server&#123; location /group1/M00/ &#123; root /opt/fdfs_storage/data; ngx_fastdfs_module; &#125; &#125;/usr/local/openresty/nginx/sbin/nginx -s reload# 参考上面测试的那一步图片url地址：http://192.168.75.5:8080/group1/M00/00/00/wKhLBV6FUl6AA0eTACSnIi51C7k037.jpg使用nginxf访问的话，实际地址是：http://192.168.75.5/group1/M00/00/00/wKhLBV6FUl6AA0eTACSnIi51C7k037.jpg需要把tracker使用的8080端口去掉，否则无法访问123456789101112131415161718192021222324252627282930313233343536373839404142# 进一步完善nginx配置文件 # 这个server设置的是storage nginx server &#123; listen 9991; server_name localhost; location / &#123; root html; index index.html index.htm; &#125; location ~/group1/M00 &#123; root /opt/fastdfs_storage/data; ngx_fastdfs_module; &#125; location = /50x.html &#123; root html; &#125; &#125; # 若访问不到图片需要配置这个软连接 # ln -s /opt/fastdfs_storage_data/data/ /opt/fastdfs_storage_data/data/M00 # 这个server设置的是tracker nginx upstream fdfs_group1 &#123; server 127.0.0.1:9991; &#125; server &#123; listen 80; server_name localhost; location /group1/M00 &#123; proxy_pass http://fdfs_group1; &#125; error_page 500 502 503 504 /50x.html; location = /50x.html &#123; root html; &#125; &#125;]]></content>
      <categories>
        <category>nginx</category>
        <category>FastDFS</category>
      </categories>
      <tags>
        <tag>nginx</tag>
        <tag>FastDFS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[FastAPI快速入门]]></title>
    <url>%2F2020%2F01%2F15%2Ffastapi%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8%2F</url>
    <content type="text"><![CDATA[fastapi是高性能的web框架。他的主要特点是：快速编码减少人为bug直观简易具有交互式文档基于API的开放标准（并与之完全兼容）：OpenAPI（以前称为Swagger）和JSON Schema。技术背景：python3.6+、Starlette、Pydantic官方文档地址：https://fastapi.tiangolo.com/安装12pip install fastapipip install uvicornquick start12345678910111213# main.pyfrom fastapi import FastAPIapp = FastAPI()@app.get(&quot;/&quot;)def read_root(): return &#123;&quot;Hello&quot;: &quot;World&quot;&#125;@app.get(&quot;/items/&#123;item_id&#125;&quot;)def read_item(item_id: int, q: str = None): return &#123;&quot;item_id&quot;: item_id, &quot;q&quot;: q&#125;或者1234567891011121314# If your code uses async / await, use async def:from fastapi import FastAPIapp = FastAPI()@app.get(&quot;/&quot;)async def read_root(): return &#123;&quot;Hello&quot;: &quot;World&quot;&#125;@app.get(&quot;/items/&#123;item_id&#125;&quot;)async def read_item(item_id: int, q: str = None): return &#123;&quot;item_id&quot;: item_id, &quot;q&quot;: q&#125;运行1uvicorn main:app --reload看到如下提示，证明运行成功main: 表示app所在文件名, the file main.py (the Python &quot;module&quot;). app：FastAPI实例, the object created inside of main.py with the line app = FastAPI(). reload：debug模式，可以自动重启,make the server restart after code changes. Only do this for development. 试着请求http://127.0.0.1:8000/items/5?q=somequery，会看到如下返回1&#123;&quot;item_id&quot;: 5, &quot;q&quot;: &quot;somequery&quot;&#125;交互文档试着打开http://127.0.0.1:8000/docsAPI文档试着打开http://127.0.0.1:8000/redocupdate通过上面的例子，我们已经用fastapi完成了第一个web服务，现在我们再添加一个接口1234567891011121314151617181920212223242526from fastapi import FastAPIfrom pydantic import BaseModelapp = FastAPI()class Item(BaseModel): name: str price: float is_offer: bool = None@app.get(&quot;/&quot;)def read_root(): return &#123;&quot;Hello&quot;: &quot;World&quot;&#125;@app.get(&quot;/items/&#123;item_id&#125;&quot;)def read_item(item_id: int, q: str = None): return &#123;&quot;item_id&quot;: item_id, &quot;q&quot;: q&#125;@app.put(&quot;/items/&#123;item_id&#125;&quot;)def update_item(item_id: int, item: Item): return &#123;&quot;item_name&quot;: item.name, &quot;item_id&quot;: item_id&#125;此时会发现，服务自动重启了，这是因为我们在启动命令后添加了–reload。再次查看文档，发现同样发生了改变。到此，你已经可以快速的用fastapi搭建起服务了～]]></content>
      <categories>
        <category>Python</category>
        <category>FastAPI</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>FastAPI</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[FastAPI教程进阶(一)]]></title>
    <url>%2F2020%2F01%2F15%2Ffastapi%E6%95%99%E7%A8%8B%E8%BF%9B%E9%98%B6%EF%BC%88%E4%B8%80%EF%BC%89%2F</url>
    <content type="text"><![CDATA[一个简单的栗子12345678from fastapi import FastAPIapp = FastAPI()@app.get(&quot;/&quot;)async def root(): return &#123;&quot;message&quot;: &quot;Hello World&quot;&#125;FASTAPI继承Starlette，因此在Starlette中的所有可调用的对象在FASTAPI中可以直接引用编写步骤步骤一：导入FastAPI1from fastapi import FastAPI步骤二：创建FastAPI实例1app = FastAPI()步骤三：创建访问路径1@app.get(&quot;/&quot;)这个路径告诉FastAPI，该装饰器下的方法是用来处理路径是“/”的GET请求步骤四：定义方法，处理请求1async def root():步骤五：返回响应信息1return &#123;&quot;message&quot;: &quot;Hello World&quot;&#125;步骤六：运行1uvicorn main:app --reload获取路径参数12345678from fastapi import FastAPIapp = FastAPI()@app.get(&quot;/items/&#123;item_id&#125;&quot;)async def read_item(item_id): return &#123;&quot;item_id&quot;: item_id&#125;路径中的item_id将会被解析，传递给方法中的item_id。请求http://127.0.0.1:8000/items/foo会返回如下结果：1&#123;&quot;item_id&quot;:&quot;foo&quot;&#125;也可以在方法中定义参数类型：12345678from fastapi import FastAPIapp = FastAPI()@app.get(&quot;/items/&#123;item_id&#125;&quot;)async def read_item(item_id: int): return &#123;&quot;item_id&quot;: item_id&#125;继续请求http://127.0.0.1:8000/items/3，会返回1&#123;&quot;item_id&quot;:3&#125;此时的item_id是int类型的3，而不是string类型，这是因为FastAPI在解析请求时，自动根据声明的类型进行了解析如果请求http://127.0.0.1:8000/items/foo，此时会返回：123456789101112&#123; &quot;detail&quot;: [ &#123; &quot;loc&quot;: [ &quot;path&quot;, &quot;item_id&quot; ], &quot;msg&quot;: &quot;value is not a valid integer&quot;, &quot;type&quot;: &quot;type_error.integer&quot; &#125; ]&#125;这是因为foo并不能转换成int类型。请求http://127.0.0.1:8000/items/4.2也会出现上述错误所有的数据类型验证，都是通过Pydantic完成的如果想对路径参数做一个预定义，可以使用Enum：123456789101112131415161718192021from enum import Enumfrom fastapi import FastAPIclass ModelName(str, Enum): alexnet = &quot;alexnet&quot; resnet = &quot;resnet&quot; lenet = &quot;lenet&quot;app = FastAPI()@app.get(&quot;/model/&#123;model_name&#125;&quot;)async def get_model(model_name: ModelName): if model_name == ModelName.alexnet: return &#123;&quot;model_name&quot;: model_name, &quot;message&quot;: &quot;Deep Learning FTW!&quot;&#125; if model_name.value == &quot;lenet&quot;: return &#123;&quot;model_name&quot;: model_name, &quot;message&quot;: &quot;LeCNN all the images&quot;&#125; return &#123;&quot;model_name&quot;: model_name, &quot;message&quot;: &quot;Have some residuals&quot;&#125;打开http://127.0.0.1:8000/docs:除此之外，假如想接收一个路径参数，它本身就是一个路径，就像/files/{file_path}，而这个file_path是home/johndoe/myfile.txt时，可以写成/files/{file_path:path}：12345678from fastapi import FastAPIapp = FastAPI()@app.get(&quot;/files/&#123;file_path:path&#125;&quot;)async def read_user_me(file_path: str): return &#123;&quot;file_path&quot;: file_path&#125;OpenAPI本身不支持在路径参数包含路径，但是可以当作Starlette内部的一个使用方法 此时访问http://127.0.0.1:8000/files/home/johndoe/myfile.txt，返回：1&#123;&quot;file_path&quot;:&quot;home/johndoe/myfile.txt&quot;&#125;如果将路径改为/files/{file_path}，会返回：1&#123;&quot;detail&quot;:&quot;Not Found&quot;&#125;获取查询参数这里依旧是一个例子：12345678910from fastapi import FastAPIapp = FastAPI()fake_items_db = [&#123;&quot;item_name&quot;: &quot;Foo&quot;&#125;, &#123;&quot;item_name&quot;: &quot;Bar&quot;&#125;, &#123;&quot;item_name&quot;: &quot;Baz&quot;&#125;]@app.get(&quot;/items/&quot;)async def read_item(skip: int = 0, limit: int = 10): return fake_items_db[skip : skip + limit]尝试访问http://127.0.0.1:8000/items/?skip=0&amp;limit=2，返回：1[&#123;&quot;item_name&quot;:&quot;Foo&quot;&#125;,&#123;&quot;item_name&quot;:&quot;Bar&quot;&#125;]尝试访问http://127.0.0.1:8000/items/，返回：1[&#123;&quot;item_name&quot;:&quot;Foo&quot;&#125;,&#123;&quot;item_name&quot;:&quot;Bar&quot;&#125;,&#123;&quot;item_name&quot;:&quot;Baz&quot;&#125;]由于我们在定义方法的时候，分别赋予skip和limit默认值，当不添加querystring时，会使用默认值。当然，我们也可以将默认值赋值为None：12345678910from fastapi import FastAPIapp = FastAPI()@app.get(&quot;/items/&#123;item_id&#125;&quot;)async def read_item(item_id: str, q: str = None): if q: return &#123;&quot;item_id&quot;: item_id, &quot;q&quot;: q&#125; return &#123;&quot;item_id&quot;: item_id&#125;此时，我们请求http://127.0.0.1:8000/items/1?q=qqq:1&#123;&quot;item_id&quot;:&quot;1&quot;,&quot;q&quot;:&quot;qqq&quot;&#125;值得放心的一点是，FastAPI很聪明，他知道参数来自哪里～假如，我们不给参数默认值会发生什么情况呢？这里还是一个例子：123456789from fastapi import FastAPIapp = FastAPI()@app.get(&quot;/items/&#123;item_id&#125;&quot;)async def read_user_item(item_id: str, needy: str): item = &#123;&quot;item_id&quot;: item_id, &quot;needy&quot;: needy&#125; return item继续请求http://127.0.0.1:8000/items/1，会发现，返回报错：123456789101112&#123; &quot;detail&quot;: [ &#123; &quot;loc&quot;: [ &quot;query&quot;, &quot;needy&quot; ], &quot;msg&quot;: &quot;field required&quot;, &quot;type&quot;: &quot;value_error.missing&quot; &#125; ]&#125;]]></content>
      <categories>
        <category>Python</category>
        <category>FastAPI</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>FastAPI</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Jenkins 使用 SonarQube 扫描 Coding]]></title>
    <url>%2F2020%2F01%2F14%2FJenkins%20%E4%BD%BF%E7%94%A8%20SonarQube%20%E6%89%AB%E6%8F%8F%20Coding%2F</url>
    <content type="text"><![CDATA[系统环境：Jenkins 版本：2.176SonarQube 版本：7.4.0一、SonarQube 介绍1、SonarQube 简介SonarQube 是一个用于代码质量管理的开源平台，用于管理源代码的质量。同时 SonarQube 还对大量的持续集成工具提供了接口支持，可以很方便地在持续集成中使用 SonarQube。此外， SonarQube 的插件还可以对 Java 以外的其他编程语言提供支持，对国际化以及报告文档化也有良好的支持。2、SonarQube工作原理SonarQube 并不是简单地将各种质量检测工具的结果直接展现给客户，而是通过不同的插件算法来对这些结果进行再加工，最终以量化的方式来衡量代码质量，从而方便地对不同规模和种类的工程进行相应的代码质量管理。3、SonarQube 特性多语言的平台： 支持超过20种编程语言，包括Java、Python、C#、C/C++、JavaScript等常用语言。 自定义规则： 用户可根据不同项目自定义Quality Profile以及Quality Gates。 丰富的插件： SonarQube 拥有丰富的插件，从而拥有强大的可扩展性。 持续集成： 通过对某项目的持续扫描，可以对该项目的代码质量做长期的把控，并且预防新增代码中的不严谨和冗余。 质量门： 在扫描代码后可以通过对“质量门”的比对判定此次“构建”的结果是否通过，质量门可以由用户定义，由多维度判定是否通过。 4、需要注意的代码质量问题(1)、不遵循代码标准： SonarQube可以通过PMD,CheckStyle,Findbugs等等代码规则检测工具规 范代码编写。 (2)、糟糕的复杂度分布： 文件、类、方法等，如果复杂度过高将难以改变，这会使得开发人员难以理解它们且如果没有自动化的单元测试，对于程序中的任何组件的改变都将可能导致需要全面的回归测试。 (3)、注释不足或者过多： 没有注释将使代码可读性变差，特别是当不可避免地出现人员变动 时，程序的可读性将大幅下降而过多的注释又会使得开发人员将精力过多地花费在阅读注释上，亦违背初衷。 (4)、缺乏单元测试： SonarQube 可以很方便地统计并展示单元测试覆盖率。 (5)、潜在的缺陷： –SonarQube 可以通过PMD,CheckStyle,Findbugs等等代码规则检测工具检 测出潜在的缺陷。 (6)、重复： 显然程序中包含大量复制粘贴的代码是质量低下的，SonarQube 源码中重复严重的地方。 (7)、糟糕的设计 二、一般执行流程在项目中一般流程为：(1)、项目人员开发代码。 (2)、将代码推送到持久化仓库，如 Git。 (3)、Jenkins 进行代码拉取，然后利用 SonarQube 扫描器进行扫描分析代码信息。 (4)、将分析结果等信息上传至 SonarQube Server 服务器进行分类处理。 (5)、SonarQube 将分析结果等信息持久化到数据库，如 Mysql。 (6)、开发人员访问 SonarQube UI 界面访问，查看扫描出的结果信息进行项目优化。 这里只描述 Jenkins 如何与 SonarQube 集成执行过程流程图 三、SonarQuke 配置1、禁用SCM传感器点击 配置—SCM—Disable the SCM Sensor 将其关闭。 2、安装 JAVA 分析插件由于这里要分析的项目是 JAVA 项目，所以需要确保安装 Java 语言分析插件，如果是别的类型的项目，可以类似安装相关分析插件即可。点击 配置—应用市场—插件 搜索 SonarJava 插件安装如果忘记安装，可能会导致 Jenkins 编译过程中提示没有语言插件的异常错误信息,确保一定要安装。3、生成 Token这里生成验证用的 Token 字符串，用于 Jenkins 在执行流水线时候将待检测信息发送到 SonarQube 时候用于的安全验证。点击 头像—我的账号—安全—生成令牌 生成验证的 Token。因为此 Token 不会显示第二次，所以这里记住此 Token。四、Jenkins 安装插件1、需要安装的插件介绍Jenkins 先提前安装好可能需要用到的插件，这里需要用到一下插件：Maven IntegrationMaven 插件，用于编译 Maven 项目和安装 Maven 工具到任务中。Pipeline Utility Steps参考：https://jenkins.io/doc/pipeline/steps/pipeline-utility-steps/用于在 Pipeline 执行过程中操作文件“读/写”的插件，这里用其创建 Sonar properties 配置文件。SonarQube Scanner参考：https://docs.sonarqube.org/display/SCAN/Analyzing+with+SonarQube+Scanner+for+JenkinsSonarQube 是一种用于连续检查代码质量的开源平台，该插件可轻松与 SonarQube 集成。2、安装 SonarQube Scanner 插件打开 系统管理—插件管理—可选插件 输入 sonarqube 进行插件筛选，如下如方式进行安装。关于安装 Pipeline Utility Steps 与 Maven Integration 插件和上面类似，请自行安装即可，这里不过多描述。五、Jenkins 配置插件1、连接 SonarQube 配置打开 系统管理—系统设置—SonarQube servers 配置下面属性参数说明：Name： 用于 Jenklins Pipeline 中构建环境指定的名称，在 Pipeline 脚本中会用到，自定义即可。Server URL： SonarQube 地址。Server authentication token： 用于连接 SonarQube 的 Token，将上面 SonarQube 中生成的 Token 输入即可。2、配置 SonarQube Scanner 插件打开 系统管理—全局工具配置—SonarQube Scanner 输入 Name，选择最新版本点击自动安装即可3、配置 Maven 插件打开 系统管理—全局工具配置—Maven 输入 Name，选择最新版本点击自动安装即可六、创建流水线项目写 Pipeline 脚本1、创建流水线任务2、设置 SonarQube 配置文件(1)、Sonar 配置文件说明在使用 SonarQube 来进行代码扫描时候需要一个名称为 sonar-project.properties 的配置文件。该文件设置了项目的一些属性用于 SonarQube 扫描的属性。例如，设置项目在 Sonar 面板中的唯一标识 Key，项目名称及其版本，要扫描项目的语言类型等等。sonar-project.properties123456789sonar.projectKey=key:valuesonar.projectName=ProjectNamesonar.projectVersion=1.0.0sonar.sources=srcsonar.language=javasonar.sourceEncoding=UTF-8sonar.java.binaries=target/classessonar.java.source=1.8sonar.java.target=1.8配置参数：sonar.projectKey： 项目在 SonarQube 的唯一标识，不能重复sonar.projectName=ProjectName： 项目名称sonar.projectVersion： 项目版本sonar.language： 项目语言，例如 Java、C#、PHP 等sonar.sourceEncoding： 编码方式sonar.sources： 项目源代码目录sonar.java.binaries： 编译后 class 文件目录(2)、Sonar 配置文件存放位置这个文件可以放在源代码根目录中，也可是设置到 Jenkins 变量。① ————————方式一：放置到源代码———————————————–直接在源代码中放置文件 sonar-project.properties，然后在此配置文件中设置这些配置参数。② ————————方式二：设置到变量并在 Jenkins 编译时候创建————————可以设置文本到环境变量中，在变量文本中设置哪些配置参数，之后在执行 Pipeline 脚本时候利用 “Pipeline Utility Steps” 插件的创建文件方法创建 sonar-project.properties 文件。在 Jenkins sonar-qube-coding 任务—&gt;配置—&gt;参数化构建过程—&gt;添加参数—&gt;文本参数 输入 Sonar 配置。变量名称：sonar_project_properties变量内容：123456sonar.sources=srcsonar.language=javasonar.sourceEncoding=UTF-8sonar.java.binaries=target/classessonar.java.source=1.8sonar.java.target=1.8注意:这里不设置 sonar.projectKey、sonar.projectName、sonar.projectVersion 这三个参数，将三个参数在执行 Pipeline 脚本的时候设置。这里为了配置更灵活方便，所以采用将 SonarQube 配置设置到环境变量3、创建 Pipeline 脚本配置 Jenkins 任务，创建脚本并加入到 “流水线” 配置项中1234567891011121314151617181920212223242526272829303132// 设置超时时间为10分钟，如果未成功则结束任务timeout(time: 600, unit: &apos;SECONDS&apos;) &#123; node () &#123; stage(&apos;Git 拉取阶段&apos;)&#123; // Git 拉取代码 git branch: &quot;master&quot; ,changelog: true , url: &quot;https://github.com/a324670547/springboot-helloworld&quot; &#125; stage(&apos;Maven 编译阶段&apos;) &#123; // 设置 Maven 工具,引用先前全局工具配置中设置工具的名称 def m3 = tool name: &apos;maven&apos; // 执行 Maven 命令 sh &quot;$&#123;m3&#125;/bin/mvn -B -e clean install -Dmaven.test.skip=true&quot; &#125; stage(&apos;SonarQube 扫描阶段&apos;)&#123; // 读取maven变量 pom = readMavenPom file: &quot;./pom.xml&quot; // 创建SonarQube配置文件 writeFile file: &apos;sonar-project.properties&apos;, text: &quot;&quot;&quot;sonar.projectKey=$&#123;pom.artifactId&#125;:$&#123;pom.version&#125;\n&quot;&quot;&quot;+ &quot;&quot;&quot;sonar.projectName=$&#123;pom.artifactId&#125;\n&quot;&quot;&quot;+ &quot;&quot;&quot;sonar.projectVersion=$&#123;pom.version&#125;\n&quot;&quot;&quot;+ &quot;&quot;&quot;$&#123;sonar_project_properties&#125;&quot;&quot;&quot; // 设置 SonarQube 代码扫描工具,引用先前全局工具配置中设置工具的名称 def sonarqubeScanner = tool name: &apos;sonar-scanner&apos; // 设置 SonarQube 环境,其中参数设置为之前系统设置中SonarQuke服务器配置的 Name withSonarQubeEnv(&apos;jenkins&apos;) &#123; // 执行代码扫描 sh &quot;$&#123;sonarqubeScanner&#125;/bin/sonar-scanner&quot; &#125; &#125; &#125;&#125;七、执行 Jenkins 任务1、执行 Jenkins Pipeline 任务点击 Build with Parameters 执行 Jenkins 任务2、查看任务执行日志查看日志信息为：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156......[Pipeline] &#123; (Git 拉取阶段)[Pipeline] echoGit 阶段 &gt; git rev-parse --is-inside-work-tree # timeout=10Fetching changes from the remote Git repository &gt; git config remote.origin.url https://github.com/a324670547/springboot-helloworld # timeout=10Fetching upstream changes from https://github.com/a324670547/springboot-helloworld &gt; git --version # timeout=10 &gt; git fetch --tags --progress https://github.com/a324670547/springboot-helloworld Commit message: &quot;修改jenkinsfile&quot; &gt; git rev-list --no-walk a34691106075d58bc99d9dcc06f5eadcc03ca759 # timeout=10[Pipeline] &#123; (Maven 编译阶段)[Pipeline] tool+ /var/jenkins_home/tools/hudson.tasks.Maven_MavenInstallation/maven/bin/mvn -B -e clean install -Dmaven.test.skip=true[INFO] Error stacktraces are turned on.[INFO] Scanning for projects...[INFO] [INFO] ------------------&lt; club.mydlq:springboot-helloworld &gt;------------------[INFO] Building springboot-helloworld 0.0.1[INFO] --------------------------------[ jar ]---------------------------------[INFO] [INFO] --- maven-clean-plugin:3.1.0:clean (default-clean) @ springboot-helloworld ---[INFO] Deleting /var/jenkins_home/workspace/sonar-qube-coding/target[INFO] [INFO] --- maven-resources-plugin:3.1.0:resources (default-resources) @ springboot-helloworld ---[INFO] Using &apos;UTF-8&apos; encoding to copy filtered resources.[INFO] Copying 1 resource[INFO] Copying 0 resource[INFO] [INFO] --- maven-compiler-plugin:3.8.0:compile (default-compile) @ springboot-helloworld ---[INFO] Changes detected - recompiling the module![INFO] Compiling 2 source files to /var/jenkins_home/workspace/sonar-qube-coding/target/classes[INFO] [INFO] --- maven-resources-plugin:3.1.0:testResources (default-testResources) @ springboot-helloworld ---[INFO] Not copying test resources[INFO] [INFO] --- maven-compiler-plugin:3.8.0:testCompile (default-testCompile) @ springboot-helloworld ---[INFO] Not compiling test sources[INFO] [INFO] --- maven-surefire-plugin:2.22.1:test (default-test) @ springboot-helloworld ---[INFO] Tests are skipped.[INFO] [INFO] --- maven-jar-plugin:3.1.1:jar (default-jar) @ springboot-helloworld ---[INFO] Building jar: /var/jenkins_home/workspace/sonar-qube-coding/target/springboot-helloworld-0.0.1.jar[INFO] [INFO] --- spring-boot-maven-plugin:2.1.4.RELEASE:repackage (repackage) @ springboot-helloworld ---[INFO] Replacing main artifact with repackaged archive[INFO] [INFO] --- maven-install-plugin:2.5.2:install (default-install) @ springboot-helloworld ---[INFO] Installing /var/jenkins_home/workspace/sonar-qube-coding/target/springboot-helloworld-0.0.1.jar to /root/.m2/repository/club/mydlq/springboot-helloworld/0.0.1/springboot-helloworld-0.0.1.jar[INFO] Installing /var/jenkins_home/workspace/sonar-qube-coding/pom.xml to /root/.m2/repository/club/mydlq/springboot-helloworld/0.0.1/springboot-helloworld-0.0.1.pom[INFO] ------------------------------------------------------------------------[INFO] BUILD SUCCESS[INFO] ------------------------------------------------------------------------[INFO] Total time: 3.695 s[INFO] Finished at: 2019-05-09T17:59:45Z[INFO] ------------------------------------------------------------------------[Pipeline] &#125;[Pipeline] &#123; (SonarQube 扫描阶段)[Pipeline] readMavenPom[Pipeline] writeFile[Pipeline] tool[Pipeline] withSonarQubeEnvInjecting SonarQube environment variables using the configuration: jenkins[Pipeline] &#123;[Pipeline] sh+ /var/jenkins_home/tools/hudson.plugins.sonar.SonarRunnerInstallation/sonar-scanner/bin/sonar-scannerINFO: Scanner configuration file: /var/jenkins_home/tools/hudson.plugins.sonar.SonarRunnerInstallation/sonar-scanner/conf/sonar-scanner.propertiesINFO: Project root configuration file: /var/jenkins_home/workspace/sonar-qube-coding/sonar-project.propertiesINFO: SonarQube Scanner 3.3.0.1492INFO: Java 1.8.0_212 Oracle Corporation (64-bit)INFO: Linux 3.10.0-957.1.3.el7.x86_64 amd64INFO: User cache: /root/.sonar/cacheINFO: SonarQube server 7.4.0INFO: Default locale: &quot;en&quot;, source code encoding: &quot;UTF-8&quot;INFO: Publish modeINFO: Load global settingsINFO: Load global settings (done) | time=89msINFO: Server id: D549D2A8-AWpYoogtP1ytl0VN9FsrINFO: User cache: /root/.sonar/cacheINFO: Load/download pluginsINFO: Load plugins indexINFO: Load plugins index (done) | time=31msINFO: Plugin [l10nzh] defines &apos;l10nen&apos; as base plugin. This metadata can be removed from manifest of l10n plugins since version 5.2.INFO: Load/download plugins (done) | time=38msINFO: Loaded core extensions: INFO: Process project propertiesINFO: Load project repositoriesINFO: Load project repositories (done) | time=11msINFO: Load quality profilesINFO: Load quality profiles (done) | time=32msINFO: Load active rulesINFO: Load active rules (done) | time=201msINFO: Load metrics repositoryINFO: Load metrics repository (done) | time=24msINFO: Project key: springboot-helloworld:0.0.1INFO: Project base dir: /var/jenkins_home/workspace/sonar-qube-codingINFO: ------------- Scan springboot-helloworldINFO: Base dir: /var/jenkins_home/workspace/sonar-qube-codingINFO: Working dir: /var/jenkins_home/workspace/sonar-qube-coding/.scannerworkINFO: Source paths: srcINFO: Source encoding: UTF-8, default locale: enINFO: Load server rulesINFO: Load server rules (done) | time=109msINFO: Language is forced to javaINFO: Index filesWARN: File &apos;/var/jenkins_home/workspace/sonar-qube-coding/src/main/resources/application.yaml&apos; is ignored because it doesn&apos;t belong to the forced language &apos;java&apos;INFO: 2 files indexedINFO: Quality profile for java: Sonar wayINFO: Sensor JavaSquidSensor [java]INFO: Configured Java source version (sonar.java.source): 8INFO: JavaClasspath initializationWARN: Bytecode of dependencies was not provided for analysis of source files, you might end up with less precise results. Bytecode can be provided using sonar.java.libraries property.INFO: JavaClasspath initialization (done) | time=8msINFO: JavaTestClasspath initializationINFO: JavaTestClasspath initialization (done) | time=0msINFO: Java Main Files AST scanINFO: 2 source files to be analyzedINFO: 2/2 source files have been analyzedINFO: Java Main Files AST scan (done) | time=609msINFO: Java Test Files AST scanINFO: 0 source files to be analyzedINFO: Java Test Files AST scan (done) | time=1msINFO: Sensor JavaSquidSensor [java] (done) | time=1334msINFO: Sensor SurefireSensor [java]INFO: 0/0 source files have been analyzedINFO: parsing [/var/jenkins_home/workspace/sonar-qube-coding/target/surefire-reports]INFO: Sensor SurefireSensor [java] (done) | time=55msINFO: Sensor JaCoCoSensor [java]INFO: Sensor JaCoCoSensor [java] (done) | time=2msINFO: Sensor JavaXmlSensor [java]INFO: Sensor JavaXmlSensor [java] (done) | time=0msINFO: Sensor Zero Coverage SensorINFO: Sensor Zero Coverage Sensor (done) | time=8msINFO: Sensor Java CPD Block IndexerINFO: Sensor Java CPD Block Indexer (done) | time=10msINFO: SCM Publisher is disabledINFO: 2 files had no CPD blocksINFO: Calculating CPD for 0 filesINFO: CPD calculation finishedINFO: Analysis report generated in 114ms, dir size=13 KBINFO: Analysis reports compressed in 39ms, zip size=6 KBINFO: Analysis report uploaded in 671msINFO: ANALYSIS SUCCESSFUL, you can browse http://10.2.5.143:9000/dashboard?id=springboot-helloworld%3A0.0.1INFO: Note that you will be able to access the updated dashboard once the server has processed the submitted analysis reportINFO: More about the report processing at http://10.2.5.143:9000/api/ce/task?id=AWqdwF0moB4s4osu4wHuINFO: Task total time: 3.412 sINFO: ------------------------------------------------------------------------INFO: EXECUTION SUCCESSINFO: ------------------------------------------------------------------------INFO: Total time: 4.469sINFO: Final Memory: 10M/158MINFO: ------------------------------------------------------------------------......Finished: SUCCESS八、SonarQube 查看代码扫描结果登录 SonarQube 平台，查看代码扫描结果转载地址：http://www.mydlq.club/article/11/]]></content>
      <categories>
        <category>Jenkins</category>
        <category>SonarQube</category>
      </categories>
      <tags>
        <tag>Jenkins</tag>
        <tag>SonarQube</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux yum安装PostgreSQL9.6]]></title>
    <url>%2F2020%2F01%2F13%2FLinux%20yum%E5%AE%89%E8%A3%85PostgreSQL9.6%2F</url>
    <content type="text"><![CDATA[PostgreSQL10版本的主从安装配置在 https://www.cnblogs.com/virtulreal/p/11675841.html一、下载安装1、创建PostgreSQL9.6的yum源文件1$ yum install https://download.postgresql.org/pub/repos/yum/9.6/redhat/rhel-7-x86_64/pgdg-centos96-9.6-3.noarch.rpm2、安装PostgreSQL客户端1$ yum install postgresql963、安装PostgreSQL服务端1$ yum install postgresql96-server4、安装PostgreSQL拓展包(可选)1$ yum install postgresql96-devel.x86_645、安装PostgreSQL的附加模块（可选）1$ yum install postgresql96-contrib.x86_64二、配置初始化初始化数据库1$ /usr/pgsql-9.6/bin/postgresql96-setup initdb启动postgresql服务，并设置为开机自动启动12$ systemctl enable postgresql-9.6$ systemctl start postgresql-9.6postgres用户初始配置安装完成后，操作系统会自动创建一个postgres用户用来管理数据库，为其初始化密码(输入命令后连输2次密码)：1$ passwd postgres数据库初始配置使用数据库自带的postgres用户登录数据库,并为其赋予密码123$ su - postgres$ psql -U postgresalter user postgres with password &apos;你的密码&apos;;配置远程连接可能在/var/lib/pgsql/9.6/data下，可以1、使用find / -name ‘pg_hba.conf’查找到pg_hba.conf，修改pg_hba.conf在最后添加允许访问IP段（全网段可访问）host all all 0.0.0.0/0 md52、使用find / -name ‘postgresql.conf’找到 postgresql.conf找到用户参数listen_address(取消掉注释),改成下面样式:1listen_address = &apos;*&apos;启用密码验证1#password_encryption = on 修改为 password_encryption = on3、重启数据库1$ systemctl restart postgresql-9.6备注:使用Navicat For PostgreSql来连接]]></content>
      <categories>
        <category>PostgreSQL</category>
      </categories>
      <tags>
        <tag>PostgreSQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[配置 Nginx 反向代理 WebSocket]]></title>
    <url>%2F2020%2F01%2F13%2F%E9%85%8D%E7%BD%AE%20Nginx%20%E5%8F%8D%E5%90%91%E4%BB%A3%E7%90%86%20WebSocket%2F</url>
    <content type="text"><![CDATA[用Nginx给网站做反向代理和负载均衡是广泛使用的一种Web服务器部署技术。不仅能够保证后端服务器的隐蔽性，还可以提高网站部署灵活性。今天我们来讲一下，如何用Nginx给WebSocket服务器实现反向代理和负载均衡。什么是反向代理和负载均衡反向代理(Reverse Proxy)方式是指以代理服务器来接受Internet上的连接请求，然后将请求转发给内部网络上的服务器。并将内部服务器上得到的结果返回给Internet上请求连接的客户端，此时代理服务器对外就表现为一个服务器。负载均衡(Load Balancing)建立在现有网络结构之上，它提供了一种廉价有效透明的方法扩展网络设备和服务器的带宽、增加吞吐量、加强网络数据处理能力、提高网络的灵活性和可用性。什么是WebSocketWebSocket协议相比较于HTTP协议成功握手后可以多次进行通讯，直到连接被关闭。但是WebSocket中的握手和HTTP中的握手兼容，它使用HTTP中的Upgrade协议头将连接从HTTP升级到WebSocket。这使得WebSocket程序可以更容易的使用现已存在的基础设施。WebSocket工作在HTTP的80和443端口并使用前缀ws://或者wss://进行协议标注，在建立连接时使用HTTP/1.1的101状态码进行协议切换，当前标准不支持两个客户端之间不借助HTTP直接建立Websocket连接。更多Websocket的介绍可参考「WebSocket教程」一文。创建基于Node的WebSocket服务Nginx在官方博客上给出了一个实践样例「Using Nginx as a Websocket Proxy」，我们以这个例子来演示WebSocket的交互过程。这个例子中将会使用到nodejs的一个WebSocket的ws模块。安装node.js和npmDebian/Ubuntu1$ apt-get install nodejs npmRHEL/CentOS1$ yum install nodejs npm创建nodejs软链在Ubuntu上创建一个名叫node软链。Centos默认为node，不用在单独创建了。12# 如果不创建，后面运行wscat时Ubuntu环境中会报错。$ ln -s /usr/bin/nodejs /usr/bin/node安装ws和wscat模块ws是nodejs的WebSocket实现，我们借助它来搭建简单的WebSocket Echo Server。wscat是一个可执行的WebSocket客户端，用来调试WebSocket服务是否正常。1$ npm install ws wscat如果访问官方仓库比较慢的话，可用淘宝提供的镜像服务。1$ npm --registry=https://registry.npm.taobao.org install ws wscat创建一个简单的服务端这个简单的服务端实现的是向客户端返回客户端发送的消息。123456789101112$ vim server.jsconsole.log(&quot;Server started&quot;);var Msg = &apos;&apos;;var WebSocketServer = require(&apos;ws&apos;).Server , wss = new WebSocketServer(&#123;port: 8010&#125;); wss.on(&apos;connection&apos;, function(ws) &#123; ws.on(&apos;message&apos;, function(message) &#123; console.log(&apos;Received from client: %s&apos;, message); ws.send(&apos;Server received from client: &apos; + message); &#125;); &#125;);运行这个简单的echo服务12$ node server.jsServer started验证服务端是否正常启动12$ netstat -tlunp|grep 8010tcp6 0 0 :::8010 :::* LISTEN 23864/nodejs使用wscat做为客户端测试wscat命令默认安装当前用户目录node_modules/wscat/目录，我这里的位置是/root/node_modules/wscat/bin/wscat。输入任意内容进行测试，得到相同返回则说明运行正常。123456789$ cd /root/node_modules/wscat/bin/$ ./wscat --connect ws://127.0.0.1:8010connected (press CTRL+C to quit)&gt; Hello&lt; Server received from client: Hello&gt; Welcome to www.hi-linux.com&lt; Server received from client: Welcome to www.hi-linux.com使用Nginx对WebSocket进行反向代理安装Nginx下载对应软件包Nginx从1.3.13版本就开始支持WebSocket了，并且可以为WebSocket应用程序做反向代理和负载均衡。这里Nginx选用1.9.2版本。12$ cd /root$ wget &apos;http://nginx.org/download/nginx-1.9.2.tar.gz&apos;编译安装Nginx12345$ apt-get install libreadline-dev libncurses5-dev libpcre3-dev libssl-dev perl make build-essential$ tar xzvf nginx-1.9.2.tar.gz$ cd nginx-1.9.2$ ./configure$ make &amp;&amp; make install配置Nginx修改Nginx主配置文件1234567891011121314151617181920212223242526272829303132333435363738$ vim /usr/local/nginx/conf/nginx.conf# 在http上下文中增加如下配置，确保Nginx能处理正常http请求。http &#123; map $http_upgrade $connection_upgrade &#123; default upgrade; &apos;&apos; close; &#125; upstream websocket &#123; #ip_hash; server localhost:8010; server localhost:8011; &#125;# 以下配置是在server上下文中添加，location指用于websocket连接的path。 server &#123; listen 80; server_name localhost; access_log /var/log/nginx/yourdomain.log; location / &#123; proxy_pass http://websocket; proxy_read_timeout 300s; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_http_version 1.1; proxy_set_header Upgrade $http_upgrade; proxy_set_header Connection $connection_upgrade; &#125; &#125;&#125;最重要的就是在反向代理的配置中增加了如下两行，其它的部分和普通的HTTP反向代理没有任何差别。12proxy_set_header Upgrade $http_upgrade;proxy_set_header Connection $connection_upgrade;这里面的关键部分在于HTTP的请求中多了如下头部：12Upgrade: websocketConnection: Upgrade这两个字段表示请求服务器升级协议为WebSocket。服务器处理完请求后，响应如下报文：1234# 状态码为101HTTP/1.1 101 Switching ProtocolsUpgrade: websocketConnection: upgrade告诉客户端已成功切换协议，升级为Websocket协议。握手成功之后，服务器端和客户端便角色对等，就像普通的Socket一样，能够双向通信。不再进行HTTP的交互，而是开始WebSocket的数据帧协议实现数据交换。这里使用map指令可以将变量组合成为新的变量，会根据客户端传来的连接中是否带有Upgrade头来决定是否给源站传递Connection头，这样做的方法比直接全部传递upgrade更加优雅。默认情况下，连接将会在无数据传输60秒后关闭，proxy_read_timeout参数可以延长这个时间或者源站通过定期发送ping帧以保持连接并确认连接是否还在使用。启动NginxNginx会默认安装到/usr/local/nginx目录下。12$ cd /usr/local/nginx/sbin$ ./nginx -c /usr/local/nginx/conf/nginx.conf如果你想以Systemd服务的方式更方便的管理Nginx，可参考「基于Upsync模块实现Nginx动态配置」 一文。测试通过Nginx访问WebSocket服务上面的配置会使NGINX监听80端口，并把接收到的任何请求传递给后端的WebSocket服务器。我们可以使用wscat作为客户端来测试一下：1234567$ cd /root/node_modules/wscat/bin/$ ./wscat --connect ws://192.168.2.210connected (press CTRL+C to quit)&gt; Hello Nginx&lt; Server received from client: Hello Nginx&gt; Welcome to www.hi-linux.com&lt; Server received from client: Welcome to www.hi-linux.com反向代理服务器在支持WebSocket时面临的挑战WebSocket是端对端的，所以当一个代理服务器从客户端拦截一个Upgrade请求，它需要去发送它自己的Upgrade请求到后端服务器，也包括合适的头。因为WebSocket是一个长连接，不像HTTP那样是典型的短连接，所以反向代理服务器需要允许连接保持着打开，而不是在它们看起来空闲时就将它们关闭。]]></content>
      <categories>
        <category>Nginx</category>
        <category>WebSocket</category>
      </categories>
      <tags>
        <tag>Nginx</tag>
        <tag>WebSocket</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[harbor helm仓库使用]]></title>
    <url>%2F2020%2F01%2F13%2Fharbor%20helm%E4%BB%93%E5%BA%93%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[harbor helm仓库使用官方文档地址：https://github.com/goharbor/harborMonocular 从1.0 开始专注于helm 的UI展示，对于部署以及维护已经去掉了，官方也提供了相关的说明以及推荐了几个可选的部署工具，从使用以及架构上来说kubeapps 就是Monocular + helm 操作的集合，比Monocular早期版本有好多提升安装下载离线安装包1wget https://github.com/goharbor/harbor/releases/download/v1.9.3/harbor-offline-installer-v1.9.3.tgz配置harbor主要是harbor.cfg文件目前主要配置hostname和port ,使用自己服务器的ip，修改默认端口号1234hostname: 192.168.75.100http: # port for http, default is 80. If https enabled, this port will redirect to https port port: 10000生成docker-compose file12345678910111213# 先安装docker-compose，地址：https://github.com/docker/compose/releases# 需要docker-compose(1.18.0+)版本curl -L https://github.com/docker/compose/releases/download/1.25.0/docker-compose-`uname -s`-`uname -m` -o /usr/local/bin/docker-composechmod +x /usr/local/bin/docker-compose# 查看docker-compose版本[root@ks-allinone harbor]# docker-compose versiondocker-compose version 1.25.0, build 0a186604docker-py version: 4.1.0CPython version: 3.7.4OpenSSL version: OpenSSL 1.1.0l 10 Sep 2019./install.sh --with-clair --with-chartmuseum使用地址：http://192.168.75.100:10000账号：admin默认密码：Harbor12345其他操作1234567891011121314151617181920212223242526272829303132# 安装helmcurl https://raw.githubusercontent.com/kubernetes/helm/master/scripts/get | bash# 安装push 插件helm init helm plugin install https://github.com/chartmuseum/helm-push# 查看安装的插件helm plugin listNAME VERSION DESCRIPTION push 0.7.1 Push chart package to ChartMuseum# 添加harbor helm 私服# 首先需要创建项目myrepo(当前设计的模式为public)# chartrepo是必备的,不可缺少，不然就会推送到默认的library上面去了helm repo add --username=admin --password=Harbor12345 myrepo http://192.168.75.100:10000/chartrepo/myrepo"myrepo" has been added to your repositories# or 添加特定仓库helm repo add --username=admin --password=Harbor12345 myrepo https://xx.xx.xx.xx/chartrepo/myproject# 创建demohelm create app Creating app# 推送到harbor,pushhelm push --username=admin --password=Harbor12345 app myrepo Pushing app-0.1.0.tgz to myrepo... Done.]]></content>
      <categories>
        <category>Harbor</category>
        <category>Helm</category>
      </categories>
      <tags>
        <tag>Harbor</tag>
        <tag>Helm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Dockfile文件解析]]></title>
    <url>%2F2020%2F01%2F13%2FDockfile%E6%96%87%E4%BB%B6%E8%A7%A3%E6%9E%90%2F</url>
    <content type="text"><![CDATA[1. Dockerfile内容基础知识每条保留字指令都必须为大写字母且后面要跟随至少一个参数指令按照从上到下，顺序执行#表示注释每条指令都会创建一个新的镜像层，并对镜像进行提交2. Docker执行Dockerfile的大致流程docker从基础镜像运行一个容器执行一条指令并对容器作出修改执行类似docker commit的操作提交一个新的镜像层docker再基于刚提交的镜像运行一个新容器执行dockerfile中的下一条指令直到所有指令都执行完成3. DockerFile体系结构(保留字指令)FROM：基础镜像，当前新镜像是基于哪个镜像的MAINTAINER：镜像维护者的姓名和邮箱地址RUN：容器构建时需要运行的命令EXPOSE：当前容器对外暴露出的端口WORKDIR：指定在创建容器后，终端默认登陆的进来工作目录，一个落脚点ENV：用来在构建镜像过程中设置环境变量 (ENV MY_PATH /usr/mytest)ADD：将宿主机目录下的文件拷贝进镜像且ADD命令会自动处理URL和解压tar压缩包COPY：类似ADD，拷贝文件和目录到镜像中。将从构建上下文目录中 &lt;源路径&gt; 的文件/目录复制到新的一层的镜像内的 &lt;目标路径&gt; 位置 (COPY src dest)(COPY [“src”, “dest”])VOLUME：容器数据卷，用于数据保存和持久化工作CMD：指定一个容器启动时要运行的命令。可以有多个 CMD 指令，但只有最后一个生效，CMD 会被 docker run 之后的参数替换ENTRYPOINT：指定一个容器启动时要运行的命令，ENTRYPOINT 的目的和 CMD 一样，都是在指定容器启动程序及参数ONBUILD：当构建一个被继承的Dockerfile时运行命令，父镜像在被子继承后父镜像的onbuild被触发4. 示例内容123456789101112131415161718192021222324FROM centosMAINTAINER zzyy&lt;zzyybs@126.com&gt;#把宿主机当前上下文的c.txt拷贝到容器/usr/local/路径下COPY c.txt /usr/local/cincontainer.txt#把java与tomcat添加到容器中ADD jdk-8u171-linux-x64.tar.gz /usr/local/ADD apache-tomcat-9.0.8.tar.gz /usr/local/#安装vim编辑器RUN yum -y install vim#设置工作访问时候的WORKDIR路径，登录落脚点ENV MYPATH /usr/localWORKDIR $MYPATH#配置java与tomcat环境变量ENV JAVA_HOME /usr/local/jdk1.8.0_171ENV CLASSPATH $JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jarENV CATALINA_HOME /usr/local/apache-tomcat-9.0.8ENV CATALINA_BASE /usr/local/apache-tomcat-9.0.8ENV PATH $PATH:$JAVA_HOME/bin:$CATALINA_HOME/lib:$CATALINA_HOME/bin#容器运行时监听的端口EXPOSE 8080#启动时运行tomcat# ENTRYPOINT [&quot;/usr/local/apache-tomcat-9.0.8/bin/startup.sh&quot; ]# CMD [&quot;/usr/local/apache-tomcat-9.0.8/bin/catalina.sh&quot;,&quot;run&quot;]CMD /usr/local/apache-tomcat-9.0.8/bin/startup.sh &amp;&amp; tail -F /usr/local/apache-tomcat-9.0.8/bin/logs/catalina.out]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Beats：如何使用Filebeat将MySQL日志发送到Elasticsearch]]></title>
    <url>%2F2020%2F01%2F13%2FBeats%EF%BC%9A%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8Filebeat%E5%B0%86MySQL%E6%97%A5%E5%BF%97%E5%8F%91%E9%80%81%E5%88%B0Elasticsearch%2F</url>
    <content type="text"><![CDATA[在今天的文章中，我们来详细地描述如果使用Filebeat把MySQL的日志信息传输到Elasticsearch中。为了说明问题的方便，我们的测试系统的配置是这样的：我有一台MacOS机器。在上面我安装了Elasticsearch及Kibana。在这个机器里，我同时安装了一个Ubuntu 18.04的虚拟机。在这个Ubunutu机器上，我安装了MySQL及Filebeat。它们的IP地址分别显示如上。针对你们自己的测试环境，你们的IP地址可能和我的不太一样。准备工作安装Elasticsearch如果大家还没安装好自己的Elastic Stack的话，那么请按照我之前的教程“如何在Linux，MacOS及Windows上进行安装Elasticsearch” 安装好自己的Elasticsearch。由于我们的Elastic Stack需要被另外一个Ubuntu VM来访问，我们需要对我们的Elasticsearch进行配置。首先使用一个编辑器打开在config目录下的elasticsearch.yml配置文件。我们需要修改network.host的IP地址。在你的Mac及Linux机器上，我们可以使用:$ ifconfig来查看到我们的机器的IP地址。针对我的情况，我的机器的IP地址是：192.168.0.100。我们也必须在elasticsearch.yml的最后加上discovery.type: single-node，表明我们是单个node。等修改完我们的IP地址后，我们保存elasticsearch.yml文件。然后重新运行我们的elasticsearch。我们可以在一个浏览器中输入刚才输入的IP地址并加上端口号9200。这样可以查看一下我们的elasticsearch是否已经正常运行了。安装Kibana我们可以按照“如何在Linux，MacOS及Windows上安装Elastic栈中的Kibana”中介绍的那样来安装我们的Kibana。由于我们的Elasticsearch的IP地址已经改变，所以我们必须修改我们的Kibana的配置文件。我们使用自己喜欢的编辑器打开在config目录下的kibana.yml文件，并找到server.host。把它的值修改为自己的电脑的IP地址。针对我的情况是：同时找到elasticsearch.hosts，并把自己的IP地址输入进去：保存我们的kibana.yml文件，并运行我们的Kibana。同时在浏览器的地址中输入自己的IP地址及5601端口：如果配置成功的话，我们就可以看到上面的画面。安装Ubuntu虚拟机这个不在我的这个教程之内。在网上我们可以找到许多的教程教我们如何安装Ubuntu虚拟机。在Ubuntu上安装MySQL我们可以按照链接https://vitux.com/how-to-install-and-configure-mysql-in-ubuntu-18-04-lts/来安装我们的MySQL。简单地说，安装步骤如下:如果尚未安装MySQL，则可以使用以下步骤安装和配置它。 您需要做的第一件事就是更新系统。sudo apt-get update然后像这样安装MySQL：sudo apt-get install mysql-server在安装过程中，系统将提示您设置root密码。 记下它，因为管理MySQL数据库将需要它。或者，你通过如下的方法来设置MySQL的密码：sudo mysql等进入到MySQL后，打入如下的指令来创建你的root用户的密码：1mysql&gt; ALTER USER &apos;root&apos;@&apos;localhost&apos; IDENTIFIED WITH mysql_native_password BY &apos;password&apos;;在上面的句子里，使用自己喜欢的密码来代替password。下一步是配置MySQL以写入常规查询日志文件和慢速查询日志文件，因为默认情况下会禁用这些配置。 要更改配置，您将需要编辑包含用户数据库设置的my.cnf文件。1sudo vi /etc/mysql/my.cnf常规查询和慢速查询的有效配置应如下所示：1234567[mysqld]general_log = 1general_log_file = /var/log/mysql/mysql.logslow_query_log = 1slow_query_log_file = /var/log/mysql/mysql-slow.loglong_query_time = 1log_queries_not_using_indexes = 1我们可以把上面的配置添加到我们的my.cnf文件当中去：请注意，在低于5.1.29的MySQL版本中，使用了变量log_slow_queries而不是slow_query_log。进行以下更改后，请确保重新启动MySQL：1sudo service mysql restart现在，您的MySQL已准备好编写慢速查询，这些查询将通过Filebeat传送到您的Elasticsearch集群中。我们可以检查一下我们的MySQL是否已经成功运行：systemctl status mysql.service等我们成功配置后我们的MySQL，我们可以开始对我们的MySQL进行一些操作，然后你可以在如下的目录中查看到相应的log文件：1ls /var/log/mysql/安装Filebeat在Ubuntu上安装Filebeat也是非常直接的。我们可以先打开我们的Kibana。点击上面的“Add log data”按钮:点击上面的“MySQL logs”按钮：选择上面的操作系统。针对我们的Ubuntu系统，它是一个DEB格式的安装文件。我们按照上面的要求一步一步地进行安装和修改。在修改filebeat.yml文件时，我们需要注意的三点：1）修改MySQL的log路径：2）填入Kibana的地址：3）填入Elasticsearch的地址及端口号：我们需要运行如下的命令来把相应的dashboard，pipeline及template信息上传到Elasticsearch和Kibana中。123sudo filebeat modules enable mysqlsudo filebeat setupsudo service filebeat start等我们启动我们的filebeat后，我们可以通过如下的命令来检查filebeat服务是否运行正常：1sudo systemctl status filebeatKibana查看我们可以打开Kibana，并在Kibana中查看由filebeat发送过来的MySQL的数据：在上面，我们可以看到MySQL的dashboard：至此，我们可以看到所有的关于MySQL的信息，这里包括以下queries及error logs等。上面我们显示了如何直接把MySQL的信息发送到Elasticsearch，并对数据进行分析。当然，我们也可以把数据发送到logstash来对数据进行处理，然后再发送到Elasticsearch中。我们的filebeat.yml文件的配置文件可以这么写：12345678filebeat.prospectors:- input_type: log paths: - /var/log/mysql/*.log document_type: syslog registry: /var/lib/filebeat/registryoutput.logstash: hosts: [&quot;mylogstashurl.example.com:5044&quot;]总结如本教程所示，Filebeat是用于MySQL数据库和Elasticsearch集群的出色日志传送解决方案。 与以前的版本相比，它非常轻巧，可以有效地发送日志事件。 Filebeat支持压缩，并且可以通过单个yaml文件轻松配置。 使用Filebeat，您可以轻松地管理日志文件，跟踪日志注册表，创建自定义字段以在日志中启用细化过滤和发现，以及使用Kibana可视化功能立即为日志数据供电。版权声明：本文为CSDN博主「Elastic 中国社区官方博客」的原创文章，遵循 CC 4.0 BY-SA 版权协议，转载请附上原文出处链接及本声明。原文链接：https://blog.csdn.net/UbuntuTouch/article/details/103954935]]></content>
      <categories>
        <category>elk</category>
      </categories>
      <tags>
        <tag>elk</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch中text与keyword的区别]]></title>
    <url>%2F2020%2F01%2F10%2FElasticsearch%E4%B8%ADtext%E4%B8%8Ekeyword%E7%9A%84%E5%8C%BA%E5%88%AB%2F</url>
    <content type="text"><![CDATA[ES更新到5版本后，取消了 string 数据类型，代替它的是 keyword 和 text 数据类型.那么 text 和keyword有什么区别呢？添加数据使用bulk往es数据库中批量添加一些document123456789POST /book/novel/_bulk&#123;&quot;index&quot;: &#123;&quot;_id&quot;: 1&#125;&#125;&#123;&quot;name&quot;: &quot;Gone with the Wind&quot;, &quot;author&quot;: &quot;Margaret Mitchell&quot;, &quot;date&quot;: &quot;2018-01-01&quot;&#125;&#123;&quot;index&quot;: &#123;&quot;_id&quot;: 2&#125;&#125;&#123;&quot;name&quot;: &quot;Robinson Crusoe&quot;, &quot;author&quot;: &quot;Daniel Defoe&quot;, &quot;date&quot;: &quot;2018-01-02&quot;&#125;&#123;&quot;index&quot;: &#123;&quot;_id&quot;: 3&#125;&#125;&#123;&quot;name&quot;: &quot;Pride and Prejudice&quot;, &quot;author&quot;: &quot;Jane Austen&quot;, &quot;date&quot;: &quot;2018-01-01&quot;&#125;&#123;&quot;index&quot;: &#123;&quot;_id&quot;: 4&#125;&#125;&#123;&quot;name&quot;: &quot;Jane Eyre&quot;, &quot;author&quot;: &quot;Charlotte Bronte&quot;, &quot;date&quot;: &quot;2018-01-02&quot;&#125;查看mapping发现name、author的type是text，还有个field是keyword，keyword的type是keyword：查询使用term查询某个小说：12345678910111213GET book/novel/_search&#123; &quot;query&quot;: &#123; &quot;constant_score&quot;: &#123; &quot;filter&quot;: &#123; &quot;term&quot;: &#123; &quot;name&quot;: &quot;Gone with the Wind&quot; &#125; &#125;, &quot;boost&quot;: 1.2 &#125; &#125;&#125;结果是什么也没有查到：然后使用name的keyword查询：12345678910111213GET book/novel/_search&#123; &quot;query&quot;: &#123; &quot;constant_score&quot;: &#123; &quot;filter&quot;: &#123; &quot;term&quot;: &#123; &quot;name.keyword&quot;: &quot;Gone with the Wind&quot; &#125; &#125;, &quot;boost&quot;: 1.2 &#125; &#125;&#125;可以查询到一条数据：实验使用name不能查到，而使用name.keyword可以查到，我们可以通过下面的实验来判断：使用name进行分词的时候，结果会有4个词出来：使用name.keyword进行分词的时候，结果只有一个词出来：结论text类型：会分词，先把对象进行分词处理，然后再再存入到es中。当使用多个单词进行查询的时候，当然查不到已经分词过的内容！keyword：不分词，没有把es中的对象进行分词处理，而是存入了整个对象！这时候当然可以进行完整地查询！默认是256个字符！作者：香山上的麻雀链接：https://www.jianshu.com/p/1189ff372c38来源：简书著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。]]></content>
      <categories>
        <category>elk</category>
      </categories>
      <tags>
        <tag>elk</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用 Logstash 和 JDBC 确保 Elasticsearch 与关系型数据库保持同步]]></title>
    <url>%2F2020%2F01%2F09%2F%E4%BD%BF%E7%94%A8%20Logstash%20%E5%92%8C%20JDBC%20%E7%A1%AE%E4%BF%9D%20Elasticsearch%20%E4%B8%8E%E5%85%B3%E7%B3%BB%E5%9E%8B%E6%95%B0%E6%8D%AE%E5%BA%93%E4%BF%9D%E6%8C%81%E5%90%8C%E6%AD%A5%2F</url>
    <content type="text"><![CDATA[为了充分利用 Elasticsearch 提供的强大搜索功能，很多公司都会在既有关系型数据库的基础上再部署 Elasticsearch。在这种情况下，很可能需要确保 Elasticsearch 与所关联关系型数据库中的数据保持同步。因此，在本篇博文中，我会演示如何使用 Logstash 来高效地复制数据并将关系型数据库中的更新同步到 Elasticsearch 中。本文中所列出的代码和方法已使用 MySQL 进行过测试，但理论上应该适用于任何关系数据库管理系统 (RDBMS)。系统配置在本篇文章中，我使用下列产品进行测试：MySQL：8.0.16Elasticsearch：7.1.1Logstash：7.1.1Java：1.8.0_162-b12JDBC 输入插件：v4.3.13JDBC 连接器：Connector/J 8.0.16同步步骤整体概览在本篇博文中，我们使用 Logstash 和 JDBC 输入插件来让 Elasticsearch 与 MySQL 保持同步。从概念上讲，Logstash 的 JDBC 输入插件会运行一个循环来定期对 MySQL 进行轮询，从而找出在此次循环的上次迭代后插入或更改的记录。如要让其正确运行，必须满足下列条件：在将 MySQL 中的文档写入 Elasticsearch 时，Elasticsearch 中的 “_id” 字段必须设置为 MySQL 中的 “id” 字段。这可在 MySQL 记录与 Elasticsearch 文档之间建立一个直接映射关系。如果在 MySQL 中更新了某条记录，那么将会在 Elasticsearch 中覆盖整条相关记录。请注意，在 Elasticsearch 中覆盖文档的效率与更新操作的效率一样高，因为从内部原理上来讲，更新便包括删除旧文档以及随后对全新文档进行索引。当在 MySQL 中插入或更新数据时，该条记录必须有一个包含更新或插入时间的字段。通过此字段，便可允许 Logstash 仅请求获得在轮询循环的上次迭代后编辑或插入的文档。Logstash 每次对 MySQL 进行轮询时，都会保存其从 MySQL 所读取最后一条记录的更新或插入时间。在下一次迭代时，Logstash 便知道其仅需请求获得符合下列条件的记录：更新或插入时间晚于在轮询循环中的上一次迭代中所收到的最后一条记录。如果满足上述条件，我们便可配置 Logstash，以定期请求从 MySQL 获得新增或已编辑的全部记录，然后将它们写入 Elasticsearch 中。完成这些操作的 Logstash 代码在本篇博文的后面会列出。MySQL 设置可以使用下列代码配置 MySQL 数据库和数据表：1234567891011CREATE DATABASE es_db;USE es_db;DROP TABLE IF EXISTS es_table;CREATE TABLE es_table ( id BIGINT(20) UNSIGNED NOT NULL, PRIMARY KEY (id), UNIQUE KEY unique_id (id), client_name VARCHAR(32) NOT NULL, modification_time TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP, insertion_time TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP);在上面的 MySQL 配置中，有几个参数需要特别注意：es_table：这是 MySQL 数据表的名称，数据会从这里读取出来并同步到 Elasticsearch。id：这是该条记录的唯一标识符。请注意 “id” 已被定义为 PRIMARY KEY（主键）和 UNIQUE KEY（唯一键）。这能确保每个 “id” 仅在当前表格中出现一次。其将会转换为 “_id”，以用于更新 Elasticsearch 中的文档及向 Elasticsearch 中插入文档。client_name：此字段表示在每条记录中所存储的用户定义数据。在本篇博文中，为简单起见，我们只有一个包含用户定义数据的字段，但您可以轻松添加更多字段。我们要更改的就是这个字段，从而向大家演示不仅新插入的 MySQL 记录被复制到了 Elasticsearch 中，而且更新的记录也被正确传播到了 Elasticsearch 中。modification_time：在 MySQL 中插入或更改任何记录时，都会将这个所定义字段的值设置为编辑时间。有了这个编辑时间，我们便能提取自从上次 Logstash 请求从 MySQL 获取记录后被编辑的任何记录。insertion_time：此字段主要用于演示目的，并非正确进行同步需满足的严格必要条件。我们用其来跟踪记录最初插入到 MySQL 中的时间。MySQL 操作完成上述配置后，可以通过下列语句向 MySQL 中写入记录：1INSERT INTO es_table (id, client_name) VALUES (&lt;id&gt;, &lt;client name&gt;);可以通过下列命令更新 MySQL 中的记录：1UPDATE es_table SET client_name = &lt;new client name&gt; WHERE id=&lt;id&gt;;可以通过下列语句完成 MySQL 更新/插入操作 (upsert)：1INSERT INTO es_table (id, client_name) VALUES (&lt;id&gt;, &lt;client name when created&gt; ON DUPLICATE KEY UPDATE client_name=&lt;client name when updated&gt;;同步代码下列 Logstash 管道会实施在前一部分中所描述的同步代码：1234567891011121314151617181920212223242526272829input &#123; jdbc &#123; jdbc_driver_library =&gt; &quot;&lt;path&gt;/mysql-connector-java-8.0.16.jar&quot; jdbc_driver_class =&gt; &quot;com.mysql.jdbc.Driver&quot; jdbc_connection_string =&gt; &quot;jdbc:mysql://&lt;MySQL host&gt;:3306/es_db&quot; jdbc_user =&gt; &lt;my username&gt; jdbc_password =&gt; &lt;my password&gt; jdbc_paging_enabled =&gt; true tracking_column =&gt; &quot;unix_ts_in_secs&quot; use_column_value =&gt; true tracking_column_type =&gt; &quot;numeric&quot; schedule =&gt; &quot;*/5 * * * * *&quot; statement =&gt; &quot;SELECT *, UNIX_TIMESTAMP(modification_time) AS unix_ts_in_secs FROM es_table WHERE (UNIX_TIMESTAMP(modification_time) &gt; :sql_last_value AND modification_time &lt; NOW()) ORDER BY modification_time ASC&quot; &#125;&#125;filter &#123; mutate &#123; copy =&gt; &#123; &quot;id&quot; =&gt; &quot;[@metadata][_id]&quot;&#125; remove_field =&gt; [&quot;id&quot;, &quot;@version&quot;, &quot;unix_ts_in_secs&quot;] &#125;&#125;output &#123; # stdout &#123; codec =&gt; &quot;rubydebug&quot;&#125; elasticsearch &#123; index =&gt; &quot;rdbms_sync_idx&quot; document_id =&gt; &quot;%&#123;[@metadata][_id]&#125;&quot; &#125;&#125;Read Less在上述管道中，应该重点强调几个区域：tracking_column：此字段会指定 “unix_ts_in_secs” 字段（用于跟踪 Logstash 从 MySQL 读取的最后一个文档，下面会进行描述），其存储在 .logstash_jdbc_last_run 中的磁盘上。该值将会用来确定 Logstash 在其轮询循环的下一次迭代中所请求文档的起始值。在 .logstash_jdbc_last_run 中所存储的值可以作为 “:sql_last_value” 通过 SELECT 语句进行访问。unix_ts_in_secs：这是一个由上述 SELECT 语句生成的字段，包含可作为标准 Unix 时间戳（自 Epoch 起秒数）的 “modification_time”。我们刚讨论的 “tracking column” 会引用该字段。Unix 时间戳用于跟踪进度，而非作为简单的时间戳；如将其作为简单时间戳，可能会导致错误，因为在 UMT 和本地时区之间正确地来回转换是一个十分复杂的过程。sql_last_value：这是一个内置参数，包括 Logstash 轮询循环中当前迭代的起始点，上面 JDBC 输入配置中的 SELECT 语句便会引用这一参数。该字段会设置为 “unix_ts_in_secs”（读取自 .logstash_jdbc_last_run）的最新值。在 Logstash 轮询循环内所执行的 MySQL 查询中，其会用作所返回文档的起点。通过在查询中加入这一变量，能够确保不会将之前传播到 Elasticsearch 的插入或更新内容重新发送到 Elasticsearch。schedule：其会使用 cron 语法来指定 Logstash 应当以什么频率对 MySQL 进行轮询以查找变更。这里所指定的 &quot;*/5 * * * * *&quot; 会告诉 Logstash 每 5 秒钟联系一次 MySQL。modification_time &lt; NOW()：SELECT 中的这一部分是一个较难解释的概念，我们会在下一部分详加解释。filter：在这一部分，我们只需简单地将 MySQL 记录中的 “id” 值复制到名为 “_id” 的元数据字段，因为我们之后输出时会引用这一字段，以确保写入 Elasticsearch 的每个文档都有正确的 “_id” 值。通过使用元数据字段，可以确保这一临时值不会导致创建新的字段。我们还从文档中删除了 “id”、“@version” 和 “unix_ts_in_secs” 字段，因为我们不希望将这些字段写入到 Elasticsearch 中。output：在这一部分，我们指定每个文档都应当写入 Elasticsearch，还需为其分配一个 “_id”（需从我们在筛选部分所创建的元数据字段提取出来）。还会有一个包含被注释掉代码的 rubydebug 输出，启用此输出后能够帮助您进行故障排查。SELECT 语句正确性分析在这一部分，我们会详加解释为什么在 SELECT 语句中添加 modification_time &lt; NOW() 至关重要。为帮助解释这一概念，我们首先给出几个反面例子，向您演示为什么两种最直观的方法行不通。然后会解释为什么添加 modification_time &lt; NOW() 能够克服那两种直观方法所导致的问题。直观方法应用情况：一在这一部分，我们会演示如果 WHERE 子句中不包括 modification_time &lt; NOW()，而仅仅指定 UNIX_TIMESTAMP(modification_time) &gt; :sql_last_value 的话，会发生什么情况。在这种情况下，SELECT 语句如下：1statement =&gt; &quot;SELECT *, UNIX_TIMESTAMP(modification_time) AS unix_ts_in_secs FROM es_table WHERE (UNIX_TIMESTAMP(modification_time) &gt; :sql_last_value) ORDER BY modification_time ASC&quot;乍看起来，上面的方法好像应可以正常运行，但是对于一些边缘情况，其可能会错过一些文档。举例说明，我们假设 MySQL 现在每秒插入两个文档，Logstash 每 5 秒执行一次 SELECT 语句。具体如下图所示，T0 到 T10 分别代表每一秒，MySQL 中的数据则以 R1 到 R22 表示。我们假定 Logstash 轮询循环的第一个迭代发生在 T5，其会读取文档 R1 到 R11，如蓝绿色的方框所示。在 sql_last_value 中存储的值现在是 T5，因为这是所读取最后一条记录 (R11) 的时间戳。我们还假设在 Logstash 从 MySQL 读取完文件后，另一个时间戳为 T5 的文档 R12 立即插入到了 MySQL 中。在上述 SELECT 语句的下一个迭代中，我们仅会提取时间晚于 T5 的文档（因为 WHERE (UNIX_TIMESTAMP(modification_time) &gt; :sql_last_value) 就是如此规定的），这也就意味着将会跳过记录 R12。您可以参看下面的图表，其中蓝绿色方框表示 Logstash 在当前迭代中读取的记录，灰色方框表示 Logstash 之前读取的记录。请注意，如果使用这种情况中的 SELECT 语句，记录 R12 永远不会写到 Elasticsearch 中。直观方法应用情况：二为了解决上面的问题，您可能决定更改 WHERE 子句为 greater than or equals（晚于或等于），具体如下：1statement =&gt; &quot;SELECT *, UNIX_TIMESTAMP(modification_time) AS unix_ts_in_secs FROM es_table WHERE (UNIX_TIMESTAMP(modification_time) &gt;= :sql_last_value) ORDER BY modification_time ASC&quot;然而，这种实施策略也并不理想。这种情况下的问题是：在最近一个时间间隔内从 MySQL 读取的最近文档会重复发送到 Elasticsearch。尽管这不会对结果的正确性造成任何影响，但的确做了无用功。和前一部分类似，在最初的 Logstash 轮询迭代后，下图显示了已经从 MySQL 读取了哪些文档。当执行后续的 Logstash 轮询迭代时，我们会将时间晚于或等于 T5 的文档全部提取出来。可以参见下面的图表。请注意：记录 11（紫色显示）会再次发送到 Elasticsearch。前面两种情况都不甚理想。在第一种情况中，会丢失数据，而在第二种情况中，会从 MySQL 读取冗余数据并将这些数据发送到 Elasticsearch。如何解决直观方法所带来的的问题鉴于前面两种情况都不太理想，应该采用另一种办法。通过指定 (UNIX_TIMESTAMP(modification_time) &gt; :sql_last_value AND modification_time &lt; NOW())，我们会将每个文档都发送到 Elasticsearch，而且只发送一次。请参见下面的图表，其中当前的 Logstash 轮询会在 T5 执行。请注意，由于必须满足 modification_time &lt; NOW()，所以只会从 MySQL 中读取截至（但不包括）时间段 T5 的文档。由于我们已经提取了 T4 的全部文档，而未读取 T5 的任何文档，所以我们知道对于下一次的Logstash 轮询迭代，sql_last_value 将会被设置为 T4。下图演示了在 Logstash 轮询的下一次迭代中将会发生什么情况。由于 UNIX_TIMESTAMP(modification_time) &gt; :sql_last_value，并且 sql_last_value 设置为 T4，我们知道仅会从 T5 开始提取文档。此外，由于只会提取满足 modification_time &lt; NOW() 的文档，所以仅会提取到截至（含）T9 的文档。再说一遍，这意味着 T9 中的所有文档都已提取出来，而且对于下一次迭代 sql_last_value 将会设置为 T9。所以这一方法消除了对于任何给定时间间隔仅检索到 MySQL 文档的一个子集的风险。测试系统可以通过一些简单测试来展示我们的实施方案能够实现预期效果。我们可以使用下列命令向 MySQL 中写入记录：123INSERT INTO es_table (id, client_name) VALUES (1, &apos;Jim Carrey&apos;);INSERT INTO es_table (id, client_name) VALUES (2, &apos;Mike Myers&apos;);INSERT INTO es_table (id, client_name) VALUES (3, &apos;Bryan Adams&apos;);JDBC 输入计划触发了从 MySQL 读取记录的操作并将记录写入 Elasticsearch 后，我们即可运行下列 Elasticsearch 查询来查看 Elasticsearch 中的文档：1GET rdbms_sync_idx/_search其会返回类似下面回复的内容：1234567891011121314151617181920&quot;hits&quot; : &#123; &quot;total&quot; : &#123; &quot;value&quot; :3, &quot;relation&quot; : &quot;eq&quot; &#125;, &quot;max_score&quot; :1.0, &quot;hits&quot; : [ &#123; &quot;_index&quot; : &quot;rdbms_sync_idx&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; :&quot;1&quot;, &quot;_score&quot; :1.0, &quot;_source&quot; : &#123; &quot;insertion_time&quot; :&quot;2019-06-18T12:58:56.000Z&quot;, &quot;@timestamp&quot; :&quot;2019-06-18T13:04:27.436Z&quot;, &quot;modification_time&quot; :&quot;2019-06-18T12:58:56.000Z&quot;, &quot;client_name&quot; :&quot;Jim Carrey&quot; &#125; &#125;,Etc …然后我们可以使用下列命令更新在 MySQL 中对应至 _id=1 的文档：1UPDATE es_table SET client_name = &apos;Jimbo Kerry&apos; WHERE id=1;其会正确更新 _id 被识别为 1 的文档。我们可以通过运行下列命令直接查看 Elasticsearch 中的文档：1GET rdbms_sync_idx/_doc/1其会返回一个类似下面的文档：123456789101112131415&#123; &quot;_index&quot; : &quot;rdbms_sync_idx&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; :&quot;1&quot;, &quot;_version&quot; :2, &quot;_seq_no&quot; :3, &quot;_primary_term&quot; :1, &quot;found&quot; : true, &quot;_source&quot; : &#123; &quot;insertion_time&quot; :&quot;2019-06-18T12:58:56.000Z&quot;, &quot;@timestamp&quot; :&quot;2019-06-18T13:09:30.300Z&quot;, &quot;modification_time&quot; :&quot;2019-06-18T13:09:28.000Z&quot;, &quot;client_name&quot; :&quot;Jimbo Kerry&quot; &#125;&#125;请注意 _version 现已设置为 2，modification_time 现在已不同于 insertion_time，并且 client_name 字段已正确更新至新值。在本例中，@timestamp 字段的用处并不大，由 Logstash 默认添加。MySQL 中的更新/插入 (upsert) 可通过下列命令完成，您可以验证正确信息是否会反映在 Elasticsearch 中：1INSERT INTO es_table (id, client_name) VALUES (4, &apos;Bob is new&apos;) ON DUPLICATE KEY UPDATE client_name=&apos;Bob exists already&apos;;那么删除文档呢？聪明的读者可能已经发现，如果从 MySQL 中删除一个文档，那么这一删除操作并不会传播到 Elasticsearch。可以考虑通过下列方法来解决这一问题：MySQL 记录可以包含一个 “is_deleted” 字段，用来显示该条记录是否仍有效。这一方法被称为“软删除”。正如对 MySQL 中的记录进行其他更新一样，”is_deleted” 字段将会通过 Logstash 传播至 Elasticsearch。如果实施这一方法，则需要编写 Elasticsearch 和 MySQL 查询，从而将 “is_deleted” 为 “true”（正）的记录/文档排除在外。 最后，可以通过后台作业来从 MySQL 和 Elastic 中移除此类文档。另一种方法是确保负责从 MySQL 中删除记录的任何系统随后也会执行一条命令，从而直接从 Elasticsearch 中删除相应文档。结论在本篇博文中，我演示了如何使用 Logstash 来将 Elasticsearch 与关系型数据库保持同步。在这里所列出的代码和方法已使用 MySQL 进行测试，但理论上应该适用于任何关系数据库管理系统 (RDBMS)。如果对 Logstash 或任何其他 Elasticsearch 相关主题有疑问，请在讨论论坛中查看各种宝贵的讨论、见解和信息。而且，不要忘记试用 Elasticsearch Service，这是由 Elasticsearch 开发公司提供支持的唯一一款托管型 Elasticsearch 和 Kibana 产品。]]></content>
      <categories>
        <category>elk</category>
      </categories>
      <tags>
        <tag>elk</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[logstash知识点]]></title>
    <url>%2F2020%2F01%2F09%2Flogstash%E7%9F%A5%E8%AF%86%E7%82%B9%2F</url>
    <content type="text"><![CDATA[Logstash是位于Data和Elasticsearch之间的一个中间件。Logstash是一个功能强大的工具，可与各种部署集成。 它提供了大量插件。 它从数据源实时地把数据进行采集，可帮助您解析，丰富，转换和缓冲来自各种来源的数据，并最终把数据传入到Elasticsearch之中。 如果您的数据需要Beats中没有的其他处理，则需要将Logstash添加到部署中。Logstash部署于ingest node之中。0.1 默认情况下，Logstash在管道（pipeline）阶段之间使用内存中有界队列（输入到过滤器和过滤器到输出）来缓冲事件。 如果Logstash不安全地终止，则存储在内存中的所有事件都将丢失。 为防止数据丢失，您可以使Logstash通过使用持久队列将正在进行的事件持久化到磁盘上。可以通过在logstash.yml文件中设置queue.type：persisted属性来启用持久队列，该文件位于LOGSTASH_HOME/config文件夹下。 logstash.yml是一个配置文件，其中包含与Logstash相关的设置。 默认情况下，文件存储在LOGSTASH_HOME/data/queue中。 您可以通过在logstash.yml中设置path.queue属性来覆盖它。在使用logstash之前,必须要先安装JAVA下载地址:https://artifacts.elastic.co/downloads/logstash/logstash-7.3.0.tar.gz (里面的版本号可以根据实际情况进行修改)运行最基本的Logstash管道12cd logstash-7.3.0bin/logstash -e &apos;input &#123; stdin &#123; &#125; &#125; output &#123; stdout &#123;&#125; &#125;&apos;创建logstash.conf文件来运行管道123456789101112# logstash.conf文件内容input &#123; stdin&#123; &#125;&#125; output &#123; stdout &#123; codec =&gt; rubydebug&#125;# 运行./bin/logstash -f logstash.conf (path_to_logstash_conf_file)提示：在运行Logstash时使用-r标志可让您在更改和保存配置后自动重新加载配置。 在测试新配置时，这将很有用，因为您可以对其进行修改，这样就不必在每次更改配置时都手动启动Logstash。获得所有的plugins1bin/logstash-plugin listinput读取csv文件1234567input &#123; file &#123; path =&gt; &quot;/Users/liuxg/data/cars.csv&quot; start_position =&gt; &quot;beginning&quot; sincedb_path =&gt; &quot;null&quot; &#125;在input中，定义了一个文件，它的path指向csv文件的位置。start_position指向beginning。如果对于一个实时的数据源来说，它通常是ending，这样表示它每次都是从最后拿到那个数据。sincedb_path通常指向一个文件。这个文件保存上次操作的位置。设置为/dev/null，表明不存储这个数据在Logstash中，按照顺序执行的处理方式被叫做一个pipeline。一个pipeline含有一个按照顺序执行的逻辑数据流。pipeline从input里获取数据，并传送给一个队列，并接着传入到一些worker去处理官方提供的lostash关于apache,nginx应用的日志处理样本，网站: https://github.com/elastic/examples/tree/master/Common%20Data%20Formats12345678910111213141516171819202122232425262728293031323334353637383940# apache_logstash.confinput &#123; stdin &#123; &#125; &#125;filter &#123; grok &#123; match =&gt; &#123; &quot;message&quot; =&gt; &apos;%&#123;IPORHOST:clientip&#125; %&#123;USER:ident&#125; %&#123;USER:auth&#125; \[%&#123;HTTPDATE:timestamp&#125;\] &quot;%&#123;WORD:verb&#125; %&#123;DATA:request&#125; HTTP/%&#123;NUMBER:httpversion&#125;&quot; %&#123;NUMBER:response:int&#125; (?:-|%&#123;NUMBER:bytes:int&#125;) %&#123;QS:referrer&#125; %&#123;QS:agent&#125;&apos; &#125; &#125; date &#123; match =&gt; [ &quot;timestamp&quot;, &quot;dd/MMM/YYYY:HH:mm:ss Z&quot; ] locale =&gt; en &#125; geoip &#123; source =&gt; &quot;clientip&quot; &#125; useragent &#123; source =&gt; &quot;agent&quot; target =&gt; &quot;useragent&quot; &#125;&#125;output &#123; stdout &#123; codec =&gt; dots &#123;&#125; &#125; elasticsearch &#123; index =&gt; &quot;apache_elastic_example&quot; template =&gt; &quot;./apache_template.json&quot; template_name =&gt; &quot;apache_elastic_example&quot; template_overwrite =&gt; true &#125;&#125;123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263# apache_template.json&#123; &quot;template&quot;: &quot;apache_elastic_example&quot;, &quot;settings&quot;: &#123; &quot;index.refresh_interval&quot;: &quot;5s&quot; &#125;, &quot;mappings&quot;: &#123; &quot;_default_&quot;: &#123; &quot;dynamic_templates&quot;: [ &#123; &quot;message_field&quot;: &#123; &quot;mapping&quot;: &#123; &quot;norms&quot;: false, &quot;type&quot;: &quot;text&quot; &#125;, &quot;match_mapping_type&quot;: &quot;string&quot;, &quot;match&quot;: &quot;message&quot; &#125; &#125;, &#123; &quot;string_fields&quot;: &#123; &quot;mapping&quot;: &#123; &quot;norms&quot;: false, &quot;type&quot;: &quot;text&quot;, &quot;fields&quot;: &#123; &quot;keyword&quot;: &#123; &quot;ignore_above&quot;: 256, &quot;type&quot;: &quot;keyword&quot; &#125; &#125; &#125;, &quot;match_mapping_type&quot;: &quot;string&quot;, &quot;match&quot;: &quot;*&quot; &#125; &#125; ], &quot;properties&quot;: &#123; &quot;geoip&quot;: &#123; &quot;dynamic&quot;: true, &quot;properties&quot;: &#123; &quot;location&quot;: &#123; &quot;type&quot;: &quot;geo_point&quot; &#125;, &quot;ip&quot;: &#123; &quot;type&quot;: &quot;ip&quot; &#125;, &quot;continent_code&quot;: &#123; &quot;type&quot;: &quot;keyword&quot; &#125;, &quot;country_name&quot;: &#123; &quot;type&quot;: &quot;keyword&quot; &#125; &#125;, &quot;type&quot;: &quot;object&quot; &#125;, &quot;@version&quot;: &#123; &quot;type&quot;: &quot;keyword&quot; &#125; &#125; &#125; &#125;&#125;12345678910111213141516171819202122232425262728293031323334353637383940# nginx_logstash.confinput &#123; stdin &#123; &#125;&#125;filter &#123; grok &#123; match =&gt; &#123; &quot;message&quot; =&gt; &apos;%&#123;IPORHOST:remote_ip&#125; - %&#123;DATA:user_name&#125; \[%&#123;HTTPDATE:time&#125;\] &quot;%&#123;WORD:request_action&#125; %&#123;DATA:request&#125; HTTP/%&#123;NUMBER:http_version&#125;&quot; %&#123;NUMBER:response&#125; %&#123;NUMBER:bytes&#125; &quot;%&#123;DATA:referrer&#125;&quot; &quot;%&#123;DATA:agent&#125;&quot;&apos; &#125; &#125; date &#123; match =&gt; [ &quot;time&quot;, &quot;dd/MMM/YYYY:HH:mm:ss Z&quot; ] locale =&gt; en &#125; geoip &#123; source =&gt; &quot;remote_ip&quot; target =&gt; &quot;geoip&quot; &#125; useragent &#123; source =&gt; &quot;agent&quot; target =&gt; &quot;user_agent&quot; &#125;&#125;output &#123;stdout &#123; codec =&gt; dots &#123;&#125; &#125; elasticsearch &#123; index =&gt; &quot;nginx_elastic_stack_example&quot; document_type =&gt; &quot;logs&quot; template =&gt; &quot;./nginx_template.json&quot; template_name =&gt; &quot;nginx_elastic_stack_example&quot; template_overwrite =&gt; true &#125;&#125;123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960# nginx_template.json&#123; &quot;template&quot;: &quot;nginx_elastic_stack_example&quot;, &quot;settings&quot;: &#123; &quot;index.refresh_interval&quot;: &quot;5s&quot; &#125;, &quot;mappings&quot;: &#123; &quot;_default_&quot;: &#123; &quot;dynamic_templates&quot;: [ &#123; &quot;message_field&quot;: &#123; &quot;mapping&quot;: &#123; &quot;index&quot;: &quot;analyzed&quot;, &quot;norms&quot;: false, &quot;type&quot;: &quot;string&quot; &#125;, &quot;match_mapping_type&quot;: &quot;string&quot;, &quot;match&quot;: &quot;message&quot; &#125; &#125;, &#123; &quot;string_fields&quot;: &#123; &quot;mapping&quot;: &#123; &quot;norms&quot;: false, &quot;type&quot;: &quot;text&quot;, &quot;fields&quot;: &#123; &quot;raw&quot;: &#123; &quot;type&quot;: &quot;keyword&quot; &#125; &#125; &#125;, &quot;match_mapping_type&quot;: &quot;string&quot;, &quot;match&quot;: &quot;*&quot; &#125; &#125; ], &quot;properties&quot;: &#123; &quot;geoip&quot;: &#123; &quot;dynamic&quot;: true, &quot;properties&quot;: &#123; &quot;location&quot;: &#123; &quot;type&quot;: &quot;geo_point&quot; &#125; &#125;, &quot;type&quot;: &quot;object&quot; &#125;, &quot;bytes&quot;: &#123; &quot;type&quot;: &quot;float&quot; &#125;, &quot;request&quot;: &#123; &quot;type&quot;: &quot;keyword&quot; &#125; &#125;, &quot;_all&quot;: &#123; &quot;enabled&quot;: true &#125; &#125; &#125;&#125;12345678910111213141516171819202122232425262728293031323334353637383940414243# nginx_json_logstash.confinput &#123; stdin &#123; codec =&gt; json &#125;&#125;filter &#123; date &#123; match =&gt; [&quot;time&quot;, &quot;dd/MMM/YYYY:HH:mm:ss Z&quot; ] locale =&gt; en &#125; geoip &#123; source =&gt; &quot;remote_ip&quot; target =&gt; &quot;geoip&quot; &#125; useragent &#123; source =&gt; &quot;agent&quot; target =&gt; &quot;user_agent&quot; &#125; grok &#123; match =&gt; [ &quot;request&quot; , &quot;%&#123;WORD:request_action&#125; %&#123;DATA:request1&#125; HTTP/%&#123;NUMBER:http_version&#125;&quot; ] &#125;&#125;output &#123; stdout &#123; codec =&gt; dots &#123;&#125; &#125; elasticsearch &#123; index =&gt; &quot;nginx_json_elastic_stack_example&quot; document_type =&gt; &quot;logs&quot; template =&gt; &quot;./nginx_json_template.json&quot; template_name =&gt; &quot;nginx_json_elastic_stack_example&quot; template_overwrite =&gt; true &#125;&#125;1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253# nginx_json_template.json&#123; &quot;index_patterns&quot;: &quot;nginx_json_elastic&quot;, &quot;settings&quot;: &#123; &quot;index.refresh_interval&quot;: &quot;5s&quot; &#125;, &quot;mappings&quot;: &#123; &quot;doc&quot;: &#123; &quot;dynamic_templates&quot;: [ &#123; &quot;message_field&quot;: &#123; &quot;mapping&quot;: &#123; &quot;norms&quot;: false, &quot;type&quot;: &quot;text&quot; &#125;, &quot;match_mapping_type&quot;: &quot;string&quot;, &quot;match&quot;: &quot;message&quot; &#125; &#125;, &#123; &quot;string_fields&quot;: &#123; &quot;mapping&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;norms&quot;: false, &quot;fields&quot;: &#123; &quot;keyword&quot;: &#123; &quot;type&quot;: &quot;keyword&quot;, &quot;ignore_above&quot;: 256 &#125; &#125; &#125;, &quot;match_mapping_type&quot;: &quot;string&quot;, &quot;match&quot;: &quot;*&quot; &#125; &#125; ], &quot;properties&quot;: &#123; &quot;geoip&quot;: &#123; &quot;dynamic&quot;: true, &quot;properties&quot;: &#123; &quot;location&quot;: &#123; &quot;type&quot;: &quot;geo_point&quot; &#125; &#125;, &quot;type&quot;: &quot;object&quot; &#125;, &quot;request&quot;: &#123; &quot;type&quot;: &quot;keyword&quot; &#125; &#125; &#125; &#125;&#125;处理多个input12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667# multi-input.confinput &#123; file &#123; path =&gt; &quot;/data/multi-input/apache.log&quot; start_position =&gt; &quot;beginning&quot; sincedb_path =&gt; &quot;/dev/null&quot; # ignore_older =&gt; 100000 type =&gt; &quot;apache&quot; &#125;&#125; input &#123; file &#123; path =&gt; &quot;/data/multi-input/apache-daily-access.log&quot; start_position =&gt; &quot;beginning&quot; sincedb_path =&gt; &quot;/dev/null&quot; type =&gt; &quot;daily&quot; &#125;&#125; filter &#123; grok &#123; match =&gt; &#123; &quot;message&quot; =&gt; &apos;%&#123;IPORHOST:clientip&#125; %&#123;USER:ident&#125; %&#123;USER:auth&#125; \[%&#123;HTTPDATE:timestamp&#125;\] &quot;%&#123;WORD:verb&#125; %&#123;DATA:request&#125; HTTP/%&#123;NUMBER:httpversion&#125;&quot; %&#123;NUMBER:response:int&#125; (?:-|%&#123;NUMBER:bytes:int&#125;) %&#123;QS:referrer&#125; %&#123;QS:agent&#125;&apos; &#125; &#125; if [type] == &quot;apache&quot; &#123; mutate &#123; add_tag =&gt; [&quot;apache&quot;] &#125; &#125; if [type] == &quot;daily&quot; &#123; mutate &#123; add_tag =&gt; [&quot;daily&quot;] &#125; &#125; &#125; output &#123; stdout &#123; codec =&gt; rubydebug &#125; if &quot;apache&quot; in [tags] &#123; elasticsearch &#123; index =&gt; &quot;apache_log&quot; template =&gt; &quot;/data/apache_template.json&quot; template_name =&gt; &quot;apache_elastic_example&quot; template_overwrite =&gt; true &#125; &#125; if &quot;daily&quot; in [tags] &#123; elasticsearch &#123; index =&gt; &quot;apache_daily&quot; template =&gt; &quot;/data/apache_template.json&quot; template_name =&gt; &quot;apache_elastic_example&quot; template_overwrite =&gt; true &#125; &#125; &#125;# 运行./bin/logstash -f multi-input.conf使用了两个input。它们分别对应不同的log文件。对于这两个input，使用了不同的type来表示：apache和daily。尽管它们的格式是一样的，它们共同使用同样的一个grok filter，但是还是想分别对它们进行处理。为此，添加了一个tag。也可以添加一个field来进行区别。在output的部分，根据在filter部分设置的tag来对它们输出到不同的index里。daily的事件最早被处理及输出,接着apache的数据才开始处理.处理多个配置文件(conf)一个pipeline含有一个逻辑的数据流，它从input接收数据，并把它们传入到队列里，经过worker的处理，最后输出到output。这个output可以是Elasticsearch或其它多个pipeline两个不同的conf配置文件12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364# apache.confinput &#123; file &#123; path =&gt; &quot;/data/multi-input/apache.log&quot; start_position =&gt; &quot;beginning&quot; sincedb_path =&gt; &quot;/dev/null&quot; # ignore_older =&gt; 100000 type =&gt; &quot;apache&quot; &#125;&#125; filter &#123; grok &#123; match =&gt; &#123; &quot;message&quot; =&gt; &apos;%&#123;IPORHOST:clientip&#125; %&#123;USER:ident&#125; %&#123;USER:auth&#125; \[%&#123;HTTPDATE:timestamp&#125;\] &quot;%&#123;WORD:verb&#125; %&#123;DATA:request&#125; HTTP/%&#123;NUMBER:httpversion&#125;&quot; %&#123;NUMBER:response:int&#125; (?:-|%&#123;NUMBER:bytes:int&#125;) %&#123;QS:referrer&#125; %&#123;QS:agent&#125;&apos; &#125; &#125;&#125; output &#123; stdout &#123; codec =&gt; rubydebug &#125; elasticsearch &#123; index =&gt; &quot;apache_log&quot; template =&gt; &quot;/data/apache_template.json&quot; template_name =&gt; &quot;apache_elastic_example&quot; template_overwrite =&gt; true &#125; &#125;# daily.confinput &#123; file &#123; path =&gt; &quot;/data/multi-pipeline/apache-daily-access.log&quot; start_position =&gt; &quot;beginning&quot; sincedb_path =&gt; &quot;/dev/null&quot; type =&gt; &quot;daily&quot; &#125;&#125; filter &#123; grok &#123; match =&gt; &#123; &quot;message&quot; =&gt; &apos;%&#123;IPORHOST:clientip&#125; %&#123;USER:ident&#125; %&#123;USER:auth&#125; \[%&#123;HTTPDATE:timestamp&#125;\] &quot;%&#123;WORD:verb&#125; %&#123;DATA:request&#125; HTTP/%&#123;NUMBER:httpversion&#125;&quot; %&#123;NUMBER:response:int&#125; (?:-|%&#123;NUMBER:bytes:int&#125;) %&#123;QS:referrer&#125; %&#123;QS:agent&#125;&apos; &#125; &#125;&#125; output &#123; stdout &#123; codec =&gt; rubydebug &#125; elasticsearch &#123; index =&gt; &quot;apache_daily&quot; template =&gt; &quot;/data/multi-pipeline/apache_template.json&quot; template_name =&gt; &quot;apache_elastic_example&quot; template_overwrite =&gt; true &#125; &#125;在logstash的安装目录下的config文件目录下,修改pipelines.yml文件.123456789# pipelines.yml- pipeline.id: daily pipeline.workers: 1 pipeline.batch.size: 1 path.config: &quot;/data/multi-pipeline/daily.conf&quot; - pipeline.id: apache queue.type: persisted path.config: &quot;/data/multi-pipeline/apache.conf&quot;启动,注意：不使用-f参数指定配置文件1/bin/logstash在终端中可以看到有两个piple在同时运行。一个pipeline修改位于Logstash安装目录下的config子目录里的pipleline.yml文件1234# pipelines.yml- pipeline.id: my_logs queue.type: persisted path.config: &quot;/data/multi-pipeline/*.conf&quot;这里把所有位于/data/multi-pipeline/下的所有的conf文件都放于一个pipeline里。启动,注意：不使用-f参数指定配置文件1/bin/logstash在终端中会看到两个同样的输出，这是因为把两个.conf文件放于一个pipleline里运行，那么有两个stdout的输出分别位于两个.conf文件了。apache_log里有20条数据，它包括两个log文件里所有的事件，这是因为它们都是一个pipleline。同样可以在apache_daily看到同样的20条数据。采用这种方式意味着会把两个不同的配置文件获取的日志输出到同一个索引中。合并数据的话可以使用这种方式。把MySQL数据导入到Elasticsearch中官方文档地址: https://www.elastic.co/guide/en/logstash/current/plugins-inputs-jdbc.html#plugins-inputs-jdbc-parametersMySQL安装,准备一些测试数据Logstash安装根据mysql的版本信息下载相应的JDBC connector驱动,下载网站: https://dev.mysql.com/downloads/connector/j/下载完这个Connector后，把这个connector存入到Logstash安装目录下的logstash-core/lib/jars/子目录中。最终地址是这样的：logstash-7.3.0/logstash-core/lib/jars/mysql-connector-java-8.0.17.jarLogstash 配置12345678910111213141516171819202122232425# sales.confinput &#123; jdbc &#123; jdbc_connection_string =&gt; &quot;jdbc:mysql://localhost:3306/data&quot; jdbc_user =&gt; &quot;root&quot; jdbc_password =&gt; &quot;YourMyQLPassword&quot; jdbc_validate_connection =&gt; true jdbc_driver_library =&gt; &quot;&quot; jdbc_driver_class =&gt; &quot;com.mysql.cj.jdbc.Driver&quot; parameters =&gt; &#123; &quot;Product_id&quot; =&gt; &quot;Product1&quot; &#125; statement =&gt; &quot;SELECT * FROM SalesJan2009 WHERE Product = :Product_id&quot; &#125; &#125; output &#123; stdout &#123; &#125; elasticsearch &#123; index =&gt; &quot;sales&quot; hosts =&gt; &quot;localhost:9200&quot; document_type =&gt; &quot;_doc&quot; &#125; &#125;替换jdbc_user和jdbc_password为自己的MySQL账号的用户名及密码。特别值得指出的是jdbc_driver_library按elastic的文档是可以放入JDBC驱动的路径及驱动名称。实践证明如果这个驱动不在JAVA的classpath里，是不能被正确地加载。正因为这样的原因，在上一步里把驱动mysql-connector-java-8.0.17.jar放入到Logstash的jar目录里，所以这里就直接填入空字符串。运行Logstash加载数据1./bin/logstash --debug -f sales.conf注意：在MySQL中删除数据的话则不会自动同步删除es中的数据，需要另作处理]]></content>
      <categories>
        <category>elk</category>
      </categories>
      <tags>
        <tag>elk</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[filebeat知识点]]></title>
    <url>%2F2020%2F01%2F09%2Ffilebeat%E7%9F%A5%E8%AF%86%E7%82%B9%2F</url>
    <content type="text"><![CDATA[在Filebeat的根目录下，有一个叫做filebeat.yml的文件。1234567filebeat.inputs:- type: log enabled: true paths: - ./sample.log output.logstash:这里需要注意的是之前有的文章里第一行写的是filebeat.prospectors。经过测试在新的版本里不再适用。通过如下的命令来运行filebeat:1./filebeat在默认的情况下，filebeat会自动寻找定义在filebeat.yml文件里的配置。如果配置文件是另外的名字，可以通过如下的命令来执行filebeat:1./filebeat -c YourYmlFile.ymlFilebeat的registry文件存储Filebeat用于跟踪上次读取位置的状态和位置信息。data/registry 针对 .tar.gz and .tgz 归档文件安装/var/lib/filebeat/registry 针对 DEB 及 RPM 安装包c:\ProgramData\filebeat\registry 针对 Windows zip 文件如果想重新运行一遍数据，可以直接到相应的目录下删除那个叫做registry的目录即可。针对.tar.gz的安装包来说，可以直接删除这个文件。那么重新运行上面的./filebeat命令即可。它将会重新把数据从头再进行处理一遍。这对于我调试来说是非常有用的。]]></content>
      <categories>
        <category>elk</category>
      </categories>
      <tags>
        <tag>elk</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[logstash中output{}的另类写法]]></title>
    <url>%2F2020%2F01%2F09%2Flogstash%E4%B8%ADoutput%7B%7D%E7%9A%84%E5%8F%A6%E7%B1%BB%E5%86%99%E6%B3%95%2F</url>
    <content type="text"><![CDATA[日志传输路径如下：filebeat-&gt;redis-&gt;logstash-&gt;es在filebeat配置文件中，收集日志的时候配置的有如下参数：12fields: log_source: messages表示的是会把log_source作为fields的二级字段若是配置如下，表示的是会把log_source作为顶级字段：123fields: log_source: messagesfields_under_root: true使用这个字段来作为区分不同应用日志的来源；在logstash中从redis读取后，output给es的时候，根据上述不同的字段来创建不同的应用日志索引。常见的写法是多使用if条件进行区分，如下所示：1234567891011121314151617if [fields][log_source] == &apos;test_custom&apos; &#123; elasticsearch &#123; hosts =&gt; [&quot;http://172.17.107.187:9203&quot;, &quot;http://172.17.107.187:9201&quot;,&quot;http://172.17.107.187:9202&quot;] index =&gt; &quot;filebeat_test_custom-%&#123;+YYYY.MM.dd&#125;&quot; user =&gt; &quot;elastic&quot; password =&gt; &quot;escluter123456&quot; &#125;&#125;if [fields][log_source] == &quot;test_user&quot; &#123; elasticsearch &#123; hosts =&gt; [&quot;http://172.17.107.187:9203&quot;,&quot;http://172.17.107.187:9201&quot;,&quot;http://172.17.107.187:9202&quot;] index =&gt; &quot;filebeat_test_user-%&#123;+YYYY.MM.dd&#125;&quot; user =&gt; &quot;elastic&quot; password =&gt; &quot;escluter123456&quot; &#125;&#125;这样写也能使用，但是考虑到假设这个区分字段比较多的话，那这得写多少个if条件呀，所以可以使用如下的用法：在创建索引的时候使用上这个区分用的字段，具体如下：123456elasticsearch &#123; hosts =&gt; [&quot;http://172.17.107.187:9203&quot;,&quot;http://172.17.107.187:9201&quot;,&quot;http://172.17.107.187:9202&quot;] index =&gt; &quot;filebeat_%&#123;[fields][log_source]&#125;-%&#123;+YYYY.MM.dd&#125;&quot; user =&gt; &quot;elastic&quot; password =&gt; &quot;escluter123456&quot;&#125;说明：%{[fields][log_source]}表示的是获取区分字段的值若是顶级字段则是这样的用法：%{[log_source]}]]></content>
      <categories>
        <category>elk</category>
      </categories>
      <tags>
        <tag>elk</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[在logstash中启动X-Pack Management功能后配置logstash的情况说明]]></title>
    <url>%2F2020%2F01%2F08%2F%E5%9C%A8logstash%E4%B8%AD%E5%90%AF%E5%8A%A8X-Pack%20Management%E5%8A%9F%E8%83%BD%E5%90%8E%E9%85%8D%E7%BD%AElogstash%E7%9A%84%E6%83%85%E5%86%B5%E8%AF%B4%E6%98%8E%2F</url>
    <content type="text"><![CDATA[开启X-Pack Management功能后，启动logstsh的时候就不用再配置logstash.conf文件了，启动的时候也不用再使用-f指定这个文件进行启动了一旦启动了logstash的集中管理，我们就可以直接启动logstash，而不用跟任何的参数Logstash集中管理，先启动logstash，然后再设置相关配置。(感觉这种方式比较节省内存)之前的是先进行相关配置，再启动的时候指定相关配置大致步骤如图：1.创建用户角色2.创建用户3.在logstash.yml文件里做相应的配置4.启动logstash,不用加任何参数5.在kibana web界面，找到logstash管道管理，创建管道管道id是在logstash.yml文件里设置的xpack.management.pipeline.id: [&quot;main&quot;, &quot;apache_logs&quot;,&quot;my_apache_logs&quot;]中的任意一个管道内容就是之前logstash.conf文件中的内容，主要是input{} out{}之类的最后点击创建并部署管道.首先我们来创建一个叫做logstash_writer的role:点击“Create role”来创建我们的role。首先让我们来创建一个具有logstash_user的用户账号：点击上面的“Create user”按钮来创建一个用户：点击“Create user”来创建一个叫做logstash_user的账号。它具有logstash_admin及logstash_system的权限。为了启动集中管理，我们必须在logstash.yml文件里做相应的配置：12345xpack.management.enabled: truexpack.management.pipeline.id: [&quot;main&quot;, &quot;apache_logs&quot;, &quot;my_apache_logs&quot;]xpack.management.elasticsearch.username: &quot;logstash_user&quot;xpack.management.elasticsearch.password: &quot;123456&quot;xpack.management.elasticsearch.hosts: [&quot;$&#123;ES_HOST&#125;&quot;]我们可以在链接https://www.elastic.co/guide/en/logstash/current/logstash-centralized-pipeline-management.html找到更多的描述。在这里，我们启动logstash的管理，同时也把我们刚才创建的logstash_user的账号填入进来，并同时取了一个叫做my_apache_logs的pipeline id。一旦启动了logstash的集中管理，我们就可以直接启动logstash，而不用跟任何的参数1sudo ./bin/logstash这样我们的logstash已经被成功运行起来了。我们接下来可以在Kibana中创建自己的pipeline。点击上面的“Create pipeline”按钮，我们可以看到如下的画面：接下来我们点击“Create and Deploy”按钮：这样我们的my_apache_logs就被创建好了，而且已经被成功执行了。我们可以在Kibana中创建一个叫apache_log的index pattern，然后打开Discover，你可以看到刚刚被Logstash导入的数据：]]></content>
      <categories>
        <category>elk</category>
      </categories>
      <tags>
        <tag>elk</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用shell脚本根据输入es版本的不同自动生成白金版破解jar包文件]]></title>
    <url>%2F2020%2F01%2F08%2F%E4%BD%BF%E7%94%A8shell%E8%84%9A%E6%9C%AC%E6%A0%B9%E6%8D%AE%E8%BE%93%E5%85%A5es%E7%89%88%E6%9C%AC%E7%9A%84%E4%B8%8D%E5%90%8C%E8%87%AA%E5%8A%A8%E7%94%9F%E6%88%90%E7%99%BD%E9%87%91%E7%89%88%E7%A0%B4%E8%A7%A3jar%E5%8C%85%E6%96%87%E4%BB%B6%2F</url>
    <content type="text"><![CDATA[说明：使用的是7.3.0版本进行演示的注意1：采用这种方式的话不用再上传license文件，切记切记注意2：这种方式适用于如下两种形式– 1. 首次配置es,未开启security,也未设置账号密码– 2. 已配置es,开启了security,也设置了账号密码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081#!/bin/bashecho &quot;创建工作目录&quot;/usr/bin/mkdir -p /opt/workBASE_HOME=/opt/workecho &quot;切换到/opt/work目录下&quot;/usr/bin/cd $&#123;BASE_HOME&#125;echo &quot;根据输入版本不同创建相应的目录,例如：7.3.0&quot;read typeecho &quot;您输入的版本是$&#123;type&#125;&quot;TYPE_HOME=/opt/work/$&#123;type&#125;TAG=&quot;v$&#123;type&#125;&quot;/usr/bin/mkdir -p $&#123;TYPE_HOME&#125;/&#123;build/src,install,src&#125;echo &quot;安装wget,git&quot;yum -y install wget gitecho &quot;下载es $&#123;type&#125;安装包&quot;wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-$&#123;type&#125;-linux-x86_64.tar.gz -P $&#123;TYPE_HOME&#125;/installecho &quot;解压安装包到指定目录&quot;tar -zxv -f $&#123;TYPE_HOME&#125;/install/elasticsearch-$&#123;type&#125;-linux-x86_64.tar.gz -C $&#123;TYPE_HOME&#125;/installecho &quot;下载指定版本的es源码,使用国内码云地址&quot;echo &quot;官方github地址：https://github.com/elastic/elasticsearch&quot;echo &quot;国内码云地址：https://gitee.com/mirrors/elasticsearch&quot;cd $&#123;TYPE_HOME&#125;/srcgit clone -b $&#123;TAG&#125; https://gitee.com/mirrors/elasticsearch cd $&#123;TYPE_HOME&#125;/build# lib moduleln -s ../install/elasticsearch-$&#123;type&#125;/lib .ln -s ../install/elasticsearch-$&#123;type&#125;/modules .# License.javafind ../src -name &quot;License.java&quot; | xargs -r -I &#123;&#125; cp &#123;&#125; .sed -i &apos;s#this.type = type;#this.type = &quot;platinum&quot;;#g&apos; License.javased -i &apos;s#validate();#// validate();#g&apos; License.java# 编译javac -cp &quot;`ls lib/elasticsearch-$&#123;type&#125;.jar`:`ls lib/elasticsearch-x-content-$&#123;type&#125;.jar`:`ls lib/lucene-core-*.jar`:`ls modules/x-pack-core/x-pack-core-$&#123;type&#125;.jar`&quot; License.java# x-pack-core-7.5.0.jarcd $&#123;TYPE_HOME&#125;/build/srcfind ../../install -name &quot;x-pack-core-$&#123;type&#125;.jar&quot; | xargs -r -I &#123;&#125; cp &#123;&#125; .jar xvf x-pack-core-$&#123;type&#125;.jarrm -f x-pack-core-$&#123;type&#125;.jar\cp -f ../License*.class org/elasticsearch/license/jar cvf x-pack-core-$&#123;type&#125;.jar .echo &quot;切换到存放破解jar包文件的路径下&quot;cd $&#123;TYPE_HOME&#125;/build/srcecho &quot;如下步骤需要手动操作&quot;echo &quot;覆盖原有的x-pack-core-$&#123;type&#125;.jar文件&quot;# 配置elasticsearch.yml# xpack.security.enabled: true# xpack.security.transport.ssl.enabled: trueecho &quot;配置重启elasticsearch&quot;echo &quot;初始化elasticsearch密码&quot;echo &quot;bin/elasticsearch-setup-passwords auto&quot;echo &quot;配置重启kibana&quot;# elasticsearch.username: kibana# elasticsearch.password: password替换jar包后，es配置文件中开启security，然后启动es，然后给es设置账号和密码，然后修改kibana中的配置，添加上访问es使用的账号和密码，然后浏览器访问kibana web界面查看;命令行查看]]></content>
      <categories>
        <category>elk</category>
      </categories>
      <tags>
        <tag>elk</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ES配置生成SSL使用的证书]]></title>
    <url>%2F2020%2F01%2F08%2FES%E9%85%8D%E7%BD%AE%E7%94%9F%E6%88%90SSL%E4%BD%BF%E7%94%A8%E7%9A%84%E8%AF%81%E4%B9%A6%2F</url>
    <content type="text"><![CDATA[1234567891011cd /usr/local/elasticsearch/bin/./elasticsearch-certgen ##################################### Please enter the desired output file [certificate-bundle.zip]: cert.zip （生成的压缩包名称，输入或者保持默认，直接回车） Enter instance name: my-application (实例名) Enter name for directories and files [my-application]: elasticsearch（存储实例证书的文件夹名，可以随意指定或保持默认） Enter IP Addresses for instance (comma-separated if more than one) []: 127.0.0.1(实例ip，多个ip用逗号隔开) Enter DNS names for instance (comma-separated if more than one) []: node-1（节点名，多个节点用逗号隔开） Would you like to specify another instance? Press &apos;y&apos; to continue entering instance information: (到达这一步,不需要按y重新设置,按空格键就完成了) Certificates written to /usr/local/elasticsearch/bin/cert.zip（这个是生成的文件存放地址，不用填写）解压cert.zip文件会得到123456 creating: ca/inflating: ca/ca.crt inflating: ca/ca.key creating: my-applicaiton/inflating: my-applicaiton/my-applicaiton.crt inflating: my-applicaiton/my-applicaiton.keyes配置文件中使用如下：1234xpack.security.transport.ssl.enabled: truexpack.ssl.key: my-applicaiton.keyxpack.ssl.certificate: my-applicaiton.crtxpack.ssl.certificate_authorities: ca.crt]]></content>
      <categories>
        <category>elk</category>
      </categories>
      <tags>
        <tag>elk</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[x-pack设置完毕后，head无法登陆的问题]]></title>
    <url>%2F2020%2F01%2F08%2Fx-pack%E8%AE%BE%E7%BD%AE%E5%AE%8C%E6%AF%95%E5%90%8E%EF%BC%8Chead%E6%97%A0%E6%B3%95%E7%99%BB%E9%99%86%E7%9A%84%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[在elasticsearch.yml中添加如下三行配置123http.cors.enabled: truehttp.cors.allow-origin: &quot;*&quot;http.cors.allow-headers: Authorization,X-Requested-With,Content-Length,Content-Type重启服务，并通过如下形式访问head端口http://192.168.36.61:9100/?auth_user=elastic&amp;auth_password=passwd]]></content>
      <categories>
        <category>elk</category>
      </categories>
      <tags>
        <tag>elk</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch 7.3.0版本破解]]></title>
    <url>%2F2020%2F01%2F07%2FElasticsearch7.3.0%E7%89%88%E6%9C%AC%E7%A0%B4%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[elasticsearch操作文件：LicenseVerifier.java路径：elasticsearch/x-pack/plugin/core/src/main/java/org/elasticsearch/license/文件：XPackBuild.java路径：elasticsearch/x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core1. ES安装前进行破解软件名称：elasticsearch-7.3.0-linux-x86_64.tar.gz操作环境：系统：windows 10系统解压缩软件：7z反编译工具:Luyten操作步骤：解压缩文件elasticsearch-7.3.0-linux-x86_64.tar.gz,得到elasticsearch-7.3.0-linux-x86_64.tar文件夹,进入该文件夹,解压缩elasticsearch-7.3.0-linux-x86_64.tar,得到目录elasticsearch-7.3.0-linux-x86_64,再进入该文件夹,里面有一个elasticsearch-7.3.0文件夹,进入.这些操作在linux系统下操作的话就很简单：tar -zxv -f elasticsearch-7.3.0-linux-x86_64.tar.gz 直接得到文件夹elasticsearch-7.3.0进入modules\x-pack-core文件夹内,找到x-pack-core-7.3.0.jar文件,这个就是要操作的文件2. 下载反编译工具Luyten破解x-pack-core-7.3.0.jar需要反编译工具Luyten(https://github.com/deathmarine/Luyten/releases),我们可以前往下载地址下载Luyten工具。我们这里下载Luyten.exe windows版本，下载下来后打开，并将x-pack-core-7.3.0.jar文件拖进去，即可展开jar包的源代码了。3. 修改X-Pack源码文件在Luyten工具中我们需要把2个文件提取出来进行修改。org.elasticsearch.license.LicenseVerifier和org.elasticsearch.xpack.core.XPackBuild。导出LicenseVerifier.class文件为LicenseVerifier.java源码文件,导出XPackBuild.class文件为XPackBuild.java源码文件导出步骤1. 修改LicenseVerifier.javaLicenseVerifier中有两个静态方法，这就是验证授权文件是否有效的方法，把它修改为全部返回true.修改后的文档如下：123456789101112131415161718192021package org.elasticsearch.license;import java.nio.*;import org.elasticsearch.common.bytes.*;import java.security.*;import java.util.*;import org.elasticsearch.common.xcontent.*;import org.apache.lucene.util.*;import org.elasticsearch.core.internal.io.*;import java.io.*;public class LicenseVerifier&#123; public static boolean verifyLicense(final License license, final byte[] publicKeyData) &#123; return true; &#125; public static boolean verifyLicense(final License license) &#123; return true; &#125;&#125;2. 修改XPackBuild.javaXPackBuild中最后一个静态代码块中try的部分全部删除，这部分会验证jar包是否被修改.修改后的文档如下：12345678910111213141516171819202122232425262728293031323334353637383940414243package org.elasticsearch.xpack.core;import org.elasticsearch.common.io.*;import java.net.*;import org.elasticsearch.common.*;import java.nio.file.*;import java.io.*;import java.util.jar.*;public class XPackBuild&#123; public static final XPackBuild CURRENT; private String shortHash; private String date; @SuppressForbidden(reason = &quot;looks up path of xpack.jar directly&quot;) static Path getElasticsearchCodebase() &#123; final URL url = XPackBuild.class.getProtectionDomain().getCodeSource().getLocation(); try &#123; return PathUtils.get(url.toURI()); &#125; catch (URISyntaxException bogus) &#123; throw new RuntimeException(bogus); &#125; &#125; XPackBuild(final String shortHash, final String date) &#123; this.shortHash = shortHash; this.date = date; &#125; public String shortHash() &#123; return this.shortHash; &#125; public String date() &#123; return this.date; &#125; static &#123; CURRENT = new XPackBuild(&quot;Unknown&quot;, &quot;Unknown&quot;); &#125;&#125;或者这样的：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950package org.elasticsearch.xpack.core;import org.elasticsearch.common.io.*;import java.net.*;import org.elasticsearch.common.*;import java.nio.file.*;import java.io.*;import java.util.jar.*;public class XPackBuild&#123; public static final XPackBuild CURRENT; private String shortHash; private String date; @SuppressForbidden(reason = &quot;looks up path of xpack.jar directly&quot;) static Path getElasticsearchCodebase() &#123; final URL url = XPackBuild.class.getProtectionDomain().getCodeSource().getLocation(); try &#123; return PathUtils.get(url.toURI()); &#125; catch (URISyntaxException bogus) &#123; throw new RuntimeException(bogus); &#125; &#125; XPackBuild(final String shortHash, final String date) &#123; this.shortHash = shortHash; this.date = date; &#125; public String shortHash() &#123; return this.shortHash; &#125; public String date() &#123; return this.date; &#125; static &#123; final Path path = getElasticsearchCodebase(); String shortHash = null; String date = null; Label_0109: &#123; shortHash = &quot;Unknown&quot;; date = &quot;Unknown&quot;; &#125; CURRENT = new XPackBuild(shortHash, date); &#125;&#125;3. 生成.class文件上述LicenseVerifier.java和XPackBuild.java两个文件在本地电脑windows修改完成后，需要将其复制到elasticsearch服务器上并编译成class文件，然后打包到x-pack-core-7.3.0.jar中。这里将这2个文件放到了/opt目录下。123456789# 生成LicenseVerifier.class文件javac -cp &quot;/opt/elasticsearch-7.3.0/lib/elasticsearch-7.3.0.jar:/opt/elasticsearch-7.3.0/lib/lucene-core-8.1.0.jar:/opt/elasticsearch-7.3.0/modules/x-pack-core/x-pack-core-7.3.0.jar:/opt/elasticsearch-7.3.0/modules/x-pack-core/netty-common-4.1.36.Final.jar:/opt/elasticsearch-7.3.0/lib/elasticsearch-core-7.3.0.jar&quot; /opt/LicenseVerifier.java# 生成XPackBuild.class文件javac -cp &quot;/opt/elasticsearch-7.3.0/lib/elasticsearch-7.3.0.jar:/opt/elasticsearch-7.3.0/lib/lucene-core-8.1.0.jar:/opt/elasticsearch-7.3.0/modules/x-pack-core/x-pack-core-7.3.0.jar:/opt/elasticsearch-7.3.0/lib/elasticsearch-core-7.3.0.jar&quot; /opt/XPackBuild.java# 查看编译后的文件LicenseVerifier.classXPackBuild.class4. 替换LicenseVerifier.class和XPackBuild.class把/opt/elasticsearch-7.3.0/modules/x-pack-core/目录下的x-pack-core-7.3.0.jar提取出来，放到/opt/tmp目录中。12345678cp /opt/elasticsearch-7.3.0/modules/x-pack-core/x-pack-core-7.3.0.jar /opt/tmpcd /opt/tmp# 解压x-pack-core-7.3.0.jarjar -xvf x-pack-core-7.3.0.jar# 替换.class文件cp /opt/XPackBuild.class /opt/tmp/org/elasticsearch/xpack/core/cp /opt/LicenseVerifier.class /opt/tmp/org/elasticsearch/license/5. 打包新x-pack-core-7.3.0.jar文件123cd /opt/tmprm -rf x-pack-core-7.3.0.jar # 删除临时拷贝过来的源文件jar cvf x-pack-core-7.3.0.jar .至此在/opt/tmp目录下会新生成一个x-pack-core-7.3.0.jar文件,也就是破解后的文件。6. 替换x-pack-core-7.3.0.jar文件1cp /opt/tmp/x-pack-core-7.3.0.jar /opt/elasticsearch-7.3.0/modules/x-pack-core/7. 申请License完成以上步骤后，还需要去elastic官网申请一个license, License申请地址，申请完成后，下载下来的License格式为json格式。并将该License的type、expiry_date_in_millis、max_nodes分别修改成platinum、2524579200999、1000。如下：12345678910111213&#123;"license": &#123; "uid":"537c5c48-c1dd-43ea-ab69-68d209d80c32", "type":"platinum", "issue_date_in_millis":1558051200000, "expiry_date_in_millis":2524579200999, "max_nodes":1000, "issued_to":"hkd", "issuer":"Web Form", "signature":"AAAAAwAAAA3fIq7NLN3Blk2olVjbAAABmC9ZN0hjZDBGYnVyRXpCOW5Bb3FjZDAxOWpSbTVoMVZwUzRxVk1PSmkxaktJRVl5MUYvUWh3bHZVUTllbXNPbzBUemtnbWpBbmlWRmRZb25KNFlBR2x0TXc2K2p1Y1VtMG1UQU9TRGZVSGRwaEJGUjE3bXd3LzRqZ05iLzRteWFNekdxRGpIYlFwYkJiNUs0U1hTVlJKNVlXekMrSlVUdFIvV0FNeWdOYnlESDc3MWhlY3hSQmdKSjJ2ZTcvYlBFOHhPQlV3ZHdDQ0tHcG5uOElCaDJ4K1hob29xSG85N0kvTWV3THhlQk9NL01VMFRjNDZpZEVXeUtUMXIyMlIveFpJUkk2WUdveEZaME9XWitGUi9WNTZVQW1FMG1DenhZU0ZmeXlZakVEMjZFT2NvOWxpZGlqVmlHNC8rWVVUYzMwRGVySHpIdURzKzFiRDl4TmM1TUp2VTBOUlJZUlAyV0ZVL2kvVk10L0NsbXNFYVZwT3NSU082dFNNa2prQ0ZsclZ4NTltbU1CVE5lR09Bck93V2J1Y3c9PQAAAQCjNd8mwy8B1sm9rGrgTmN2Gjm/lxqfnTEpTc+HOEmAgwQ7Q1Ye/FSGVNIU/enZ5cqSzWS2mY8oZ7FM/7UPKVQ4hkarWn2qye964MW+cux54h7dqxlSB19fG0ZJOJZxxwVxxi8iyJPUSQBa+QN8m7TFkK2kVmP+HnhU7mGUrqXt3zTk5d3pZw3QBQ/Rr3wmSYC5pxV6/o2UHFgu1OPDcX+kEb+UZtMrVNneR+cEwyx7o5Bg3rbKC014T+lMtt69Y080JDI5KfHa7e9Ul0c3rozIL975fP45dU175D4PKZy98cvHJgtsCJF3K8XUZKo2lOcbsWzhK2mZ5kFp0BMXF3Hs", "start_date_in_millis":1558051200000 &#125;&#125;文件存为license.json将过期时间写到2049年，type改为platinum 白金版，这样我们就会拥有全部的x-pack功能。8. 配置elasticsearch安全协议完成以上所有操作在启动elasticsearch前，需要配置elasticsearch的SSL/TLS安全协议,如果不配置的话，需要禁止security才能配置License。当License配置完成后我们需要再开启security，并开启SSL\TLS。12345# 加载License到elasticsearch之前操作echo &quot;xpack.security.enabled: false&quot; &gt;&gt; /opt/elasticsearch-7.3.0/config/elasticsearch.yml# 加载License到elasticsearch之后操作echo &quot;xpack.security.transport.ssl.enabled: true&quot; &gt;&gt; /opt/elasticsearch-7.3.0/config/elasticsearch.yml./bin/elasticsearch -d # 启动elasticsearch9. 加载License到elasticsearch12345678curl -XPUT &apos;http://localhost:9200/_xpack/license&apos; -H &quot;Content-Type: application/json&quot; -d @license.json&#123;&quot;acknowledged&quot;:true,&quot;license_status&quot;:&quot;valid&quot;&#125; # license写入成功# 在es日志中可以查看到如下信息,license [65eafbab-c360-4f64-900d-449499b3d530] mode [basic] - validActive license is now [BASIC]; Security is disabledlicense [537c5c48-c1dd-43ea-ab69-68d209d80c32] mode [platinum] - validActive license is now [PLATINUM]; Security is enabled但是再次查看证书信息的话会报错，因为没有开启ssl/tlscurl &#39;http://localhost:9200/_xpack/license&#39;1`&#123;&quot;error&quot;:&#123;&quot;root_cause&quot;:[&#123;&quot;type&quot;:&quot;security_exception&quot;,&quot;reason&quot;:&quot;missing authentication credentials for REST request [/_xpack/license]&quot;,&quot;header&quot;:&#123;&quot;WWW-Authenticate&quot;:&quot;Basic realm=\&quot;security\&quot; charset=\&quot;UTF-8\&quot;&quot;&#125;&#125;],&quot;type&quot;:&quot;security_exception&quot;,&quot;reason&quot;:&quot;missing authentication credentials for REST request [/_xpack/license]&quot;,&quot;header&quot;:&#123;&quot;WWW-Authenticate&quot;:&quot;Basic realm=\&quot;security\&quot; charset=\&quot;UTF-8\&quot;&quot;&#125;&#125;,&quot;status&quot;:401&#125;`12345678910111213141516171819202122232425262728293031# 开启ssl/tls,打开认证sed -i &apos;s/xpack.security.enabled: false/xpack.security.enabled: true/g&apos; /opt/elasticsearch-7.3.0/config/elasticsearch.yml# 如果需要重新设置密码,手动设置密码./bin/elasticsearch-setup-passwords interactive# 自动生成密码：./bin/elasticsearch-setup-passwords autoInitiating the setup of passwords for reserved users elastic,apm_system,kibana,logstash_system,beats_system,remote_monitoring_user.The passwords will be randomly generated and printed to the console.Please confirm that you would like to continue [y/N]yChanged password for user apm_systemPASSWORD apm_system = 7zkIYUXQpq8xZdaapTuQChanged password for user kibanaPASSWORD kibana = aQi7k57swBAaDOtvzdm2Changed password for user logstash_systemPASSWORD logstash_system = CboSzpSIq60Zkk0SgpAoChanged password for user beats_systemPASSWORD beats_system = ZErooCE4ybll3UcLTsAPChanged password for user remote_monitoring_userPASSWORD remote_monitoring_user = EDYQShIhk5P1vjvpeijIChanged password for user elasticPASSWORD elastic = 3tdAyUIFLMdg79EBnrsg10. 查看License12345678910111213141516curl -XGET -u elastic:3tdAyUIFLMdg79EBnrsg http://localhost:9200/_license&#123; &quot;license&quot; : &#123; &quot;status&quot; : &quot;active&quot;, &quot;uid&quot; : &quot;537c5c48-c1dd-43ea-ab69-68d209d80c32&quot;, &quot;type&quot; : &quot;platinum&quot;, &quot;issue_date&quot; : &quot;2019-05-17T00:00:00.000Z&quot;, &quot;issue_date_in_millis&quot; : 1558051200000, &quot;expiry_date&quot; : &quot;2049-12-31T16:00:00.999Z&quot;, &quot;expiry_date_in_millis&quot; : 2524579200999, &quot;max_nodes&quot; : 1000, &quot;issued_to&quot; : &quot;hkd&quot;, &quot;issuer&quot; : &quot;Web Form&quot;, &quot;start_date_in_millis&quot; : 1558051200000 &#125;&#125;由结果可以看出x-pack到期时间为2049-12-31，破解完成。也可以在kibana web页面管理中查看破解详情。kibana操作12345678910111213useradd kibanacd /opttar -zxv -f kibana-7.3.0-linux-x86_64.tar.gzchown -R kibana:kibana kibana-7.3.0-linux-x86_64su - kibanacd /opt/kibana-7.3.0-linux-x86_64cat config/kibana.yml server.port: 5601 server.host: &quot;192.168.0.253&quot; elasticsearch.hosts: [&quot;http://localhost:9200&quot;] elasticsearch.username: &quot;kibana&quot; elasticsearch.password: &quot;aQi7k57swBAaDOtvzdm2&quot;浏览器访问：http://192.168.0.253:5601左侧导航查看ES安装后操作之前已经开启ssl/tls并设置账号等停用es，关闭security验证，替换jar包，开启es，导入license，启动es说明：此方法暂时走不通。以上说的步骤指的是尚未开启security，未设置账号密码的情况。破解文件和license下载地址：https://files.cnblogs.com/files/sanduzxcvbnm/7.3.0%E7%A0%B4%E8%A7%A3%E6%96%87%E4%BB%B6%E5%92%8Clicense.7zES版本升级操作先按照上述步骤生成破解文件，然后再替换操作]]></content>
      <categories>
        <category>elk</category>
      </categories>
      <tags>
        <tag>elk</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch 理解mapping中的store属性]]></title>
    <url>%2F2020%2F01%2F06%2FElasticsearch%20%E7%90%86%E8%A7%A3mapping%E4%B8%AD%E7%9A%84store%E5%B1%9E%E6%80%A7%2F</url>
    <content type="text"><![CDATA[默认情况下，对字段值进行索引以使其可搜索，但不存储它们 (store)。 这意味着可以查询该字段，但是无法检索原始字段值。在这里我们必须理解的一点是: 如果一个字段的mapping中含有store属性为true，那么有一个单独的存储空间为这个字段做存储，而且这个存储是独立于_source的存储的。它具有更快的查询。存储该字段会占用磁盘空间。如果需要从文档中提取（即在脚本中和聚合），它会帮助减少计算。在聚合时，具有store属性的字段会比不具有这个属性的字段快。 此选项的可能值为false和true。通常这无关紧要。 该字段值已经是_source字段的一部分，默认情况下已存储。 如果您只想检索单个字段或几个字段的值，而不是整个_source的值，则可以使用source filtering来实现。在某些情况下，存储字段可能很有意义。 例如，如果您有一个带有标题，日期和很大的内容字段的文档，则可能只想检索标题和日期，而不必从较大的_source字段中提取这些字段。接下来我们还是通过一个具体的例子来解释这个，虽然上面的描述有点绕口。首先我们来创建一个叫做my_index的索引：123456789101112131415161718PUT my_index&#123; &quot;mappings&quot;: &#123; &quot;properties&quot;: &#123; &quot;title&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;store&quot;: true &#125;, &quot;date&quot;: &#123; &quot;type&quot;: &quot;date&quot;, &quot;store&quot;: true &#125;, &quot;content&quot;: &#123; &quot;type&quot;: &quot;text&quot; &#125; &#125; &#125;&#125;在上面的mapping中，我们把title及date字段里的store属性设置为true，表明有一个单独的index fragement是为它们而配备的，并存储它们的值。我们来写入一个文档到my_index索引中：123456PUT my_index/_doc/1&#123; &quot;title&quot;: &quot;Some short title&quot;, &quot;date&quot;: &quot;2015-01-01&quot;, &quot;content&quot;: &quot;A very long content field...&quot;&#125;接下来，我们来做一个搜索：1GET my_index/_search显示的结果是：1234567891011121314151617181920&quot;hits&quot; : &#123; &quot;total&quot; : &#123; &quot;value&quot; : 1, &quot;relation&quot; : &quot;eq&quot; &#125;, &quot;max_score&quot; : 1.0, &quot;hits&quot; : [ &#123; &quot;_index&quot; : &quot;my_index&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;1&quot;, &quot;_score&quot; : 1.0, &quot;_source&quot; : &#123; &quot;title&quot; : &quot;Some short title&quot;, &quot;date&quot; : &quot;2015-01-01&quot;, &quot;content&quot; : &quot;A very long content field...&quot; &#125; &#125; ]&#125;在上面我们可以在_source中看到这个文档的title，date及content字段。我们可以通过source filtering的方法提前我们想要的字段：1234GET my_index/_search&#123; &quot;_source&quot;: [&quot;title&quot;, &quot;date&quot;]&#125;显示的结果是：12345678910111213141516171819&quot;hits&quot; : &#123; &quot;total&quot; : &#123; &quot;value&quot; : 1, &quot;relation&quot; : &quot;eq&quot; &#125;, &quot;max_score&quot; : 1.0, &quot;hits&quot; : [ &#123; &quot;_index&quot; : &quot;my_index&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;1&quot;, &quot;_score&quot; : 1.0, &quot;_source&quot; : &#123; &quot;date&quot; : &quot;2015-01-01&quot;, &quot;title&quot; : &quot;Some short title&quot; &#125; &#125; ]&#125;显然上面的结果显示我们想要的字段date及title是可以从_source里获取的。我们也可以通过如下的方法来获取这两个字段的值：1234567GET my_index/_search&#123; &quot;stored_fields&quot;: [ &quot;title&quot;, &quot;date&quot; ]&#125;返回的结果是：1234567891011121314151617181920212223&quot;hits&quot; : &#123; &quot;total&quot; : &#123; &quot;value&quot; : 1, &quot;relation&quot; : &quot;eq&quot; &#125;, &quot;max_score&quot; : 1.0, &quot;hits&quot; : [ &#123; &quot;_index&quot; : &quot;my_index&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;1&quot;, &quot;_score&quot; : 1.0, &quot;fields&quot; : &#123; &quot;date&quot; : [ &quot;2015-01-01T00:00:00.000Z&quot; ], &quot;title&quot; : [ &quot;Some short title&quot; ] &#125; &#125; ]&#125;在上面，我们可以看出来在fields里有一个date及title的数组返回查询的结果。也许我们很多人想知道到底这个store到底有什么用途呢？如果都能从_source里得到字段的值。有一种就是我们在开头我们已经说明的情况：我们有时候并不想存下所有的字段在_source里，因为该字段的内容很大，或者我们根本就不想存_source，但是有些字段，我们还是想要获取它们的内容。那么在这种情况下，我们就可以使用store来实现。我们还是用一个例子来说明。首先创建一个叫做my_index1的索引：12345678910111213141516171819202122PUT my_index1&#123; &quot;mappings&quot;: &#123; &quot;_source&quot;: &#123; &quot;enabled&quot;: false &#125;, &quot;properties&quot;: &#123; &quot;title&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;store&quot;: true &#125;, &quot;date&quot;: &#123; &quot;type&quot;: &quot;date&quot;, &quot;store&quot;: true &#125;, &quot;content&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;store&quot;: false &#125; &#125; &#125;&#125;因为我们认为content字段的内容可能会很大，那么我不想存这个字段。在上面，我们也把_source的enabled开关设置为false，表明将不存储任何的source字段。接下来写入一个文档到my_index1里去：123456PUT my_index1/_doc/1&#123; &quot;title&quot;: &quot;Some short title&quot;, &quot;date&quot;: &quot;2015-01-01&quot;, &quot;content&quot;: &quot;A very long content field...&quot;&#125;同样我们来做一个搜索：12345678GET my_index1/_search&#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;content&quot;: &quot;content&quot; &#125; &#125;&#125;我们可以看到搜索的结果：123456789101112131415&quot;hits&quot; : &#123; &quot;total&quot; : &#123; &quot;value&quot; : 1, &quot;relation&quot; : &quot;eq&quot; &#125;, &quot;max_score&quot; : 0.2876821, &quot;hits&quot; : [ &#123; &quot;_index&quot; : &quot;my_index1&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;1&quot;, &quot;_score&quot; : 0.2876821 &#125; ]&#125;在这次的显示中，我们没有看到_source字段，这是因为我们已经把它给disabled了。但是我们可以通过如下的方法来获取那些store 字段：123456789101112GET my_index1/_search&#123; &quot;stored_fields&quot;: [ &quot;title&quot;, &quot;date&quot; ], &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;content&quot;: &quot;content&quot; &#125; &#125;&#125;返回结果是：1234567891011121314151617181920212223&quot;hits&quot; : &#123; &quot;total&quot; : &#123; &quot;value&quot; : 1, &quot;relation&quot; : &quot;eq&quot; &#125;, &quot;max_score&quot; : 0.2876821, &quot;hits&quot; : [ &#123; &quot;_index&quot; : &quot;my_index1&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;1&quot;, &quot;_score&quot; : 0.2876821, &quot;fields&quot; : &#123; &quot;date&quot; : [ &quot;2015-01-01T00:00:00.000Z&quot; ], &quot;title&quot; : [ &quot;Some short title&quot; ] &#125; &#125; ]&#125;我们可以在返回结果里查看到date及title的值。可以合理地存储字段的另一种情况是，对于那些未出现在_source字段（例如copy_to字段）中的字段。您可以参阅我的另外一篇文章“如何使用Elasticsearch中的copy_to来提高搜索效率”。如果你想了解更多关于Elasticsearch的存储，可以阅读文章“Elasticsearch：inverted index，doc_values及source”。参考：https://www.elastic.co/guide/en/elasticsearch/reference/current/mapping-store.htmlhttps://stackoverflow.com/questions/17103047/why-do-i-need-storeyes-in-elasticsearch版权声明：本文为CSDN博主「Elastic 中国社区官方博客」的原创文章，遵循 CC 4.0 BY-SA 版权协议，转载请附上原文出处链接及本声明。原文链接：https://blog.csdn.net/UbuntuTouch/article/details/103810863]]></content>
      <categories>
        <category>elk</category>
      </categories>
      <tags>
        <tag>elk</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elastic：使用ElastAlert发送通知]]></title>
    <url>%2F2020%2F01%2F06%2FElastic%EF%BC%9A%E4%BD%BF%E7%94%A8ElastAlert%E5%8F%91%E9%80%81%E9%80%9A%E7%9F%A5%2F</url>
    <content type="text"><![CDATA[ElastAlert是一个简单的框架，用于从Elasticsearch中的数据中发出异常，尖峰或其他感兴趣模式的警报。我们可以在地址https://elastalert.readthedocs.io/en/latest/elastalert.html找到它的使用说明。在今天的教程中，我将一步一步地介绍如何搭配环境，并从Elasticsearch发送通知给Slack。为了说明问题的方便，我的环境如下：在我的环境中，我使用iMac电脑运行Elasticsearch及Kibana，而在另外一个虚拟机上运行我们的filebeat。filebeat把Ubuntu机器里的syslog传入到Elasticsearch中供分析，同时ElastAlert周期性地从Elasticsearch中获取数据，并依据制定的规则来发送通知。准备工作创建Slack账号我们首先需要创建一个自己的Slack账号，并具有自己的管理员权限。你可以参考链接 “Configuring Slack Account”来配置自己的Slack账号，并生成一个相应的一个Webhook URL。这个URL将会在Elasticsearch里进行使用。我们先把上面创建的webhook url记下来供下面的配置使用。安装Elasticsearch我们可以按照“如何在Linux，MacOS及Windows上进行安装Elasticsearch”介绍的那样安装好我们的Elasticsearch。不过由于我们需要使我们的Elasticsearch被另外一个虚拟机所见，在这里我们需要对我们的Elasticsearch进行配置。首先使用一个编辑器打开在config目录下的elasticsearch.yml配置文件。我们需要修改network.host的IP地址。在你的Mac及Linux机器上，我们可以使用:$ ifconfig来查看到我们的机器的IP地址。针对我的情况，我的机器的IP地址是：10.211.55.2。等修改完我们的IP地址后，我们保存elasticsearch.yml文件。然后重新运行我们的elasticsearch。我们可以在一个浏览器中输入刚才输入的IP地址并加上端口号9200。这样可以查看一下我们的elasticsearch是否已经正常运行了。安装Kibana我们可以按照“如何在Linux，MacOS及Windows上安装Elastic栈中的Kibana”中介绍的那样来安装我们的Kibana。由于我们的Elasticsearch的IP地址已经改变，所以我们必须修改我们的Kibana的配置文件。我们使用自己喜欢的编辑器打开在config目录下的kibana.yml文件，并找到server.host。把它的值修改为自己的电脑的IP地址。针对我的情况是：同时找到elasticsearch.hosts，并把自己的IP地址输入进去：保存我们的kibana.yml文件，并运行我们的Kibana。同时在浏览器的地址中输入自己的IP地址及5601端口：如果配置成功的话，我们就可以看到上面的画面。安装Ubuntu虚拟机这个不在我的这个教程之内。在网上我们可以找到许多的教程教我们如何安装Ubuntu虚拟机。安装filebeat我们想在Ubuntu机器上安装我们的filebeat来手机system log信息。我们首先打开我们的Kibana。点击左上角的Kibana图标：点击“Add log data”按钮：然后点击“System logs”由于Ubuntu是debian系统，我们选择DEB。安装上面的步骤一步一步地进行安装。在配置filebeat.yml时，我们需要把我们的IP地址输入到相应的地方：123456output.elasticsearch: hosts: [&quot;http://10.211.55.2:9200&quot;] username: &quot;elastic&quot; password: &quot;123456&quot;setup.kibana: host: &quot;10.211.55.2:5601&quot;上面是我的配置情况。你可以根据自己的实际的IP地址进行配置。当我们成功地启动filebeat服务后，我们可以通过如下的命令来检查我们的服务是否已经成功运行：sudo systemctl status filebeat安装ElastAlert我们可以参考链接https://elastalert.readthedocs.io/en/latest/running_elastalert.html来安装我们的ElastAlert。在这里我们使用python3来运行ElastAlert。首先我们需要在我们的Ubuntu上安装python3。我们安装如下的步骤进行安装：1） 下载elastalert源码：git clone https://github.com/Yelp/elastalert.git2）安装模块：123sudo pip3 install &quot;setuptools&gt;=11.3&quot;sudo python3 setup.py installsudo pip3 install -U PyYAML根据Elasticsearch的版本，您可能需要手动安装正确版本的elasticsearch-py。Elasticsearch 5.0+:sudo pip3 install &quot;elasticsearch&gt;=5.0.0&quot;Elasticsearch 2.X:sudo pip3 install &quot;elasticsearch&lt;3.0.0&quot;这样我们的安装工作就完成了。配置ElastAlert配置文件我们可以在ElastAlert源码文件的根目录下找到一个叫做config.yaml.example的文件：我们可以把这个文件修改为config.yaml文件：mv config.yaml.example config.yaml我们使用我们喜欢的编辑器打开这个文件，并修改这个文件：我们可以根据自己的IP地址来进行修改。如果我们对Elasticsearch做了安全设置，我们同时也需要填写用户名及密码：做完上面的修改后，我们保存config.yaml文件。配置ElasticsearchElastAlert将有关其查询和警报的信息和元数据保存回Elasticsearch。 这对于审核和调试很有用，它使ElastAlert可以重新启动并完全从中断处恢复。 ElastAlert不需要运行，但强烈建议使用。首先，我们需要通过运行elastalert-create-index并按照说明为ElastAlert创建要写入的索引。我们进入到ElastAlert的源码根目录，并打入如下的命令：elastalert-create-index创建rule每个规则都定义要执行的查询，触发匹配的参数以及每个匹配要触发的警报列表。 我们将使用example_rules/example_frequency.yaml作为模板。我们删除其中一些不需要的项目，最终的文件是这样的：example_frequency.yaml12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455# Alert when the rate of events exceeds a threshold # Elasticsearch hostes_host: 10.211.55.2 # Elasticsearch portes_port: 9200 # (OptionaL) Connect with SSL to Elasticsearch#use_ssl: True # (Optional) basic-auth username and password for Elasticsearches_username: &quot;elastic&quot;es_password: &quot;123456&quot; # (Required)# Rule name, must be uniquename: Slack demo # (Required)# Type of alert.# the frequency rule type alerts when num_events events occur with timeframe timetype: frequency # (Required)# Index to search, wildcard supportedindex: filebeat-* # (Required, frequency specific)# Alert when this many documents matching the query occur within a timeframenum_events: 3 # (Required, frequency specific)# num_events must occur within this amount of time to trigger an alerttimeframe: hours: 1 # (Required)# A list of Elasticsearch filters used for find events# These filters are joined with AND and nested in a filtered query# For more info: http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/query-dsl.htmlfilter:- term: process.name: &quot;JUSTME&quot; # (Required)# The alert is use when a match is foundalert:- &quot;slack&quot; # (required, email specific)# a list of email addresses to send alerts toslack:slack_webhook_url: Your_Webhook_Urlslack_username_override: &quot;liuxg&quot;在上面请修改es_host为自己的IP地址，同时也需要把自己的webhook url写入到slack_webhook_url中去。在上面我们使用index为filebeat-*作为查询的索引，同时我们使用一个filter。它检查process.name是否为JUSTME字符串。如果是，并且在1个小时（timeframe）里出现3次（num_events），那么将触发通知。测试rule运行elastalert-test-rule工具将测试您的配置文件是否成功加载并在过去的24小时内以调试模式运行它：elastalert-test-rule example_rules/example_frequency.yaml运行ElastAlert我们使用Python来直接运行Elastalert：python3 -m elastalert.elastalert --verbose --rule example_frequency.yaml这样我们的Elastalert已经被成功运行起来了。我们在这个时候可以打开我们的Kibana来监视filebeat-*索引，如果在一个小时内有三次process.name信息有JUSTME字样，那么我们就会在我们的Slack里收到一个通知。我们在Ubuntu中打开另外的一个terminal，并输入如下的命令：123sudo logger -t JUSTME this is message 1sudo logger -t JUSTME this is message 2sudo logger -t JUSTME this is message 3那么我们可以打开Kibana查看这些消息：那么这个时候，在我们的Slack中，我们可以看到如下的消息：我们收到了我们所需要的通知信息。我们也可以把通知写入到我们的邮件中去。这个由你们自己来实践了。在Elastalert的官方网站上，我们可以看到很多的通知类型。详细地址为https://elastalert.readthedocs.io/en/latest/ruletypes.html————————————————版权声明：本文为CSDN博主「Elastic 中国社区官方博客」的原创文章，遵循 CC 4.0 BY-SA 版权协议，转载请附上原文出处链接及本声明。原文链接：https://blog.csdn.net/UbuntuTouch/article/details/103820572]]></content>
      <categories>
        <category>elk</category>
      </categories>
      <tags>
        <tag>elk</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[不给字段创建索引，字段不存放在source中，字段无法聚合查询等]]></title>
    <url>%2F2020%2F01%2F06%2F%E4%B8%8D%E7%BB%99%E5%AD%97%E6%AE%B5%E5%88%9B%E5%BB%BA%E7%B4%A2%E5%BC%95%EF%BC%8C%E5%AD%97%E6%AE%B5%E4%B8%8D%E5%AD%98%E6%94%BE%E5%9C%A8source%E4%B8%AD%EF%BC%8C%E5%AD%97%E6%AE%B5%E6%97%A0%E6%B3%95%E8%81%9A%E5%90%88%E6%9F%A5%E8%AF%A2%E7%AD%89%2F</url>
    <content type="text"><![CDATA[某个字段不被搜索，也就是说不想为这个字段建立inverted index(反向索引)，可以这么做：12345678910111213PUT twitter&#123; &quot;mappings&quot;: &#123; &quot;uid&quot;: &#123; &quot;type&quot;: &quot;long&quot; &#125;, &quot;user&quot;: &#123; &quot;type&quot;: &quot;object&quot;, &quot;enabled&quot;: false &#125; &#125; &#125;&#125;通过mapping对user字段进行了修改：1234&quot;user&quot;: &#123; &quot;type&quot;: &quot;object&quot;, &quot;enabled&quot;: false &#125;不想我们的整个文档被搜索:1234567PUT twitter &#123; &quot;mappings&quot;: &#123; &quot;enabled&quot;: false &#125;&#125;不想存储任何的字段,也就是说不在source中存储数据,它有完好的inverted index供查询，虽然它没有字的source。12345678PUT twitter&#123; &quot;mappings&quot;: &#123; &quot;_source&quot;: &#123; &quot;enabled&quot;: false &#125; &#125;&#125;想节省自己的存储空间，只存储那些需要的字段到source里去使用include来包含我们想要的字段，同时我们通过exclude来去除那些不需要的字段123456789101112131415PUT twitter&#123; &quot;mappings&quot;: &#123; &quot;_source&quot;: &#123; &quot;includes&quot;: [ &quot;*.lat&quot;, &quot;address&quot;, &quot;name.*&quot; ], &quot;excludes&quot;: [ &quot;name.surname&quot; ] &#125; &#125;&#125;默认情况下，所有支持doc值的字段均已启用它们。如果您确定不需要对字段进行排序或汇总，也不需要通过脚本访问字段值，则可以禁用doc值以节省磁盘空间：123456789101112131415161718192021222324PUT twitter&#123; &quot;mappings&quot;: &#123; &quot;properties&quot;: &#123; &quot;city&quot;: &#123; &quot;type&quot;: &quot;keyword&quot;, &quot;doc_values&quot;: false, &quot;ignore_above&quot;: 256 &#125;, &quot;address&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;fields&quot;: &#123; &quot;keyword&quot;: &#123; &quot;type&quot;: &quot;keyword&quot;, &quot;ignore_above&quot;: 256 &#125; &#125; &#125;, &quot;age&quot;: &#123; &quot;type&quot;: &quot;long&quot; &#125; &#125; &#125;&#125;把city字段的doc_values设置为false]]></content>
      <categories>
        <category>elk</category>
      </categories>
      <tags>
        <tag>elk</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[X-Pack：创建阈值检查警报]]></title>
    <url>%2F2020%2F01%2F02%2FX-Pack%EF%BC%9A%E5%88%9B%E5%BB%BA%E9%98%88%E5%80%BC%E6%A3%80%E6%9F%A5%E8%AD%A6%E6%8A%A5%2F</url>
    <content type="text"><![CDATA[简单的事情应该简单(Simple things should be simple)，这是Elastic {ON} ‘17的主题之一，Elastics收到了许多关于使用简单易用的UI创建警报的请求。事实证明，创建单个UI以对所有类型的警报均有效地工作非常困难。例如，可以在平均CPU使用率超过50％时创建警报的UI与可以在同一IP地址上有许多并发登录的情况下创建警报的UI看起来截然不同。由于很难为所有类型的警报构建通用的UI，因此Elastic决定首先针对最常请求的警报处理UI：当指标超过或低于给定阈值时触发的简单阈值警报。在开始示例之前，请确保您具有最低版本的Elasticsearch和Kibana的6.0.0版本，并且两者都安装了X-Pack。在最新的7.x版本里，X-Pack已经是发布版的一部分，不需要安装。另外，请确保您为Elasticsearch配置了具有足够权限的用户。现在，我们需要一些有趣的数据来构建警报。 Metricbeat是监视机器上的系统和用户进程的绝佳拍子。在今天的练习里，我们来展示如何通过阈值检查，并发送通知到Slack。大家也可以尝试发送到电子邮件等方式。创建Slack账号我们首先需要创建一个自己的Slack账号(https://slack.com/)，并具有自己的管理员权限。你可以参考链接 “Configuring Slack Account”(https://www.elastic.co/guide/en/elasticsearch/reference/7.5/actions-slack.html#configuring-slack)来配置自己的Slack账号，并生成一个相应的一个Webhook URL。这个URL将会在Elasticsearch里进行使用。配置elasticsearch.yml首先watcher必须是在有账号的情况下才可以工作的。如果你还不知道如何开通一个Elasticsearch的安全，那么请参阅我之前的文章“Elasticsearch：设置Elastic账户安全”。因为这是一个付费的功能，你需要接受30天试用的条件才可以看到这个功能。为了能够使得watcher能够正常工作，我们必须配置elasticsearch.yml文件。打开elasticsearch安装目录下的config/elasticsearch.yml文件，并加入如下的配置：1234567891011121314151617xpack.security.enabled: truediscovery.type: single-node xpack.notification.slack: account: monitoring: message_defaults: from: x-pack to: notifications icon: http://example.com/images/watcher-icon.jpg attachment: fallback: &quot;X-Pack Notification&quot; color: &quot;#36a64f&quot; title: &quot;X-Pack Notification&quot; title_link: &quot;https://www.elastic.co/guide/en/x-pack/current/index.html&quot; text: &quot;One of your watches generated this notification.&quot; mrkdwn_in: &quot;pretext, text&quot;前面的两行是为了启动安全功能才进行加入的。后面的关于xpack的配置才是为watcher而设置的。配置好我们的elasticsearch.yml文件后，我们在命令行中打入如下的命令：1./bin/elasticsearch-keystore add xpack.notification.slack.account.monitoring.secure_url在这里，我们选择y。如果你是第一次运行这个命令的话，就不会有这样的一个提示了。你可以把你从Slack中配置的那个Webhook URL复制并粘贴到这里。这样我们的配置就完成了。然后，我们启动Elasticsearch。安装及配置Metricbeat只启动system模块即可。等安装好Metricbeat后，就可启动我们的metricbeat了。配置Watcher打开浏览器并导航到Kibana。单击侧面导航栏中的“Management”应用程序，然后单击Elasticsearch标题下的Watcher。我们点击Create，然后，我们就可以开始配置我们的一个watcher了。我们选择Create threashold alert:然后，我们可以按照上面的配置进行设置。再点击“Add action”：我们选择Slack作为我们的通知方法。里面还有其它的几种方式，你们可以自己去尝试。我们可以选择Send a sample message按钮来测试一下我们的Slack配置是否成功。最后，我们选择Create alert。这样就创建了一个Watcher。我们可以在Watcher页面看到我们配置的每个Watcher。上面显示我们的其中的一个watcher已经发送通知了，而且是4分钟之前发送的。我们可以在我们的Slack界面看到如下的消息：我们可以看到许多的通知信息不断地进来。它表明我们的配置是已经成功了。上面我们通过Kibana的界面配置了Watcher。事实上，我们也可以通过API的方式来配置。请详细阅读我们的文档(https://www.elastic.co/guide/en/elasticsearch/reference/7.5/how-watcher-works.html)。参考：【1】https://www.elastic.co/guide/en/elasticsearch/reference/7.5/how-watcher-works.html]]></content>
      <categories>
        <category>elk</category>
      </categories>
      <tags>
        <tag>elk</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Solutions：如何运用Elastic App Search快速建立出色的React搜索体验]]></title>
    <url>%2F2020%2F01%2F02%2FSolutions%EF%BC%9A%E5%A6%82%E4%BD%95%E8%BF%90%E7%94%A8Elastic%20App%20Search%E5%BF%AB%E9%80%9F%E5%BB%BA%E7%AB%8B%E5%87%BA%E8%89%B2%E7%9A%84React%E6%90%9C%E7%B4%A2%E4%BD%93%E9%AA%8C%2F</url>
    <content type="text"><![CDATA[建立搜索体验是一项艰苦的工作。 乍一看似乎很容易：建立一个搜索栏，将数据放入数据库，然后让用户输入对该数据库的查询。 但是，在数据建模，底层逻辑以及（当然）总体设计和用户体验方面，有很多事情要考虑。我们将逐步介绍如何使用Elastic的开源Search UI库构建出色的基于React的搜索体验。 大约需要30分钟，然后您便可以将搜索带到需要它的任何应用程序中。但是首先，是什么使创建搜索如此具有挑战性？搜索是很难创建的开发人员在搜索开发中采用许多错误的假设。比如许多相信的假设：“知道他们要寻找的客户将按照您期望的方式进行搜索。”“您可以编写一个查询解析器，该解析器将始终成功解析查询。”“一旦设置，下周搜索将以相同的方式进行。”“同义词很容易。”…得出的结论是，搜索面临许多挑战–而且这些挑战并不简单。 您需要考虑如何管理状态，构建用于过滤，构面，排序，分页，同义词，语言处理等等的组件，等等。 但是，总而言之：建立出色的搜索需要两个复杂的部分：(1)搜索引擎，它提供用于增强搜索功能的API(2)搜索库，它描绘了搜索体验。对于搜索引擎，我们将查看Elastic App Search。为了获得搜索体验，我们将介绍一个操作系统搜索库：Search UI。完成后，将如下所示。您也可以在地址(https://codesandbox.io/embed/happy-wilbur-hwzsh?view=preview&amp;initialpath=%3Fq%3Dfinal%20fantasy)上进行在线体验。搜索引擎: Elastic App SearchApp Search可作为付费托管服务或免费的自助托管发行版(https://www.elastic.co/downloads/app-search?ultron=searchui-howto-react&amp;blade=codeburst&amp;hulk=content)提供。 我们将在本教程中使用托管服务，但是请记住，如果您自己托管，您的团队可以免费使用带有基本许可的Search UI和App Search。计划：将代表有史以来最好的视频游戏的文档编入搜索引擎，然后设计和优化搜索体验以对其进行搜索。首先，注册14天的试用期(https://www.elastic.co/products/app-search/service?ultron=searchui-howto-react&amp;blade=codeburst&amp;hulk=content)-无需信用卡。创建一个引擎。 您可以选择13种不同的语言。我们将其命名为video-games，并将语言设置为英语。下载最佳视频游戏数据集(https://drive.google.com/file/d/14-3wzemyLzJh6XHVUotFsdl0tZ7K2v1E/view)，然后使用导入程序将其上传到App Search。接下来，单击进入引擎，然后选择“Credentials”选项卡。使用仅对video-games引擎具有Limited Engine Access的方式创建新的Public Search Key。我们可以记下我们刚创建的Public Search Key及Host Indentifier以便下面之用。尽管看起来我们目前做的并不多，但我们现在拥有功能全面的搜索引擎，可以使用完善的搜索API来搜索我们的视频游戏数据。到目前为止，这是我们所做的：创建了一个搜索引擎建立了索引文档创建一个默认的索引schema创建了一个有限的可以用于外界访问的凭证（credential）让我们开始使用“Search UI”来建立我们的搜索体验。搜索库：Search UI我们将使用create-react-app(https://github.com/facebook/create-react-app)脚手架实用程序创建一个React应用：123npm install -g create-react-appcreate-react-app video-game-search --use-npmcd video-game-search在此基础上，我们将安装Search UI和App Search连接器：1npm install --save @elastic/react-search-ui @elastic/search-ui-app-search-connector并以开发模式启动该应用程序：1npm start在您喜欢的文本编辑器中打开src/App.js我们将从一些样板代码开始，注意评论部分！12345678910111213141516171819202122232425262728293031// Step #1, import statementsimport React from &quot;react&quot;;import AppSearchAPIConnector from &quot;@elastic/search-ui-app-search-connector&quot;;import &#123; SearchProvider, Results, SearchBox &#125; from &quot;@elastic/react-search-ui&quot;;import &#123; Layout &#125; from &quot;@elastic/react-search-ui-views&quot;;import &quot;@elastic/react-search-ui-views/lib/styles/styles.css&quot;;// Step #2, The connectorconst connector = new AppSearchAPIConnector(&#123; searchKey: &quot;[YOUR_SEARCH_KEY]&quot;, engineName: &quot;video-games&quot;, hostIdentifier: &quot;[YOUR_HOST_IDENTIFIER]&quot;&#125;); // Step #3: Configuration optionsconst configurationOptions = &#123; apiConnector: connector // Let&apos;s fill this in together.&#125;; // Step #4, SearchProvider: The finishing touchesexport default function App() &#123; return ( &lt;SearchProvider config=&#123;configurationOptions&#125;&gt; &lt;div className=&quot;App&quot;&gt; &lt;Layout // Let&apos;s fill this in together. /&gt; &lt;/div&gt; &lt;/SearchProvider&gt; );&#125;Step 1: 导入声明我们需要导入我们的Search UI依赖关系和React。核心组件，连接器和视图组件包含在三个不同的程序包中：123@elastic/search-ui-app-search-connector@elastic/react-search-ui@elastic/react-search-ui-views继续进行时，我们将详细了解它们1234import React from &quot;react&quot;;import AppSearchAPIConnector from &quot;@elastic/search-ui-app-search-connector&quot;;import &#123; SearchProvider, Results, SearchBox &#125; from &quot;@elastic/react-search-ui&quot;;import &#123; Layout &#125; from &quot;@elastic/react-search-ui-views&quot;;我们还将为该项目导入默认样式表，这将使我们拥有良好的外观，而无需编写我们自己的CSS行：1import &quot;@elastic/react-search-ui-views/lib/styles/styles.css&quot;;Step 2: 连接器我们有来自App Search的Public Search Key和Host Identifier。是时候让他们工作了！Search UI中的连接器对象使用credentials连接到App Search和超级搜索：12345const connector = new AppSearchAPIConnector(&#123; searchKey: &quot;[YOUR_SEARCH_KEY]&quot;, engineName: &quot;video-games&quot;, hostIdentifier: &quot;[YOUR_HOST_IDENTIFIER]&quot;&#125;);搜索用户界面可与任何搜索API配合使用。 但是通过连接器可以使搜索API正常工作，而无需进行任何更深入的配置。Step 3: configurationOptions在深入探讨configurationOptions之前，让我们花点时间进行反思。我们将一组数据导入了搜索引擎。 但是，它是什么样的数据？我们对数据了解的越多，我们就会越了解如何将数据呈现给搜索者。 这样一来，您便可以了解如何配置搜索体验。我们来看一个对象，这是该数据集中所有对象中的一个：12345678910111213&#123; &quot;id&quot;:&quot;final-fantasy-vii-ps-1997&quot;, &quot;name&quot;:&quot;Final Fantasy VII&quot;, &quot;year&quot;:1997, &quot;platform&quot;:&quot;PS&quot;, &quot;genre&quot;:&quot;Role-Playing&quot;, &quot;publisher&quot;:&quot;Sony Computer Entertainment&quot;, &quot;global_sales&quot;:9.72, &quot;critic_score&quot;:92, &quot;user_score&quot;:9, &quot;developer&quot;:&quot;SquareSoft&quot;, &quot;image_url&quot;:&quot;https://r.hswstatic.com/w_907/gif/finalfantasyvii-MAIN.jpg&quot;&#125;我们看到它有几个文本字段，例如name，year，platform等等，还有一些数字字段，例如critic_score，global_sales和user_score。如果我们提出三个关键问题，我们将足够了解，以提供扎实的搜索体验：大多数人将如何搜索？ 以视频游戏的名称命名。大多数人想要看到的结果是什么？ 视频游戏的名称，类型，发行商，得分和平台。大多数人将如何过滤，排序和构面？ 按得分，体裁，发布者和平台分类。然后，我们可以将这些答案转换为我们的configurationOptions：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071const configurationOptions = &#123; apiConnector: connector, searchQuery: &#123; search_fields: &#123; // 1. Search by name of video game. name: &#123;&#125; &#125;, // 2. Results: name, genre, publisher, scores, and platform. result_fields: &#123; name: &#123; // A snippet means that matching search terms will be wrapped in &lt;em&gt; tags. snippet: &#123; size: 75, // Limit the snippet to 75 characters. fallback: true // Fallback to a &quot;raw&quot; result. &#125; &#125;, genre: &#123; snippet: &#123; size: 50, fallback: true &#125; &#125;, publisher: &#123; snippet: &#123; size: 50, fallback: true &#125; &#125;, critic_score: &#123; // Scores are numeric, so we won&apos;t snippet. raw: &#123;&#125; &#125;, user_score: &#123; raw: &#123;&#125; &#125;, platform: &#123; snippet: &#123; size: 50, fallback: true &#125; &#125;, image_url: &#123; raw: &#123;&#125; &#125; &#125;, // 3. Facet by scores, genre, publisher, and platform, which we&apos;ll use to build filters later. facets: &#123; user_score: &#123; type: &quot;range&quot;, ranges: [ &#123; from: 0, to: 5, name: &quot;Not good&quot; &#125;, &#123; from: 5, to: 7, name: &quot;Not bad&quot; &#125;, &#123; from: 7, to: 9, name: &quot;Pretty good&quot; &#125;, &#123; from: 9, to: 10, name: &quot;Must play!&quot; &#125; ] &#125;, critic_score: &#123; type: &quot;range&quot;, ranges: [ &#123; from: 0, to: 50, name: &quot;Not good&quot; &#125;, &#123; from: 50, to: 70, name: &quot;Not bad&quot; &#125;, &#123; from: 70, to: 90, name: &quot;Pretty good&quot; &#125;, &#123; from: 90, to: 100, name: &quot;Must play!&quot; &#125; ] &#125;, genre: &#123; type: &quot;value&quot;, size: 100 &#125;, publisher: &#123; type: &quot;value&quot;, size: 100 &#125;, platform: &#123; type: &quot;value&quot;, size: 100 &#125; &#125; &#125;&#125;;我们已经将Search UI连接到我们的搜索引擎，现在我们有一些选项可以控制我们如何搜索数据，显示结果并探索这些结果。 但是我们需要一些东西来将所有内容绑定到Search UI的动态前端组件。Step 4: SearchProvider这是统治所有对象的对象。 SearchProvider是所有其他组件嵌套的地方。Search UI提供了一个Layout组件，用于绘制典型的搜索布局。 有很深的自定义选项，但我们不会在本教程中介绍。我们将做两件事：将configurationOptions传递给SearchProvider。将一些结构性构建基块放入Layout中，并添加两个基本组件：SearchBox和Results。12345678910111213export default function App() &#123; return ( &lt;SearchProvider config=&#123;configurationOptions&#125;&gt; &lt;div className=&quot;App&quot;&gt; &lt;Layout header=&#123;&lt;SearchBox /&gt;&#125; // titleField is the most prominent field within a result: the result header. bodyContent=&#123;&lt;Results titleField=&quot;name&quot; urlField=&quot;image_url&quot; /&gt;&#125; /&gt; &lt;/div&gt; &lt;/SearchProvider&gt; );&#125;至此，我们已经在前端建立了基础。 在运行此后端之前，还有一些其他细节需要在后端解决。 我们还应该研究相关性模型，以便针对该项目的独特需求微调搜索。重新进入搜索平台App Search具有强大且完善的搜索引擎功能。 它使曾经复杂的调优变得更加有趣。 只需单击几下，我们便可以进行细粒度的相关性调整和无缝的模式更改。我们将首先调整schema以使其实际运行。登录到App Search，输入video-games引擎，然后单击“Manage”部分下的“Schema”。出现架构。 默认情况下，这11个字段中的每一个均被视为文本。在configurationOptions对象中，我们定义了两个范围构面来帮助我们搜索数字：user_score和critic_score。 为了使range facet按预期工作，字段类型必须为数字(number)。单击每个字段旁边的下拉菜单，将其更改为数字，然后单击“Update Types”。引擎会即时重新更新索引。 然后，当我们将构面（facet）组件添加到布局中时，范围过滤器将按预期运行。 现在，进入真正的漂亮东西。下面的部分是高度相关的具有三个关键的相关功能：Synonyms，Curations和Relevance Tuning。在边栏中的“Search Settings”部分下选择每个功能：Synonyms世界各地的人们使用不同的词来形容事物。 同义词可帮助您创建被视为一个或一组相同的术语集。就video game搜索引擎而言，我们知道人们会希望找到Final Fantasy。 但是也许他们会改用FF。单击进入同义词，然后选择创建同义词集并输入术语：单击Save。 您可以根据需要添加任意多个同义词集。现在，搜索FF与搜索Final Fantasy的权重相同。CurationsCurations是最最让人喜欢的。 如果有人搜索Final Fantasy或FF，该怎么办？ 系列赛中有很多游戏-他们会得到哪些？默认情况下，前五个结果如下所示：1.最终幻想VIII2.最终幻想X3.最终幻想策略4.最终幻想IX5.最终幻想XIII这似乎不正确……Final Fantasy VII是所有游戏中最好的Final Fantasy游戏。 而且Final Fantasy XIII不是很好！ 😜我们可以做到这一点，以便搜索Final Fantasy的人会收到Final Fantasy VII作为第一结果吗？ 我们可以从搜索结果中删除Final Fnatasy XIII吗？我们可以！单击“Curations”，然后输入查询：“Final Fantasy”。接下来，通过抓住表格最左侧的把手将“Final FantasyVII”文档拖到“Promoted Documents”部分。 然后单击“Final Fantasy XIII”文档上的“Hide Result”按钮（那个有一条线穿过眼睛的图标，下图列表中第三个图标）：现在，执行“Final Fantasy”或“FF”搜索的任何人都将首先看到“Final Fantasy VII”。他们根本看不到Final Fantasy XIII。 哈！我们可以升级和隐藏许多文档。 我们甚至可以对升级后的文档进行排序，因此我们可以完全控制每个查询顶部显示的内容。Relevance tuning单击边栏中的“Relevance Tuning”。我们搜索一个文本字段：name字段。 但是，如果我们有多个文本字段可供人们搜索，例如name字段和description字段，该怎么办？ 我们正在使用的video game数据集不包含description字段，因此我们假想一些文档以进行仔细考虑。说我们的文档看起来像这样：12345678&#123; &quot;name&quot;:&quot;Magical Quest&quot;, &quot;description&quot;: &quot;A dangerous journey through caves and such.&quot; &#125;,&#123; &quot;name&quot;:&quot;Dangerous Quest&quot;, &quot;description&quot;: &quot;A magical journey filled with magical magic. Highly magic.&quot; &#125;如果有人想找到游戏Magical Quest，他们会输入该内容作为查询。 但是第一个结果将是Dangerous Quest：为什么？ 因为在“Dangerous”的description中“Magical”一词出现了3次，所以搜索引擎不会知道一个字段比另一个字段更重要。 然后，它将使“Dangerous Quest”的排名更高。 这就是为什么存在相关性调整的难题。我们可以选择一个字段，除其他外，还可以增加其相关性的权重：我们看到，当我们增加权重时，正确的项目“ Magical Quest”上升到顶部，因为name字段变得更重要。 我们需要做的就是将滑块拖动到更高的值，然后单击“Save”。现在，我们已经使用App Search实现了如下的任务：调整schema，并将user_score和critic_score更改为数字字段。微调关联（relevance）模型。这样就总结出了精美的“仪表板”功能-每个功能都有一个匹配的API端点，如果您不是GUI的用户，则可以使用它们使程序以编程方式工作。现在，让我们结束UI。最后加工此时，您的UI应该可以正常工作了。 尝试一些查询。 首先要说的是，我们缺少探索结果的工具，例如过滤，分面(facet)，排序等，但是搜索有效。 我们需要完善用户界面。在初始的src/App.js文件中，我们导入了三个基本组件：1import &#123; SearchProvider, Results, SearchBox &#125; from &quot;@elastic/react-search-ui&quot;;根据我们为配置选项定义的内容，让我们添加更多内容。导入以下组件将启用UI中缺少的功能：PagingInfo：在当前页面上显示信息。ResultsPerPage：配置每页上显示多少个结果。Paging：浏览不同的页面。Facet：以数据类型独有的方式过滤和浏览数据。Sort：重新定向给定字段的结果。12345678910import &#123; PagingInfo, ResultsPerPage, Paging, Facet, SearchProvider, Results, SearchBox, Sorting&#125; from &quot;@elastic/react-search-ui&quot;;导入后，可以将组件放置到布局中。布局组件将页面分为多个部分，可以通过prop将组件放置在这些部分中。它包含以下部分：header：搜索框/栏bodyContent：结果容器sideContent：侧边栏，其中包含构面和排序选项bodyHeader：围绕结果的“包装器”，其中包含上下文丰富的信息，例如当前页面和每页结果数bodyFooter：用于在页面之间快速导航的分页选项组件呈现数据。根据我们在configurationOptions中提供的搜索设置获取数据。现在，我们将每个组件放置在适当的布局部分中。例如，我们在configurationOptions中描述了五个方面的维度，因此我们将创建五个方面的组件。每个Facet组件都将使用“字段”属性作为返回数据的键。我们将它们与我们的Sorting组件一起放在sideContent部分中，然后将Paging，PagingInfo和ResultsPerPage组件放在最适合它们的部分中：1234567891011121314151617181920212223242526272829303132333435&lt;Layout header=&#123;&lt;SearchBox /&gt;&#125; bodyContent=&#123;&lt;Results titleField=&quot;name&quot; urlField=&quot;image_url&quot; /&gt;&#125; sideContent=&#123; &lt;div&gt; &lt;Sorting label=&#123;&quot;Sort by&quot;&#125; sortOptions=&#123;[ &#123; name: &quot;Relevance&quot;, value: &quot;&quot;, direction: &quot;&quot; &#125;, &#123; name: &quot;Name&quot;, value: &quot;name&quot;, direction: &quot;asc&quot; &#125; ]&#125; /&gt; &lt;Facet field=&quot;user_score&quot; label=&quot;User Score&quot; /&gt; &lt;Facet field=&quot;critic_score&quot; label=&quot;Critic Score&quot; /&gt; &lt;Facet field=&quot;genre&quot; label=&quot;Genre&quot; /&gt; &lt;Facet field=&quot;publisher&quot; label=&quot;Publisher&quot; isFilterable=&#123;true&#125; /&gt; &lt;Facet field=&quot;platform&quot; label=&quot;Platform&quot; /&gt; &lt;/div&gt; &#125; bodyHeader=&#123; &lt;&gt; &lt;PagingInfo /&gt; &lt;ResultsPerPage /&gt; &lt;/&gt; &#125; bodyFooter=&#123;&lt;Paging /&gt;&#125;/&gt;现在，让我们看一下本地开发环境中的搜索体验。好多了！ 我们提供了丰富的选项来探索搜索结果。我们引入了一些额外的好处，例如多种排序选项，并且通过添加单个标志使发布者的面可过滤。 尝试使用空白查询进行搜索并浏览所有选项。最后，让我们看一下搜索体验的最后一项功能。 这是一个受欢迎的…自动完成 (Autocomplete)搜索者喜欢自动完成功能，因为它可以提供即时反馈。 它的建议有两种形式：结果和查询。 取决于哪种口味，搜索者将收到相关结果或可能导致结果的潜在查询。我们将重点关注自动填充作为一种查询建议形式。这需要两个快速更改。首先，我们需要将自动完成功能添加到configurationOptions对象中：123456789101112131415const configurationOptions = &#123; autocompleteQuery: &#123; suggestions: &#123; types: &#123; documents: &#123; // Which fields to search for suggestions fields: [&quot;name&quot;] &#125; &#125;, // How many suggestions appear size: 5 &#125; &#125;, ...&#125;;其次，我们需要根据SearchBox启用自动填充功能：123456... &lt;Layout ... header=&#123;&lt;SearchBox autocompleteSuggestions=&#123;true&#125; /&gt;&#125;/&gt;...是的，就是这样。尝试搜索-键入时，将显示自动完成查询建议。总结现在，我们拥有美观的功能性搜索体验。 而且，我们避免了人们在尝试实施搜索时经常会遇到的一堆陷阱。 30分钟还不错，你不是说吗？你可以在地址进行一个完美的体验。如果你想进一步动态生成数据集，请参阅文章https://swiftype.com/documentation/app-search/api/documents#create你可以在如下地址找到这个项目的源码：https://github.com/liu-xiao-guo/swiftype-video-game-search参考：【1】How to Build Great React Search Experiences Quickly————————————————版权声明：本文为CSDN博主「Elastic官方博客」的原创文章，遵循 CC 4.0 BY-SA 版权协议，转载请附上原文出处链接及本声明。原文链接：https://blog.csdn.net/UbuntuTouch/article/details/103101698]]></content>
      <categories>
        <category>elk</category>
      </categories>
      <tags>
        <tag>elk</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Logstash 启动监控及集中管理]]></title>
    <url>%2F2020%2F01%2F02%2FLogstash%20%E5%90%AF%E5%8A%A8%E7%9B%91%E6%8E%A7%E5%8F%8A%E9%9B%86%E4%B8%AD%E7%AE%A1%E7%90%86%2F</url>
    <content type="text"><![CDATA[在本篇文章里，我将详细介绍如果启动Logstash的监控及集中管理。前提条件安装好Logstash，设置Elasticsearch及Kibana的安全密码。如何监控Logstash?我们安装如下的步骤来实现监控Logstash的目的：Step 1: 在Kibana中启动监控：然后，我们可以看到如下的画面：Step 2：配置Logstash如果我们在没有配置Logstash的情况下直接运行Logstash，我们会发现如下的错误：123456789101112131415161718liuxg-2:logstash-7.5.0 liuxg$ ./bin/logstashJava HotSpot(TM) 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.WARNING: An illegal reflective access operation has occurredWARNING: Illegal reflective access by com.headius.backport9.modules.Modules (file:/Users/liuxg/elastic5/logstash-7.5.0/logstash-core/lib/jars/jruby-complete-9.2.8.0.jar) to field java.io.FileDescriptor.fdWARNING: Please consider reporting this to the maintainers of com.headius.backport9.modules.ModulesWARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operationsWARNING: All illegal access operations will be denied in a future releaseThread.exclusive is deprecated, use Thread::MutexSending Logstash logs to /Users/liuxg/elastic5/logstash-7.5.0/logs which is now configured via log4j2.propertiesERROR: Pipelines YAML file is empty. Location: /Users/liuxg/elastic5/logstash-7.5.0/config/pipelines.ymlusage: bin/logstash -f CONFIG_PATH [-t] [-r] [] [-w COUNT] [-l LOG] bin/logstash --modules MODULE_NAME [-M &quot;MODULE_NAME.var.PLUGIN_TYPE.PLUGIN_NAME.VARIABLE_NAME=VALUE&quot;] [-t] [-w COUNT] [-l LOG] bin/logstash -e CONFIG_STR [-t] [--log.level fatal|error|warn|info|debug|trace] [-w COUNT] [-l LOG] bin/logstash -i SHELL [--log.level fatal|error|warn|info|debug|trace] bin/logstash -V [--log.level fatal|error|warn|info|debug|trace] bin/logstash --help[2019-12-30T15:32:49,899][ERROR][org.logstash.Logstash ] java.lang.IllegalStateException: Logstash stopped processing because of an error: (SystemExit) exit首先在Logstash的安装目录中找到logstash的配置文件logstash.yml：我们可以在Logstash的根目录下运行一下的命令：1./bin/logstash-keystore create上面的命令将创建一个Created Logstash keystore：我们可以利用如下的命令来创建一些key: ES_HOST及ES_PWD。1./bin/logstash-keystore add ES_HOST当我们运行时，可以把我们的Elasticsearch的host地址粘贴过来：比如针对我们的情况，我们粘贴的地址是http://localhost:9200/。按照同样的方法，我们可以创建另外一个ES_PWD key：1./bin/logstash-keystore add ES_PWD这些key可以在logstash的配置文件中所使用。这样我们可以不暴露我们的密码给别人看到。我们打开logstash.yml文件，并同时使用如下的配置：1234xpack.monitoring.enabled: truexpack.monitoring.elasticsearch.username: logstash_systemxpack.monitoring.elasticsearch.password: &quot;$&#123;ES_PWD&#125;&quot;xpack.monitoring.elasticsearch.hosts: [&quot;$&#123;ES_HOST&#125;&quot;]这里，我们打开monitoring的开关，并同时使用我们在创建安全账户已经创建好的用户名logstash_system:现在我们下载一个我之前做个的一个练习：1git clone https://github.com/liu-xiao-guo/logstash_multi-pipeline我们可以下载到我们指定的目录里。但是记得修改在apache.conf中的path路径，否则我们会错的。123456789101112131415161718192021222324252627282930313233343536apache.confinput &#123; file &#123; path =&gt; &quot;/Users/liuxg/data/multi-pipeline/apache.log&quot; start_position =&gt; &quot;beginning&quot; sincedb_path =&gt; &quot;/dev/null&quot; # ignore_older =&gt; 100000 type =&gt; &quot;apache&quot; &#125;&#125;filter &#123; grok &#123; match =&gt; &#123; &quot;message&quot; =&gt; &apos;%&#123;IPORHOST:clientip&#125; %&#123;USER:ident&#125; %&#123;USER:auth&#125; \[%&#123;HTTPDATE:timestamp&#125;\] &quot;%&#123;WORD:verb&#125; %&#123;DATA:request&#125; HTTP/%&#123;NUMBER:httpversion&#125;&quot; %&#123;NUMBER:response:int&#125; (?:-|%&#123;NUMBER:bytes:int&#125;) %&#123;QS:referrer&#125; %&#123;QS:agent&#125;&apos; &#125; &#125;&#125;output &#123; stdout &#123; codec =&gt; rubydebug &#125; elasticsearch &#123; hosts =&gt; [&quot;$&#123;ES_HOST&#125;&quot;] user =&gt; &quot;elastic&quot; password =&gt; &quot;$&#123;ES_PWD&#125;&quot; index =&gt; &quot;apache_log&quot; template =&gt; &quot;/Users/liuxg/data/multi-pipeline/apache_template.json&quot; template_name =&gt; &quot;apache_elastic_example&quot; template_overwrite =&gt; true &#125; &#125;同时，我们需要添加hosts, user及password的定义。这是因为我们现在我们是需要有用户名及密码才可以连接到Elasticsearch。这个和之前的练习是不一样的。同时我们可以创建自己的用户名及密码。我们可以参考“Elasticsearch：用户安全设置”来创建自己喜欢的账号。在这里，为了方便，我们使用elastic账号。在这里，我们是用${ES_HOST}及${ES_PWD}来代表我们的Elasticsearch地址及密码。这样的好处是我们不暴露我们的密码在配置文件中。一旦上面的配置已经做好了，我们可以使用如下的命令来把我们的apache log文件上传到Elasticsearch之中：1sudo ./bin/logstash -f ~/data/multi-pipeline/apache.confStep3：打开Stack Monitoring UI我们安装如下的步骤来查看Logstash的monitoring：我们会发现在Logstash运行的情况下，有一个Logstash的类别出现了。这在之前是没有的。我们点击Nodes 1：我们看到一个Logstash的运行实例。它显示了目前CPU的使用情况和Load Average及JVM head的使用情况。点击上面的超链接：我们可以看到更加详细的使用情况。我们也可以查看pipeline的状况：Logstash集中管理首先我们来创建一个叫做logstash_writer的role:点击“Create role”来创建我们的role。首先让我们来创建一个具有logstash_user的用户账号：点击上面的“Create user”按钮来创建一个用户：点击“Create user”来创建一个叫做logstash_user的账号。它具有logstash_admin及logstash_system的权限。为了启动集中管理，我们必须在logstash.yml文件里做相应的配置：12345xpack.management.enabled: truexpack.management.pipeline.id: [&quot;main&quot;, &quot;apache_logs&quot;, &quot;my_apache_logs&quot;]xpack.management.elasticsearch.username: &quot;logstash_user&quot;xpack.management.elasticsearch.password: &quot;123456&quot;xpack.management.elasticsearch.hosts: [&quot;$&#123;ES_HOST&#125;&quot;]我们可以在链接https://www.elastic.co/guide/en/logstash/current/logstash-centralized-pipeline-management.html找到更多的描述。在这里，我们启动logstash的管理，同时也把我们刚才创建的logstash_user的账号填入进来，并同时取了一个叫做my_apache_logs的pipeline id。一旦启动了logstash的集中管理，我们就可以直接启动logstash，而不用跟任何的参数：1sudo ./bin/logstash这样我们的logstash已经被成功运行起来了。我们接下来可以在Kibana中创建自己的pipeline。点击上面的“Create pipeline”按钮，我们可以看到如下的画面：接下来我们点击“Create and Deploy”按钮：这样我们的my_apache_logs就被创建好了，而且已经被成功执行了。我们可以在Kibana中创建一个叫apache_log的index pattern，然后打开Discover，你可以看到刚刚被Logstash导入的数据：好了到此，我们关于如何启动Logstash的监控及集中管理讲完了。————————————————版权声明：本文为CSDN博主「Elastic 中国社区官方博客」的原创文章，遵循 CC 4.0 BY-SA 版权协议，转载请附上原文出处链接及本声明。原文链接：https://blog.csdn.net/UbuntuTouch/article/details/103767088]]></content>
      <categories>
        <category>elk</category>
      </categories>
      <tags>
        <tag>elk</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[lasticsearch：Index alias]]></title>
    <url>%2F2020%2F01%2F02%2Flasticsearch%EF%BC%9AIndex%20alias%2F</url>
    <content type="text"><![CDATA[现在让我们来谈谈Elasticsearch最简单和最有用的功能之一：别名 （alias)。为了区分这里alias和文章“Elasticsearch : alias数据类型”，这里的别名（alias）指的是index的别名。 别名正是他们听起来的样子; 它们是您可以使用的指针或名称，对应于一个或多个具体索引。 事实证明这非常有用，因为它在扩展集群和管理数据在索引中的布局方式时提供了灵活性。 即使使用Elasticsearch 只有一个索引的集群，使用别名。 您将在以后感谢我们给予您的灵活性。别名到底是什么？您可能想知道别名究竟是什么，以及Elasticsearch在创建别名时涉及何种开销。 别名将其生命置于群集状态内，由主节点（master node)管理; 这意味着如果你有一个名为idaho的别名指向一个名为potato的索引，那么开销就是群集状态映射中的一个额外键，它将名称idaho映射到具体的索引字符串。 这意味着与其他指数相比，别名的重量要轻得多; 可以维护数千个而不会对集群产生负面影响。 也就是说，我们会警告不要创建数十万或数百万个别名，因为在这一点上，即使映射中单个条目的最小开销也会导致集群状态增长到大小。 这意味着创建新群集状态的操作将花费更长时间，因为每次更改时都会将整个群集状态发送到每个节点。为什么别名是有用的？我们建议每个人都为他们的Elasticsearch索引使用别名，因为在重新索引时，它将在未来提供更大的灵活性。 假设您首先创建一个包含单个主分片的索引，然后再决定是否需要更多索引容量。 如果您使用原始别名index，您现在可以将该别名更改为指向另外创建的索引，而无需更改您正在搜索的索引的名称（假设您从头开始使用别名进行搜索）。 另一个有用的功能是可以创建不同索引的窗口; 例如，如果您为数据创建每日索引，则可能需要创建一个名为last-7-days的别名的上周数据的滑动窗口; 然后每天创建新的每日索引时，可以将其添加到别名中，同时删除8天的索引。另外的一种场景是，当我们修改了我们的index的mapping，让后通过reindex API来把我们的现有的index转移到新的index上，那么如果在我们的应用中，我们利用alias就可以很方便地做这间事。在我们成功转移到新的index之后，我们只需要重新定义我们的alias指向新的index，而在我们的客户端代码中，我们一直使用alias来访问我们的index，这样我们的代码不需要任何的改动。建立index为了验证我们的API，我们先建立一些数据：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647PUT twitter/_doc/1&#123; &quot;user&quot; : &quot;双榆树-张三&quot;, &quot;message&quot; : &quot;今儿天气不错啊，出去转转去&quot;, &quot;uid&quot; : 2, &quot;age&quot; : 20, &quot;city&quot; : &quot;北京&quot;, &quot;province&quot; : &quot;北京&quot;, &quot;country&quot; : &quot;中国&quot;, &quot;address&quot; : &quot;中国北京市海淀区&quot;, &quot;location&quot; : &#123; &quot;lat&quot; : &quot;39.970718&quot;, &quot;lon&quot; : &quot;116.325747&quot; &#125;&#125; PUT twitter/_doc/2&#123; &quot;user&quot; : &quot;东城区-老刘&quot;, &quot;message&quot; : &quot;出发，下一站云南！&quot;, &quot;uid&quot; : 3, &quot;age&quot; : 30, &quot;city&quot; : &quot;北京&quot;, &quot;province&quot; : &quot;北京&quot;, &quot;country&quot; : &quot;中国&quot;, &quot;address&quot; : &quot;中国北京市东城区台基厂三条3号&quot;, &quot;location&quot; : &#123; &quot;lat&quot; : &quot;39.904313&quot;, &quot;lon&quot; : &quot;116.412754&quot; &#125;&#125; PUT twitter/_doc/3&#123; &quot;user&quot; : &quot;虹桥-老吴&quot;, &quot;message&quot; : &quot;好友来了都今天我生日，好友来了,什么 birthday happy 就成!&quot;, &quot;uid&quot; : 7, &quot;age&quot; : 90, &quot;city&quot; : &quot;上海&quot;, &quot;province&quot; : &quot;上海&quot;, &quot;country&quot; : &quot;中国&quot;, &quot;address&quot; : &quot;中国上海市闵行区&quot;, &quot;location&quot; : &#123; &quot;lat&quot; : &quot;31.175927&quot;, &quot;lon&quot; : &quot;121.383328&quot; &#125;&#125;这样，我们建立了三个文档的twitter索引。管理别名添加一个index alias一个index别名就是一个用来引用一个或多个已经存在的索引的另外一个名字，我们可以用如下的方法来创建1PUT /twitter/_alias/alias1请求的格式：1234PUT /&lt;index&gt;/_alias/&lt;alias&gt;POST /&lt;index&gt;/_alias/&lt;alias&gt;PUT /&lt;index&gt;/_aliases/&lt;alias&gt;POST /&lt;index&gt;/_aliases/&lt;alias&gt;路径参数：123&lt;index&gt;:要添加到别名的索引名称的逗号分隔列表或通配符表达式。 要将群集中的所有索引添加到别名，请使用_all值。&lt;alias&gt;:(必需,字符串)要创建或更新的索引别名的名称。比如经过上面的REST 请求，我们为twitter创建了另外一个别名alias1。我们以后可以通过alias1来访问这个index:1GET alias1/_search显示的结果：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677&#123; &quot;took&quot; : 0, &quot;timed_out&quot; : false, &quot;_shards&quot; : &#123; &quot;total&quot; : 1, &quot;successful&quot; : 1, &quot;skipped&quot; : 0, &quot;failed&quot; : 0 &#125;, &quot;hits&quot; : &#123; &quot;total&quot; : &#123; &quot;value&quot; : 3, &quot;relation&quot; : &quot;eq&quot; &#125;, &quot;max_score&quot; : 1.0, &quot;hits&quot; : [ &#123; &quot;_index&quot; : &quot;twitter&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;4&quot;, &quot;_score&quot; : 1.0, &quot;_source&quot; : &#123; &quot;user&quot; : &quot;朝阳区-老贾&quot;, &quot;message&quot; : &quot;123,gogogo&quot;, &quot;uid&quot; : 5, &quot;age&quot; : 35, &quot;city&quot; : &quot;北京&quot;, &quot;province&quot; : &quot;北京&quot;, &quot;country&quot; : &quot;中国&quot;, &quot;address&quot; : &quot;中国北京市朝阳区建国门&quot;, &quot;location&quot; : &#123; &quot;lat&quot; : &quot;39.718256&quot;, &quot;lon&quot; : &quot;116.367910&quot; &#125; &#125; &#125;, &#123; &quot;_index&quot; : &quot;twitter&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;5&quot;, &quot;_score&quot; : 1.0, &quot;_source&quot; : &#123; &quot;user&quot; : &quot;朝阳区-老王&quot;, &quot;message&quot; : &quot;Happy BirthDay My Friend!&quot;, &quot;uid&quot; : 6, &quot;age&quot; : 50, &quot;city&quot; : &quot;北京&quot;, &quot;province&quot; : &quot;北京&quot;, &quot;country&quot; : &quot;中国&quot;, &quot;address&quot; : &quot;中国北京市朝阳区国贸&quot;, &quot;location&quot; : &#123; &quot;lat&quot; : &quot;39.918256&quot;, &quot;lon&quot; : &quot;116.467910&quot; &#125; &#125; &#125;, &#123; &quot;_index&quot; : &quot;twitter&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;6&quot;, &quot;_score&quot; : 1.0, &quot;_source&quot; : &#123; &quot;user&quot; : &quot;虹桥-老吴&quot;, &quot;message&quot; : &quot;好友来了都今天我生日，好友来了,什么 birthday happy 就成!&quot;, &quot;uid&quot; : 7, &quot;age&quot; : 90, &quot;city&quot; : &quot;上海&quot;, &quot;province&quot; : &quot;上海&quot;, &quot;country&quot; : &quot;中国&quot;, &quot;address&quot; : &quot;中国上海市闵行区&quot;, &quot;location&quot; : &#123; &quot;lat&quot; : &quot;31.175927&quot;, &quot;lon&quot; : &quot;121.383328&quot; &#125; &#125; &#125;, ...显然这样做的好处是非常明显的，我们可以把我们想要的进行搜索的index取一个和我们搜索方法里一样的别名就可以了，这样我们可以不修改我们的搜索方法，就可以分别对不同的index进行搜索。比如我们可以用同样的搜索方法对每天的log进行分析。只有把每天的log的index的名字都改成一样的alias就可以了。创建一个基于城市的alias：12345678PUT twitter/_alias/city_beijing&#123; &quot;filter&quot;: &#123; &quot;term&quot;: &#123; &quot;city&quot;: &quot;北京&quot; &#125; &#125;&#125;在这里，我们创建了一个名称为city_beijing的alias。如果我们运行如下的搜索：1GET city_beijing/_search它将返回所有关于城市为北京的搜索结果：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455&#123; &quot;took&quot; : 1, &quot;timed_out&quot; : false, &quot;_shards&quot; : &#123; &quot;total&quot; : 1, &quot;successful&quot; : 1, &quot;skipped&quot; : 0, &quot;failed&quot; : 0 &#125;, &quot;hits&quot; : &#123; &quot;total&quot; : &#123; &quot;value&quot; : 4, &quot;relation&quot; : &quot;eq&quot; &#125;, &quot;max_score&quot; : 1.0, &quot;hits&quot; : [ &#123; &quot;_index&quot; : &quot;twitter&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;4&quot;, &quot;_score&quot; : 1.0, &quot;_source&quot; : &#123; &quot;user&quot; : &quot;朝阳区-老贾&quot;, &quot;message&quot; : &quot;123,gogogo&quot;, &quot;uid&quot; : 5, &quot;age&quot; : 35, &quot;city&quot; : &quot;北京&quot;, &quot;province&quot; : &quot;北京&quot;, &quot;country&quot; : &quot;中国&quot;, &quot;address&quot; : &quot;中国北京市朝阳区建国门&quot;, &quot;location&quot; : &#123; &quot;lat&quot; : &quot;39.718256&quot;, &quot;lon&quot; : &quot;116.367910&quot; &#125; &#125; &#125;, &#123; &quot;_index&quot; : &quot;twitter&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;5&quot;, &quot;_score&quot; : 1.0, &quot;_source&quot; : &#123; &quot;user&quot; : &quot;朝阳区-老王&quot;, &quot;message&quot; : &quot;Happy BirthDay My Friend!&quot;, &quot;uid&quot; : 6, &quot;age&quot; : 50, &quot;city&quot; : &quot;北京&quot;, &quot;province&quot; : &quot;北京&quot;, &quot;country&quot; : &quot;中国&quot;, &quot;address&quot; : &quot;中国北京市朝阳区国贸&quot;, &quot;location&quot; : &#123; &quot;lat&quot; : &quot;39.918256&quot;, &quot;lon&quot; : &quot;116.467910&quot; &#125;...alias也可以在创建index时被创建，比如：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778DELETE twitter PUT twitter&#123; &quot;mappings&quot; : &#123; &quot;properties&quot; : &#123; &quot;address&quot; : &#123; &quot;type&quot; : &quot;text&quot;, &quot;fields&quot; : &#123; &quot;keyword&quot; : &#123; &quot;type&quot; : &quot;keyword&quot;, &quot;ignore_above&quot; : 256 &#125; &#125; &#125;, &quot;age&quot; : &#123; &quot;type&quot; : &quot;long&quot; &#125;, &quot;city&quot; : &#123; &quot;type&quot; : &quot;keyword&quot;, &quot;copy_to&quot; : [ &quot;region&quot; ] &#125;, &quot;country&quot; : &#123; &quot;type&quot; : &quot;keyword&quot;, &quot;copy_to&quot; : [ &quot;region&quot; ] &#125;, &quot;explain&quot; : &#123; &quot;type&quot; : &quot;boolean&quot; &#125;, &quot;location&quot; : &#123; &quot;type&quot; : &quot;geo_point&quot; &#125;, &quot;message&quot; : &#123; &quot;type&quot; : &quot;text&quot;, &quot;fields&quot; : &#123; &quot;keyword&quot; : &#123; &quot;type&quot; : &quot;keyword&quot;, &quot;ignore_above&quot; : 256 &#125; &#125; &#125;, &quot;province&quot; : &#123; &quot;type&quot; : &quot;keyword&quot;, &quot;copy_to&quot; : [ &quot;region&quot; ] &#125;, &quot;region&quot; : &#123; &quot;type&quot; : &quot;text&quot; &#125;, &quot;uid&quot; : &#123; &quot;type&quot; : &quot;long&quot; &#125;, &quot;user&quot; : &#123; &quot;type&quot; : &quot;text&quot;, &quot;fields&quot; : &#123; &quot;keyword&quot; : &#123; &quot;type&quot; : &quot;keyword&quot;, &quot;ignore_above&quot; : 256 &#125; &#125; &#125; &#125; &#125;, &quot;aliases&quot;: &#123; &quot;city_beijing&quot;: &#123; &quot;filter&quot;: &#123; &quot;term&quot;: &#123; &quot;city&quot;: &quot;北京&quot; &#125; &#125; &#125; &#125;&#125;在这里，我们删除了twitter索引，同时我们重新定义twitter索引的mapping，并同时定义了city_beijing你别名。重新index我们上面的三个文档，那么我们再次搜索我们的数据：1GET city_beijing/_search我们可以看到两个城市为北京的搜索结果：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859&#123; &quot;took&quot; : 0, &quot;timed_out&quot; : false, &quot;_shards&quot; : &#123; &quot;total&quot; : 1, &quot;successful&quot; : 1, &quot;skipped&quot; : 0, &quot;failed&quot; : 0 &#125;, &quot;hits&quot; : &#123; &quot;total&quot; : &#123; &quot;value&quot; : 2, &quot;relation&quot; : &quot;eq&quot; &#125;, &quot;max_score&quot; : 1.0, &quot;hits&quot; : [ &#123; &quot;_index&quot; : &quot;twitter&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;1&quot;, &quot;_score&quot; : 1.0, &quot;_source&quot; : &#123; &quot;user&quot; : &quot;双榆树-张三&quot;, &quot;message&quot; : &quot;今儿天气不错啊，出去转转去&quot;, &quot;uid&quot; : 2, &quot;age&quot; : 20, &quot;city&quot; : &quot;北京&quot;, &quot;province&quot; : &quot;北京&quot;, &quot;country&quot; : &quot;中国&quot;, &quot;address&quot; : &quot;中国北京市海淀区&quot;, &quot;location&quot; : &#123; &quot;lat&quot; : &quot;39.970718&quot;, &quot;lon&quot; : &quot;116.325747&quot; &#125; &#125; &#125;, &#123; &quot;_index&quot; : &quot;twitter&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;2&quot;, &quot;_score&quot; : 1.0, &quot;_source&quot; : &#123; &quot;user&quot; : &quot;东城区-老刘&quot;, &quot;message&quot; : &quot;出发，下一站云南！&quot;, &quot;uid&quot; : 3, &quot;age&quot; : 30, &quot;city&quot; : &quot;北京&quot;, &quot;province&quot; : &quot;北京&quot;, &quot;country&quot; : &quot;中国&quot;, &quot;address&quot; : &quot;中国北京市东城区台基厂三条3号&quot;, &quot;location&quot; : &#123; &quot;lat&quot; : &quot;39.904313&quot;, &quot;lon&quot; : &quot;116.412754&quot; &#125; &#125; &#125; ] &#125;&#125;获取alias我们可以通过如下的API来获取当前以及定义好的alias:123GET /_aliasGET /_alias/&lt;alias&gt;GET /&lt;index&gt;/_alias/&lt;alias&gt;比如：1GET /twitter/_alias/alias1这里获取在twitter下的名字叫做alias1的别名。针对我们的情况，我们使用如下的接口：1GET /twitter/_alias/city_beijing我们获取我们之前得到的city_beijing的alias。显示的结果如下：12345678910111213&#123; &quot;twitter&quot; : &#123; &quot;aliases&quot; : &#123; &quot;city_beijing&quot; : &#123; &quot;filter&quot; : &#123; &quot;term&quot; : &#123; &quot;city&quot; : &quot;北京&quot; &#125; &#125; &#125; &#125; &#125;&#125;你也可以通过如下的wild card方式来获取所有的alias:1GET /twitter/_alias/*比如，我们新增加一个alias1的别名：1PUT /twitter/_alias/alias1上面的wild card方式返回来得结果为：1234567891011121314&#123; &quot;twitter&quot; : &#123; &quot;aliases&quot; : &#123; &quot;alias1&quot; : &#123; &#125;, &quot;city_beijing&quot; : &#123; &quot;filter&quot; : &#123; &quot;term&quot; : &#123; &quot;city&quot; : &quot;北京&quot; &#125; &#125; &#125; &#125; &#125;&#125;显然这里有两个别名：alias1及city_beijing。你可以通过如下的方式来搜寻你的alias:1GET /_alias/city_*它将显示名字以city开始的所有的alias。检查一个alias是否存在我们可以通过如下的方式来检查一个alias是否存在：12HEAD /_alias/&lt;alias&gt;HEAD /&lt;index&gt;/_alias/&lt;alias&gt;比如：1HEAD /_alias/alias1它显示的结果是：1200 - OK同样你也可通过wild card方式来查询：1HEAD /_alias/city*这个用来检查所有以city为开头的alias。更新alias我们这里所说的更新包括：添加及删除接口为：1POST /_aliases比如：123456POST /_aliases&#123; &quot;actions&quot; : [ &#123; &quot;add&quot; : &#123; &quot;index&quot; : &quot;twitter&quot;, &quot;alias&quot; : &quot;alias2&quot; &#125; &#125; ]&#125;在这里，我们为twitter索引添加了一个叫做alias2的别名。运行后，我们可以通过alias2来重新搜索我们的twitter1GET /alias2/_search我们可以看到我们想要的结果：1234567891011121314151617181920212223242526272829303132333435363738&#123; &quot;took&quot; : 0, &quot;timed_out&quot; : false, &quot;_shards&quot; : &#123; &quot;total&quot; : 1, &quot;successful&quot; : 1, &quot;skipped&quot; : 0, &quot;failed&quot; : 0 &#125;, &quot;hits&quot; : &#123; &quot;total&quot; : &#123; &quot;value&quot; : 3, &quot;relation&quot; : &quot;eq&quot; &#125;, &quot;max_score&quot; : 1.0, &quot;hits&quot; : [ &#123; &quot;_index&quot; : &quot;twitter&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;1&quot;, &quot;_score&quot; : 1.0, &quot;_source&quot; : &#123; &quot;user&quot; : &quot;双榆树-张三&quot;, &quot;message&quot; : &quot;今儿天气不错啊，出去转转去&quot;, &quot;uid&quot; : 2, &quot;age&quot; : 20, &quot;city&quot; : &quot;北京&quot;, &quot;province&quot; : &quot;北京&quot;, &quot;country&quot; : &quot;中国&quot;, &quot;address&quot; : &quot;中国北京市海淀区&quot;, &quot;location&quot; : &#123; &quot;lat&quot; : &quot;39.970718&quot;, &quot;lon&quot; : &quot;116.325747&quot; &#125; &#125; &#125;, ...在action里，我们可以有如下的几种：123add: 添加一个别名remove: 删除一个别名remove_index: 删除一个index或它的别名比如我们可以通过如下的方法来删除一个alias123456POST /_aliases&#123; &quot;actions&quot; : [ &#123; &quot;remove&quot;: &#123; &quot;index&quot; : &quot;twitter&quot;, &quot;alias&quot; : &quot;alias2&quot; &#125; &#125; ]&#125;一旦删除后，之前的定义的alias2就不可以用了。重新命名一个alias重命名别名是一个简单的删除然后在同一API中添加操作。 此操作是原子操作，无需担心别名未指向索引的短时间段：1234567POST /_aliases&#123; &quot;actions&quot; : [ &#123; &quot;remove&quot; : &#123; &quot;index&quot; : &quot;twitter&quot;, &quot;alias&quot; : &quot;alias1&quot; &#125; &#125;, &#123; &quot;add&quot; : &#123; &quot;index&quot; : &quot;twitter&quot;, &quot;alias&quot; : &quot;alias2&quot; &#125; &#125; ]&#125;上面的操作，删除alias1，同时创建一个新的叫做alias2的别名。我们也可以把同一个alias在指向不同时期的index，比如我们的log index滚动下一个月，我们可以修改我们的alias总是指向最新的index。为多个索引添加同样一个alias将别名与多个索引相关联只需几个添加操作：1234567POST /_aliases&#123; &quot;actions&quot; : [ &#123; &quot;add&quot; : &#123; &quot;index&quot; : &quot;test1&quot;, &quot;alias&quot; : &quot;alias1&quot; &#125; &#125;, &#123; &quot;add&quot; : &#123; &quot;index&quot; : &quot;test2&quot;, &quot;alias&quot; : &quot;alias1&quot; &#125; &#125; ]&#125;你也可以通过如下的方式，通过一个add命令来完成：123456POST /_aliases&#123; &quot;actions&quot; : [ &#123; &quot;add&quot; : &#123; &quot;indices&quot; : [&quot;test1&quot;, &quot;test2&quot;], &quot;alias&quot; : &quot;alias1&quot; &#125; &#125; ]&#125;甚至：123456POST /_aliases&#123; &quot;actions&quot; : [ &#123; &quot;add&quot; : &#123; &quot;index&quot; : &quot;test*&quot;, &quot;alias&quot; : &quot;all_test_indices&quot; &#125; &#125; ]&#125;这样所有以test*为开头的索引都共同一个别名。当我们index我们的文档时，对一个指向多个index的别名进行索引是错误的。也可以在一个操作中使用别名交换索引：123456789PUT test PUT test_2 POST /_aliases&#123; &quot;actions&quot; : [ &#123; &quot;add&quot;: &#123; &quot;index&quot;: &quot;test_2&quot;, &quot;alias&quot;: &quot;test&quot; &#125; &#125;, &#123; &quot;remove_index&quot;: &#123; &quot;index&quot;: &quot;test&quot; &#125; &#125; ]&#125;在上面的例子中，假如我们地添加了一个叫做test的index，而test_2是我们想要的。我们直接可以通过上面的方法吧test中的数据交换到test_2中，并同时把test索引删除。Filtered alias带有过滤器的别名提供了一种创建同一索引的不同“视图”的简便方法。 可以使用Query DSL定义过滤器，并使用此别名将其应用于所有“搜索”，“计数”，“按查询删除”和“更多此类操作”。要创建过滤后的别名，首先我们需要确保映射中已存在这些字段：12345678910PUT /test1&#123; &quot;mappings&quot;: &#123; &quot;properties&quot;: &#123; &quot;user&quot; : &#123; &quot;type&quot;: &quot;keyword&quot; &#125; &#125; &#125;&#125;现在我们可以利用filter来创建一个alias，是基于user字段123456789101112POST /_aliases&#123; &quot;actions&quot; : [ &#123; &quot;add&quot; : &#123; &quot;index&quot; : &quot;test1&quot;, &quot;alias&quot; : &quot;alias2&quot;, &quot;filter&quot; : &#123; &quot;term&quot; : &#123; &quot;user&quot; : &quot;kimchy&quot; &#125; &#125; &#125; &#125; ]&#125;Write index可以将别名指向的索引关联为write索引。 指定后，针对指向多个索引的别名的所有索引和更新请求将尝试解析为write索引的一个索引。 每个别名只能将一个索引分配为一次write索引。 如果未指定write索引且别名引用了多个索引，则不允许写入。可以使用别名API和索引创建API将与别名关联的索引指定为write索引。123456789101112131415161718POST /_aliases&#123; &quot;actions&quot; : [ &#123; &quot;add&quot; : &#123; &quot;index&quot; : &quot;test&quot;, &quot;alias&quot; : &quot;alias1&quot;, &quot;is_write_index&quot; : true &#125; &#125;, &#123; &quot;add&quot; : &#123; &quot;index&quot; : &quot;test2&quot;, &quot;alias&quot; : &quot;alias1&quot; &#125; &#125; ]&#125;在这里，我们定义了alias1同时指向test及test2两个索引。其中test中，注明了is_write_index，那么，如下的操作：1234PUT /alias1/_doc/1&#123; &quot;foo&quot;: &quot;bar&quot;&#125;相当于如下的操作：1PUT /test/_doc/1也就是写入到test索引中，而不会写入到test2中。要交换哪个索引是别名的写入索引，可以利用别名API进行原子交换。 交换不依赖于操作的顺序。123456789101112131415161718POST /_aliases&#123; &quot;actions&quot; : [ &#123; &quot;add&quot; : &#123; &quot;index&quot; : &quot;test&quot;, &quot;alias&quot; : &quot;alias1&quot;, &quot;is_write_index&quot; : false &#125; &#125;, &#123; &quot;add&quot; : &#123; &quot;index&quot; : &quot;test2&quot;, &quot;alias&quot; : &quot;alias1&quot;, &quot;is_write_index&quot; : true &#125; &#125; ]&#125;参考：【1】https://www.elastic.co/guide/en/elasticsearch/reference/7.3/indices-aliases.html【2】https://www.elastic.co/guide/en/elasticsearch/reference/7.3/indices-get-alias.html【3】https://www.elastic.co/guide/en/elasticsearch/reference/7.3/indices-add-alias.html]]></content>
      <categories>
        <category>elk</category>
      </categories>
      <tags>
        <tag>elk</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[lasticsearch：IK中文分词器]]></title>
    <url>%2F2020%2F01%2F02%2Flasticsearch%EF%BC%9AIK%E4%B8%AD%E6%96%87%E5%88%86%E8%AF%8D%E5%99%A8%2F</url>
    <content type="text"><![CDATA[Elasticsearch内置的分词器对中文不友好，只会一个字一个字的分，无法形成词语，比如：12345POST /_analyze&#123; &quot;text&quot;: &quot;我爱北京天安门&quot;, &quot;analyzer&quot;: &quot;standard&quot;&#125;如果我们使用的是standard的分词器，那么结果就是：1234567891011121314151617181920212223242526&#123; &quot;tokens&quot; : [ &#123; &quot;token&quot; : &quot;我&quot;, &quot;start_offset&quot; : 0, &quot;end_offset&quot; : 1, &quot;type&quot; : &quot;&lt;IDEOGRAPHIC&gt;&quot;, &quot;position&quot; : 0 &#125;, &#123; &quot;token&quot; : &quot;爱&quot;, &quot;start_offset&quot; : 1, &quot;end_offset&quot; : 2, &quot;type&quot; : &quot;&lt;IDEOGRAPHIC&gt;&quot;, &quot;position&quot; : 1 &#125;, ... &#123; &quot;token&quot; : &quot;门&quot;, &quot;start_offset&quot; : 6, &quot;end_offset&quot; : 7, &quot;type&quot; : &quot;&lt;IDEOGRAPHIC&gt;&quot;, &quot;position&quot; : 6 &#125; ]&#125;显然这对中文来说并不友好，它显示的每一个汉字。好在Elastic的大拿medcl已经为我们做好IK中文分词器。下面我们来详细介绍如何安装并使用中文分词器。具体的安装步骤可以在地址https://github.com/medcl/elasticsearch-analysis-ik找到。安装首先，我们可以到如下的地址查看一下是否有最新的版本对应你的Elasticsearch的发行版：https://github.com/medcl/elasticsearch-analysis-ik/releases到目前截止日期，我们可以看到有最新的v7.3.1发行版。那么，我们直接进入到我们的Elasticsearch的安装目录下，并打入如下的命令：1./bin/elasticsearch-plugin nstall https://github.com/medcl/elasticsearch-analysis-ik/releases/download/v7.3.1/elasticsearch-analysis-ik-7.3.1.zip替代上面的7.3.1安装你自己想要的版本：安装好后，我们可以通过如下的命令来检查是否已经安装好：12localhost:elasticsearch-7.3.0 liuxg$ ./bin/elasticsearch-plugin listanalysis-ik上面的命令显示我们的IK已经安装成功了。这个时候需要我们重新启动一下我们的Elasticsearch，以便这个plugin能装被加载。使用IK分词器首先我们创建一个index:1PUT chinese接下来，我们来为这个index 创建一个mapping12345678910PUT /chinese/_mapping&#123; &quot;properties&quot;: &#123; &quot;content&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;analyzer&quot;: &quot;ik_max_word&quot;, &quot;search_analyzer&quot;: &quot;ik_smart&quot; &#125; &#125;&#125;运行上面的命令后，如果出现如下的信息：123&#123; &quot;acknowledged&quot; : true&#125;它表明我们的安装时成功的。接下来，我们来index一些文档：12345GET /chinese/_analyze&#123; &quot;text&quot;: &quot;我爱北京天安门&quot;, &quot;analyzer&quot;: &quot;ik_max_word&quot;&#125;显示的结果为：12345678910111213141516171819202122232425262728293031323334353637383940414243444546&#123; &quot;tokens&quot; : [ &#123; &quot;token&quot; : &quot;我&quot;, &quot;start_offset&quot; : 0, &quot;end_offset&quot; : 1, &quot;type&quot; : &quot;CN_CHAR&quot;, &quot;position&quot; : 0 &#125;, &#123; &quot;token&quot; : &quot;爱&quot;, &quot;start_offset&quot; : 1, &quot;end_offset&quot; : 2, &quot;type&quot; : &quot;CN_CHAR&quot;, &quot;position&quot; : 1 &#125;, &#123; &quot;token&quot; : &quot;北京&quot;, &quot;start_offset&quot; : 2, &quot;end_offset&quot; : 4, &quot;type&quot; : &quot;CN_WORD&quot;, &quot;position&quot; : 2 &#125;, &#123; &quot;token&quot; : &quot;天安门&quot;, &quot;start_offset&quot; : 4, &quot;end_offset&quot; : 7, &quot;type&quot; : &quot;CN_WORD&quot;, &quot;position&quot; : 3 &#125;, &#123; &quot;token&quot; : &quot;天安&quot;, &quot;start_offset&quot; : 4, &quot;end_offset&quot; : 6, &quot;type&quot; : &quot;CN_WORD&quot;, &quot;position&quot; : 4 &#125;, &#123; &quot;token&quot; : &quot;门&quot;, &quot;start_offset&quot; : 6, &quot;end_offset&quot; : 7, &quot;type&quot; : &quot;CN_CHAR&quot;, &quot;position&quot; : 5 &#125; ]&#125;从上面的结果我们可以看出来，在我们的token中显示“北京”，“天安”及“天安门”。这个和我们之前的是不一样的。下面，我们输入两个文档：123456789PUT /chinese/_doc/1&#123; &quot;content&quot;:&quot;我爱北京天安门&quot;&#125; PUT /chinese/_doc/2&#123; &quot;content&quot;: &quot;北京，你好&quot;&#125;那么我们可以，通过如下的方式来进行搜索：12345678GET /chinese/_search&#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;content&quot;: &quot;北京&quot; &#125; &#125;&#125;我们显示的结果是：12345678910111213141516171819202122232425262728293031323334353637&#123; &quot;took&quot; : 1, &quot;timed_out&quot; : false, &quot;_shards&quot; : &#123; &quot;total&quot; : 1, &quot;successful&quot; : 1, &quot;skipped&quot; : 0, &quot;failed&quot; : 0 &#125;, &quot;hits&quot; : &#123; &quot;total&quot; : &#123; &quot;value&quot; : 2, &quot;relation&quot; : &quot;eq&quot; &#125;, &quot;max_score&quot; : 0.15965709, &quot;hits&quot; : [ &#123; &quot;_index&quot; : &quot;chinese&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;2&quot;, &quot;_score&quot; : 0.15965709, &quot;_source&quot; : &#123; &quot;content&quot; : &quot;北京，你好&quot; &#125; &#125;, &#123; &quot;_index&quot; : &quot;chinese&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;1&quot;, &quot;_score&quot; : 0.100605845, &quot;_source&quot; : &#123; &quot;content&quot; : &quot;我爱北京天安门&quot; &#125; &#125; ] &#125;&#125;因为两个文档里都含有“北京”，我们可以看出来两个文档都被显示出来了。我们同时做另外一个搜索：12345678GET /chinese/_search&#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;content&quot;: &quot;天安门&quot; &#125; &#125;&#125;那么显示的结果是：12345678910111213141516171819202122232425262728&#123; &quot;took&quot; : 0, &quot;timed_out&quot; : false, &quot;_shards&quot; : &#123; &quot;total&quot; : 1, &quot;successful&quot; : 1, &quot;skipped&quot; : 0, &quot;failed&quot; : 0 &#125;, &quot;hits&quot; : &#123; &quot;total&quot; : &#123; &quot;value&quot; : 1, &quot;relation&quot; : &quot;eq&quot; &#125;, &quot;max_score&quot; : 0.73898095, &quot;hits&quot; : [ &#123; &quot;_index&quot; : &quot;chinese&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;1&quot;, &quot;_score&quot; : 0.73898095, &quot;_source&quot; : &#123; &quot;content&quot; : &quot;我爱北京天安门&quot; &#125; &#125; ] &#125;&#125;因为“天安门”只出现在第二个文档里，所以，我们可以看出来只有一个结果。我们也同时做另外一个搜索：12345678GET /chinese/_search&#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;content&quot;: &quot;北京天安门&quot; &#125; &#125;&#125;在这里，我们来搜索“北京天安门”。请注意我们在mapping中使用了1&quot;search_analyzer&quot;: &quot;ik_smart&quot;也就是说，search_analyzer会把我们的“北京天安门”，分解成两个词“北京”及“天安门”。这两个词将被用于搜索。通常对于match来说是OR关系，也就是说只要匹配到“北京”或“天安门”，这两个之中的任何一个，那么就是匹配：12345678910111213141516171819202122232425262728293031323334353637&#123; &quot;took&quot; : 3, &quot;timed_out&quot; : false, &quot;_shards&quot; : &#123; &quot;total&quot; : 1, &quot;successful&quot; : 1, &quot;skipped&quot; : 0, &quot;failed&quot; : 0 &#125;, &quot;hits&quot; : &#123; &quot;total&quot; : &#123; &quot;value&quot; : 2, &quot;relation&quot; : &quot;eq&quot; &#125;, &quot;max_score&quot; : 0.7268042, &quot;hits&quot; : [ &#123; &quot;_index&quot; : &quot;chinese&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;1&quot;, &quot;_score&quot; : 0.7268042, &quot;_source&quot; : &#123; &quot;content&quot; : &quot;我爱北京天安门&quot; &#125; &#125;, &#123; &quot;_index&quot; : &quot;chinese&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;2&quot;, &quot;_score&quot; : 0.22920427, &quot;_source&quot; : &#123; &quot;content&quot; : &quot;北京，你好&quot; &#125; &#125; ] &#125;&#125;上面显示的结果显示“我爱北京天安门”是最贴切的搜索结果。参考：【1】https://github.com/medcl/elasticsearch-analysis-ik]]></content>
      <categories>
        <category>elk</category>
      </categories>
      <tags>
        <tag>elk</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[lasticsearch：ICU分词器介绍]]></title>
    <url>%2F2020%2F01%2F02%2Flasticsearch%EF%BC%9AICU%E5%88%86%E8%AF%8D%E5%99%A8%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[ICU Analysis插件是一组将Lucene ICU模块集成到Elasticsearch中的库。 本质上，ICU的目的是增加对Unicode和全球化的支持，以提供对亚洲语言更好的文本分割分析。 从Elasticsearch的角度来看，此插件提供了文本分析中的新组件，如下表所示:安装我们可以首先到Elasticsearch的安装目录打入如下的命令：123456$ pwd/Users/liuxg/elastic/elasticsearch-7.3.0(base) localhost:elasticsearch-7.3.0 liuxg$ ./bin/elasticsearch-plugin listanalysis-icuanalysis-ikpinyin上面显示我已经安装好了三个插件。上面的analysis-ik及pinyin都是为中文而准备的。注意：如果你们在使用上面的elasticsearch-plug list命名出现如下的错误的话：那么请使用如下的命令来删除在当前目录下的.DS_Store目录：1sudo find /Path/to/your/elasticsearch-folder -name &quot;.DS_Store&quot; -depth -exec rm &#123;&#125; \;然后重新运行上面的命令就不会有问题了。上面显示我已经安装好了。如果在你的电脑里没有安装好，可以使用如下的命令来进行安装：1./bin/elasticsearch-plugin install analysis-icu上面的命令在Elasticsearch的安装目录里进行运行。等安装好后，我们需要重新启动Elasticsearch让它起作用。重新运行：1./bin/elasticsearch-plugin list来检查analysis-icu是否已经被成功安装好了。例子等我们完全安装好了analysis_icu，那么，我们可以使用如下的例子在Kibana中来做一个实验：12345POST _analyze &#123; &quot;text&quot;: &quot;我爱北京天安门&quot;, &quot;analyzer&quot;: &quot;icu_analyzer&quot;&#125;那么显示的结果是：上面显示，我们analysis可以正确地帮我们把中文词语安装中文的分词方法正确地进行分词。我们可以和standard分词器来进行一个比较：我们从上面可以看出来，在默认的情况下，icu_analyzer通常是一个及以上的字符的token，而standard的analyzer只有一个字符。通过更改字符过滤器和token的方法和模式参数，ICU分析器可以具有多种自定义变量类型。 下表描述了不同类型的ICU分析仪的组合：让我们尝试nfkd_normalized分析器。 遵循定义并在Kibana Dev Tools控制台中对其进行测试。 响应显示在以下屏幕截图中。 但是，由于使用nfkd_normalized分析器和icu_analyzer分析器，我们无法在结果中找到任何差异：123456POST _analyze &#123; &quot;text&quot;: &quot;股市投资稳赚不赔必修课：如何做好仓位管理和情绪管理&quot;, &quot;char_filter&quot;: [&#123;&quot;type&quot;: &quot;icu_normalizer&quot;, &quot;name&quot;: &quot;nfkc&quot;, &quot;mode&quot;:&quot;decompose&quot;&#125;], &quot;tokenizer&quot;: &quot;icu_tokenizer&quot;&#125;运行结果：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109&#123; &quot;tokens&quot; : [ &#123; &quot;token&quot; : &quot;股市&quot;, &quot;start_offset&quot; : 0, &quot;end_offset&quot; : 2, &quot;type&quot; : &quot;&lt;IDEOGRAPHIC&gt;&quot;, &quot;position&quot; : 0 &#125;, &#123; &quot;token&quot; : &quot;投资&quot;, &quot;start_offset&quot; : 2, &quot;end_offset&quot; : 4, &quot;type&quot; : &quot;&lt;IDEOGRAPHIC&gt;&quot;, &quot;position&quot; : 1 &#125;, &#123; &quot;token&quot; : &quot;稳赚&quot;, &quot;start_offset&quot; : 4, &quot;end_offset&quot; : 6, &quot;type&quot; : &quot;&lt;IDEOGRAPHIC&gt;&quot;, &quot;position&quot; : 2 &#125;, &#123; &quot;token&quot; : &quot;不&quot;, &quot;start_offset&quot; : 6, &quot;end_offset&quot; : 7, &quot;type&quot; : &quot;&lt;IDEOGRAPHIC&gt;&quot;, &quot;position&quot; : 3 &#125;, &#123; &quot;token&quot; : &quot;赔&quot;, &quot;start_offset&quot; : 7, &quot;end_offset&quot; : 8, &quot;type&quot; : &quot;&lt;IDEOGRAPHIC&gt;&quot;, &quot;position&quot; : 4 &#125;, &#123; &quot;token&quot; : &quot;必修&quot;, &quot;start_offset&quot; : 8, &quot;end_offset&quot; : 10, &quot;type&quot; : &quot;&lt;IDEOGRAPHIC&gt;&quot;, &quot;position&quot; : 5 &#125;, &#123; &quot;token&quot; : &quot;课&quot;, &quot;start_offset&quot; : 10, &quot;end_offset&quot; : 11, &quot;type&quot; : &quot;&lt;IDEOGRAPHIC&gt;&quot;, &quot;position&quot; : 6 &#125;, &#123; &quot;token&quot; : &quot;如何&quot;, &quot;start_offset&quot; : 12, &quot;end_offset&quot; : 14, &quot;type&quot; : &quot;&lt;IDEOGRAPHIC&gt;&quot;, &quot;position&quot; : 7 &#125;, &#123; &quot;token&quot; : &quot;做好&quot;, &quot;start_offset&quot; : 14, &quot;end_offset&quot; : 16, &quot;type&quot; : &quot;&lt;IDEOGRAPHIC&gt;&quot;, &quot;position&quot; : 8 &#125;, &#123; &quot;token&quot; : &quot;仓&quot;, &quot;start_offset&quot; : 16, &quot;end_offset&quot; : 17, &quot;type&quot; : &quot;&lt;IDEOGRAPHIC&gt;&quot;, &quot;position&quot; : 9 &#125;, &#123; &quot;token&quot; : &quot;位&quot;, &quot;start_offset&quot; : 17, &quot;end_offset&quot; : 18, &quot;type&quot; : &quot;&lt;IDEOGRAPHIC&gt;&quot;, &quot;position&quot; : 10 &#125;, &#123; &quot;token&quot; : &quot;管理&quot;, &quot;start_offset&quot; : 18, &quot;end_offset&quot; : 20, &quot;type&quot; : &quot;&lt;IDEOGRAPHIC&gt;&quot;, &quot;position&quot; : 11 &#125;, &#123; &quot;token&quot; : &quot;和&quot;, &quot;start_offset&quot; : 20, &quot;end_offset&quot; : 21, &quot;type&quot; : &quot;&lt;IDEOGRAPHIC&gt;&quot;, &quot;position&quot; : 12 &#125;, &#123; &quot;token&quot; : &quot;情绪&quot;, &quot;start_offset&quot; : 21, &quot;end_offset&quot; : 23, &quot;type&quot; : &quot;&lt;IDEOGRAPHIC&gt;&quot;, &quot;position&quot; : 13 &#125;, &#123; &quot;token&quot; : &quot;管理&quot;, &quot;start_offset&quot; : 23, &quot;end_offset&quot; : 25, &quot;type&quot; : &quot;&lt;IDEOGRAPHIC&gt;&quot;, &quot;position&quot; : 14 &#125; ]&#125;要使用新定义的分析器，我们必须在Index setting中对其进行定义。请参阅我之前的文章“Elasticsearch: analyzer”。]]></content>
      <categories>
        <category>elk</category>
      </categories>
      <tags>
        <tag>elk</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[lasticsearch：fuzzy 搜索(模糊搜索)]]></title>
    <url>%2F2020%2F01%2F02%2Flasticsearch%EF%BC%9Afuzzy%20%E6%90%9C%E7%B4%A2%20%EF%BC%88%E6%A8%A1%E7%B3%8A%E6%90%9C%E7%B4%A2)%2F</url>
    <content type="text"><![CDATA[在实际的搜索中，我们有时候会打错字，从而导致搜索不到。在Elasticsearch中，我们可以使用fuzziness属性来进行模糊查询，从而达到搜索有错别字的情形。match查询具有“fuziness”属性。它可以被设置为“0”， “1”， “2”或“auto”。“auto”是推荐的选项，它会根据查询词的长度定义距离。Fuzzy query返回包含与搜索词相似的词的文档，以Levenshtein编辑距离测量。编辑距离是将一个术语转换为另一个术语所需的一个字符更改的次数。 这些更改可以包括：更改字符（box→fox）删除字符（black→lack）插入字符（sic→sick）转置两个相邻字符（act→cat）为了找到相似的词，模糊查询会在指定的编辑距离内创建搜索词的所有可能变化或扩展的集合。 查询然后返回每个扩展的完全匹配。例子我们首先输入如下的一个文档到fuzzyindex索引中：1234PUT fuzzyindex/_doc/1&#123; &quot;content&quot;: &quot;I like blue sky&quot;&#125;如果这个时候，我们进行如下的搜索：12345678GET fuzzyindex/_search&#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;content&quot;: &quot;ski&quot; &#125; &#125;&#125;那么是没有任何被搜索到的结果，这是因为“I like blue sky” 里分词后没有ski这个词。123456789101112131415161718&#123; &quot;took&quot; : 0, &quot;timed_out&quot; : false, &quot;_shards&quot; : &#123; &quot;total&quot; : 1, &quot;successful&quot; : 1, &quot;skipped&quot; : 0, &quot;failed&quot; : 0 &#125;, &quot;hits&quot; : &#123; &quot;total&quot; : &#123; &quot;value&quot; : 0, &quot;relation&quot; : &quot;eq&quot; &#125;, &quot;max_score&quot; : null, &quot;hits&quot; : [ ] &#125;&#125;这个时候，如果我们使用如下的搜索：1234567891011GET fuzzyindex/_search&#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;content&quot;: &#123; &quot;query&quot;: &quot;ski&quot;, &quot;fuzziness&quot;: &quot;1&quot; &#125; &#125; &#125;&#125;那么显示的结果是：12345678910111213141516171819202122232425262728&#123; &quot;took&quot; : 18, &quot;timed_out&quot; : false, &quot;_shards&quot; : &#123; &quot;total&quot; : 1, &quot;successful&quot; : 1, &quot;skipped&quot; : 0, &quot;failed&quot; : 0 &#125;, &quot;hits&quot; : &#123; &quot;total&quot; : &#123; &quot;value&quot; : 1, &quot;relation&quot; : &quot;eq&quot; &#125;, &quot;max_score&quot; : 0.19178805, &quot;hits&quot; : [ &#123; &quot;_index&quot; : &quot;fuzzyindex&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;1&quot;, &quot;_score&quot; : 0.19178805, &quot;_source&quot; : &#123; &quot;content&quot; : &quot;I like blue sky&quot; &#125; &#125; ] &#125;&#125;显然是找到我们需要的结果了。这是因为sky和ski时间上是只差别一个字母。同样，如果我们选用“auto”选项看看：1234567891011GET fuzzyindex/_search&#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;content&quot;: &#123; &quot;query&quot;: &quot;ski&quot;, &quot;fuzziness&quot;: &quot;auto&quot; &#125; &#125; &#125;&#125;它显示的结果和上面的是一样的。也可以进行匹配。如果我们进行如下的匹配：1234567891011GET fuzzyindex/_search&#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;content&quot;: &#123; &quot;query&quot;: &quot;bxxe&quot;, &quot;fuzziness&quot;: &quot;auto&quot; &#125; &#125; &#125;&#125;那么它不能匹配任何的结果，但是，如果我们进行如下的搜索：1234567891011GET fuzzyindex/_search&#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;content&quot;: &#123; &quot;query&quot;: &quot;bxxe&quot;, &quot;fuzziness&quot;: &quot;2&quot; &#125; &#125; &#125;&#125;我们也可以使用如下的格式：1234567891011GET /_search&#123; &quot;query&quot;: &#123; &quot;fuzzy&quot;: &#123; &quot;content&quot;: &#123; &quot;value&quot;: &quot;bxxe&quot;, &quot;fuzziness&quot;: &quot;2&quot; &#125; &#125; &#125;&#125;那么它可以显示搜索的结果，这是因为我们能够容许两个编辑的错误。模糊性是拼写错误的简单解决方案，但具有很高的CPU开销和非常低的精度。参考：【1】https://www.elastic.co/guide/en/elasticsearch/reference/7.4/query-dsl-fuzzy-query.html]]></content>
      <categories>
        <category>elk</category>
      </categories>
      <tags>
        <tag>elk</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch：运用shard filtering来控制索引分配给哪个节点]]></title>
    <url>%2F2020%2F01%2F02%2FElasticsearch%EF%BC%9A%E8%BF%90%E7%94%A8shard%20filtering%E6%9D%A5%E6%8E%A7%E5%88%B6%E7%B4%A2%E5%BC%95%E5%88%86%E9%85%8D%E7%BB%99%E5%93%AA%E4%B8%AA%E8%8A%82%E7%82%B9%2F</url>
    <content type="text"><![CDATA[在我们的实际部署中，我们的各个node（节点）的能力是不一样的。比如有的节点的计算能力比较强，而且配有高性能的存储，速度也比较快，同时我们可能有一些node的能力稍微差一点，比如计算能力及存储器的速度都比较差一点。针对这两种情况，我们其实可以把这两种节点用来做不同的用途：运算能力较强的节点可以用来做indexing（建立索引表格）的工作，而那些能力较差一点的节点，我们可以用来做搜索用途。我们可以把这两种节点分别叫做：hot node：用于支持索引并写入新文档warm node：用于处理不太频繁查询的只读索引这种架构在Elasticsearch中，我们称之为hot/warm架构。Hot node我们可以使用hot node来做indexing：indexing是CPU和IO的密集操作，因此热节点应该是功能强大的服务器比warm node更快的存储Warm node对较旧的只读索引使用热节点：倾向于利用大型附加磁盘（通常是旋转磁盘）大量数据可能需要其他节点才能满足性能要求Shard filteringShard filtering在Elasticsearch中，我们可以利用这个能力来把我们想要的index放入到我们想要的node里。我们可以使用在elasticsearch.yml配置文件中的：node.attr来指定我们node属性：hot或是warm。在index的settings里通过index.routing.allocation来指定索引（index)到一个满足要求的node为节点分配索引有三种规则：就像上面的表格说明的一样：include指的是至少包含其中的一个值；exclude指的是不包含任何值；require指的是必须包含里面索引的值。这些值实际上我们用来标识node的tag。针对自己的配置这些tag可以由厂商自己标识。标识node在上面的图中，我们标识my_temp属性为hot或是warm，表明我们的cluster中分为两类：hot或是warm。在这里特别指出：这里的my_temp，hot及warm都是我们任意取的可以让我们记住的属性及名称。只要在使用时和index.routing.allocation.include index.routing.allocation.exclude及index.routing.allocation.require中的值相对应即可。配置index的settings我们可以通过配置在Index中的settings来分配我们的index到相应的具有哪些属性的node里，比如：123456PUT logs-2019-03&#123; &quot;settings&quot;: &#123; &quot;index.routing.allocation.require.my_temp&quot;: &quot;hot&quot; &#125;&#125;在上面我们通过logs-2019-03的这个index的settings来控制这个index必须分配到具有hot属性的node里。假如我们上面的index logs-2019-03由于一些原因不再是当前的用来做indexing的index，比如我们可以通过rollover API接口来自动滚动我们的index名字。我们可以通过如下的命令把该index移动到warm node里：123456PUT logs-2019-03&#123; &quot;settings&quot;: &#123; &quot;index.routing.allocation.require.my_temp&quot;: &quot;warm&quot; &#125;&#125;这样Elasticsearch会自动帮我们把logs-2019-03索引移动到warm node中，以便直供搜索之用。例子首先，我们我们按照如下的方式来做一个实验，虽然不能应用于实际的生产环境中：按照“如何在Linux，MacOS及Windows上进行安装Elasticsearch”安装好自己的Elasticsearh，但是不要运行Elasticsearch按照“如何在Linux及MacOS上安装Elastic栈中的Kibana”安装好自己的Kibana在我们完成上面的两个安装后，我们分别打开两个terminal，然后分别在两个terminal中运行如下的指令：1./bin/elasticsearch -E node.name=node1 -E node.attr.data=hot -Enode.max_local_storage_nodes=2上面的指令运行一个名字叫做node1的，data属性为hot的node。1./bin/elasticsearch -E node.name=node2 -E node.attr.data=warm -Enode.max_local_storage_nodes=2上面的指令运行一个名字叫做node2的，data属性为hot的warm。我们可以在Kibana里查看我们的nodes：我们可以看出来有两个node正在运行：node1及node2。如果我们想了解这两个node的更多属性，我们可以打入如下的命令：1GET _cat/nodeattrs?v&amp;s=name显示的结果为：我们可以看到node被标识为hot node，而node2被标识为warm node。接下来，我们运用我们上面命令来把我们的logs-2019-03置于我们的hot node里。我们可以通过如下的命令：12345678PUT logs-2019-03&#123; &quot;settings&quot;: &#123; &quot;number_of_shards&quot;: 1, &quot;number_of_replicas&quot;: 0, &quot;index.routing.allocation.require.data&quot;: &quot;hot&quot; &#125;&#125;运行上面的结果后，可以通过如下的命令来查看：1GET _cat/shards/logs-*?v&amp;h=index,shard,prirep,state,node&amp;s=index,shard,prirep显示的结果为：从上面我们可以看出来我们的logs-2019-03是分配到node1上面的。假如我们由于某种原因，想把logs-2019-03分配到node2上面，那么该怎么做呢？我们可以通过如下的命令来实现：1234PUT logs-2019-03/_settings&#123; &quot;index.routing.allocation.require.data&quot;: &quot;warm&quot;&#125;运行上面的指令显示的结果是：显然我们logs-2019-03已经成功地移到node2了。针对硬件的shard filtering上面我们说了，对于node.attr来说，我们可以添加任意的属性。在上面的我们已经使用hot/warm来标识我们的my_temp属性。其实我们也可以同时定义一些能标识硬件的属性my_server，这个属性值可以为small，medium及large。有多个属性组成的集群就像是如下的结构：那么这样的集群里的每个node可能具有不同的属性。我们可以通过如下的方法来分配index到同时具有两个或以上属性的node里:123456789PUT my_index1 &#123; &quot;settings&quot;: &#123; &quot;number_of_shards&quot;: 2, &quot;number_of_replicas&quot;: 1, &quot;index.routing.allocation.include.my_server&quot;: &quot;medium&quot;, &quot;index.routing.allocation.require.my_temp&quot;: &quot;hot&quot; &#125;&#125;如上所示，我们把我们的my_index1分配到这么一个node：这个node必须具有hot属性，同时也具有medium的属性。针对我们上面显示的图片，只有node1满足我们的要求。总结：在今天的这篇文章中，我们介绍了如何使用shard filtering来控制我们的index的分配。在实际的操作中，可能大家会觉得麻烦一点，因为这个比较需要我们自己来管理这个。这个技术可以和我之前的文章“Elasticsearch: rollover API”一起配合使用。Elasticsearch实际已经帮我做好了。在接下来的文章里，我会来介绍如何使用Index life cycle policy来自动管理我们的Index。]]></content>
      <categories>
        <category>elk</category>
      </categories>
      <tags>
        <tag>elk</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch：运用search_after来进行深度分页]]></title>
    <url>%2F2020%2F01%2F02%2FElasticsearch%EF%BC%9A%E8%BF%90%E7%94%A8search_after%E6%9D%A5%E8%BF%9B%E8%A1%8C%E6%B7%B1%E5%BA%A6%E5%88%86%E9%A1%B5%2F</url>
    <content type="text"><![CDATA[在上一篇文章 “Elasticsearch：运用scroll接口对大量数据实现更好的分页”，我们讲述了如何运用scroll接口来对大量数据来进行有效地分页。在那篇文章中，我们讲述了两种方法：from加上size的方法来进行分页运用scroll接口来进行分页对于大量的数据而言，我们尽量避免使用from+size这种方法。这里的原因是index.max_result_window的默认值是10K，也就是说from+size的最大值是1万。搜索请求占用堆内存和时间与from+size成比例，这限制了内存。假如你想hit从990到1000，那么每个shard至少需要1000个文档：为了避免过度使得我们的cluster繁忙，通常Scroll接口被推荐作为深层次的scrolling，但是因为维护scroll上下文也是非常昂贵的，所以这种方法不推荐作为实时用户请求。search_after参数通过提供实时cursor来解决此问题。 我们的想法是使用上一页的结果来帮助检索下一页。我们先输入如下的文档到twitter索引中：12345678910111213POST _bulk&#123; &quot;index&quot; : &#123; &quot;_index&quot; : &quot;twitter&quot;, &quot;_id&quot;: 1&#125; &#125;&#123;&quot;user&quot;:&quot;双榆树-张三&quot;, &quot;DOB&quot;:&quot;1980-01-01&quot;, &quot;message&quot;:&quot;今儿天气不错啊，出去转转去&quot;,&quot;uid&quot;:2,&quot;age&quot;:20,&quot;city&quot;:&quot;北京&quot;,&quot;province&quot;:&quot;北京&quot;,&quot;country&quot;:&quot;中国&quot;,&quot;address&quot;:&quot;中国北京市海淀区&quot;,&quot;location&quot;:&#123;&quot;lat&quot;:&quot;39.970718&quot;,&quot;lon&quot;:&quot;116.325747&quot;&#125;&#125;&#123; &quot;index&quot; : &#123; &quot;_index&quot; : &quot;twitter&quot;, &quot;_id&quot;: 2 &#125;&#125;&#123;&quot;user&quot;:&quot;东城区-老刘&quot;, &quot;DOB&quot;:&quot;1981-01-01&quot;, &quot;message&quot;:&quot;出发，下一站云南！&quot;,&quot;uid&quot;:3,&quot;age&quot;:30,&quot;city&quot;:&quot;北京&quot;,&quot;province&quot;:&quot;北京&quot;,&quot;country&quot;:&quot;中国&quot;,&quot;address&quot;:&quot;中国北京市东城区台基厂三条3号&quot;,&quot;location&quot;:&#123;&quot;lat&quot;:&quot;39.904313&quot;,&quot;lon&quot;:&quot;116.412754&quot;&#125;&#125;&#123; &quot;index&quot; : &#123; &quot;_index&quot; : &quot;twitter&quot;, &quot;_id&quot;: 3&#125; &#125;&#123;&quot;user&quot;:&quot;东城区-李四&quot;, &quot;DOB&quot;:&quot;1982-01-01&quot;, &quot;message&quot;:&quot;happy birthday!&quot;,&quot;uid&quot;:4,&quot;age&quot;:30,&quot;city&quot;:&quot;北京&quot;,&quot;province&quot;:&quot;北京&quot;,&quot;country&quot;:&quot;中国&quot;,&quot;address&quot;:&quot;中国北京市东城区&quot;,&quot;location&quot;:&#123;&quot;lat&quot;:&quot;39.893801&quot;,&quot;lon&quot;:&quot;116.408986&quot;&#125;&#125;&#123; &quot;index&quot; : &#123; &quot;_index&quot; : &quot;twitter&quot;, &quot;_id&quot;: 4&#125; &#125;&#123;&quot;user&quot;:&quot;朝阳区-老贾&quot;,&quot;DOB&quot;:&quot;1983-01-01&quot;, &quot;message&quot;:&quot;123,gogogo&quot;,&quot;uid&quot;:5,&quot;age&quot;:35,&quot;city&quot;:&quot;北京&quot;,&quot;province&quot;:&quot;北京&quot;,&quot;country&quot;:&quot;中国&quot;,&quot;address&quot;:&quot;中国北京市朝阳区建国门&quot;,&quot;location&quot;:&#123;&quot;lat&quot;:&quot;39.718256&quot;,&quot;lon&quot;:&quot;116.367910&quot;&#125;&#125;&#123; &quot;index&quot; : &#123; &quot;_index&quot; : &quot;twitter&quot;, &quot;_id&quot;: 5&#125; &#125;&#123;&quot;user&quot;:&quot;朝阳区-老王&quot;,&quot;DOB&quot;:&quot;1984-01-01&quot;, &quot;message&quot;:&quot;Happy BirthDay My Friend!&quot;,&quot;uid&quot;:6,&quot;age&quot;:50,&quot;city&quot;:&quot;北京&quot;,&quot;province&quot;:&quot;北京&quot;,&quot;country&quot;:&quot;中国&quot;,&quot;address&quot;:&quot;中国北京市朝阳区国贸&quot;,&quot;location&quot;:&#123;&quot;lat&quot;:&quot;39.918256&quot;,&quot;lon&quot;:&quot;116.467910&quot;&#125;&#125;&#123; &quot;index&quot; : &#123; &quot;_index&quot; : &quot;twitter&quot;, &quot;_id&quot;: 6&#125; &#125;&#123;&quot;user&quot;:&quot;虹桥-老吴&quot;, &quot;DOB&quot;:&quot;1985-01-01&quot;, &quot;message&quot;:&quot;好友来了都今天我生日，好友来了,什么 birthday happy 就成!&quot;,&quot;uid&quot;:7,&quot;age&quot;:90,&quot;city&quot;:&quot;上海&quot;,&quot;province&quot;:&quot;上海&quot;,&quot;country&quot;:&quot;中国&quot;,&quot;address&quot;:&quot;中国上海市闵行区&quot;,&quot;location&quot;:&#123;&quot;lat&quot;:&quot;31.175927&quot;,&quot;lon&quot;:&quot;121.383328&quot;&#125;&#125;这里共有6个文档。假设检索第一页的查询如下所示：123456789101112131415161718192021GET twitter/_search&#123; &quot;size&quot;: 2, &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;city&quot;: &quot;北京&quot; &#125; &#125;, &quot;sort&quot;: [ &#123; &quot;DOB&quot;: &#123; &quot;order&quot;: &quot;asc&quot; &#125; &#125;, &#123; &quot;user.keyword&quot;: &#123; &quot;order&quot;: &quot;asc&quot; &#125; &#125; ]&#125;显示的结果为：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869&#123; &quot;took&quot; : 29, &quot;timed_out&quot; : false, &quot;_shards&quot; : &#123; &quot;total&quot; : 1, &quot;successful&quot; : 1, &quot;skipped&quot; : 0, &quot;failed&quot; : 0 &#125;, &quot;hits&quot; : &#123; &quot;total&quot; : &#123; &quot;value&quot; : 5, &quot;relation&quot; : &quot;eq&quot; &#125;, &quot;max_score&quot; : null, &quot;hits&quot; : [ &#123; &quot;_index&quot; : &quot;twitter&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;1&quot;, &quot;_score&quot; : null, &quot;_source&quot; : &#123; &quot;user&quot; : &quot;双榆树-张三&quot;, &quot;DOB&quot; : &quot;1980-01-01&quot;, &quot;message&quot; : &quot;今儿天气不错啊，出去转转去&quot;, &quot;uid&quot; : 2, &quot;age&quot; : 20, &quot;city&quot; : &quot;北京&quot;, &quot;province&quot; : &quot;北京&quot;, &quot;country&quot; : &quot;中国&quot;, &quot;address&quot; : &quot;中国北京市海淀区&quot;, &quot;location&quot; : &#123; &quot;lat&quot; : &quot;39.970718&quot;, &quot;lon&quot; : &quot;116.325747&quot; &#125; &#125;, &quot;sort&quot; : [ 315532800000, &quot;双榆树-张三&quot; ] &#125;, &#123; &quot;_index&quot; : &quot;twitter&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;2&quot;, &quot;_score&quot; : null, &quot;_source&quot; : &#123; &quot;user&quot; : &quot;东城区-老刘&quot;, &quot;DOB&quot; : &quot;1981-01-01&quot;, &quot;message&quot; : &quot;出发，下一站云南！&quot;, &quot;uid&quot; : 3, &quot;age&quot; : 30, &quot;city&quot; : &quot;北京&quot;, &quot;province&quot; : &quot;北京&quot;, &quot;country&quot; : &quot;中国&quot;, &quot;address&quot; : &quot;中国北京市东城区台基厂三条3号&quot;, &quot;location&quot; : &#123; &quot;lat&quot; : &quot;39.904313&quot;, &quot;lon&quot; : &quot;116.412754&quot; &#125; &#125;, &quot;sort&quot; : [ 347155200000, &quot;东城区-老刘&quot; ] &#125; ] &#125;&#125;上述请求的结果包括每个文档的sort值数组。 这些sort值可以与search_after参数一起使用，以开始返回在这个结果列表之后的任何文档。 例如，我们可以使用上一个文档的sort值并将其传递给search_after以检索下一页结果：12345678910111213141516171819202122232425GET twitter/_search&#123; &quot;size&quot;: 2, &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;city&quot;: &quot;北京&quot; &#125; &#125;, &quot;search_after&quot;: [ 347155200000, &quot;东城区-老刘&quot; ], &quot;sort&quot;: [ &#123; &quot;DOB&quot;: &#123; &quot;order&quot;: &quot;asc&quot; &#125; &#125;, &#123; &quot;user.keyword&quot;: &#123; &quot;order&quot;: &quot;asc&quot; &#125; &#125; ]&#125;在这里在search_after中，我们把上一个搜索结果的sort值放进来。 显示的结果为：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869&#123; &quot;took&quot; : 47, &quot;timed_out&quot; : false, &quot;_shards&quot; : &#123; &quot;total&quot; : 1, &quot;successful&quot; : 1, &quot;skipped&quot; : 0, &quot;failed&quot; : 0 &#125;, &quot;hits&quot; : &#123; &quot;total&quot; : &#123; &quot;value&quot; : 5, &quot;relation&quot; : &quot;eq&quot; &#125;, &quot;max_score&quot; : null, &quot;hits&quot; : [ &#123; &quot;_index&quot; : &quot;twitter&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;3&quot;, &quot;_score&quot; : null, &quot;_source&quot; : &#123; &quot;user&quot; : &quot;东城区-李四&quot;, &quot;DOB&quot; : &quot;1982-01-01&quot;, &quot;message&quot; : &quot;happy birthday!&quot;, &quot;uid&quot; : 4, &quot;age&quot; : 30, &quot;city&quot; : &quot;北京&quot;, &quot;province&quot; : &quot;北京&quot;, &quot;country&quot; : &quot;中国&quot;, &quot;address&quot; : &quot;中国北京市东城区&quot;, &quot;location&quot; : &#123; &quot;lat&quot; : &quot;39.893801&quot;, &quot;lon&quot; : &quot;116.408986&quot; &#125; &#125;, &quot;sort&quot; : [ 378691200000, &quot;东城区-李四&quot; ] &#125;, &#123; &quot;_index&quot; : &quot;twitter&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;4&quot;, &quot;_score&quot; : null, &quot;_source&quot; : &#123; &quot;user&quot; : &quot;朝阳区-老贾&quot;, &quot;DOB&quot; : &quot;1983-01-01&quot;, &quot;message&quot; : &quot;123,gogogo&quot;, &quot;uid&quot; : 5, &quot;age&quot; : 35, &quot;city&quot; : &quot;北京&quot;, &quot;province&quot; : &quot;北京&quot;, &quot;country&quot; : &quot;中国&quot;, &quot;address&quot; : &quot;中国北京市朝阳区建国门&quot;, &quot;location&quot; : &#123; &quot;lat&quot; : &quot;39.718256&quot;, &quot;lon&quot; : &quot;116.367910&quot; &#125; &#125;, &quot;sort&quot; : [ 410227200000, &quot;朝阳区-老贾&quot; ] &#125; ] &#125;&#125;注意：当我们使用search_after时，from值必须设置为0或者-1。search_after不是自由跳转到随机页面而是并行scroll多个查询的解决方案。 它与scroll API非常相似，但与它不同，search_after参数是无状态的，它始终针对最新版本的搜索器进行解析。 因此，排序顺序可能会在步行期间发生变化，具体取决于索引的更新和删除。]]></content>
      <categories>
        <category>elk</category>
      </categories>
      <tags>
        <tag>elk</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch：运用scroll接口对大量数据实现更好的分页]]></title>
    <url>%2F2020%2F01%2F02%2FElasticsearch%EF%BC%9A%E8%BF%90%E7%94%A8scroll%E6%8E%A5%E5%8F%A3%E5%AF%B9%E5%A4%A7%E9%87%8F%E6%95%B0%E6%8D%AE%E5%AE%9E%E7%8E%B0%E6%9B%B4%E5%A5%BD%E7%9A%84%E5%88%86%E9%A1%B5%2F</url>
    <content type="text"><![CDATA[在Elasticsearch中，我们可以通过size和from来对我们的结果来进行分页。但是对于数据量很大的索引，这是有效的吗？Scroll API可用于从单个搜索请求中检索大量结果（甚至所有结果），这与在传统数据库上使用cursor的方式非常相似。Scroll不是用于实时用户请求，而是用于处理大量数据，例如，用于处理大量数据。 为了将一个索引的内容重新索引到具有不同配置的新索引中。为了说明问题，我们今天先创建一个叫做twitter的Index:12345678910111213POST _bulk&#123; &quot;index&quot; : &#123; &quot;_index&quot; : &quot;twitter&quot;, &quot;_id&quot;: 1&#125; &#125;&#123;&quot;user&quot;:&quot;双榆树-张三&quot;,&quot;message&quot;:&quot;今儿天气不错啊，出去转转去&quot;,&quot;uid&quot;:2,&quot;age&quot;:20,&quot;city&quot;:&quot;北京&quot;,&quot;province&quot;:&quot;北京&quot;,&quot;country&quot;:&quot;中国&quot;,&quot;address&quot;:&quot;中国北京市海淀区&quot;,&quot;location&quot;:&#123;&quot;lat&quot;:&quot;39.970718&quot;,&quot;lon&quot;:&quot;116.325747&quot;&#125;&#125;&#123; &quot;index&quot; : &#123; &quot;_index&quot; : &quot;twitter&quot;, &quot;_id&quot;: 2 &#125;&#125;&#123;&quot;user&quot;:&quot;东城区-老刘&quot;,&quot;message&quot;:&quot;出发，下一站云南！&quot;,&quot;uid&quot;:3,&quot;age&quot;:30,&quot;city&quot;:&quot;北京&quot;,&quot;province&quot;:&quot;北京&quot;,&quot;country&quot;:&quot;中国&quot;,&quot;address&quot;:&quot;中国北京市东城区台基厂三条3号&quot;,&quot;location&quot;:&#123;&quot;lat&quot;:&quot;39.904313&quot;,&quot;lon&quot;:&quot;116.412754&quot;&#125;&#125;&#123; &quot;index&quot; : &#123; &quot;_index&quot; : &quot;twitter&quot;, &quot;_id&quot;: 3&#125; &#125;&#123;&quot;user&quot;:&quot;东城区-李四&quot;,&quot;message&quot;:&quot;happy birthday!&quot;,&quot;uid&quot;:4,&quot;age&quot;:30,&quot;city&quot;:&quot;北京&quot;,&quot;province&quot;:&quot;北京&quot;,&quot;country&quot;:&quot;中国&quot;,&quot;address&quot;:&quot;中国北京市东城区&quot;,&quot;location&quot;:&#123;&quot;lat&quot;:&quot;39.893801&quot;,&quot;lon&quot;:&quot;116.408986&quot;&#125;&#125;&#123; &quot;index&quot; : &#123; &quot;_index&quot; : &quot;twitter&quot;, &quot;_id&quot;: 4&#125; &#125;&#123;&quot;user&quot;:&quot;朝阳区-老贾&quot;,&quot;message&quot;:&quot;123,gogogo&quot;,&quot;uid&quot;:5,&quot;age&quot;:35,&quot;city&quot;:&quot;北京&quot;,&quot;province&quot;:&quot;北京&quot;,&quot;country&quot;:&quot;中国&quot;,&quot;address&quot;:&quot;中国北京市朝阳区建国门&quot;,&quot;location&quot;:&#123;&quot;lat&quot;:&quot;39.718256&quot;,&quot;lon&quot;:&quot;116.367910&quot;&#125;&#125;&#123; &quot;index&quot; : &#123; &quot;_index&quot; : &quot;twitter&quot;, &quot;_id&quot;: 5&#125; &#125;&#123;&quot;user&quot;:&quot;朝阳区-老王&quot;,&quot;message&quot;:&quot;Happy BirthDay My Friend!&quot;,&quot;uid&quot;:6,&quot;age&quot;:50,&quot;city&quot;:&quot;北京&quot;,&quot;province&quot;:&quot;北京&quot;,&quot;country&quot;:&quot;中国&quot;,&quot;address&quot;:&quot;中国北京市朝阳区国贸&quot;,&quot;location&quot;:&#123;&quot;lat&quot;:&quot;39.918256&quot;,&quot;lon&quot;:&quot;116.467910&quot;&#125;&#125;&#123; &quot;index&quot; : &#123; &quot;_index&quot; : &quot;twitter&quot;, &quot;_id&quot;: 6&#125; &#125;&#123;&quot;user&quot;:&quot;虹桥-老吴&quot;,&quot;message&quot;:&quot;好友来了都今天我生日，好友来了,什么 birthday happy 就成!&quot;,&quot;uid&quot;:7,&quot;age&quot;:90,&quot;city&quot;:&quot;上海&quot;,&quot;province&quot;:&quot;上海&quot;,&quot;country&quot;:&quot;中国&quot;,&quot;address&quot;:&quot;中国上海市闵行区&quot;,&quot;location&quot;:&#123;&quot;lat&quot;:&quot;31.175927&quot;,&quot;lon&quot;:&quot;121.383328&quot;&#125;&#125;在上面，我们创建了6个文档。这些文档的数量虽然不是很多，但是我们想为了说明问题的方便。在实际的使用中，我们可能有成百上千的文档。下面，我们通过size和from的方法来进行分页。假如我们把sizs设置为2，那么，我们可以通过如下写的方法来进行分页。123GET twitter/_search?size=2&amp;from=0GET twitter/_search?size=2&amp;from=2GET twitter/_search?size=2&amp;from=4这样，我们每次可以得到2个文档，从而对我们的Index进行分页。我们可以得到这些数据并在自己的页面上或应用里进行展示。通常这样的每个请求返回的上线是10K。如果超过这个上限的话，这样的方法将不再适合。上面的这种方法，对于小量的数据是可行的，但是对于大量的数据，而且我们需要进行sort时，这个有可能变得力不从心，比如：1234567891011121314151617GET twitter/_search&#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;city&quot;: &quot;北京&quot; &#125; &#125;, &quot;from&quot;: 2, &quot;size&quot;: 2, &quot;sort&quot;: [ &#123; &quot;user.keyword&quot;: &#123; &quot;order&quot;: &quot;desc&quot; &#125; &#125; ]&#125;你可以想象当你更深入地进行分页时，它会变得多么低效。 例如，如果更改mapping并希望将所有现有数据重新索引到新索引中，您可能没有足够的内存来对所有结果进行排序以返回最后一页的数据。对于这种应用场景，你可以使用scan搜索类型。我们可以这么做：1. 使用scroll来返回一个初始的搜索，并返回一个scroll ID123456789GET twitter/_search?scroll=1m&#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;city&quot;: &quot;北京&quot; &#125; &#125;, &quot;size&quot;: 2&#125;这里的scroll=1m，表明Elasticsearch允许等待的时间是1分钟。如果在一分钟之内，接下来的scroll请求没有到达的话，那么当前的请求的上下文将会丢失。返回的结果是：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960&#123; &quot;_scroll_id&quot; : &quot;DXF1ZXJ5QW5kRmV0Y2gBAAAAAAAAFh8WWUdCVlRMUllRb3UzMkdqb0IxVnZNUQ==&quot;, &quot;took&quot; : 31, &quot;timed_out&quot; : false, &quot;_shards&quot; : &#123; &quot;total&quot; : 1, &quot;successful&quot; : 1, &quot;skipped&quot; : 0, &quot;failed&quot; : 0 &#125;, &quot;hits&quot; : &#123; &quot;total&quot; : &#123; &quot;value&quot; : 5, &quot;relation&quot; : &quot;eq&quot; &#125;, &quot;max_score&quot; : 0.48232412, &quot;hits&quot; : [ &#123; &quot;_index&quot; : &quot;twitter&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;1&quot;, &quot;_score&quot; : 0.48232412, &quot;_source&quot; : &#123; &quot;user&quot; : &quot;双榆树-张三&quot;, &quot;message&quot; : &quot;今儿天气不错啊，出去转转去&quot;, &quot;uid&quot; : 2, &quot;age&quot; : 20, &quot;city&quot; : &quot;北京&quot;, &quot;province&quot; : &quot;北京&quot;, &quot;country&quot; : &quot;中国&quot;, &quot;address&quot; : &quot;中国北京市海淀区&quot;, &quot;location&quot; : &#123; &quot;lat&quot; : &quot;39.970718&quot;, &quot;lon&quot; : &quot;116.325747&quot; &#125; &#125; &#125;, &#123; &quot;_index&quot; : &quot;twitter&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;2&quot;, &quot;_score&quot; : 0.48232412, &quot;_source&quot; : &#123; &quot;user&quot; : &quot;东城区-老刘&quot;, &quot;message&quot; : &quot;出发，下一站云南！&quot;, &quot;uid&quot; : 3, &quot;age&quot; : 30, &quot;city&quot; : &quot;北京&quot;, &quot;province&quot; : &quot;北京&quot;, &quot;country&quot; : &quot;中国&quot;, &quot;address&quot; : &quot;中国北京市东城区台基厂三条3号&quot;, &quot;location&quot; : &#123; &quot;lat&quot; : &quot;39.904313&quot;, &quot;lon&quot; : &quot;116.412754&quot; &#125; &#125; &#125; ] &#125;&#125;在这里，我们可以看到一个返回的_scroll_id。这个_scroll_id将会被用于接下来的请求。2. 使用_scroll_id，再次请求利用上次请求返回来的_scroll_id，再次请求以获得下一个page的信息：12345GET _search/scroll&#123; &quot;scroll&quot;: &quot;1m&quot;, &quot;scroll_id&quot;:&quot;DXF1ZXJ5QW5kRmV0Y2gBAAAAAAAAHC8WWUdCVlRMUllRb3UzMkdqb0IxVnZNUQ==&quot;&#125;在这里必须指出的是：这里填写的scroll_id是上一个请求返回的值这个scroll_id的有效期是我们在第一次搜索时定义的1m，也就是1分钟。如果超过了，这个就没有用运行后返回的结果是：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960&#123; &quot;_scroll_id&quot; : &quot;DXF1ZXJ5QW5kRmV0Y2gBAAAAAAAAHS0WWUdCVlRMUllRb3UzMkdqb0IxVnZNUQ==&quot;, &quot;took&quot; : 2, &quot;timed_out&quot; : false, &quot;_shards&quot; : &#123; &quot;total&quot; : 1, &quot;successful&quot; : 1, &quot;skipped&quot; : 0, &quot;failed&quot; : 0 &#125;, &quot;hits&quot; : &#123; &quot;total&quot; : &#123; &quot;value&quot; : 5, &quot;relation&quot; : &quot;eq&quot; &#125;, &quot;max_score&quot; : 0.48232412, &quot;hits&quot; : [ &#123; &quot;_index&quot; : &quot;twitter&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;3&quot;, &quot;_score&quot; : 0.48232412, &quot;_source&quot; : &#123; &quot;user&quot; : &quot;东城区-李四&quot;, &quot;message&quot; : &quot;happy birthday!&quot;, &quot;uid&quot; : 4, &quot;age&quot; : 30, &quot;city&quot; : &quot;北京&quot;, &quot;province&quot; : &quot;北京&quot;, &quot;country&quot; : &quot;中国&quot;, &quot;address&quot; : &quot;中国北京市东城区&quot;, &quot;location&quot; : &#123; &quot;lat&quot; : &quot;39.893801&quot;, &quot;lon&quot; : &quot;116.408986&quot; &#125; &#125; &#125;, &#123; &quot;_index&quot; : &quot;twitter&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;4&quot;, &quot;_score&quot; : 0.48232412, &quot;_source&quot; : &#123; &quot;user&quot; : &quot;朝阳区-老贾&quot;, &quot;message&quot; : &quot;123,gogogo&quot;, &quot;uid&quot; : 5, &quot;age&quot; : 35, &quot;city&quot; : &quot;北京&quot;, &quot;province&quot; : &quot;北京&quot;, &quot;country&quot; : &quot;中国&quot;, &quot;address&quot; : &quot;中国北京市朝阳区建国门&quot;, &quot;location&quot; : &#123; &quot;lat&quot; : &quot;39.718256&quot;, &quot;lon&quot; : &quot;116.367910&quot; &#125; &#125; &#125; ] &#125;&#125;显然这次返回的是2个文档。我们需要再次使用同样的办法来得到最后一个page的结果：12345678910111213141516171819202122232425262728293031323334353637383940&#123; &quot;_scroll_id&quot; : &quot;DXF1ZXJ5QW5kRmV0Y2gBAAAAAAAADeoWeno1UkF2RWZRd202VW1HQXRlOWFUdw==&quot;, &quot;took&quot; : 2, &quot;timed_out&quot; : false, &quot;_shards&quot; : &#123; &quot;total&quot; : 1, &quot;successful&quot; : 1, &quot;skipped&quot; : 0, &quot;failed&quot; : 0 &#125;, &quot;hits&quot; : &#123; &quot;total&quot; : &#123; &quot;value&quot; : 5, &quot;relation&quot; : &quot;eq&quot; &#125;, &quot;max_score&quot; : 0.48232412, &quot;hits&quot; : [ &#123; &quot;_index&quot; : &quot;twitter&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;5&quot;, &quot;_score&quot; : 0.48232412, &quot;_source&quot; : &#123; &quot;user&quot; : &quot;朝阳区-老王&quot;, &quot;message&quot; : &quot;Happy BirthDay My Friend!&quot;, &quot;uid&quot; : 6, &quot;age&quot; : 50, &quot;city&quot; : &quot;北京&quot;, &quot;province&quot; : &quot;北京&quot;, &quot;country&quot; : &quot;中国&quot;, &quot;address&quot; : &quot;中国北京市朝阳区国贸&quot;, &quot;location&quot; : &#123; &quot;lat&quot; : &quot;39.918256&quot;, &quot;lon&quot; : &quot;116.467910&quot; &#125; &#125; &#125; ] &#125;&#125;显然，这次返回的结果只有一个数值，比我们请求的page大小2要小。如果我们利用返回的_scroll_id再次请求时，我们可以看返回的结果是：12345678910111213141516171819&#123; &quot;_scroll_id&quot; : &quot;DXF1ZXJ5QW5kRmV0Y2gBAAAAAAAADeoWeno1UkF2RWZRd202VW1HQXRlOWFUdw==&quot;, &quot;took&quot; : 1, &quot;timed_out&quot; : false, &quot;_shards&quot; : &#123; &quot;total&quot; : 1, &quot;successful&quot; : 1, &quot;skipped&quot; : 0, &quot;failed&quot; : 0 &#125;, &quot;hits&quot; : &#123; &quot;total&quot; : &#123; &quot;value&quot; : 5, &quot;relation&quot; : &quot;eq&quot; &#125;, &quot;max_score&quot; : 0.48232412, &quot;hits&quot; : [ ] &#125;&#125;这次是一个结果都没有。如果完成此过程，则需要清理上下文，因为上下文在超时之前仍会占用计算资源。 如下面的屏幕快照所示，您可以使用scroll_id参数在DELETE API中指定一个或多个上下文：1234DELTE_search/scroll&#123; &quot;scroll_id&quot;:&quot;DXF1ZXJ5QW5kRmV0Y2gBAAAAAAAAHC8WWUdCVlRMUllRb3UzMkdqb0IxVnZNUQ==&quot;&#125;参考：【1】Elasticsearch Scroll【2】https://www.elastic.co/guide/en/elasticsearch/reference/7.4/search-request-body.html#request-body-search-scroll]]></content>
      <categories>
        <category>elk</category>
      </categories>
      <tags>
        <tag>elk</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch：用户安全设置]]></title>
    <url>%2F2020%2F01%2F02%2FElasticsearch%EF%BC%9A%E7%94%A8%E6%88%B7%E5%AE%89%E5%85%A8%E8%AE%BE%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[Elastic Stack的组件是不安全的，因为它没有内置的固有安全性。 这意味着任何人都可以访问它。 在生产环境中运行Elastic Stack时，这会带来安全风险。 为了防止生产中未经授权的访问，采用了不同的机制来施加安全性，例如在防火墙后运行Elastic Stack并通过反向代理（例如nginx，HAProxy等）进行保护。 Elastic提供商业产品来保护Elastic Stack。 此产品是X-Pack的一部分，模块称为安全性。在今天的文章中，我们来讲述如何为我们的Elastics索引设置字段级的安全。这样有的字段对有些用户是可见的，而对另外一些用户是不可见的。我们也可以通过对用户安全的设置，使得不同的用户有不同的权限。User authentication在X-Pack安全性中，安全资源是基于用户的安全性的基础。 安全资源是需要访问以执行Elasticsearch集群操作的资源，例如索引，文档或字段。 X-Pack安全性通过分配给用户的角色的权限来实现。 权限是针对受保护资源的一项或多项特权。 特权是一个命名的组，代表用户可以针对安全资源执行的一个或多个操作。 用户可以具有一个或多个角色，并且用户拥有的总权限集定义为其所有角色的权限的并集，如下图所示：从上面的图上可以看出来：一个用户可以用多个role，而每个role可以对应多个permission(权限）。在接下来的练习中，我们来展示如何创建用户，role（角色）以及把permission分配到每个role。通过这样的组合，我们可以实现对字段级的安全控制。为Elastic设置安全及创建用户当我们设置完我们的安全账户后，最开始我们使用最原始的elastic的账号进行登录。请注意这里的密码是我们设置elastic账号的密码：![](https://img-blog.csdnimg.cn/20190904225704187.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1VidW50dVRvdWNo,size_16,color_FFFFFF,t_70等登录进去之后，现在我们去Manage/Sercurity/Users页面：我们来创建一个新的账号。针对我的情况，我想创建一个叫做liuxg的用户名。点击当前页面的Create User按钮：然后填入我们所需要的信息：点击Create User按钮，这样我们就创建了我们的用户。按照同样的步骤，我们来创建另外一个叫做user1的用户。准备实验数据在我们还没退出elastic用户的情况下，我们使用bulk API来把如下的文档输入到Elasticsearch中。1234567POST employee/_bulk&#123;&quot;index&quot;:&#123;&quot;_index&quot;:&quot;employee&quot;&#125;&#125;&#123;&quot;name&quot;:&quot;user1&quot;,&quot;email&quot;:&quot;user1@packt.com&quot;,&quot;salary&quot;:5000,&quot;gender&quot;:&quot;M&quot;,&quot;address1&quot;:&quot;312 Main St&quot;,&quot;address2&quot;:&quot;Walthill&quot;,&quot;state&quot;:&quot;NE&quot;&#125;&#123;&quot;index&quot;:&#123;&quot;_index&quot;:&quot;employee&quot;&#125;&#125;&#123;&quot;name&quot;:&quot;user2&quot;,&quot;email&quot;:&quot;user2@packt.com&quot;,&quot;salary&quot;:10000,&quot;gender&quot;:&quot;F&quot;,&quot;address1&quot;:&quot;5658 N Denver Ave&quot;,&quot;address2&quot;:&quot;Portland&quot;,&quot;state&quot;:&quot;OR&quot;&#125;&#123;&quot;index&quot;:&#123;&quot;_index&quot;:&quot;employee&quot;&#125;&#125;&#123;&quot;name&quot;:&quot;user3&quot;,&quot;email&quot;:&quot;user3@packt.com&quot;,&quot;salary&quot;:7000,&quot;gender&quot;:&quot;F&quot;,&quot;address1&quot;:&quot;300 Quinterra Ln&quot;,&quot;address2&quot;:&quot;Danville&quot;,&quot;state&quot;:&quot;CA&quot;&#125;这样我们把三个文档存入到employee的索引之中。创建新的role请注意：如下的操作是在elastic用户登录的情况下进行操作的。要创建新用户，请导航到管理UI并在“Security”部分中选择“role”，或者如果您当前在“Users”屏幕上，请单击“Roles”选项。 角色屏幕显示所有已定义/可用的角色:当我们点击roles后：我们点击Create role按钮。在这里，我们定义了一个叫做monitor_role，它具有monitor的权限。把role赋予给用户我们打开我们的用户列表。针对我的情况，我们打开liuxg用户：我们修改liuxg账号的Roles。把刚才创建的monitor_role赋予给liuxg用户。点击Update User按钮。这样我们的设定就好了。设定好的账号是这样的：从上面，我们可以看出来liuxg账号是有monitor_role的，而user1账号是没有的。下面我们来做一些基本的测试。我们在一个terminal中打入如下的命令：1curl -u liuxg:123456 &quot;http://localhost:9200/_cluster/health?pretty&quot;注意这里的123456是liuxg的账号密码。执行上面的显示结果是：我们显然看到了结果。那么我们同样地对use1账号来进行实验：1curl -u user1:123456 &quot;http://localhost:9200/_cluster/health?pretty&quot;显示的结果是：显然，user1账号没有得到任何结果。这个根本的原因是因为这个账号没有相应的权限。文档级或字段级安全现在，我们知道了如何创建新用户，创建新角色以及将角色分配给用户，让我们探讨如何针对给定的索引/文档对文档和字段施加安全性。接下来，我们使用我之前给大家输入进的employee索引来展示。案例1当用户搜索员工详细信息时，该用户不允许包含在属于员工索引的文档中的薪水/地址详细信息。这就是我们所说的字段级安全。首先，让我们来创建一个叫做employee_read的role。这个role只具有employ索引的read权限。为了限制字段，我们可以在设置里做相应的配置：我们只允许这个employee_read role访问gender，state及email字段，而且只有read权限。运用我们刚才设置的employee_read role，我们赋予给我们的user1用户：设置好的用户界面为：![] (https://img-blog.csdnimg.cn/20191029213613951.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1VidW50dVRvdWNo,size_16,color_FFFFFF,t_70)上面显示我们的user1具有employ_read的role。在我们的一个terminal里打入如下的命令：1curl -u user1:123456 &quot;http://localhost:9200/employee/_search?pretty&quot;请注意：这里的123456是user1用户的密码。上面命令显示的结果为：显然，user1只能访问在employee_read中的三个字段。案例2我们想定义一个role。这个role具有read的权限，并且只能访问state为OR的那些文档。我们做一下的设置：我们创建了一个叫做OR_state的role。它通过一个query:1&#123;&quot;match&quot;: &#123;&quot;state.keyword&quot;:&quot;OR&quot;&#125;&#125;来匹配项对应的文档。我们接着把这个role赋予给liuxg用户：在我们设置完后，我们接着在一个terminal中打入如下的命令：1curl -u liuxg:123456 &quot;http://localhost:9200/employee/_search?pretty&quot;显示的结果：我们可以看出来这次的显示的结果只有一个，而且这个文档的state是OR。]]></content>
      <categories>
        <category>elk</category>
      </categories>
      <tags>
        <tag>elk</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch：使用function_score及soft_score定制搜索结果的分数]]></title>
    <url>%2F2020%2F01%2F02%2FElasticsearch%EF%BC%9A%E4%BD%BF%E7%94%A8function_score%E5%8F%8Asoft_score%E5%AE%9A%E5%88%B6%E6%90%9C%E7%B4%A2%E7%BB%93%E6%9E%9C%E7%9A%84%E5%88%86%E6%95%B0%2F</url>
    <content type="text"><![CDATA[我们将介绍使用function_score的基础知识，并介绍一些function core技术非常有用和有效的用例。介绍评分的概念是任何搜索引擎（包括Elasticsearch）的核心。评分可以粗略地定义为：找到符合一组标准的数据并按相关性顺序将其返回。相关性通常是通过类似TF-IDF的算法来实现的，该算法试图找出文本上与提交的查询最相似的文档。尽管TF-IDF及其表亲（例如BM25）非常棒，但有时必须通过其他算法或通过其他评分启发式方法来解决相关性问题。在这里，Elasticsearch的script_score和function_score功能变得非常有用。本文将介绍这些工具的用法。文本相似性不是最重要因素的一个域示例是地理搜索。如果正在寻找在给定点附近的好咖啡店，则按与查询在文本上的相似程度对咖啡店进行排名对用户而言不是很有用，但按地理位置在附近的排名对他们。另一个示例可能是视频共享站点上的视频，其中搜索结果可能应该考虑视频的相对受欢迎程度。如果某个流行歌星上传了具有给定标题的视频，从而获得了数百万的观看次数，那么该视频可能应该比具有相似文字相关性的不受欢迎的视频更胜一筹。在使用Elasticsearch进行全文搜索的时候，默认是使用BM25计算的_score字段进行降序排序的。当我们需要用其他字段进行降序或者升序排序的时候，可以使用sort字段，传入我们想要的排序字段和方式。 当简单的使用几个字段升降序排列组合无法满足我们的需求的时候，我们就需要自定义排序的特性，Elasticsearch提供了function_score的DSL来自定义打分，这样就可以根据自定义的_score来进行排序。在实际的使用中，我们必须注意的是：soft_score和function_score是耗资源的。您只需要计算一组经过过滤的文档的分数。下面我们来用一个例子来具体说明如何来通过script_core和function_core来定制我们的分数。准备数据我们首先来下载我们的测试数据：1git clone https://github.com/liu-xiao-guo/best_games_json_data然后我们通过Kibana把这个数据来导入到我们的Elasticsearch中：在导入的过程中，我们选择Time field为year，并且指定相应的日期格式：我们指定我们的索引名字为best_games：我们可以查看一下一个样本的文档就像是下面的格式一样：1234567891011121314&quot;_source&quot; : &#123; &quot;global_sales&quot; : 82.53, &quot;year&quot; : 2006, &quot;image_url&quot; : &quot;https://upload.wikimedia.org/wikipedia/en/thumb/e/e0/Wii_Sports_Europe.jpg/220px-Wii_Sports_Europe.jpg&quot;, &quot;platform&quot; : &quot;Wii&quot;, &quot;@timestamp&quot; : &quot;2006-01-01T00:00:00.000+08:00&quot;, &quot;user_score&quot; : 8, &quot;critic_score&quot; : 76, &quot;name&quot; : &quot;Wii Sports&quot;, &quot;genre&quot; : &quot;Sports&quot;, &quot;publisher&quot; : &quot;Nintendo&quot;, &quot;developer&quot; : &quot;Nintendo&quot;, &quot;id&quot; : &quot;wii-sports-wii-2006&quot;&#125;在上面我们可以看出来这个文档里有两个很重要的字段：critic_score及user_score。一个是表示这个游戏的难度，另外一个表示游戏的受欢迎的程度。正常查询首先我们来看看如果不使用任何的分数定制，那么情况是怎么样的。12345678910111213GET best_games/_search&#123; &quot;_source&quot;: [ &quot;name&quot;, &quot;critic_score&quot;, &quot;user_score&quot; ], &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;name&quot;: &quot;Final Fantasy&quot; &#125; &#125;&#125;在上面的查询中，为了说明问题的方便，在返回的结果中，我们只返回name, critic_score和user_score字段。我们在name字段里含有“Final Fantasy”的所有游戏，那么显示显示的结果是：1234567891011121314151617181920212223242526272829303132333435&quot;hits&quot; : [ &#123; &quot;_index&quot; : &quot;best_games&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;2qccJ28BCSSrjaXdSOnC&quot;, &quot;_score&quot; : 8.138414, &quot;_source&quot; : &#123; &quot;user_score&quot; : 9, &quot;critic_score&quot; : 92, &quot;name&quot; : &quot;Final Fantasy VII&quot; &#125; &#125;, &#123; &quot;_index&quot; : &quot;best_games&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;6KccJ28BCSSrjaXdSOnC&quot;, &quot;_score&quot; : 8.138414, &quot;_source&quot; : &#123; &quot;user_score&quot; : 8, &quot;critic_score&quot; : 92, &quot;name&quot; : &quot;Final Fantasy X&quot; &#125; &#125;, &#123; &quot;_index&quot; : &quot;best_games&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;6qccJ28BCSSrjaXdSOnC&quot;, &quot;_score&quot; : 8.138414, &quot;_source&quot; : &#123; &quot;user_score&quot; : 8, &quot;critic_score&quot; : 90, &quot;name&quot; : &quot;Final Fantasy VIII&quot; &#125; &#125;, ...从上面的结果中，我们可以看出来Final Fantasy VII是最匹配的结果。它的分数是最高的。Soft_score 查询加入我们我们是游戏的运营商，那么我们也许我们自己想要的排名的方法。比如，虽然所有的结果都很匹配，但是我们也许不只单单是匹配Final Fantasy，而且我们想把user_score和critic_score加进来（虽然你可以使用其中的一个）。我们想这样来算我们的分数。最终score = score*(user_score*10 + critic_score)/2/100也就是我们把user_score乘以10，从而变成100分制。它和critic_score加起来，然后除以2，并除以100，这样就得出来最后的分数的加权系数。这个加权系数再乘以先前在上一步得出来的分数才是最终的分数值。经过这样的改造后，我们发现我们的分数其实不光是全文搜索的相关性，同时它也紧紧地关联了我们的用户体验和游戏的难道系数。那么我们如何使用这个呢？参照Elastics的官方文档soft_score，我们现在做如下的搜索：1234567891011121314151617181920GET best_games/_search&#123; &quot;_source&quot;: [ &quot;name&quot;, &quot;critic_score&quot;, &quot;user_score&quot; ], &quot;query&quot;: &#123; &quot;script_score&quot;: &#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;name&quot;: &quot;Final Fantasy&quot; &#125; &#125;, &quot;script&quot;: &#123; &quot;source&quot;: &quot;_score * (doc[&apos;user_score&apos;].value*10+doc[&apos;critic_score&apos;].value)/2/100&quot; &#125; &#125; &#125;&#125;在上面的查询中，我们可以看到我们使用了新的公式：123&quot;script&quot;: &#123; &quot;source&quot;: &quot;_score * (doc[&apos;user_score&apos;].value*10+doc[&apos;critic_score&apos;].value)/2/100&quot;&#125;那么我查询后的结果为：12345678910111213141516171819202122232425262728293031323334353637383940414243444546 &quot;hits&quot; : [ &#123; &quot;_index&quot; : &quot;best_games&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;2qccJ28BCSSrjaXdSOnC&quot;, &quot;_score&quot; : 7.405957, &quot;_source&quot; : &#123; &quot;user_score&quot; : 9, &quot;critic_score&quot; : 92, &quot;name&quot; : &quot;Final Fantasy VII&quot; &#125; &#125;, &#123; &quot;_index&quot; : &quot;best_games&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;K6ccJ28BCSSrjaXdSOrC&quot;, &quot;_score&quot; : 7.0804205, &quot;_source&quot; : &#123; &quot;user_score&quot; : 8, &quot;critic_score&quot; : 94, &quot;name&quot; : &quot;Final Fantasy IX&quot; &#125; &#125;, &#123; &quot;_index&quot; : &quot;best_games&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;6KccJ28BCSSrjaXdSOnC&quot;, &quot;_score&quot; : 6.9990363, &quot;_source&quot; : &#123; &quot;user_score&quot; : 8, &quot;critic_score&quot; : 92, &quot;name&quot; : &quot;Final Fantasy X&quot; &#125; &#125;, &#123; &quot;_index&quot; : &quot;best_games&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;6qccJ28BCSSrjaXdSOnC&quot;, &quot;_score&quot; : 6.917652, &quot;_source&quot; : &#123; &quot;user_score&quot; : 8, &quot;critic_score&quot; : 90, &quot;name&quot; : &quot;Final Fantasy VIII&quot; &#125; &#125;,...我们从上面可以看出来最终的分数_score是完全不一样的值。我们同时也看出来尽管第一名的Final Fantasy VII没有发生变化，但是第二名的位置由Final Fantasy X变为Final Fantasy IX了。针对script的运算，有一些预定义好的函数可以供我们调用，它们可以帮我们加速我们的计算。SaturationSigmoidRandom score functionDecay functions for numeric fieldsDecay functions for geo fieldsDecay functions for date fieldsFunctions for vector fields我们可以参考Elastic的官方文档来帮我们更深入地了解。Function score 查询function_score允许您修改查询检索的文档分数。 例如，如果分数函数在计算上很昂贵，并且足以在过滤后的文档集上计算分数，则此功能很有用。要使用function_score，用户必须定义一个查询和一个或多个函数，这些函数为查询返回的每个文档计算一个新分数。function_score可以只与一个函数一起使用，比如：12345678910111213GET /_search&#123; &quot;query&quot;: &#123; &quot;function_score&quot;: &#123; &quot;query&quot;: &#123; &quot;match_all&quot;: &#123;&#125; &#125;, &quot;boost&quot;: &quot;5&quot;, &quot;random_score&quot;: &#123;&#125;, &quot;boost_mode&quot;: &quot;multiply&quot; &#125; &#125;&#125;这里它把所有的文档的分数由5和一个由random_score (返回0到1之间的值)相乘而得到。那么这个分数就是一个从0到5之间的一个数值：12345678910111213141516171819202122232425262728&quot;hits&quot; : [ &#123; &quot;_index&quot; : &quot;chicago_employees&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;Hrz0_W4BDM8YqwyDD06A&quot;, &quot;_score&quot; : 4.9999876, &quot;_source&quot; : &#123; &quot;Name&quot; : &quot;ADKINS, WILLIAM J&quot;, &quot;Job Titles&quot; : &quot;SUPERVISING FIRE COMMUNICATIONS OPERATOR&quot;, &quot;Department&quot; : &quot;OEMC&quot;, &quot;Full or Part-Time&quot; : &quot;F&quot;, &quot;Salary or Hourly&quot; : &quot;Salary&quot;, &quot;Annual Salary&quot; : 121472.04 &#125; &#125;, &#123; &quot;_index&quot; : &quot;kibana_sample_data_logs&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;eXNIHm8BjrINWI3xYF0J&quot;, &quot;_score&quot; : 4.9999495, &quot;_source&quot; : &#123; &quot;agent&quot; : &quot;Mozilla/5.0 (X11; Linux i686) AppleWebKit/534.24 (KHTML, like Gecko) Chrome/11.0.696.50 Safari/534.24&quot;, &quot;bytes&quot; : 6630, &quot;clientip&quot; : &quot;77.5.51.49&quot;, &quot;extension&quot; : &quot;&quot;, &quot;geo&quot; : &#123; &quot;srcdest&quot; : &quot;CN:ID&quot;, ...尽管这个分数没有多大实际的意思，但是它可以让我们每次进入一个网页看到不同的文档，而不是严格按照固定的匹配而得到的固定的结果。我们也可以配合soft_score一起来使用function_score：1234567891011121314151617181920GET best_games/_search&#123; &quot;_source&quot;: [ &quot;name&quot;, &quot;critic_score&quot;, &quot;user_score&quot; ], &quot;query&quot;: &#123; &quot;function_score&quot;: &#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;name&quot;: &quot;Final Fantasy&quot; &#125; &#125;, &quot;script_score&quot;: &#123; &quot;script&quot;: &quot;_score * (doc[&apos;user_score&apos;].value*10+doc[&apos;critic_score&apos;].value)/2/100&quot; &#125; &#125; &#125;&#125;那么显示的结果是：12345678910111213141516171819202122232425262728293031323334353637383940414243444546&quot;hits&quot; : [ &#123; &quot;_index&quot; : &quot;best_games&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;2qccJ28BCSSrjaXdSOnC&quot;, &quot;_score&quot; : 60.272747, &quot;_source&quot; : &#123; &quot;user_score&quot; : 9, &quot;critic_score&quot; : 92, &quot;name&quot; : &quot;Final Fantasy VII&quot; &#125; &#125;, &#123; &quot;_index&quot; : &quot;best_games&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;K6ccJ28BCSSrjaXdSOrC&quot;, &quot;_score&quot; : 57.623398, &quot;_source&quot; : &#123; &quot;user_score&quot; : 8, &quot;critic_score&quot; : 94, &quot;name&quot; : &quot;Final Fantasy IX&quot; &#125; &#125;, &#123; &quot;_index&quot; : &quot;best_games&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;6KccJ28BCSSrjaXdSOnC&quot;, &quot;_score&quot; : 56.96106, &quot;_source&quot; : &#123; &quot;user_score&quot; : 8, &quot;critic_score&quot; : 92, &quot;name&quot; : &quot;Final Fantasy X&quot; &#125; &#125;, &#123; &quot;_index&quot; : &quot;best_games&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;6qccJ28BCSSrjaXdSOnC&quot;, &quot;_score&quot; : 56.29872, &quot;_source&quot; : &#123; &quot;user_score&quot; : 8, &quot;critic_score&quot; : 90, &quot;name&quot; : &quot;Final Fantasy VIII&quot; &#125; &#125;, ...细心的读者可能看出来了。我们的分数和之前的那个soft_score结果是不一样的，但是我们搜索的结果的排序是一样的。在上面的script的写法中，我们使用了硬编码，也就是把10硬写入到script中了。假如有一种情况，我将来想修改这个值为20或其它的值，重新看看查询的结果。由于script的改变，需要重新进行编译，这样的效率并不高。一种较好的办法是如下的写法：1234567891011121314151617181920212223GET best_games/_search&#123; &quot;_source&quot;: [ &quot;name&quot;, &quot;critic_score&quot;, &quot;user_score&quot; ], &quot;query&quot;: &#123; &quot;script_score&quot;: &#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;name&quot;: &quot;Final Fantasy&quot; &#125; &#125;, &quot;script&quot;: &#123; &quot;params&quot;:&#123; &quot;multiplier&quot;: 10 &#125;, &quot;source&quot;: &quot;_score * (doc[&apos;user_score&apos;].value*params.multiplier+doc[&apos;critic_score&apos;].value)/2/100&quot; &#125; &#125; &#125;&#125;脚本编译被缓存以加快执行速度。 如果脚本具有需要考虑的参数，则最好重用相同的脚本并为其提供参数。boost_modeboost_mode是用来定义最新计算出来的分数如何和查询的分数来相结合的。mulitply 查询分数和功能分数相乘（默认）replace 仅使用功能分数，查询分数将被忽略sum 查询分数和功能分数相加avg 平均值max 查询分数和功能分数的最大值min 查询分数和功能分数的最小值field_value_factorfield_value_factor函数使您可以使用文档中的字段来影响得分。 与使用script_score函数类似，但是它避免了脚本编写的开销。 如果用于多值字段，则在计算中仅使用该字段的第一个值。例如，假设您有一个用数字likes字段索引的文档，并希望通过该字段影响文档的得分，那么这样做的示例如下所示：12345678910111213GET /_search&#123; &quot;query&quot;: &#123; &quot;function_score&quot;: &#123; &quot;field_value_factor&quot;: &#123; &quot;field&quot;: &quot;likes&quot;, &quot;factor&quot;: 1.2, &quot;modifier&quot;: &quot;sqrt&quot;, &quot;missing&quot;: 1 &#125; &#125; &#125;&#125;上面的function_score将根据field_value_factore按照如下的方式来计算分数：sqrt(1.2 * doc[&#39;likes&#39;].value)field_value_factor函数有许多选项：field 要从文档中提取的字段。factor 字段值乘以的可选因子，默认为1。modifier 应用于字段值的修饰符可以是以下之一：none，log，log1p，log2p，ln，ln1p，ln2p，平方，sqrt或reciprocal。 默认为无。missing 如果文档没有该字段，则使用该值。 就像从文档中读取一样，修饰符和因数仍然适用于它。针对我们的例子，我们也可以使用如下的方法来重新计算分数：1234567891011121314151617181920212223GET best_games/_search&#123; &quot;_source&quot;: [ &quot;name&quot;, &quot;critic_score&quot;, &quot;user_score&quot; ], &quot;query&quot;: &#123; &quot;function_score&quot;: &#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;name&quot;: &quot;Final Fantasy&quot; &#125; &#125;, &quot;field_value_factor&quot;: &#123; &quot;field&quot;: &quot;user_score&quot;, &quot;factor&quot;: 1.2, &quot;modifier&quot;: &quot;none&quot;, &quot;missing&quot;: 1 &#125; &#125; &#125;&#125;在上面的例子里，我们使用user_score字段，并把这个字段的factor设置为1.2。这样加大这个字段的重要性。重新进行搜索：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647 &quot;hits&quot; : [ &#123; &quot;_index&quot; : &quot;best_games&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;2qccJ28BCSSrjaXdSOnC&quot;, &quot;_score&quot; : 87.89488, &quot;_source&quot; : &#123; &quot;user_score&quot; : 9, &quot;critic_score&quot; : 92, &quot;name&quot; : &quot;Final Fantasy VII&quot; &#125; &#125;, &#123; &quot;_index&quot; : &quot;best_games&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;6KccJ28BCSSrjaXdSOnC&quot;, &quot;_score&quot; : 78.128784, &quot;_source&quot; : &#123; &quot;user_score&quot; : 8, &quot;critic_score&quot; : 92, &quot;name&quot; : &quot;Final Fantasy X&quot; &#125; &#125;, &#123; &quot;_index&quot; : &quot;best_games&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;6qccJ28BCSSrjaXdSOnC&quot;, &quot;_score&quot; : 78.128784, &quot;_source&quot; : &#123; &quot;user_score&quot; : 8, &quot;critic_score&quot; : 90, &quot;name&quot; : &quot;Final Fantasy VIII&quot; &#125; &#125;, &#123; &quot;_index&quot; : &quot;best_games&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;K6ccJ28BCSSrjaXdSOrC&quot;, &quot;_score&quot; : 78.128784, &quot;_source&quot; : &#123; &quot;user_score&quot; : 8, &quot;critic_score&quot; : 94, &quot;name&quot; : &quot;Final Fantasy IX&quot; &#125; &#125;, ...我们可以看出来我们的分数又有些变化。而且排序也有变化。functions上面的例子中，每一个doc都会乘以相同的系数，有时候我们需要对不同的doc采用不同的权重。这时，使用functions是一种不错的选择。几个function可以组合。 在这种情况下，可以选择仅在文档与给定的过滤查询匹配时才应用该function:12345678910111213141516171819202122232425262728293031323334GET /_search&#123; &quot;query&quot;: &#123; &quot;function_score&quot;: &#123; &quot;query&quot;: &#123; &quot;match_all&quot;: &#123;&#125; &#125;, &quot;boost&quot;: &quot;5&quot;, &quot;functions&quot;: [ &#123; &quot;filter&quot;: &#123; &quot;match&quot;: &#123; &quot;test&quot;: &quot;bar&quot; &#125; &#125;, &quot;random_score&quot;: &#123;&#125;, &quot;weight&quot;: 23 &#125;, &#123; &quot;filter&quot;: &#123; &quot;match&quot;: &#123; &quot;test&quot;: &quot;cat&quot; &#125; &#125;, &quot;weight&quot;: 42 &#125; ], &quot;max_boost&quot;: 42, &quot;score_mode&quot;: &quot;max&quot;, &quot;boost_mode&quot;: &quot;multiply&quot;, &quot;min_score&quot;: 42 &#125; &#125;&#125;上面的boost为5，也即所有的文档的加权都是5。我们同时也看到几个定义的functions。它们是针对相应的匹配的文档分别进行加权的。如果匹配了，就可以乘以相应的加权。针对我们的例子，我们也可以做如下的实验。123456789101112131415161718192021222324GET best_games/_search&#123; &quot;query&quot;: &#123; &quot;function_score&quot;: &#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;name&quot;: &quot;Final Fantasy&quot; &#125; &#125;, &quot;boost&quot;: &quot;1&quot;, &quot;functions&quot;: [ &#123; &quot;filter&quot;: &#123; &quot;match&quot;: &#123; &quot;name&quot;: &quot; XIII&quot; &#125; &#125;, &quot;weight&quot;: 10000000 &#125; ], &quot;boost_mode&quot;: &quot;multiply&quot; &#125; &#125;&#125;我们想把name含有XIII的所有游戏都加一个权。这样它可以排到最前面。我们给它的加权值很大：10000000。搜索后的结果是：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182 &quot;hits&quot; : [ &#123; &quot;_index&quot; : &quot;best_games&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;KqccJ28BCSSrjaXdSOrC&quot;, &quot;_score&quot; : 8.1384144E7, &quot;_source&quot; : &#123; &quot;global_sales&quot; : 5.33, &quot;year&quot; : 2009, &quot;image_url&quot; : &quot;https://www.wired.com/images_blogs/gamelife/2009/09/ffxiii-01.jpg&quot;, &quot;platform&quot; : &quot;PS3&quot;, &quot;@timestamp&quot; : &quot;2009-01-01T00:00:00.000+08:00&quot;, &quot;user_score&quot; : 7, &quot;critic_score&quot; : 83, &quot;name&quot; : &quot;Final Fantasy XIII&quot;, &quot;genre&quot; : &quot;Role-Playing&quot;, &quot;publisher&quot; : &quot;Square Enix&quot;, &quot;developer&quot; : &quot;Square Enix&quot;, &quot;id&quot; : &quot;final-fantasy-xiii-ps3-2009&quot; &#125; &#125;, &#123; &quot;_index&quot; : &quot;best_games&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;OKccJ28BCSSrjaXdSOvC&quot;, &quot;_score&quot; : 7.2601472E7, &quot;_source&quot; : &#123; &quot;global_sales&quot; : 2.63, &quot;year&quot; : 2011, &quot;image_url&quot; : &quot;https://i.ytimg.com/vi/tSJH_vhaYUk/maxresdefault.jpg&quot;, &quot;platform&quot; : &quot;PS3&quot;, &quot;@timestamp&quot; : &quot;2011-01-01T00:00:00.000+08:00&quot;, &quot;user_score&quot; : 6, &quot;critic_score&quot; : 79, &quot;name&quot; : &quot;Final Fantasy XIII-2&quot;, &quot;genre&quot; : &quot;Role-Playing&quot;, &quot;publisher&quot; : &quot;Square Enix&quot;, &quot;developer&quot; : &quot;Square Enix&quot;, &quot;id&quot; : &quot;final-fantasy-xiii-2-ps3-2011&quot; &#125; &#125;, &#123; &quot;_index&quot; : &quot;best_games&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;2qccJ28BCSSrjaXdSOnC&quot;, &quot;_score&quot; : 8.138414, &quot;_source&quot; : &#123; &quot;global_sales&quot; : 9.72, &quot;year&quot; : 1997, &quot;image_url&quot; : &quot;https://r.hswstatic.com/w_907/gif/finalfantasyvii-MAIN.jpg&quot;, &quot;platform&quot; : &quot;PS&quot;, &quot;@timestamp&quot; : &quot;1997-01-01T00:00:00.000+08:00&quot;, &quot;user_score&quot; : 9, &quot;critic_score&quot; : 92, &quot;name&quot; : &quot;Final Fantasy VII&quot;, &quot;genre&quot; : &quot;Role-Playing&quot;, &quot;publisher&quot; : &quot;Sony Computer Entertainment&quot;, &quot;developer&quot; : &quot;SquareSoft&quot;, &quot;id&quot; : &quot;final-fantasy-vii-ps-1997&quot; &#125; &#125;, &#123; &quot;_index&quot; : &quot;best_games&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;6KccJ28BCSSrjaXdSOnC&quot;, &quot;_score&quot; : 8.138414, &quot;_source&quot; : &#123; &quot;global_sales&quot; : 8.05, &quot;year&quot; : 2001, &quot;image_url&quot; : &quot;https://www.mobygames.com/images/promo/l/192477-final-fantasy-x-screenshot.jpg&quot;, &quot;platform&quot; : &quot;PS2&quot;, &quot;@timestamp&quot; : &quot;2001-01-01T00:00:00.000+08:00&quot;, &quot;user_score&quot; : 8, &quot;critic_score&quot; : 92, &quot;name&quot; : &quot;Final Fantasy X&quot;, &quot;genre&quot; : &quot;Role-Playing&quot;, &quot;publisher&quot; : &quot;Sony Computer Entertainment&quot;, &quot;developer&quot; : &quot;SquareSoft&quot;, &quot;id&quot; : &quot;final-fantasy-x-ps2-2001&quot; &#125; &#125;,...我们可以看出来，在这一次的搜索中Final Fantasy XIII的排名变成第一了。Elasticsearch中的衰变函数在Elasticsearch中，常见的Decay function （衰变函数）有一下的几种：Function评分技术不仅可以修改默认的Elasticsearch评分算法，还可以用于完全替代它。 一个很好的例子是“trending”搜索，显示主题中正在迅速流行的项目。这样的分数不能基于简单的指标（例如“喜欢”或“观看次数”），而必须根据当前时间不断调整。 与在24小时内获得10000次观看的视频相比，在1小时内获得1000次观看的视频通常被认为“更热”。 Elasticsearch附带了几个衰减函数，这些函数使解决此类问题变得轻而易举。我们现在以gauss来为例展示如何使用这个衰变函数的。曲线的形状可以通过orgin，scale，offset和decay来控制。 这三个变量是控制曲线形状的主要工具。 可以将origin和scale参数视为您的最小值和最大值，它定义了将在其中定义曲线的边界框。 如果我们希望趋势视频列表涵盖一整天，则最好将原点定义为当前时间戳，比例尺定义为24小时。 offset可用于在开始时将曲线完全平坦，例如将其设置为1h，可消除最近视频的所有惩罚，也即最近1个小时里的所有视频不受影响 。最后，衰减选项会根据文档的位置更改文档降级的严重程度。 默认的衰减值是0.5，较大的值会使曲线更陡峭，其效果也更明显。我们还是拿我们的best_games来为例：123456789101112131415161718192021222324252627282930GET best_games/_search&#123; &quot;_source&quot;: [ &quot;name&quot;, &quot;critic_score&quot;, &quot;user_score&quot; ], &quot;query&quot;: &#123; &quot;function_score&quot;: &#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;name&quot;: &quot;Final Fantasy&quot; &#125; &#125;, &quot;functions&quot;: [ &#123; &quot;gauss&quot;: &#123; &quot;@timestamp&quot;: &#123; &quot;origin&quot;: &quot;2016-01-01T00:00:00&quot;, &quot;scale&quot;: &quot;365d&quot;, &quot;offset&quot;: &quot;0h&quot;, &quot;decay&quot;: 0.1 &#125; &#125; &#125; ], &quot;boost_mode&quot;: &quot;multiply&quot; &#125; &#125;&#125;上面的查询是基于2016-010-01这一天开始，在365天之内的文档不收衰减，那么超过这个时间的所有文档，衰减的加权值为0.1。也就是说1年开外的所有文档对我的意义并不是太多。重新运行我们的查询，结果显示：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657 &quot;hits&quot; : [ &#123; &quot;_index&quot; : &quot;best_games&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;OKccJ28BCSSrjaXdSOvC&quot;, &quot;_score&quot; : 6.6742494E-25, &quot;_source&quot; : &#123; &quot;user_score&quot; : 6, &quot;critic_score&quot; : 79, &quot;name&quot; : &quot;Final Fantasy XIII-2&quot; &#125; &#125;, &#123; &quot;_index&quot; : &quot;best_games&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;2qccJ28BCSSrjaXdSOnC&quot;, &quot;_score&quot; : 0.0, &quot;_source&quot; : &#123; &quot;user_score&quot; : 9, &quot;critic_score&quot; : 92, &quot;name&quot; : &quot;Final Fantasy VII&quot; &#125; &#125;, &#123; &quot;_index&quot; : &quot;best_games&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;6KccJ28BCSSrjaXdSOnC&quot;, &quot;_score&quot; : 0.0, &quot;_source&quot; : &#123; &quot;user_score&quot; : 8, &quot;critic_score&quot; : 92, &quot;name&quot; : &quot;Final Fantasy X&quot; &#125; &#125;, &#123; &quot;_index&quot; : &quot;best_games&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;6qccJ28BCSSrjaXdSOnC&quot;, &quot;_score&quot; : 0.0, &quot;_source&quot; : &#123; &quot;user_score&quot; : 8, &quot;critic_score&quot; : 90, &quot;name&quot; : &quot;Final Fantasy VIII&quot; &#125; &#125;, &#123; &quot;_index&quot; : &quot;best_games&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;FqccJ28BCSSrjaXdSOrC&quot;, &quot;_score&quot; : 0.0, &quot;_source&quot; : &#123; &quot;user_score&quot; : 7, &quot;critic_score&quot; : 92, &quot;name&quot; : &quot;Final Fantasy XII&quot; &#125; &#125;,...这次的搜索结果显示Final Fantasy XIII-2是得分最高的文档。参考：【1】https://www.elastic.co/blog/found-function-scoring【2】https://medium.com/horrible-hacks/customizing-scores-in-elasticsearch-for-product-recommendations-9e0d02ce1dbd【3】https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-function-score-query.html#function-field-value-factor【4】https://juejin.im/post/5df8f465518825123751c089【5】https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-script-score-query.html]]></content>
      <categories>
        <category>elk</category>
      </categories>
      <tags>
        <tag>elk</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch：如何把Elasticsearch中的数据导出为CSV格式的文件]]></title>
    <url>%2F2020%2F01%2F02%2FElasticsearch%EF%BC%9A%E5%A6%82%E4%BD%95%E6%8A%8AElasticsearch%E4%B8%AD%E7%9A%84%E6%95%B0%E6%8D%AE%E5%AF%BC%E5%87%BA%E4%B8%BACSV%E6%A0%BC%E5%BC%8F%E7%9A%84%E6%96%87%E4%BB%B6%2F</url>
    <content type="text"><![CDATA[本教程向您展示如何将数据从Elasticsearch导出到CSV文件。 想象一下，您想要在Excel中打开一些Elasticsearch中的数据，并根据这些数据创建数据透视表。 这只是一个用例，其中将数据从Elasticsearch导出到CSV文件将很有用。方法一其实这种方法最简单了。我们可以直接使用Kibana中提供的功能实现这个需求。我们首先来准备数据：再接着选择Add data。这样我们的Elasticsearch中就会有我们的eCommerce索引了。我们接着选择Discover，并选择我们刚才建立的eCommerce索引。我们同时要记得在time picker里选择我们所需要的时间段：我们可以做一些我们想要的搜索：我们点击左上角的Save按钮：接下来，我们点击Share按钮：这样我们就可以得到我们当前搜索结果的csv文件。我们只需要在Kibana中下载即可：方法二我们可以使用Logstash提供的功能来做这个。这个的好处是可以通过编程的方式来进行。Logstash不只光可以把数据传上Elasticsearch，同时它还可以把数据从Elasticsearch中导出。我们首先必须安装和Elasticsearch相同版本的 Logstash。如果大家还不指定如安装Logstash的话，请参阅我的文章“如何安装Elastic栈中的Logstash”。我们可以进一步查看我们的Logstash是否支持csv的output:1./bin/logstash-plugin list --group output显示：123456789101112131415161718192021logstash-output-cloudwatchlogstash-output-csvlogstash-output-elastic_app_searchlogstash-output-elasticsearchlogstash-output-emaillogstash-output-filelogstash-output-graphitelogstash-output-httplogstash-output-lumberjacklogstash-output-nagioslogstash-output-nulllogstash-output-pipelogstash-output-rabbitmqlogstash-output-redislogstash-output-s3logstash-output-snslogstash-output-sqslogstash-output-stdoutlogstash-output-tcplogstash-output-udplogstash-output-webhdfs显然logstash-ouput-csv是在列表中。也就是说我们logstash支持csv格式的输出。我们建立如下的Logstash的配置文件：12345678910111213141516171819202122232425262728293031323334353637383940convert_csv.confinput &#123; elasticsearch &#123; hosts =&gt; &quot;localhost:9200&quot; index =&gt; &quot;kibana_sample_data_ecommerce&quot; query =&gt; &apos;&#123; &quot;query&quot;: &#123; &quot;bool&quot;: &#123; &quot;must&quot;: [ &#123; &quot;match&quot;: &#123; &quot;currency&quot;: &quot;EUR&quot; &#125; &#125;, &#123; &quot;match&quot;: &#123; &quot;products.quantity&quot;: 1 &#125; &#125; ] &#125; &#125; &#125;&apos; &#125;&#125; output &#123; csv &#123; # This is the fields that you would like to output in CSV format. # The field needs to be one of the fields shown in the output when you run your # Elasticsearch query fields =&gt; [&quot;category&quot;, &quot;customer_birth_date&quot;, &quot;customer_first_name&quot;, &quot;customer_full_name&quot;, &quot;day_of_week&quot;] # This is where we store output. We can use several files to store our output # by using a timestamp to determine the filename where to store output. path =&gt; &quot;/Users/liuxg/tmp/csv-export.csv&quot; &#125;&#125;请注意上面的path需要自己去定义时候自己环境的路径。这里我们在fields里定义了我们想要的字段。然后，我们可以运行我们的Logstash应用：1./bin/logstash -f ~/data/convert_csv.conf这样在我们定义的文件路径/Users/liuxg/tmp/csv-export.csv可以看到一个输出的csv文件。我们可以打开这个文件，并看到像这样的文档：]]></content>
      <categories>
        <category>elk</category>
      </categories>
      <tags>
        <tag>elk</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch：top_hits aggregation]]></title>
    <url>%2F2020%2F01%2F02%2FElasticsearch%EF%BC%9Atop_hits%20aggregation%2F</url>
    <content type="text"><![CDATA[top_hits指标聚合器跟踪要聚合的最相关文档。 该聚合器旨在用作子聚合器，以便可以按存储分区汇总最匹配的文档。top_hits聚合器可以有效地用于通过存储桶聚合器按某些字段对结果集进行分组。 一个或多个存储桶聚合器确定将结果集切成哪些属性。选项：from-要获取的第一个结果的偏移量。size-每个存储桶要返回的最匹配匹配项的最大数目。 默认情况下，返回前三个匹配项。排序-匹配的热门匹配的排序方式。 默认情况下，命中按主要查询的分数排序。我们还是来用一个例子来展示如何使用这个：准备数据：我们选用Kibana里带的官方的Sample web logs来作为我们的索引：然后加载我们的索引：这样我们的数据就加载完成了。Top hits aggregation首先，我们先做一个简单的基于hosts的aggregation:123456789101112GET kibana_sample_data_logs/_search&#123; &quot;size&quot;: 0, &quot;aggs&quot;: &#123; &quot;hosts&quot;: &#123; &quot;terms&quot;: &#123; &quot;field&quot;: &quot;host.keyword&quot;, &quot;size&quot;: 2 &#125; &#125; &#125;&#125;上面的搜索的结果是我们想得到2个桶的数据（这里为了说明问题的方便，设定为2）。而这两个桶是基于hosts的值。搜索的结果是：12345678910111213141516&quot;aggregations&quot; : &#123; &quot;hosts&quot; : &#123; &quot;doc_count_error_upper_bound&quot; : 0, &quot;sum_other_doc_count&quot; : 2807, &quot;buckets&quot; : [ &#123; &quot;key&quot; : &quot;artifacts.elastic.co&quot;, &quot;doc_count&quot; : 6488 &#125;, &#123; &quot;key&quot; : &quot;www.elastic.co&quot;, &quot;doc_count&quot; : 4779 &#125; ] &#125; &#125;现在的要求是：我们想针对这里的每个桶得到按照我们需要排序的前面的几个结果，比如下面的搜索：12345678910111213141516171819202122232425262728293031323334GET kibana_sample_data_logs/_search&#123; &quot;size&quot;: 0, &quot;aggs&quot;: &#123; &quot;hosts&quot;: &#123; &quot;terms&quot;: &#123; &quot;field&quot;: &quot;host.keyword&quot;, &quot;size&quot;: 2 &#125;, &quot;aggs&quot;: &#123; &quot;most_bytes&quot;: &#123; &quot;top_hits&quot;: &#123; &quot;sort&quot;: [ &#123; &quot;bytes&quot;: &#123; &quot;order&quot;: &quot;desc&quot; &#125; &#125; ], &quot;_source&quot;: &#123; &quot;includes&quot;: [ &quot;bytes&quot;, &quot;hosts&quot;, &quot;ip&quot;, &quot;clientip&quot; ] &#125;, &quot;size&quot;: 2 &#125; &#125; &#125; &#125; &#125;&#125;上面实际上市一个pipleline的聚合。它在针对上面的桶来做了一个top_hits的聚合。针对每个桶，我们需要安装bytes的大小，降序排列，并且每个桶只需要两个数据：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394&quot;aggregations&quot; : &#123; &quot;hosts&quot; : &#123; &quot;doc_count_error_upper_bound&quot; : 0, &quot;sum_other_doc_count&quot; : 2807, &quot;buckets&quot; : [ &#123; &quot;key&quot; : &quot;artifacts.elastic.co&quot;, &quot;doc_count&quot; : 6488, &quot;most_bytes&quot; : &#123; &quot;hits&quot; : &#123; &quot;total&quot; : &#123; &quot;value&quot; : 6488, &quot;relation&quot; : &quot;eq&quot; &#125;, &quot;max_score&quot; : null, &quot;hits&quot; : [ &#123; &quot;_index&quot; : &quot;kibana_sample_data_logs&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;dnNIHm8BjrINWI3xXlRc&quot;, &quot;_score&quot; : null, &quot;_source&quot; : &#123; &quot;bytes&quot; : 19929, &quot;ip&quot; : &quot;127.155.255.9&quot;, &quot;clientip&quot; : &quot;127.155.255.9&quot; &#125;, &quot;sort&quot; : [ 19929 ] &#125;, &#123; &quot;_index&quot; : &quot;kibana_sample_data_logs&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;OXNIHm8BjrINWI3xX1td&quot;, &quot;_score&quot; : null, &quot;_source&quot; : &#123; &quot;bytes&quot; : 19904, &quot;ip&quot; : &quot;100.177.58.231&quot;, &quot;clientip&quot; : &quot;100.177.58.231&quot; &#125;, &quot;sort&quot; : [ 19904 ] &#125; ] &#125; &#125; &#125;, &#123; &quot;key&quot; : &quot;www.elastic.co&quot;, &quot;doc_count&quot; : 4779, &quot;most_bytes&quot; : &#123; &quot;hits&quot; : &#123; &quot;total&quot; : &#123; &quot;value&quot; : 4779, &quot;relation&quot; : &quot;eq&quot; &#125;, &quot;max_score&quot; : null, &quot;hits&quot; : [ &#123; &quot;_index&quot; : &quot;kibana_sample_data_logs&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;4nNIHm8BjrINWI3xYWQl&quot;, &quot;_score&quot; : null, &quot;_source&quot; : &#123; &quot;bytes&quot; : 19986, &quot;ip&quot; : &quot;233.204.30.48&quot;, &quot;clientip&quot; : &quot;233.204.30.48&quot; &#125;, &quot;sort&quot; : [ 19986 ] &#125;, &#123; &quot;_index&quot; : &quot;kibana_sample_data_logs&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;wnNIHm8BjrINWI3xW0Rj&quot;, &quot;_score&quot; : null, &quot;_source&quot; : &#123; &quot;bytes&quot; : 19956, &quot;ip&quot; : &quot;129.237.102.30&quot;, &quot;clientip&quot; : &quot;129.237.102.30&quot; &#125;, &quot;sort&quot; : [ 19956 ] &#125; ] &#125; &#125; &#125; ] &#125;&#125;从上面的返回结果可以看出来两个hosts artifacts.elastic.co及www.elastic.co各返回两个结果，并且它们是按照bytes的大小进行降序排列的。细心的读者可能会发现这个和我之前介绍的field collapsing有些类似。只是field collapsing里针对每个桶有一个结果，并且是按照我们的要求进行排序的最高结果的那个。当然我们也可以含有多几个返回结果在inner_hits之中。参考：【1】https://www.elastic.co/guide/en/elasticsearch/reference/current/search-aggregations-metrics-top-hits-aggregation.html]]></content>
      <categories>
        <category>elk</category>
      </categories>
      <tags>
        <tag>elk</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch：Smart Chinese Analysis plugin]]></title>
    <url>%2F2020%2F01%2F02%2FElasticsearch%EF%BC%9ASmart%20Chinese%20Analysis%20plugin%2F</url>
    <content type="text"><![CDATA[Smart Chinese Analysis插件将Lucene的Smart Chinese分析模块集成到Elasticsearch中，用于分析中文或中英文混合文本。 支持的分析器在大型训练语料库上使用基于隐马尔可夫（Markov）模型的概率知识来查找简体中文文本的最佳分词。 它使用的策略是首先将输入文本分解为句子，然后对句子进行切分以获得单词。 该插件提供了一个称为smartcn分析器的分析器，以及一个称为smartcn_tokenizer的标记器。 请注意，两者均不能使用任何参数进行配置。要将smartcn Analysis插件安装在Elasticsearch Docker容器中，请使用以下屏幕截图中显示的命令。 然后，我们重新启动容器以使插件生效：1./bin/elasticsearch-plugin install analysis-smartcn在Elasticsearch的安装目录运行上面的命令。显示的结果如下：1234567891011121314$ ./bin/elasticsearch-plugin install analysis-smartcn-&gt; Downloading analysis-smartcn from elastic[=================================================] 100% WARNING: An illegal reflective access operation has occurredWARNING: Illegal reflective access by org.bouncycastle.jcajce.provider.drbg.DRBG (file:/Users/liuxg/elastic/elasticsearch-7.3.0/lib/tools/plugin-cli/bcprov-jdk15on-1.61.jar) to constructor sun.security.provider.Sun()WARNING: Please consider reporting this to the maintainers of org.bouncycastle.jcajce.provider.drbg.DRBGWARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operationsWARNING: All illegal access operations will be denied in a future release-&gt; Installed analysis-smartcn(base) localhost:elasticsearch-7.3.0 liuxg$ ./bin/elasticsearch-plugin listanalysis-icuanalysis-ikanalysis-smartcnpinyin上面显示我们已经成功地把analysis-smartcn安装成功了。针对docker的安装，我们可以通过如下的命令来进入到docker里，再进行安装：12345$ docker exec -it es01 /bin/bash[root@ec4d19f59a7d elasticsearch]# lsLICENSE.txt README.textile config jdk logs pluginsNOTICE.txt bin data lib modules[root@ec4d19f59a7d elasticsearch]#在这里es01是docker中的Elasticsearch实例。具体安装请参阅我的文章“Elastic：用Docker部署Elastic栈”。注意：在我们安装好smartcn分析器后，我们必须重新启动Elasticsearch使它开始起作用。实例在下面，我们在Kibana中用一个实例来展示这个用法：12345POST _analyze &#123; &quot;text&quot;: &quot;股市，投资，稳，赚，不，赔，必修课，如何，做，好，仓，位，管理，和，情绪，管理&quot;, &quot;analyzer&quot;: &quot;smartcn&quot;&#125;显示结果：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116&#123; &quot;tokens&quot; : [ &#123; &quot;token&quot; : &quot;股市&quot;, &quot;start_offset&quot; : 0, &quot;end_offset&quot; : 2, &quot;type&quot; : &quot;word&quot;, &quot;position&quot; : 0 &#125;, &#123; &quot;token&quot; : &quot;投资&quot;, &quot;start_offset&quot; : 3, &quot;end_offset&quot; : 5, &quot;type&quot; : &quot;word&quot;, &quot;position&quot; : 2 &#125;, &#123; &quot;token&quot; : &quot;稳&quot;, &quot;start_offset&quot; : 6, &quot;end_offset&quot; : 7, &quot;type&quot; : &quot;word&quot;, &quot;position&quot; : 4 &#125;, &#123; &quot;token&quot; : &quot;赚&quot;, &quot;start_offset&quot; : 8, &quot;end_offset&quot; : 9, &quot;type&quot; : &quot;word&quot;, &quot;position&quot; : 6 &#125;, &#123; &quot;token&quot; : &quot;不&quot;, &quot;start_offset&quot; : 10, &quot;end_offset&quot; : 11, &quot;type&quot; : &quot;word&quot;, &quot;position&quot; : 8 &#125;, &#123; &quot;token&quot; : &quot;赔&quot;, &quot;start_offset&quot; : 12, &quot;end_offset&quot; : 13, &quot;type&quot; : &quot;word&quot;, &quot;position&quot; : 10 &#125;, &#123; &quot;token&quot; : &quot;必修课&quot;, &quot;start_offset&quot; : 14, &quot;end_offset&quot; : 17, &quot;type&quot; : &quot;word&quot;, &quot;position&quot; : 12 &#125;, &#123; &quot;token&quot; : &quot;如何&quot;, &quot;start_offset&quot; : 18, &quot;end_offset&quot; : 20, &quot;type&quot; : &quot;word&quot;, &quot;position&quot; : 14 &#125;, &#123; &quot;token&quot; : &quot;做&quot;, &quot;start_offset&quot; : 21, &quot;end_offset&quot; : 22, &quot;type&quot; : &quot;word&quot;, &quot;position&quot; : 16 &#125;, &#123; &quot;token&quot; : &quot;好&quot;, &quot;start_offset&quot; : 23, &quot;end_offset&quot; : 24, &quot;type&quot; : &quot;word&quot;, &quot;position&quot; : 18 &#125;, &#123; &quot;token&quot; : &quot;仓&quot;, &quot;start_offset&quot; : 25, &quot;end_offset&quot; : 26, &quot;type&quot; : &quot;word&quot;, &quot;position&quot; : 20 &#125;, &#123; &quot;token&quot; : &quot;位&quot;, &quot;start_offset&quot; : 27, &quot;end_offset&quot; : 28, &quot;type&quot; : &quot;word&quot;, &quot;position&quot; : 22 &#125;, &#123; &quot;token&quot; : &quot;管理&quot;, &quot;start_offset&quot; : 29, &quot;end_offset&quot; : 31, &quot;type&quot; : &quot;word&quot;, &quot;position&quot; : 24 &#125;, &#123; &quot;token&quot; : &quot;和&quot;, &quot;start_offset&quot; : 32, &quot;end_offset&quot; : 33, &quot;type&quot; : &quot;word&quot;, &quot;position&quot; : 26 &#125;, &#123; &quot;token&quot; : &quot;情绪&quot;, &quot;start_offset&quot; : 34, &quot;end_offset&quot; : 36, &quot;type&quot; : &quot;word&quot;, &quot;position&quot; : 28 &#125;, &#123; &quot;token&quot; : &quot;管理&quot;, &quot;start_offset&quot; : 37, &quot;end_offset&quot; : 39, &quot;type&quot; : &quot;word&quot;, &quot;position&quot; : 30 &#125; ]&#125;]]></content>
      <categories>
        <category>elk</category>
      </categories>
      <tags>
        <tag>elk</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch：significant terms aggregation]]></title>
    <url>%2F2020%2F01%2F02%2FElasticsearch%EF%BC%9Asignificant%20terms%20aggregation%2F</url>
    <content type="text"><![CDATA[在本文中，我们将重点关注significant terms和significant text聚合。这些聚合旨在搜索数据集中有趣和/或不寻常的术语，这些术语可以告诉您有关数据的隐藏属性的更多信息。此功能对于以下用例特别有用：为用户查询标识包含同义词，首字母缩略词等的相关文档。例如，当用户搜索H1N1时，重要术语聚合可能会建议带有“bird flu”的文档。识别数据中的异常和有趣的事件。例如，通过基于位置过滤文档，我们可以确定特定区域中最常见的犯罪类型。使用对整数字段（例如身高，体重，收入等）的significant term聚合来确定一组主题的最重要属性。应当注意，重要术语和重要文本聚合都对直接查询（前景集）和索引中所有其他文档（背景集）检索的文档执行复杂的统计计算。因此，两种聚合都需要大量计算，因此应正确配置以快速工作。但是，一旦在本教程的帮助下掌握了它们，您将获得一个强大的工具，可以在应用程序中构建非常有用的功能并从数据集中获取有用的见解。让我们开始吧！在教程开始，我们假定您已经把Elasticsearch及Kibana完整地安装好了。创建Index mapping为了说明significant terms和significant text的工作方式，我们首先需要创建一个测试“news”索引来存储新闻文章的集合。 索引映射将包含诸如作者，出版日期，文章标题，视图数和主题之类的字段。 让我们创建映射：1234567891011121314151617181920212223PUT news&#123; &quot;mappings&quot;: &#123; &quot;properties&quot;: &#123; &quot;published&quot;: &#123; &quot;type&quot;: &quot;date&quot;, &quot;format&quot;: &quot;dateOptionalTime&quot; &#125;, &quot;author&quot;: &#123; &quot;type&quot;: &quot;keyword&quot; &#125;, &quot;title&quot;: &#123; &quot;type&quot;: &quot;text&quot; &#125;, &quot;topic&quot;: &#123; &quot;type&quot;: &quot;keyword&quot; &#125;, &quot;views&quot;: &#123; &quot;type&quot;: &quot;integer&quot; &#125; &#125; &#125;&#125;如您所见，我们在topic和author字段中使用了keyword数据类型，在title字段中使用了text数据类型。 提醒您，关键字字段只能按其确切值进行搜索，而文本字段可用于全文搜索。接下来，让我们使用Bulk API将一些任意新闻文档添加到索引中。123456789101112131415161718192021POST news/_bulk&#123;&quot;index&quot;:&#123;&quot;_index&quot;:&quot;news&quot;&#125;&#125;&#123;&quot;author&quot;:&quot;John Michael&quot;,&quot;published&quot;:&quot;2018-07-08&quot;,&quot;title&quot;:&quot;Tesla is flirting with its lowest close in over 1 1/2 years (TSLA)&quot;,&quot;topic&quot;:&quot;automobile&quot;,&quot;views&quot;:&quot;431&quot;&#125;&#123;&quot;index&quot;:&#123;&quot;_index&quot;:&quot;news&quot;&#125;&#125;&#123;&quot;author&quot;:&quot;John Michael&quot;,&quot;published&quot;:&quot;2018-07-22&quot;,&quot;title&quot;:&quot;Tesla to end up like Lehman Brothers (TSLA)&quot;,&quot;topic&quot;:&quot;automobile&quot;,&quot;views&quot;:&quot;1921&quot;&#125;&#123;&quot;index&quot;:&#123;&quot;_index&quot;:&quot;news&quot;&#125;&#125;&#123;&quot;author&quot;:&quot;John Michael&quot;,&quot;published&quot;:&quot;2018-07-29&quot;,&quot;title&quot;:&quot;Tesla (TSLA) official says that they are going to release a new self-driving car model in the coming year&quot;,&quot;topic&quot;:&quot;automobile&quot;,&quot;views&quot;:&quot;1849&quot;&#125;&#123;&quot;index&quot;:&#123;&quot;_index&quot;:&quot;news&quot;&#125;&#125;&#123;&quot;author&quot;:&quot;John Michael&quot;,&quot;published&quot;:&quot;2018-08-14&quot;,&quot;title&quot;:&quot;Five ways Tesla uses AI and Big Data&quot;,&quot;topic&quot;:&quot;ai&quot;,&quot;views&quot;:&quot;871&quot;&#125;&#123;&quot;index&quot;:&#123;&quot;_index&quot;:&quot;news&quot;&#125;&#125;&#123;&quot;author&quot;:&quot;John Michael&quot;,&quot;published&quot;:&quot;2018-08-14&quot;,&quot;title&quot;:&quot;Toyota partners with Tesla (TSLA) to improve the security of self-driving cars&quot;,&quot;topic&quot;:&quot;automobile&quot;,&quot;views&quot;:&quot;871&quot;&#125;&#123;&quot;index&quot;:&#123;&quot;_index&quot;:&quot;news&quot;&#125;&#125;&#123;&quot;author&quot;:&quot;Robert Cann&quot;,&quot;published&quot;:&quot;2018-08-25&quot;,&quot;title&quot;:&quot;Is AI dangerous for humanity&quot;,&quot;topic&quot;:&quot;ai&quot;,&quot;views&quot;:&quot;981&quot;&#125;&#123;&quot;index&quot;:&#123;&quot;_index&quot;:&quot;news&quot;&#125;&#125;&#123;&quot;author&quot;:&quot;Robert Cann&quot;,&quot;published&quot;:&quot;2018-09-13&quot;,&quot;title&quot;:&quot;Is AI dangerous for humanity&quot;,&quot;topic&quot;:&quot;ai&quot;,&quot;views&quot;:&quot;871&quot;&#125;&#123;&quot;index&quot;:&#123;&quot;_index&quot;:&quot;news&quot;&#125;&#125;&#123;&quot;author&quot;:&quot;Robert Cann&quot;,&quot;published&quot;:&quot;2018-09-27&quot;,&quot;title&quot;:&quot;Introduction to Generative Adversarial Networks (GANs) in self-driving cars&quot;,&quot;topic&quot;:&quot;automobile&quot;,&quot;views&quot;:&quot;1183&quot;&#125;&#123;&quot;index&quot;:&#123;&quot;_index&quot;:&quot;news&quot;&#125;&#125;&#123;&quot;author&quot;:&quot;Robert Cann&quot;,&quot;published&quot;:&quot;2018-10-09&quot;,&quot;title&quot;:&quot;Introduction to Natural Language Processing&quot;,&quot;topic&quot;:&quot;ai&quot;,&quot;views&quot;:&quot;786&quot;&#125;&#123;&quot;index&quot;:&#123;&quot;_index&quot;:&quot;news&quot;&#125;&#125;&#123;&quot;author&quot;:&quot;Robert Cann&quot;,&quot;published&quot;:&quot;2018-10-15&quot;,&quot;title&quot;:&quot;New Distant Objects Found in the Fight for Planet X &quot;,&quot;topic&quot;:&quot;astronomy&quot;,&quot;views&quot;:&quot;542&quot;&#125;在这里，我们共同插入了20条数据。Significant Terms Aggregation正如我们已经提到的，重要的术语聚合可以识别数据中异常和有趣的术语。 对于以下用例，聚合功能非常强大：识别与用户查询相关的相关术语/文档。 例如，当用户查询“Spain”时，聚合可能会建议诸如“Madrid”，“Corrida”之类的术语，或有关Spain的文档中常见的其他任何术语。Significant term聚合可用于自动新闻分类器，其中基于频繁连接的术语图对文档进行分类。发现数据中的异常。 例如，借助这种汇总，我们可以识别某些地理区域中的异常犯罪类型或疾病。重要的是要理解，significant terms聚合选择的术语不仅是文档集中最受欢迎的术语。 例如，即使首字母缩略词“ MSFT”仅存在于一千万个文档索引中的10个文档中，但如果在与用户查询“ Microsoft”相匹配的50个文档中有10个找到了这个MSFT，则它仍然是相关的。 该频率使acronym（比如MSFT）与用户的搜索相关。为了识别重要术语，聚合对与查询匹配的搜索结果以及从中收集结果的索引执行复杂的统计分析。 与查询直接匹配的搜索结果代表前景集，而从中检索它们的索引代表背景集。 重要术语聚合的任务是比较这些集合并找到最常与用户查询关联的术语。上面的意思可以用上面的一幅图来解释。比如上面的绿色代表一个很大的索引，它里面可能含有比如Nokia这个term很高的出现率。即便如此，只要我们所搜索的FG那个红色的结果里，它出现的几率非常低，也不能够出现在significant terms的聚合里。相反，如果一个term比如TECNO（中国一个非常出名的在非洲的品牌）出现我们所搜索的set里（比如搜索 africa phone），那么我们搜索的聚合将会是是TECNO尽管TECNO可能在整个BG所包含的文档里出现的几率非常之低。让我们使用真实示例，演示聚合如何工作。 在下面的示例中，我们将尝试在索引中查找每个author的重要topics。 为此，我们首先在author字段上使用术语“桶聚合(bucket aggregation)”。 您还记得，terms aggregation为找到索引的所有唯一术语（即author）构造了存储桶。 接下来，我们在“topics”字段上使用significant terms聚合，以找出每个author的最重要topic。 看一下下面的查询：123456789101112131415161718GET news/_search&#123; &quot;size&quot;: 0, &quot;aggregations&quot;: &#123; &quot;authors&quot;: &#123; &quot;terms&quot;: &#123; &quot;field&quot;: &quot;author&quot; &#125;, &quot;aggregations&quot;: &#123; &quot;significant_topic_types&quot;: &#123; &quot;significant_terms&quot;: &#123; &quot;field&quot;: &quot;topic&quot; &#125; &#125; &#125; &#125; &#125;&#125;显示的结果为：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758&#123; &quot;took&quot; : 1, &quot;timed_out&quot; : false, &quot;_shards&quot; : &#123; &quot;total&quot; : 1, &quot;successful&quot; : 1, &quot;skipped&quot; : 0, &quot;failed&quot; : 0 &#125;, &quot;hits&quot; : &#123; &quot;total&quot; : &#123; &quot;value&quot; : 20, &quot;relation&quot; : &quot;eq&quot; &#125;, &quot;max_score&quot; : null, &quot;hits&quot; : [ ] &#125;, &quot;aggregations&quot; : &#123; &quot;authors&quot; : &#123; &quot;doc_count_error_upper_bound&quot; : 0, &quot;sum_other_doc_count&quot; : 0, &quot;buckets&quot; : [ &#123; &quot;key&quot; : &quot;John Michael&quot;, &quot;doc_count&quot; : 10, &quot;significant_topic_types&quot; : &#123; &quot;doc_count&quot; : 10, &quot;bg_count&quot; : 20, &quot;buckets&quot; : [ &#123; &quot;key&quot; : &quot;automobile&quot;, &quot;doc_count&quot; : 8, &quot;score&quot; : 0.4800000000000001, &quot;bg_count&quot; : 10 &#125; ] &#125; &#125;, &#123; &quot;key&quot; : &quot;Robert Cann&quot;, &quot;doc_count&quot; : 10, &quot;significant_topic_types&quot; : &#123; &quot;doc_count&quot; : 10, &quot;bg_count&quot; : 20, &quot;buckets&quot; : [ &#123; &quot;key&quot; : &quot;ai&quot;, &quot;doc_count&quot; : 6, &quot;score&quot; : 0.2999999999999999, &quot;bg_count&quot; : 8 &#125; ] &#125; &#125; ] &#125; &#125;&#125;显然对于作者John Michael来说，在他所发表的书里automobile是最经常出现的词。共有8次，而bg_count是10。同样对于作者Robert Cann来说，在他发布的作品里，ai是最最经常出现的词，在他的8个作品中，有6词提到ai。可以断定他就是一个ai专家！针对上面的significant terms聚合查询，我们也可以通过如下的方法来查询针对某个作者（author）的聚合。12345678910111213141516GET news/_search&#123; &quot;size&quot;: 0, &quot;query&quot;: &#123; &quot;term&quot;: &#123; &quot;author&quot;: &quot;John Michael&quot; &#125; &#125;, &quot;aggregations&quot;: &#123; &quot;significant_topics&quot;: &#123; &quot;significant_terms&quot;: &#123; &quot;field&quot;: &quot;topic&quot; &#125; &#125; &#125;&#125;显示的结果为：1234567891011121314151617181920212223242526272829303132&#123; &quot;took&quot; : 0, &quot;timed_out&quot; : false, &quot;_shards&quot; : &#123; &quot;total&quot; : 1, &quot;successful&quot; : 1, &quot;skipped&quot; : 0, &quot;failed&quot; : 0 &#125;, &quot;hits&quot; : &#123; &quot;total&quot; : &#123; &quot;value&quot; : 10, &quot;relation&quot; : &quot;eq&quot; &#125;, &quot;max_score&quot; : null, &quot;hits&quot; : [ ] &#125;, &quot;aggregations&quot; : &#123; &quot;significant_topics&quot; : &#123; &quot;doc_count&quot; : 10, &quot;bg_count&quot; : 20, &quot;buckets&quot; : [ &#123; &quot;key&quot; : &quot;automobile&quot;, &quot;doc_count&quot; : 8, &quot;score&quot; : 0.4800000000000001, &quot;bg_count&quot; : 10 &#125; ] &#125; &#125;&#125;这种表述更适合解释我们上面的那个BG和FG的图。针对significant text aggregation，基本它和significant terms aggregation非常相似，只是它作用于一个text字段而不是一个keyword字段。比如:12345678910111213141516GET news/_search&#123; &quot;size&quot;: 0, &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;title&quot;: &quot;Tesla ai&quot; &#125; &#125;, &quot;aggregations&quot;: &#123; &quot;significant_topics&quot;: &#123; &quot;significant_text&quot;: &#123; &quot;field&quot;: &quot;topic&quot; &#125; &#125; &#125;&#125;注意这里的title字段是text，它同时搜索Telsa及ai，再根据这两个词来进行聚合：1234567891011121314151617181920212223242526272829303132333435363738&#123; &quot;took&quot; : 1, &quot;timed_out&quot; : false, &quot;_shards&quot; : &#123; &quot;total&quot; : 1, &quot;successful&quot; : 1, &quot;skipped&quot; : 0, &quot;failed&quot; : 0 &#125;, &quot;hits&quot; : &#123; &quot;total&quot; : &#123; &quot;value&quot; : 14, &quot;relation&quot; : &quot;eq&quot; &#125;, &quot;max_score&quot; : null, &quot;hits&quot; : [ ] &#125;, &quot;aggregations&quot; : &#123; &quot;significant_topics&quot; : &#123; &quot;doc_count&quot; : 14, &quot;bg_count&quot; : 20, &quot;buckets&quot; : [ &#123; &quot;key&quot; : &quot;automobile&quot;, &quot;doc_count&quot; : 8, &quot;score&quot; : 0.08163265306122446, &quot;bg_count&quot; : 10 &#125;, &#123; &quot;key&quot; : &quot;ai&quot;, &quot;doc_count&quot; : 6, &quot;score&quot; : 0.030612244897959134, &quot;bg_count&quot; : 8 &#125; ] &#125; &#125;&#125;参考：【1】significant terms aggregation(https://www.elastic.co/guide/en/elasticsearch/reference/current/search-aggregations-bucket-significantterms-aggregation.html)【2】significant text aggregation(https://www.elastic.co/guide/en/elasticsearch/reference/master/search-aggregations-bucket-significanttext-aggregation.html)]]></content>
      <categories>
        <category>elk</category>
      </categories>
      <tags>
        <tag>elk</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch：search template]]></title>
    <url>%2F2020%2F01%2F02%2FElasticsearch%EF%BC%9Asearch%20template%2F</url>
    <content type="text"><![CDATA[我们发现一些用户经常编写了一些非常冗长和复杂的查询 - 在很多情况下，相同的查询会一遍又一遍地执行，但是会有一些不同的值作为参数来查询。在这种情况下，我们觉得使用一个search template（搜索模板）来做这样的工作非常合适。搜索模板允许您使用可在执行时定义的参数定义查询。Search template的好处是：避免在多个地方重复代码更容易测试和执行您的查询在应用程序间共享查询允许用户只执行一些预定义的查询将搜索逻辑与应用程序逻辑分离定义一个Search template首先，我们来定义一个search template来看看它到底是什么东西。使用_scripts端点将模板存储在集群状态中。在search template中使用的语言叫做mustache。(http://mustache.github.io/mustache.5.html)12345678910111213POST _scripts/my_search_template&#123; &quot;script&quot;: &#123; &quot;lang&quot;: &quot;mustache&quot;, &quot;source&quot;: &#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;&#123;&#123;my_field&#125;&#125;&quot;: &quot;&#123;&#123;my_value&#125;&#125;&quot; &#125; &#125; &#125; &#125;&#125;在这里，我们定义了一个叫做my_search_template的search template。如果我们想更新这个search template，我们可以直接进行修改，然后再次运行上面的命令即可。在match的字段里，我们定义了两个参数：my_field及my_value。下面，我们来首先建立一个叫做twitter的数据库：12345678910111213141516171819202122232425262728293031PUT twitter/_doc/1&#123; &quot;user&quot; : &quot;双榆树-张三&quot;, &quot;message&quot; : &quot;今儿天气不错啊，出去转转去&quot;, &quot;uid&quot; : 2, &quot;age&quot; : 20, &quot;city&quot; : &quot;北京&quot;, &quot;province&quot; : &quot;北京&quot;, &quot;country&quot; : &quot;中国&quot;, &quot;address&quot; : &quot;中国北京市海淀区&quot;, &quot;location&quot; : &#123; &quot;lat&quot; : &quot;39.970718&quot;, &quot;lon&quot; : &quot;116.325747&quot; &#125;&#125; PUT twitter/_doc/2&#123; &quot;user&quot; : &quot;虹桥-老吴&quot;, &quot;message&quot; : &quot;好友来了都今天我生日，好友来了,什么 birthday happy 就成!&quot;, &quot;uid&quot; : 7, &quot;age&quot; : 90, &quot;city&quot; : &quot;上海&quot;, &quot;province&quot; : &quot;上海&quot;, &quot;country&quot; : &quot;中国&quot;, &quot;address&quot; : &quot;中国上海市闵行区&quot;, &quot;location&quot; : &#123; &quot;lat&quot; : &quot;31.175927&quot;, &quot;lon&quot; : &quot;121.383328&quot; &#125;&#125;我们这里把上面的两个文档存于到twitter的index之中。我们现在可以使用我们刚才定义的search template来进行搜索：12345678GET twitter/_search/template&#123; &quot;id&quot;: &quot;my_search_template&quot;, &quot;params&quot;: &#123; &quot;my_field&quot;: &quot;city&quot;, &quot;my_value&quot;: &quot;北京&quot; &#125;&#125;显示的结果是：123456789101112131415161718192021222324252627282930313233343536373839&#123; &quot;took&quot; : 1, &quot;timed_out&quot; : false, &quot;_shards&quot; : &#123; &quot;total&quot; : 1, &quot;successful&quot; : 1, &quot;skipped&quot; : 0, &quot;failed&quot; : 0 &#125;, &quot;hits&quot; : &#123; &quot;total&quot; : &#123; &quot;value&quot; : 1, &quot;relation&quot; : &quot;eq&quot; &#125;, &quot;max_score&quot; : 0.9808292, &quot;hits&quot; : [ &#123; &quot;_index&quot; : &quot;twitter&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;1&quot;, &quot;_score&quot; : 0.9808292, &quot;_source&quot; : &#123; &quot;user&quot; : &quot;双榆树-张三&quot;, &quot;message&quot; : &quot;今儿天气不错啊，出去转转去&quot;, &quot;uid&quot; : 2, &quot;age&quot; : 20, &quot;city&quot; : &quot;北京&quot;, &quot;province&quot; : &quot;北京&quot;, &quot;country&quot; : &quot;中国&quot;, &quot;address&quot; : &quot;中国北京市海淀区&quot;, &quot;location&quot; : &#123; &quot;lat&quot; : &quot;39.970718&quot;, &quot;lon&quot; : &quot;116.325747&quot; &#125; &#125; &#125; ] &#125;&#125;显示它只显示了我们的city为北京的一个文档，另外一个上海的文档没有做任何的显示。说明我们定义的search template是工作的。条件判断在Mustache语言中，它没有if/else这样的判断，但是你可以定section来跳过它如果那个变量是false还是没有被定义：123&#123;&#123;#param1&#125;&#125; &quot;This section is skipped if param1 is null or false&quot;&#123;&#123;/param1&#125;&#125;我们定义如下的一个search template:1234567891011121314151617181920212223242526272829303132POST _scripts/docs_from_beijing_and_age&#123; &quot;script&quot;: &#123; &quot;lang&quot;: &quot;mustache&quot;, &quot;source&quot;: &quot;&quot;&quot; &#123; &quot;query&quot;: &#123; &quot;bool&quot;: &#123; &quot;must&quot;: [ &#123; &quot;match&quot;: &#123; &quot;city&quot;: &quot;&#123;&#123;search_term&#125;&#125;&quot; &#125; &#125; &#123;&#123;#search_age&#125;&#125; , &#123; &quot;range&quot;: &#123; &quot;age&quot;: &#123; &quot;gte&quot;: &#123;&#123;search_age&#125;&#125; &#125; &#125; &#125; &#123;&#123;/search_age&#125;&#125; ] &#125; &#125; &#125;&quot;&quot;&quot; &#125;&#125;在这里，我们同时定义了两个变量：search_term及search_age。针对search_age，我们做了一个判断，如果它有定义，及做一个range的查询。如果没有定义，就只用search_term。那么我们来做如下的实验：1234567GET twitter/_search/template&#123; &quot;id&quot;: &quot;docs_from_beijing_and_age&quot;, &quot;params&quot;: &#123; &quot;search_term&quot;: &quot;北京&quot; &#125;&#125;显示的结果是：123456789101112131415161718192021222324252627282930313233343536373839&#123; &quot;took&quot; : 0, &quot;timed_out&quot; : false, &quot;_shards&quot; : &#123; &quot;total&quot; : 1, &quot;successful&quot; : 1, &quot;skipped&quot; : 0, &quot;failed&quot; : 0 &#125;, &quot;hits&quot; : &#123; &quot;total&quot; : &#123; &quot;value&quot; : 1, &quot;relation&quot; : &quot;eq&quot; &#125;, &quot;max_score&quot; : 0.9808292, &quot;hits&quot; : [ &#123; &quot;_index&quot; : &quot;twitter&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;1&quot;, &quot;_score&quot; : 0.9808292, &quot;_source&quot; : &#123; &quot;user&quot; : &quot;双榆树-张三&quot;, &quot;message&quot; : &quot;今儿天气不错啊，出去转转去&quot;, &quot;uid&quot; : 2, &quot;age&quot; : 20, &quot;city&quot; : &quot;北京&quot;, &quot;province&quot; : &quot;北京&quot;, &quot;country&quot; : &quot;中国&quot;, &quot;address&quot; : &quot;中国北京市海淀区&quot;, &quot;location&quot; : &#123; &quot;lat&quot; : &quot;39.970718&quot;, &quot;lon&quot; : &quot;116.325747&quot; &#125; &#125; &#125; ] &#125;&#125;显然，city为北京的文档已经被搜索到了。如果我们做如下的查询：12345678GET twitter/_search/template&#123; &quot;id&quot;: &quot;docs_from_beijing_and_age&quot;, &quot;params&quot;: &#123; &quot;search_term&quot;: &quot;北京&quot;, &quot;search_age&quot;: &quot;30&quot; &#125;&#125;我们将搜索不到任何的结果，这是因为在这次查询中search_age已经被启用，而且在数据库中没有一个文档是来自“北京”，并且年龄大于30的。我们可以做如下的查询：12345678GET twitter/_search/template&#123; &quot;id&quot;: &quot;docs_from_beijing_and_age&quot;, &quot;params&quot;: &#123; &quot;search_term&quot;: &quot;北京&quot;, &quot;search_age&quot;: &quot;20&quot; &#125;&#125;那么这次的显示结果为：123456789101112131415161718192021222324252627282930313233343536373839&#123; &quot;took&quot; : 0, &quot;timed_out&quot; : false, &quot;_shards&quot; : &#123; &quot;total&quot; : 1, &quot;successful&quot; : 1, &quot;skipped&quot; : 0, &quot;failed&quot; : 0 &#125;, &quot;hits&quot; : &#123; &quot;total&quot; : &#123; &quot;value&quot; : 1, &quot;relation&quot; : &quot;eq&quot; &#125;, &quot;max_score&quot; : 1.9808292, &quot;hits&quot; : [ &#123; &quot;_index&quot; : &quot;twitter&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;1&quot;, &quot;_score&quot; : 1.9808292, &quot;_source&quot; : &#123; &quot;user&quot; : &quot;双榆树-张三&quot;, &quot;message&quot; : &quot;今儿天气不错啊，出去转转去&quot;, &quot;uid&quot; : 2, &quot;age&quot; : 20, &quot;city&quot; : &quot;北京&quot;, &quot;province&quot; : &quot;北京&quot;, &quot;country&quot; : &quot;中国&quot;, &quot;address&quot; : &quot;中国北京市海淀区&quot;, &quot;location&quot; : &#123; &quot;lat&quot; : &quot;39.970718&quot;, &quot;lon&quot; : &quot;116.325747&quot; &#125; &#125; &#125; ] &#125;&#125;显然这次我们搜索到我们想要的结果。查询search template1GET _scripts/&lt;templateid&gt;针对我们的情况：1GET _scripts/docs_from_beijing_and_age显示的结果为：1234567891011121314151617181920212223242526272829303132&#123; &quot;_id&quot; : &quot;docs_from_beijing_and_age&quot;, &quot;found&quot; : true, &quot;script&quot; : &#123; &quot;lang&quot; : &quot;mustache&quot;, &quot;source&quot; : &quot;&quot;&quot; &#123; &quot;query&quot;: &#123; &quot;bool&quot;: &#123; &quot;must&quot;: [ &#123; &quot;match&quot;: &#123; &quot;city&quot;: &quot;&#123;&#123;search_term&#125;&#125;&quot; &#125; &#125; &#123;&#123;#search_age&#125;&#125; , &#123; &quot;range&quot;: &#123; &quot;age&quot;: &#123; &quot;gte&quot;: &#123;&#123;search_age&#125;&#125; &#125; &#125; &#125; &#123;&#123;/search_age&#125;&#125; ] &#125; &#125; &#125;&quot;&quot;&quot; &#125;&#125;这个正是我们之前定义的一个search template。删除一个search template我们可以通过如下的命令来删除一个已经创建的search template:1DELETE _scripts/&lt;templateid&gt;验证search template我们可以通过_render端点来验证我们的search template。比如：1234567891011121314151617181920212223242526272829303132GET _render/template&#123; &quot;source&quot;: &quot;&quot;&quot; &#123; &quot;query&quot;: &#123; &quot;bool&quot;: &#123; &quot;must&quot;: [ &#123; &quot;match&quot;: &#123; &quot;city&quot;: &quot;&#123;&#123;search_term&#125;&#125;&quot; &#125; &#125; &#123;&#123;#search_age&#125;&#125; , &#123; &quot;range&quot;: &#123; &quot;age&quot;: &#123; &quot;gte&quot;: &#123;&#123;search_age&#125;&#125; &#125; &#125; &#125; &#123;&#123;/search_age&#125;&#125; ] &#125; &#125; &#125;&quot;&quot;&quot;, &quot;params&quot;: &#123; &quot;search_term&quot;: &quot;北京&quot;, &quot;search_age&quot;: &quot;20&quot; &#125;&#125;那么显示的结果是：12345678910111213141516171819202122&#123; &quot;template_output&quot; : &#123; &quot;query&quot; : &#123; &quot;bool&quot; : &#123; &quot;must&quot; : [ &#123; &quot;match&quot; : &#123; &quot;city&quot; : &quot;北京&quot; &#125; &#125;, &#123; &quot;range&quot; : &#123; &quot;age&quot; : &#123; &quot;gte&quot; : 20 &#125; &#125; &#125; ] &#125; &#125; &#125;&#125;显然，这个就是我们想要的结果。参考：【1】https://www.elastic.co/guide/en/elasticsearch/reference/7.4/search-template.html]]></content>
      <categories>
        <category>elk</category>
      </categories>
      <tags>
        <tag>elk</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch：Pinyin 分词器]]></title>
    <url>%2F2020%2F01%2F02%2FElasticsearch%EF%BC%9APinyin%20%E5%88%86%E8%AF%8D%E5%99%A8%2F</url>
    <content type="text"><![CDATA[Elastic的Medcl提供了一种搜索Pinyin搜索的方法。拼音搜索在很多的应用场景中都有被用到。比如在百度搜索中，我们使用拼音就可以出现汉字：对于我们中国人来说，拼音搜索也是非常直接的。那么在Elasticsearch中我们该如何使用pinyin来进行搜索呢？答案是我们采用Medcl所创建的elasticsearch-analysis-pinyin分析器。下面我们简单介绍一下如何进行安装和测试。下载Pinyin分析器源码进行编译及安装由于elasticsearch-analysis-pinyin目前没有可以下载的可以安装的发布文件，我们必须自己下载源码，并编译。首先，我们可以通过如下的命名来进行下载：1$ git clone https://github.com/medcl/elasticsearch-analysis-pinyin下载源码后，进入到项目的根目录。整个项目的源码显示为：12345678910$ tree -L 2.├── LICENSE.txt├── README.md├── lib│ └── nlp-lang-1.7.jar├── pom.xml└── src ├── main └── test这样在我们的电脑里就会发现下载好的elasticsearch-analysis-pinyin源码。在进行编译之前，我们必须修改一下我们的版本号以便和我们的Elasticsearch的版本号是一致的。否则我们的plugin将不会被正确装载。我们已知我们的Elasticsearch版本号码是7.3.0，那么我们修改我们的pom.xml文件：在我们的电脑上必须安装好Maven。然后进入项目的根目录，并在命令行中打入如下的命令：1$ mvn install这样整个项目的编译工作就完成了。我们在命令行中打入如下的命令：12$ find ./ -name &quot;*.zip&quot;.//target/releases/elasticsearch-analysis-pinyin-7.3.0.zip它显示在tagert目录下已经生产了一个叫做elasticsearch-analysis-pinyin-7.3.0.zip的压缩文件。这个版本号码刚好和我们的Elasticsearch的版本是一样的。我们到Elasticsearch的安装目录下的plugin目录下创建一个叫做pinyin的子目录：123/Users/liuxg/elastic/elasticsearch-7.3.0/pluginslocalhost:plugins liuxg$ ls analysis-ik pinyin然后，把我们刚才在上一步生产的elasticsearch-analysis-pinyin-7.0.0.zip文件进行解压，并把文件放入到我们刚才创建的pinyin目录下。这样整个pinyin文件夹的文件显示如下：1234567$ lsanalysis-ik pinyinlocalhost:plugins liuxg$ tree pinyin/ -L 3pinyin/├── elasticsearch-analysis-pinyin-7.3.0.jar├── nlp-lang-1.7.jar└── plugin-descriptor.properties至此，我们的安装工作已经完成，我需要重新启动我们的Elasticsearch。测试Pinyin analyzer下面我们来测试一下我们已经安装好的Pinyin分词器是否已经工作。我们可以仿照https://github.com/medcl/elasticsearch-analysis-pinyin上面的介绍来做一些简单的测试：创建一个定制的pinyin分词器1234567891011121314151617181920212223PUT /medcl/ &#123; &quot;settings&quot; : &#123; &quot;analysis&quot; : &#123; &quot;analyzer&quot; : &#123; &quot;pinyin_analyzer&quot; : &#123; &quot;tokenizer&quot; : &quot;my_pinyin&quot; &#125; &#125;, &quot;tokenizer&quot; : &#123; &quot;my_pinyin&quot; : &#123; &quot;type&quot; : &quot;pinyin&quot;, &quot;keep_separate_first_letter&quot; : false, &quot;keep_full_pinyin&quot; : true, &quot;keep_original&quot; : true, &quot;limit_first_letter_length&quot; : 16, &quot;lowercase&quot; : true, &quot;remove_duplicated_term&quot; : true &#125; &#125; &#125; &#125;&#125;测试一些中文汉字12345678910111213141516171819202122232425262728293031323334353637383940414243444546GET /medcl/_analyze&#123; &quot;text&quot;: [&quot;天安门&quot;], &quot;analyzer&quot;: &quot;pinyin_analyzer&quot;&#125;# 显示结果为：&#123; &quot;tokens&quot; : [ &#123; &quot;token&quot; : &quot;tian&quot;, &quot;start_offset&quot; : 0, &quot;end_offset&quot; : 0, &quot;type&quot; : &quot;word&quot;, &quot;position&quot; : 0 &#125;, &#123; &quot;token&quot; : &quot;天安门&quot;, &quot;start_offset&quot; : 0, &quot;end_offset&quot; : 0, &quot;type&quot; : &quot;word&quot;, &quot;position&quot; : 0 &#125;, &#123; &quot;token&quot; : &quot;tam&quot;, &quot;start_offset&quot; : 0, &quot;end_offset&quot; : 0, &quot;type&quot; : &quot;word&quot;, &quot;position&quot; : 0 &#125;, &#123; &quot;token&quot; : &quot;an&quot;, &quot;start_offset&quot; : 0, &quot;end_offset&quot; : 0, &quot;type&quot; : &quot;word&quot;, &quot;position&quot; : 1 &#125;, &#123; &quot;token&quot; : &quot;men&quot;, &quot;start_offset&quot; : 0, &quot;end_offset&quot; : 0, &quot;type&quot; : &quot;word&quot;, &quot;position&quot; : 2 &#125; ]&#125;上面的token显示，如果我们打入搜索tam是完全可以搜索到我们的结果的。创建mapping1234567891011121314151617POST /medcl/_mapping&#123; &quot;properties&quot;: &#123; &quot;name&quot;: &#123; &quot;type&quot;: &quot;keyword&quot;, &quot;fields&quot;: &#123; &quot;pinyin&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;store&quot;: false, &quot;term_vector&quot;: &quot;with_offsets&quot;, &quot;analyzer&quot;: &quot;pinyin_analyzer&quot;, &quot;boost&quot;: 10 &#125; &#125; &#125; &#125;&#125;Index文档12POST /medcl/_create/andy&#123;&quot;name&quot;:&quot;刘德华&quot;&#125;搜索文档12345curl http://localhost:9200/medcl/_search?q=name:%E5%88%98%E5%BE%B7%E5%8D%8Ecurl http://localhost:9200/medcl/_search?q=name.pinyin:%e5%88%98%e5%be%b7curl http://localhost:9200/medcl/_search?q=name.pinyin:liucurl http://localhost:9200/medcl/_search?q=name.pinyin:ldhcurl http://localhost:9200/medcl/_search?q=name.pinyin:de+hua或者：12345GET medcl/_search?q=name:%E5%88%98%E5%BE%B7%E5%8D%8EGET medcl/_search?q=name.pinyin:%e5%88%98%e5%be%b7GET medcl/_search?q=name.pinyin:liuGET medcl/_search?q=name.pinyin:ldhGET medcl/_search?q=name.pinyin:de+hua上面的第一个Unicode是“刘德华”，第二个是“刘德”。使用pinyin-tokenFilter1234567891011121314151617181920212223242526PUT /medcl1/ &#123; &quot;settings&quot; : &#123; &quot;analysis&quot; : &#123; &quot;analyzer&quot; : &#123; &quot;user_name_analyzer&quot; : &#123; &quot;tokenizer&quot; : &quot;whitespace&quot;, &quot;filter&quot; : &quot;pinyin_first_letter_and_full_pinyin_filter&quot; &#125; &#125;, &quot;filter&quot; : &#123; &quot;pinyin_first_letter_and_full_pinyin_filter&quot; : &#123; &quot;type&quot; : &quot;pinyin&quot;, &quot;keep_first_letter&quot; : true, &quot;keep_full_pinyin&quot; : false, &quot;keep_none_chinese&quot; : true, &quot;keep_original&quot; : false, &quot;limit_first_letter_length&quot; : 16, &quot;lowercase&quot; : true, &quot;trim_whitespace&quot; : true, &quot;keep_none_chinese_in_first_letter&quot; : true &#125; &#125; &#125; &#125;&#125;Token Test:刘德华 张学友 郭富城 黎明 四大天王12345GET /medcl1/_analyze&#123; &quot;text&quot;: [&quot;刘德华 张学友 郭富城 黎明 四大天王&quot;], &quot;analyzer&quot;: &quot;user_name_analyzer&quot;&#125;12345678910111213141516171819202122232425262728293031323334353637383940&#123; &quot;tokens&quot; : [ &#123; &quot;token&quot; : &quot;ldh&quot;, &quot;start_offset&quot; : 0, &quot;end_offset&quot; : 3, &quot;type&quot; : &quot;word&quot;, &quot;position&quot; : 0 &#125;, &#123; &quot;token&quot; : &quot;zxy&quot;, &quot;start_offset&quot; : 4, &quot;end_offset&quot; : 7, &quot;type&quot; : &quot;word&quot;, &quot;position&quot; : 1 &#125;, &#123; &quot;token&quot; : &quot;gfc&quot;, &quot;start_offset&quot; : 8, &quot;end_offset&quot; : 11, &quot;type&quot; : &quot;word&quot;, &quot;position&quot; : 2 &#125;, &#123; &quot;token&quot; : &quot;lm&quot;, &quot;start_offset&quot; : 12, &quot;end_offset&quot; : 14, &quot;type&quot; : &quot;word&quot;, &quot;position&quot; : 3 &#125;, &#123; &quot;token&quot; : &quot;sdtw&quot;, &quot;start_offset&quot; : 15, &quot;end_offset&quot; : 19, &quot;type&quot; : &quot;word&quot;, &quot;position&quot; : 4 &#125; ]&#125;其它请参阅链接https://github.com/medcl/elasticsearch-analysis-pinyin。如果想了解中文IK分词器，请参阅文章“Elasticsearch：IK中文分词器”。]]></content>
      <categories>
        <category>elk</category>
      </categories>
      <tags>
        <tag>elk</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch：Java 运用示例]]></title>
    <url>%2F2020%2F01%2F02%2FElasticsearch%EF%BC%9AJava%20%E8%BF%90%E7%94%A8%E7%A4%BA%E4%BE%8B%2F</url>
    <content type="text"><![CDATA[在今天的文章中，我们来介绍如何使用Java来访问Elasticsearch。首先，我们必须在我们的系统中安装Elasticsearch。Maven 配置针对Java的开发，我们必须在pom.xml中配置相应的Elasticsearch的信息。Mavev dependency定义如下：12345&lt;dependency&gt; &lt;groupId&gt;org.elasticsearch&lt;/groupId&gt; &lt;artifactId&gt;elasticsearch&lt;/artifactId&gt; &lt;version&gt;7.3.1&lt;/version&gt;&lt;/dependency&gt;这也是目前截止最新的Elasticsearch的版本。您可以随时使用之前提供的链接查看Maven Central托管的最新版本。完成数据库的查询建立一个简单的model1234567891011121314151617181920212223242526272829303132333435363738package com.javacodegeeks.example; public class Person &#123; private String personId; private String name; private String number; public String getNumber() &#123; return number; &#125; public void setNumber(String number) &#123; this.number = number; &#125; public String getPersonId() &#123; return personId; &#125; public void setPersonId(String personId) &#123; this.personId = personId; &#125; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; @Override public String toString() &#123; return String.format(&quot;Person&#123;personId=&apos;%s&apos;, name=&apos;%s&apos;, number=&apos;%s&#125;&quot;, personId, name, number); &#125;&#125;在这里，我们定义了一个简单的Person Model。定义连接参数我们将使用默认连接参数与Elasticsearch建立连接。 默认情况下，ES使用两个端口：9200和920112345678910private static final String HOST = &quot;localhost&quot;;private static final int PORT_ONE = 9200;private static final int PORT_TWO = 9201;private static final String SCHEME = &quot;http&quot;; private static RestHighLevelClient restHighLevelClient;private static ObjectMapper objectMapper = new ObjectMapper(); private static final String INDEX = &quot;persondata&quot;;private static final String TYPE = &quot;_doc&quot;;这里我们定义了一个叫做persondata的index，它的type是_doc。在最新的版本中，每个index只支持一个type。如上面参数中所述，Elasticsearch使用两个端口9200和9201.第一个端口9200由Elasticsearch查询服务器使用，我们可以使用它通过RESTful API直接查询数据库。 第二个端口9201由REST服务器使用，外部客户端可以使用该端口连接并执行操作。建立一个连接我们将创建一个与Elasticsearch数据库建立连接的方法。 在建立与数据库的连接时，我们必须提供两个端口，因为只有这样，我们的应用程序才能连接到Elasticsearch服务器，我们将能够执行数据库操作。 以下是建立连接的代码。1234567891011private static synchronized RestHighLevelClient makeConnection() &#123; if(restHighLevelClient == null) &#123; restHighLevelClient = new RestHighLevelClient( RestClient.builder( new HttpHost(HOST, PORT_ONE, SCHEME), new HttpHost(HOST, PORT_TWO, SCHEME))); &#125; return restHighLevelClient;&#125;在这里，我们建立一个RestHighLevelClient的实例。具体的参数，可以参官方文档 Java High Level REST Client (https://www.elastic.co/guide/en/elasticsearch/client/java-rest/7.3/java-rest-high.html)。请注意，我们在此处实现了Singleton Design模式，因此不会为ES创建多个连接，从而节省大量内存。由于存在RestHighLevelClient，与Elasticsearch的连接是线程安全的。 初始化此连接的最佳时间是应用程序请求或向客户端发出第一个请求时。 初始化此连接客户端后，可以使用它来执行任何支持的API。关掉一个连接就像在早期版本的Elasticsearch中一样，我们使用TransportClient，一旦完成查询就关闭它，一旦数据库交互完成RestHighLevelClient，也需要关闭连接。 以下是如何做到这一点：1234private static synchronized void closeConnection() throws IOException &#123; restHighLevelClient.close(); restHighLevelClient = null;&#125;我们还为RestHighLevelClient对象分配了null，以便Singleton模式可以保持一致。插入一个文档我们可以通过将键和值转换为HashMap将数据插入数据库。 ES数据库仅接受HashMap形式的值。 让我们看看如何实现这一目标的代码片段：123456789101112131415161718192021222324252627282930313233343536373839private static Person insertPerson(Person person) &#123; person.setPersonId(UUID.randomUUID().toString()); Map&lt;String, Object&gt; dataMap = new HashMap&lt;String, Object&gt;(); dataMap.put(&quot;name&quot;, person.getName()); dataMap.put(&quot;number&quot;, person.getNumber()); IndexRequest indexRequest = new IndexRequest(INDEX) .id(person.getPersonId()).source(dataMap); try &#123; IndexResponse response = restHighLevelClient.index(indexRequest, RequestOptions.DEFAULT); &#125; catch(ElasticsearchException e) &#123; e.getDetailedMessage(); &#125; catch (java.io.IOException ex)&#123; ex.getLocalizedMessage(); &#125; /* // The following is another way to do it // More information https://www.elastic.co/guide/en/elasticsearch/client/java-rest/7.3/java-rest-high-document-index.html String id = UUID.randomUUID().toString(); person.setPersonId(id); IndexRequest request = new IndexRequest(INDEX); request.id(id); String jsonString = &quot;&#123;&quot; + &quot;\&quot;name\&quot;:&quot; + &quot;\&quot;&quot; + person.getName() + &quot;\&quot;&quot; + &quot;&#125;&quot;; System.out.println(&quot;jsonString: &quot; + jsonString); request.source(jsonString, XContentType.JSON); try &#123; IndexResponse response = restHighLevelClient.index(request, RequestOptions.DEFAULT); &#125; catch(ElasticsearchException e) &#123; e.getDetailedMessage(); &#125; catch (java.io.IOException ex)&#123; ex.getLocalizedMessage(); &#125; */ return person; &#125;就像上面代码中注释的那样。注释的代码的那一部分是另外一种方法。大家可以参照链接(https://www.elastic.co/guide/en/elasticsearch/client/java-rest/7.3/java-rest-high-document-index.html)获得更多的信息。上面，我们使用Java的UUID类来创建对象的唯一标识符。 这样，我们就可以控制对象标识符的制作方式。我们其实也可以固定一个id去写。如果是这样的话，运行多次，只会更新之前的数据，并且version会自动每次运行后增加1。请求上面存入的文档完成将数据插入数据库后，我们可以通过向Elasticsearch数据库服务器发出GET请求来确认操作。 让我们看看如何完成此操作的代码片段：1234567891011121314 private static Person getPersonById(String id)&#123; GetRequest getPersonRequest = new GetRequest(INDEX, id); GetResponse getResponse = null; try &#123; getResponse = restHighLevelClient.get(getPersonRequest, RequestOptions.DEFAULT); &#125; catch (java.io.IOException e)&#123; e.getLocalizedMessage(); &#125; return getResponse != null ? objectMapper.convertValue(getResponse.getSourceAsMap(), Person.class) : null; &#125;在这里，我们根据上面返回来得id来进行query，并返回数据。在这个查询中，我们只提供了可以识别它的对象的主要信息，即索引，和它的唯一标识符id。 此外，我们得到的实际上是一个值的映射。更新文档我们可以通过首先使用其索引，类型和唯一标识符来标识资源，从而轻松地向Elasticsearch发出更新请求。 然后我们可以使用新的HashMap对象来更新Object中的任意数量的值。 这是一个示例代码段：1234567891011121314151617private static Person updatePersonById(String id, Person person)&#123; UpdateRequest updateRequest = new UpdateRequest(INDEX, id) .fetchSource(true); // Fetch Object after its update try &#123; String personJson = objectMapper.writeValueAsString(person); updateRequest.doc(personJson, XContentType.JSON); UpdateResponse updateResponse = restHighLevelClient.update(updateRequest, RequestOptions.DEFAULT); return objectMapper.convertValue(updateResponse.getGetResult().sourceAsMap(), Person.class); &#125;catch (JsonProcessingException e)&#123; e.getMessage(); &#125; catch (java.io.IOException e)&#123; e.getLocalizedMessage(); &#125; System.out.println(&quot;Unable to update person&quot;); return null; &#125;删除文档最后，我们可以通过简单地使用其索引，类型和唯一标识符来标识资源来删除数据。 让我们看一下如何完成此操作的代码片段12345678private static void deletePersonById(String id) &#123; DeleteRequest deleteRequest = new DeleteRequest(INDEX, TYPE, id); try &#123; DeleteResponse deleteResponse = restHighLevelClient.delete(deleteRequest, RequestOptions.DEFAULT); &#125; catch (java.io.IOException e) &#123; e.getLocalizedMessage(); &#125;&#125;我们根据传入的id来删除相应的文档。当然我们也可以做查询删除。运行我们的应用让我们通过执行上面提到的所有操作来尝试我们的应用程序。 由于这是一个普通的Java应用程序，我们将调用这些方法中的每一个并打印操作结果：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051public static void main(String[] args) throws IOException &#123; makeConnection(); Person person = new Person(); person.setName(&quot;张三&quot;); System.out.println(&quot;Inserting a new Person with name &quot; + person.getName()); person.setNumber(&quot;111111&quot;); person = insertPerson(person); System.out.println(&quot;Person inserted --&gt; &quot; + person); person = new Person(); person.setName(&quot;姚明&quot;); System.out.println(&quot;Inserting a new Person with name &quot; + person.getName()); person.setNumber(&quot;222222&quot;); person = insertPerson(person); System.out.println(&quot;Person inserted --&gt; &quot; + person); person.setName(&quot;李四&quot;); System.out.println(&quot;Changing name to &quot; + person.getName()); updatePersonById(person.getPersonId(), person); System.out.println(&quot;Person updated --&gt; &quot; + person); System.out.println(&quot;Searching for all documents&quot;); SearchResponse response = searchAll(); System.out.println(response); System.out.println(&quot;Searching for a term&quot;); response = searchTerm(); System.out.println(response); System.out.println(&quot;Match a query&quot;); response = matchQuery(); System.out.println(response); System.out.println(&quot;Getting 李四&quot;); Person personFromDB = getPersonById(person.getPersonId()); System.out.println(&quot;Person from DB --&gt; &quot; + personFromDB); System.out.println(&quot;Deleting &quot; + person.getName()); deletePersonById(personFromDB.getPersonId()); System.out.println(&quot;Person &quot; + person.getName() + &quot; deleted!&quot;); closeConnection(); &#125;运行的结果如下：1234567891011121314Inserting a new Person with name 张三Person inserted --&gt; Person&#123;personId=&apos;33f4162e-0a68-4e66-8717-851516272185&apos;, name=&apos;张三&apos;, number=&apos;111111&#125;Inserting a new Person with name 姚明Person inserted --&gt; Person&#123;personId=&apos;9b477529-6e79-42e8-a50a-21b2d8bc4c13&apos;, name=&apos;姚明&apos;, number=&apos;222222&#125;Changing name to 李四Person updated --&gt; Person&#123;personId=&apos;9b477529-6e79-42e8-a50a-21b2d8bc4c13&apos;, name=&apos;李四&apos;, number=&apos;222222&#125;Searching for all documents&#123;&quot;took&quot;:0,&quot;timed_out&quot;:false,&quot;_shards&quot;:&#123;&quot;total&quot;:1,&quot;successful&quot;:1,&quot;skipped&quot;:0,&quot;failed&quot;:0&#125;,&quot;hits&quot;:&#123;&quot;total&quot;:&#123;&quot;value&quot;:4,&quot;relation&quot;:&quot;eq&quot;&#125;,&quot;max_score&quot;:1.0,&quot;hits&quot;:[&#123;&quot;_index&quot;:&quot;persondata&quot;,&quot;_type&quot;:&quot;_doc&quot;,&quot;_id&quot;:&quot;52425a44-dc06-49ca-b3df-26a8b341391c&quot;,&quot;_score&quot;:1.0,&quot;_source&quot;:&#123;&quot;number&quot;:&quot;111111&quot;,&quot;name&quot;:&quot;张三&quot;&#125;&#125;,&#123;&quot;_index&quot;:&quot;persondata&quot;,&quot;_type&quot;:&quot;_doc&quot;,&quot;_id&quot;:&quot;c76b8670-ed00-4212-b47b-46bc85d588b6&quot;,&quot;_score&quot;:1.0,&quot;_source&quot;:&#123;&quot;number&quot;:&quot;111111&quot;,&quot;name&quot;:&quot;张三&quot;&#125;&#125;,&#123;&quot;_index&quot;:&quot;persondata&quot;,&quot;_type&quot;:&quot;_doc&quot;,&quot;_id&quot;:&quot;b8bf0466-0ea5-43e0-8188-c0712812fb9a&quot;,&quot;_score&quot;:1.0,&quot;_source&quot;:&#123;&quot;number&quot;:&quot;111111&quot;,&quot;name&quot;:&quot;张三&quot;&#125;&#125;,&#123;&quot;_index&quot;:&quot;persondata&quot;,&quot;_type&quot;:&quot;_doc&quot;,&quot;_id&quot;:&quot;468dabe4-8f50-4667-a165-9ce6e015cb76&quot;,&quot;_score&quot;:1.0,&quot;_source&quot;:&#123;&quot;number&quot;:&quot;222222&quot;,&quot;name&quot;:&quot;李四&quot;,&quot;personId&quot;:&quot;468dabe4-8f50-4667-a165-9ce6e015cb76&quot;&#125;&#125;]&#125;&#125;Searching for a term&#123;&quot;took&quot;:0,&quot;timed_out&quot;:false,&quot;_shards&quot;:&#123;&quot;total&quot;:1,&quot;successful&quot;:1,&quot;skipped&quot;:0,&quot;failed&quot;:0&#125;,&quot;hits&quot;:&#123;&quot;total&quot;:&#123;&quot;value&quot;:3,&quot;relation&quot;:&quot;eq&quot;&#125;,&quot;max_score&quot;:0.9444616,&quot;hits&quot;:[&#123;&quot;_index&quot;:&quot;persondata&quot;,&quot;_type&quot;:&quot;_doc&quot;,&quot;_id&quot;:&quot;52425a44-dc06-49ca-b3df-26a8b341391c&quot;,&quot;_score&quot;:0.9444616,&quot;_source&quot;:&#123;&quot;number&quot;:&quot;111111&quot;,&quot;name&quot;:&quot;张三&quot;&#125;&#125;,&#123;&quot;_index&quot;:&quot;persondata&quot;,&quot;_type&quot;:&quot;_doc&quot;,&quot;_id&quot;:&quot;c76b8670-ed00-4212-b47b-46bc85d588b6&quot;,&quot;_score&quot;:0.9444616,&quot;_source&quot;:&#123;&quot;number&quot;:&quot;111111&quot;,&quot;name&quot;:&quot;张三&quot;&#125;&#125;,&#123;&quot;_index&quot;:&quot;persondata&quot;,&quot;_type&quot;:&quot;_doc&quot;,&quot;_id&quot;:&quot;b8bf0466-0ea5-43e0-8188-c0712812fb9a&quot;,&quot;_score&quot;:0.9444616,&quot;_source&quot;:&#123;&quot;number&quot;:&quot;111111&quot;,&quot;name&quot;:&quot;张三&quot;&#125;&#125;]&#125;&#125;Match a query&#123;&quot;took&quot;:0,&quot;timed_out&quot;:false,&quot;_shards&quot;:&#123;&quot;total&quot;:1,&quot;successful&quot;:1,&quot;skipped&quot;:0,&quot;failed&quot;:0&#125;,&quot;hits&quot;:&#123;&quot;total&quot;:&#123;&quot;value&quot;:3,&quot;relation&quot;:&quot;eq&quot;&#125;,&quot;max_score&quot;:1.8889232,&quot;hits&quot;:[&#123;&quot;_index&quot;:&quot;persondata&quot;,&quot;_type&quot;:&quot;_doc&quot;,&quot;_id&quot;:&quot;52425a44-dc06-49ca-b3df-26a8b341391c&quot;,&quot;_score&quot;:1.8889232,&quot;_source&quot;:&#123;&quot;number&quot;:&quot;111111&quot;,&quot;name&quot;:&quot;张三&quot;&#125;&#125;,&#123;&quot;_index&quot;:&quot;persondata&quot;,&quot;_type&quot;:&quot;_doc&quot;,&quot;_id&quot;:&quot;c76b8670-ed00-4212-b47b-46bc85d588b6&quot;,&quot;_score&quot;:1.8889232,&quot;_source&quot;:&#123;&quot;number&quot;:&quot;111111&quot;,&quot;name&quot;:&quot;张三&quot;&#125;&#125;,&#123;&quot;_index&quot;:&quot;persondata&quot;,&quot;_type&quot;:&quot;_doc&quot;,&quot;_id&quot;:&quot;b8bf0466-0ea5-43e0-8188-c0712812fb9a&quot;,&quot;_score&quot;:1.8889232,&quot;_source&quot;:&#123;&quot;number&quot;:&quot;111111&quot;,&quot;name&quot;:&quot;张三&quot;&#125;&#125;]&#125;&#125;Getting 李四Person from DB --&gt; Person&#123;personId=&apos;9b477529-6e79-42e8-a50a-21b2d8bc4c13&apos;, name=&apos;李四&apos;, number=&apos;222222&#125;整个项目的源码可以在地址找到：https://github.com/liu-xiao-guo/elastic-java更多资料：【1】使用RestHighLevelClient时单个索引速度很慢(https://discuss.elastic.co/t/resthighlevelclient/170293)]]></content>
      <categories>
        <category>elk</category>
      </categories>
      <tags>
        <tag>elk</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch：inverted index，doc_values及source]]></title>
    <url>%2F2020%2F01%2F02%2FElasticsearch%EF%BC%9Ainverted%20index%EF%BC%8Cdoc_values%E5%8F%8Asource%2F</url>
    <content type="text"><![CDATA[当我们学习Elasticsearch时，经常会遇到如下的几个概念：Reverted indexdoc_valuessource？这个几个概念分别指的是什么？有什么用处？如何配置它们？只有我们熟练地掌握了这些概念，我们才可以正确地使用它们。Inverted indexinverted index（反向索引）是Elasticsearch和任何其他支持全文搜索的系统的核心数据结构。 反向索引类似于您在任何书籍结尾处看到的索引。 它将出现在文档中的术语映射到文档。例如，您可以从以下字符串构建反向索引：Elasticsearch从已建立索引的三个文档中构建数据结构。 以下数据结构称为反向索引(inverted index)：Term Frequency Document (postings)choice 1 3day 1 2is 3 1,2,3it 1 1last 1 2of 1 2of 1 2sunday 2 1,2the 3 2,3tomorrow 1 1week 1 2yours 1 3在这里反向索引指的的是，我们根据term来寻找相应的文档ids。这和常规的根据文档id来寻找term相反。请注意以下几点：删除标点符号并将其小写后，文档会按术语进行细分。术语按字母顺序排序“Frequency”列捕获该术语在整个文档集中出现的次数第三列捕获了在其中找到该术语的文档。 此外，它还可能包含找到该术语的确切位置（文档中的偏移）在文档中搜索术语时，查找给定术语出现在其中的文档非常快捷。 如果用户搜索术语“sunday”，那么从“Term”列中查找sunday将非常快，因为这些术语在索引中进行了排序。 即使有数百万个术语，也可以在对术语进行排序时快速查找它们。随后，考虑一种情况，其中用户搜索两个单词，例如last sunday。 反向索引可用于分别搜索last和sunday的发生； 文档2包含这两个术语，因此比仅包含一个术语的文档1更好。反向索引是执行快速搜索的基础。 同样，很容易查明索引中出现了多少次术语。 这是一个简单的计数汇总。 当然，Elasticsearch在我们在这里解释的简单的反向排索引的基础上使用了很多创新。 它兼顾搜索和分析。默认情况下，Elasticsearch在文档中的所有字段上构建一个反向索引，指向该字段所在的Elasticsearch文档。也就是说在每个Elasticsearch的Lucene里，有一个位置存放这个inverted index。在Kibana中，我们建立一个如下的文档：12345678910111213141516171819202122PUT twitter/_doc/1&#123; &quot;user&quot; : &quot;双榆树-张三&quot;, &quot;message&quot; : &quot;今儿天气不错啊，出去转转去&quot;, &quot;uid&quot; : 2, &quot;age&quot; : 20, &quot;city&quot; : &quot;北京&quot;, &quot;province&quot; : &quot;北京&quot;, &quot;country&quot; : &quot;中国&quot;, &quot;name&quot;: &#123; &quot;firstname&quot;: &quot;三&quot;, &quot;surname&quot;: &quot;张&quot; &#125;, &quot;address&quot; : [ &quot;中国北京市海淀区&quot;, &quot;中关村29号&quot; ], &quot;location&quot; : &#123; &quot;lat&quot; : &quot;39.970718&quot;, &quot;lon&quot; : &quot;116.325747&quot; &#125;&#125;当这个文档被建立好以后，Elastic就已经帮我们建立好了相应的inverted index供我们进行搜索，比如：12345678GET twitter/_search&#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;user&quot;: &quot;张三&quot; &#125; &#125;&#125;我们可与得到相应的搜索结果：12345678910111213141516171819202122232425262728293031323334353637383940414243444546&#123; &quot;took&quot; : 0, &quot;timed_out&quot; : false, &quot;_shards&quot; : &#123; &quot;total&quot; : 1, &quot;successful&quot; : 1, &quot;skipped&quot; : 0, &quot;failed&quot; : 0 &#125;, &quot;hits&quot; : &#123; &quot;total&quot; : &#123; &quot;value&quot; : 1, &quot;relation&quot; : &quot;eq&quot; &#125;, &quot;max_score&quot; : 0.5753642, &quot;hits&quot; : [ &#123; &quot;_index&quot; : &quot;twitter&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;1&quot;, &quot;_score&quot; : 0.5753642, &quot;_source&quot; : &#123; &quot;user&quot; : &quot;双榆树-张三&quot;, &quot;message&quot; : &quot;今儿天气不错啊，出去转转去&quot;, &quot;uid&quot; : 2, &quot;age&quot; : 20, &quot;city&quot; : &quot;北京&quot;, &quot;province&quot; : &quot;北京&quot;, &quot;country&quot; : &quot;中国&quot;, &quot;name&quot; : &#123; &quot;firstname&quot; : &quot;三&quot;, &quot;surname&quot; : &quot;张&quot; &#125;, &quot;address&quot; : [ &quot;中国北京市海淀区&quot;, &quot;中关村29号&quot; ], &quot;location&quot; : &#123; &quot;lat&quot; : &quot;39.970718&quot;, &quot;lon&quot; : &quot;116.325747&quot; &#125; &#125; &#125; ] &#125;&#125;如果我们想不让我们的某个字段不被搜索，也就是说不想为这个字段建立inverted index，那么我们可以这么做：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125DELETE twitterPUT twitter&#123; &quot;mappings&quot;: &#123; &quot;properties&quot;: &#123; &quot;city&quot;: &#123; &quot;type&quot;: &quot;keyword&quot;, &quot;ignore_above&quot;: 256 &#125;, &quot;address&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;fields&quot;: &#123; &quot;keyword&quot;: &#123; &quot;type&quot;: &quot;keyword&quot;, &quot;ignore_above&quot;: 256 &#125; &#125; &#125;, &quot;age&quot;: &#123; &quot;type&quot;: &quot;long&quot; &#125;, &quot;country&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;fields&quot;: &#123; &quot;keyword&quot;: &#123; &quot;type&quot;: &quot;keyword&quot;, &quot;ignore_above&quot;: 256 &#125; &#125; &#125;, &quot;location&quot;: &#123; &quot;properties&quot;: &#123; &quot;lat&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;fields&quot;: &#123; &quot;keyword&quot;: &#123; &quot;type&quot;: &quot;keyword&quot;, &quot;ignore_above&quot;: 256 &#125; &#125; &#125;, &quot;lon&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;fields&quot;: &#123; &quot;keyword&quot;: &#123; &quot;type&quot;: &quot;keyword&quot;, &quot;ignore_above&quot;: 256 &#125; &#125; &#125; &#125; &#125;, &quot;message&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;fields&quot;: &#123; &quot;keyword&quot;: &#123; &quot;type&quot;: &quot;keyword&quot;, &quot;ignore_above&quot;: 256 &#125; &#125; &#125;, &quot;name&quot;: &#123; &quot;properties&quot;: &#123; &quot;firstname&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;fields&quot;: &#123; &quot;keyword&quot;: &#123; &quot;type&quot;: &quot;keyword&quot;, &quot;ignore_above&quot;: 256 &#125; &#125; &#125;, &quot;surname&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;fields&quot;: &#123; &quot;keyword&quot;: &#123; &quot;type&quot;: &quot;keyword&quot;, &quot;ignore_above&quot;: 256 &#125; &#125; &#125; &#125; &#125;, &quot;province&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;fields&quot;: &#123; &quot;keyword&quot;: &#123; &quot;type&quot;: &quot;keyword&quot;, &quot;ignore_above&quot;: 256 &#125; &#125; &#125;, &quot;uid&quot;: &#123; &quot;type&quot;: &quot;long&quot; &#125;, &quot;user&quot;: &#123; &quot;type&quot;: &quot;object&quot;, &quot;enabled&quot;: false &#125; &#125; &#125;&#125; PUT twitter/_doc/1&#123; &quot;user&quot; : &quot;双榆树-张三&quot;, &quot;message&quot; : &quot;今儿天气不错啊，出去转转去&quot;, &quot;uid&quot; : 2, &quot;age&quot; : 20, &quot;city&quot; : &quot;北京&quot;, &quot;province&quot; : &quot;北京&quot;, &quot;country&quot; : &quot;中国&quot;, &quot;name&quot;: &#123; &quot;firstname&quot;: &quot;三&quot;, &quot;surname&quot;: &quot;张&quot; &#125;, &quot;address&quot; : [ &quot;中国北京市海淀区&quot;, &quot;中关村29号&quot; ], &quot;location&quot; : &#123; &quot;lat&quot; : &quot;39.970718&quot;, &quot;lon&quot; : &quot;116.325747&quot; &#125;&#125;在上面，我们通过mapping对user字段进行了修改：1234&quot;user&quot;: &#123; &quot;type&quot;: &quot;object&quot;, &quot;enabled&quot;: false &#125;也就是说这个字段将不被建立索引，我们如果使用这个字段进行搜索的话，不会产生任何的结果：12345678GET twitter/_search&#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;user&quot;: &quot;张三&quot; &#125; &#125;&#125;搜索的结果为：123456789101112131415161718&#123; &quot;took&quot; : 0, &quot;timed_out&quot; : false, &quot;_shards&quot; : &#123; &quot;total&quot; : 1, &quot;successful&quot; : 1, &quot;skipped&quot; : 0, &quot;failed&quot; : 0 &#125;, &quot;hits&quot; : &#123; &quot;total&quot; : &#123; &quot;value&quot; : 0, &quot;relation&quot; : &quot;eq&quot; &#125;, &quot;max_score&quot; : null, &quot;hits&quot; : [ ] &#125;&#125;显然是没有任何的结果。但是如果我们对这个文档进行查询的话：1GET twitter/_doc/1显示的结果是：123456789101112131415161718192021222324252627282930&#123; &quot;_index&quot; : &quot;twitter&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;1&quot;, &quot;_version&quot; : 1, &quot;_seq_no&quot; : 0, &quot;_primary_term&quot; : 1, &quot;found&quot; : true, &quot;_source&quot; : &#123; &quot;user&quot; : &quot;双榆树-张三&quot;, &quot;message&quot; : &quot;今儿天气不错啊，出去转转去&quot;, &quot;uid&quot; : 2, &quot;age&quot; : 20, &quot;city&quot; : &quot;北京&quot;, &quot;province&quot; : &quot;北京&quot;, &quot;country&quot; : &quot;中国&quot;, &quot;name&quot; : &#123; &quot;firstname&quot; : &quot;三&quot;, &quot;surname&quot; : &quot;张&quot; &#125;, &quot;address&quot; : [ &quot;中国北京市海淀区&quot;, &quot;中关村29号&quot; ], &quot;location&quot; : &#123; &quot;lat&quot; : &quot;39.970718&quot;, &quot;lon&quot; : &quot;116.325747&quot; &#125; &#125;&#125;显然user的信息是存放于source里的。只是它不被我们所搜索而已。如果我们不想我们的整个文档被搜索，我们甚至可以直接采用如下的方法：12345678DELETE twitter PUT twitter &#123; &quot;mappings&quot;: &#123; &quot;enabled&quot;: false &#125;&#125;那么整个twitter索引将不建立任何的inverted index，那么我们通过如下的命令：12345678910111213141516171819202122232425262728293031PUT twitter/_doc/1&#123; &quot;user&quot; : &quot;双榆树-张三&quot;, &quot;message&quot; : &quot;今儿天气不错啊，出去转转去&quot;, &quot;uid&quot; : 2, &quot;age&quot; : 20, &quot;city&quot; : &quot;北京&quot;, &quot;province&quot; : &quot;北京&quot;, &quot;country&quot; : &quot;中国&quot;, &quot;name&quot;: &#123; &quot;firstname&quot;: &quot;三&quot;, &quot;surname&quot;: &quot;张&quot; &#125;, &quot;address&quot; : [ &quot;中国北京市海淀区&quot;, &quot;中关村29号&quot; ], &quot;location&quot; : &#123; &quot;lat&quot; : &quot;39.970718&quot;, &quot;lon&quot; : &quot;116.325747&quot; &#125;&#125; GET twitter/_search&#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;city&quot;: &quot;北京&quot; &#125; &#125;&#125;上面的命令执行的结果是，没有任何搜索的结果。更多阅读，可以参阅“Mapping parameters: enabled”(https://www.elastic.co/guide/en/elasticsearch/reference/current/enabled.html)。Source在Elasticsearch中，通常每个文档的每一个字段都会被存储在shard里存放source的地方，比如：12345678910111213141516171819202122PUT twitter/_doc/2&#123; &quot;user&quot; : &quot;双榆树-张三&quot;, &quot;message&quot; : &quot;今儿天气不错啊，出去转转去&quot;, &quot;uid&quot; : 2, &quot;age&quot; : 20, &quot;city&quot; : &quot;北京&quot;, &quot;province&quot; : &quot;北京&quot;, &quot;country&quot; : &quot;中国&quot;, &quot;name&quot;: &#123; &quot;firstname&quot;: &quot;三&quot;, &quot;surname&quot;: &quot;张&quot; &#125;, &quot;address&quot; : [ &quot;中国北京市海淀区&quot;, &quot;中关村29号&quot; ], &quot;location&quot; : &#123; &quot;lat&quot; : &quot;39.970718&quot;, &quot;lon&quot; : &quot;116.325747&quot; &#125;&#125;在这里，我们创建了一个id为2的文档。我们可以通过如下的命令来获得它的所有的存储的信息。1GET twitter/_doc/2它将返回：123456789101112131415161718192021222324252627282930&#123; &quot;_index&quot; : &quot;twitter&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;2&quot;, &quot;_version&quot; : 1, &quot;_seq_no&quot; : 1, &quot;_primary_term&quot; : 1, &quot;found&quot; : true, &quot;_source&quot; : &#123; &quot;user&quot; : &quot;双榆树-张三&quot;, &quot;message&quot; : &quot;今儿天气不错啊，出去转转去&quot;, &quot;uid&quot; : 2, &quot;age&quot; : 20, &quot;city&quot; : &quot;北京&quot;, &quot;province&quot; : &quot;北京&quot;, &quot;country&quot; : &quot;中国&quot;, &quot;name&quot; : &#123; &quot;firstname&quot; : &quot;三&quot;, &quot;surname&quot; : &quot;张&quot; &#125;, &quot;address&quot; : [ &quot;中国北京市海淀区&quot;, &quot;中关村29号&quot; ], &quot;location&quot; : &#123; &quot;lat&quot; : &quot;39.970718&quot;, &quot;lon&quot; : &quot;116.325747&quot; &#125; &#125;&#125;在上面的_source里我们可以看到Elasticsearch为我们所存下的所有的字段。如果我们不想存储任何的字段，那么我们可以做如下的设置：12345678910DELETE twitter PUT twitter&#123; &quot;mappings&quot;: &#123; &quot;_source&quot;: &#123; &quot;enabled&quot;: false &#125; &#125;&#125;那么我们使用如下的命令来创建一个id为1的文档：12345678910111213141516171819202122PUT twitter/_doc/1&#123; &quot;user&quot; : &quot;双榆树-张三&quot;, &quot;message&quot; : &quot;今儿天气不错啊，出去转转去&quot;, &quot;uid&quot; : 2, &quot;age&quot; : 20, &quot;city&quot; : &quot;北京&quot;, &quot;province&quot; : &quot;北京&quot;, &quot;country&quot; : &quot;中国&quot;, &quot;name&quot;: &#123; &quot;firstname&quot;: &quot;三&quot;, &quot;surname&quot;: &quot;张&quot; &#125;, &quot;address&quot; : [ &quot;中国北京市海淀区&quot;, &quot;中关村29号&quot; ], &quot;location&quot; : &#123; &quot;lat&quot; : &quot;39.970718&quot;, &quot;lon&quot; : &quot;116.325747&quot; &#125;&#125;那么同样地，我们来查询一下这个文档：1GET witter/_doc/1显示的结果为：123456789&#123; &quot;_index&quot; : &quot;twitter&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;1&quot;, &quot;_version&quot; : 1, &quot;_seq_no&quot; : 0, &quot;_primary_term&quot; : 1, &quot;found&quot; : true&#125;显然我们的文档是被找到了，但是我们看不到任何的source。那么我们能对这个文档进行搜索吗？尝试如下的命令：12345678GET twitter/_search&#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;city&quot;: &quot;北京&quot; &#125; &#125;&#125;显示的结果为：12345678910111213141516171819202122232425&#123; &quot;took&quot; : 0, &quot;timed_out&quot; : false, &quot;_shards&quot; : &#123; &quot;total&quot; : 1, &quot;successful&quot; : 1, &quot;skipped&quot; : 0, &quot;failed&quot; : 0 &#125;, &quot;hits&quot; : &#123; &quot;total&quot; : &#123; &quot;value&quot; : 1, &quot;relation&quot; : &quot;eq&quot; &#125;, &quot;max_score&quot; : 0.5753642, &quot;hits&quot; : [ &#123; &quot;_index&quot; : &quot;twitter&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;1&quot;, &quot;_score&quot; : 0.5753642 &#125; ] &#125;&#125;显然这个文档id为1的文档可以被正确地搜索，也就是说它有完好的inverted index供我们查询，虽然它没有字的source。那么我们如何有选择地进行存储我们想要的字段呢？这种情况适用于我们想节省自己的存储空间，只存储那些我们需要的字段到source里去。我们可以做如下的设置：1234567891011121314151617DELETE twitter PUT twitter&#123; &quot;mappings&quot;: &#123; &quot;_source&quot;: &#123; &quot;includes&quot;: [ &quot;*.lat&quot;, &quot;address&quot;, &quot;name.*&quot; ], &quot;excludes&quot;: [ &quot;name.surname&quot; ] &#125; &#125;&#125;在上面，我们使用include来包含我们想要的字段，同时我们通过exclude来去除那些不需要的字段。我们尝试如下的文档输入：12345678910111213141516171819202122PUT twitter/_doc/1&#123; &quot;user&quot; : &quot;双榆树-张三&quot;, &quot;message&quot; : &quot;今儿天气不错啊，出去转转去&quot;, &quot;uid&quot; : 2, &quot;age&quot; : 20, &quot;city&quot; : &quot;北京&quot;, &quot;province&quot; : &quot;北京&quot;, &quot;country&quot; : &quot;中国&quot;, &quot;name&quot;: &#123; &quot;firstname&quot;: &quot;三&quot;, &quot;surname&quot;: &quot;张&quot; &#125;, &quot;address&quot; : [ &quot;中国北京市海淀区&quot;, &quot;中关村29号&quot; ], &quot;location&quot; : &#123; &quot;lat&quot; : &quot;39.970718&quot;, &quot;lon&quot; : &quot;116.325747&quot; &#125;&#125;通过如下的命令来进行查询，我们可以看到：1GET twitter/_doc/1结果是：123456789101112131415161718192021&#123; &quot;_index&quot; : &quot;twitter&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;1&quot;, &quot;_version&quot; : 1, &quot;_seq_no&quot; : 0, &quot;_primary_term&quot; : 1, &quot;found&quot; : true, &quot;_source&quot; : &#123; &quot;address&quot; : [ &quot;中国北京市海淀区&quot;, &quot;中关村29号&quot; ], &quot;name&quot; : &#123; &quot;firstname&quot; : &quot;三&quot; &#125;, &quot;location&quot; : &#123; &quot;lat&quot; : &quot;39.970718&quot; &#125; &#125;&#125;显然，我们只有很少的几个字段被存储下来了。通过这样的方法，我们可以有选择地存储我们想要的字段。在实际的使用中，我们在查询文档时，也可以有选择地进行显示我们想要的字段，尽管有很多的字段被存于source中：1GET twitter/_doc/1?_source=name,location在这里，我们只想显示和name及location相关的字段，那么显示的结果为：1234567891011121314151617&#123; &quot;_index&quot; : &quot;twitter&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;1&quot;, &quot;_version&quot; : 1, &quot;_seq_no&quot; : 0, &quot;_primary_term&quot; : 1, &quot;found&quot; : true, &quot;_source&quot; : &#123; &quot;name&quot; : &#123; &quot;firstname&quot; : &quot;三&quot; &#125;, &quot;location&quot; : &#123; &quot;lat&quot; : &quot;39.970718&quot; &#125; &#125;&#125;更多的阅读，可以参阅文档“Mapping meta-field: _source”(https://www.elastic.co/guide/en/elasticsearch/reference/current/mapping-source-field.html)Doc_values默认情况下，大多数字段都已编入索引，这使它们可搜索。反向索引允许查询在唯一的术语排序列表中查找搜索词，并从中立即访问包含该词的文档列表。sort，aggregtion和访问脚本中的字段值需要不同的数据访问模式。除了查找术语和查找文档外，我们还需要能够查找文档并查找其在字段中具有的术语。Doc values是在文档索引时构建的磁盘数据结构，这使这种数据访问模式成为可能。它们存储与_source相同的值，但以面向列的方式存储，这对于排序和聚合而言更为有效。几乎所有字段类型都支持Doc值，但对字符串字段除外。默认情况下，所有支持doc值的字段均已启用它们。如果您确定不需要对字段进行排序或汇总，也不需要通过脚本访问字段值，则可以禁用doc值以节省磁盘空间：比如我们可以通过如下的方式来使得city字段不可以做sort或aggregation：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108DELETE twitterPUT twitter&#123; &quot;mappings&quot;: &#123; &quot;properties&quot;: &#123; &quot;city&quot;: &#123; &quot;type&quot;: &quot;keyword&quot;, &quot;doc_values&quot;: false, &quot;ignore_above&quot;: 256 &#125;, &quot;address&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;fields&quot;: &#123; &quot;keyword&quot;: &#123; &quot;type&quot;: &quot;keyword&quot;, &quot;ignore_above&quot;: 256 &#125; &#125; &#125;, &quot;age&quot;: &#123; &quot;type&quot;: &quot;long&quot; &#125;, &quot;country&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;fields&quot;: &#123; &quot;keyword&quot;: &#123; &quot;type&quot;: &quot;keyword&quot;, &quot;ignore_above&quot;: 256 &#125; &#125; &#125;, &quot;location&quot;: &#123; &quot;properties&quot;: &#123; &quot;lat&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;fields&quot;: &#123; &quot;keyword&quot;: &#123; &quot;type&quot;: &quot;keyword&quot;, &quot;ignore_above&quot;: 256 &#125; &#125; &#125;, &quot;lon&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;fields&quot;: &#123; &quot;keyword&quot;: &#123; &quot;type&quot;: &quot;keyword&quot;, &quot;ignore_above&quot;: 256 &#125; &#125; &#125; &#125; &#125;, &quot;message&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;fields&quot;: &#123; &quot;keyword&quot;: &#123; &quot;type&quot;: &quot;keyword&quot;, &quot;ignore_above&quot;: 256 &#125; &#125; &#125;, &quot;name&quot;: &#123; &quot;properties&quot;: &#123; &quot;firstname&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;fields&quot;: &#123; &quot;keyword&quot;: &#123; &quot;type&quot;: &quot;keyword&quot;, &quot;ignore_above&quot;: 256 &#125; &#125; &#125;, &quot;surname&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;fields&quot;: &#123; &quot;keyword&quot;: &#123; &quot;type&quot;: &quot;keyword&quot;, &quot;ignore_above&quot;: 256 &#125; &#125; &#125; &#125; &#125;, &quot;province&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;fields&quot;: &#123; &quot;keyword&quot;: &#123; &quot;type&quot;: &quot;keyword&quot;, &quot;ignore_above&quot;: 256 &#125; &#125; &#125;, &quot;uid&quot;: &#123; &quot;type&quot;: &quot;long&quot; &#125;, &quot;user&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;fields&quot;: &#123; &quot;keyword&quot;: &#123; &quot;type&quot;: &quot;keyword&quot;, &quot;ignore_above&quot;: 256 &#125; &#125; &#125; &#125; &#125;&#125;在上面，我们把city字段的doc_values设置为false。12345&quot;city&quot;: &#123; &quot;type&quot;: &quot;keyword&quot;, &quot;doc_values&quot;: false, &quot;ignore_above&quot;: 256&#125;,我们通过如下的方法来创建一个文档：12345678910111213141516171819202122PUT twitter/_doc/1&#123; &quot;user&quot; : &quot;双榆树-张三&quot;, &quot;message&quot; : &quot;今儿天气不错啊，出去转转去&quot;, &quot;uid&quot; : 2, &quot;age&quot; : 20, &quot;city&quot; : &quot;北京&quot;, &quot;province&quot; : &quot;北京&quot;, &quot;country&quot; : &quot;中国&quot;, &quot;name&quot;: &#123; &quot;firstname&quot;: &quot;三&quot;, &quot;surname&quot;: &quot;张&quot; &#125;, &quot;address&quot; : [ &quot;中国北京市海淀区&quot;, &quot;中关村29号&quot; ], &quot;location&quot; : &#123; &quot;lat&quot; : &quot;39.970718&quot;, &quot;lon&quot; : &quot;116.325747&quot; &#125;&#125;那么，当我们使用如下的方法来进行aggregation时：123456789101112GET twitter/_search&#123; &quot;size&quot;: 0, &quot;aggs&quot;: &#123; &quot;city_bucket&quot;: &#123; &quot;terms&quot;: &#123; &quot;field&quot;: &quot;city&quot;, &quot;size&quot;: 10 &#125; &#125; &#125;&#125;在我们的Kibana上我们可以看到：12345678910111213141516171819202122232425262728293031323334&#123; &quot;error&quot;: &#123; &quot;root_cause&quot;: [ &#123; &quot;type&quot;: &quot;illegal_argument_exception&quot;, &quot;reason&quot;: &quot;Can&apos;t load fielddata on [city] because fielddata is unsupported on fields of type [keyword]. Use doc values instead.&quot; &#125; ], &quot;type&quot;: &quot;search_phase_execution_exception&quot;, &quot;reason&quot;: &quot;all shards failed&quot;, &quot;phase&quot;: &quot;query&quot;, &quot;grouped&quot;: true, &quot;failed_shards&quot;: [ &#123; &quot;shard&quot;: 0, &quot;index&quot;: &quot;twitter&quot;, &quot;node&quot;: &quot;IyyZ30-hRi2rnOpfx4n1-A&quot;, &quot;reason&quot;: &#123; &quot;type&quot;: &quot;illegal_argument_exception&quot;, &quot;reason&quot;: &quot;Can&apos;t load fielddata on [city] because fielddata is unsupported on fields of type [keyword]. Use doc values instead.&quot; &#125; &#125; ], &quot;caused_by&quot;: &#123; &quot;type&quot;: &quot;illegal_argument_exception&quot;, &quot;reason&quot;: &quot;Can&apos;t load fielddata on [city] because fielddata is unsupported on fields of type [keyword]. Use doc values instead.&quot;, &quot;caused_by&quot;: &#123; &quot;type&quot;: &quot;illegal_argument_exception&quot;, &quot;reason&quot;: &quot;Can&apos;t load fielddata on [city] because fielddata is unsupported on fields of type [keyword]. Use doc values instead.&quot; &#125; &#125; &#125;, &quot;status&quot;: 400&#125;显然，我们的操作是失败的。尽管我们不能做aggregation及sort，但是我们还是可以通过如下的命令来得到它的source：1GET twitter/_doc/1显示结果为：123456789101112131415161718192021222324252627282930&#123; &quot;_index&quot; : &quot;twitter&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;1&quot;, &quot;_version&quot; : 1, &quot;_seq_no&quot; : 0, &quot;_primary_term&quot; : 1, &quot;found&quot; : true, &quot;_source&quot; : &#123; &quot;user&quot; : &quot;双榆树-张三&quot;, &quot;message&quot; : &quot;今儿天气不错啊，出去转转去&quot;, &quot;uid&quot; : 2, &quot;age&quot; : 20, &quot;city&quot; : &quot;北京&quot;, &quot;province&quot; : &quot;北京&quot;, &quot;country&quot; : &quot;中国&quot;, &quot;name&quot; : &#123; &quot;firstname&quot; : &quot;三&quot;, &quot;surname&quot; : &quot;张&quot; &#125;, &quot;address&quot; : [ &quot;中国北京市海淀区&quot;, &quot;中关村29号&quot; ], &quot;location&quot; : &#123; &quot;lat&quot; : &quot;39.970718&quot;, &quot;lon&quot; : &quot;116.325747&quot; &#125; &#125;&#125;更多阅读请参阅“Mapping parameters: doc_values”(https://www.elastic.co/guide/en/elasticsearch/reference/7.4/doc-values.html)。]]></content>
      <categories>
        <category>elk</category>
      </categories>
      <tags>
        <tag>elk</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch：Index生命周期管理入门]]></title>
    <url>%2F2020%2F01%2F02%2FElasticsearch%EF%BC%9AIndex%E7%94%9F%E5%91%BD%E5%91%A8%E6%9C%9F%E7%AE%A1%E7%90%86%E5%85%A5%E9%97%A8%2F</url>
    <content type="text"><![CDATA[如果您要处理时间序列数据，则不想将所有内容连续转储到单个索引中。 取而代之的是，您可以定期将数据滚动到新索引，以防止数据过大而又缓慢又昂贵。 随着索引的老化和查询频率的降低，您可能会将其转移到价格较低的硬件上，并减少分片和副本的数量。要在索引的生命周期内自动移动索引，可以创建策略来定义随着索引的老化对索引执行的操作。 索引生命周期策略在与Beats数据发件人一起使用时特别有用，Beats数据发件人不断将运营数据（例如指标和日志）发送到Elasticsearch。 当现有索引达到指定的大小或期限时，您可以自动滚动到新索引。 这样可以确保所有索引具有相似的大小，而不是每日索引，其大小可以根beats数和发送的事件数而有所不同。让我们通过动手操作场景跳入索引生命周期管理（Index cycle management: ILM）。 本文章将利用您可能不熟悉的ILM独有的许多新概念。 我们先用一个示例来展示。本示例的目标是建立一组索引，这些索引将封装来自时间序列数据源的数据。 我们可以想象有一个像Filebeat这样的系统，可以将文档连续索引到我们的书写索引中。 我们希望在索引达到50 GB，或文档的数量超过10000，或已在30天前创建索引后对其进行rollover，然后在90天后删除该索引。上图显示一个Log文档在Elasticsearch中生命周期。运行两个node的Elasticsearch集群我们可以参考文章“Elasticsearch：运用shard filtering来控制索引分配给哪个节点”运行起来两个node的cluster。其实非常简单，当我们安装好Elasticsearch后，打开一个terminal，并运行如下的命令：1./bin/elasticsearch -E node.name=node1 -E node.attr.data=hot -Enode.max_local_storage_nodes=2它将运行起来一个叫做node1的节点。同时在另外terminal中运行如下的命令：1./bin/elasticsearch -E node.name=node2 -E node.attr.data=warm -Enode.max_local_storage_nodes=2它运行另外一个叫做node2的节点。我们可以通过如下的命令来进行查看：1GET _cat/nodes?v显示两个节点：我们可以用如下的命令来检查这两个node的属性：1GET _cat/nodeattrs?v&amp;s=name显然其中的一个node是hot，另外一个是warm。准备数据运行起来我们的Kibana:我们分别点击上面的1和2处：点击上面的“Add data”。这样我们就可以把我们的kibana_sample_data_logs索引加载到Elasticsearch中。我们可以通过如下的命令进行查看：1GET _cat/indices/kibana_sample_data_logs命令显示结果为：它显示kibana_sample_data_logs具有11.1M的数据，并且它有14074个文档。建立ILM policy我们可以通过如下的方法来建立一个ILM的policy.123456789101112131415161718192021222324252627PUT _ilm/policy/logs_policy&#123; &quot;policy&quot;: &#123; &quot;phases&quot;: &#123; &quot;hot&quot;: &#123; &quot;min_age&quot;: &quot;0ms&quot;, &quot;actions&quot;: &#123; &quot;rollover&quot;: &#123; &quot;max_size&quot;: &quot;50gb&quot;, &quot;max_age&quot;: &quot;30d&quot;, &quot;max_docs&quot;: 10000 &#125;, &quot;set_priority&quot;: &#123; &quot;priority&quot;: 100 &#125; &#125; &#125;, &quot;delete&quot;: &#123; &quot;min_age&quot;: &quot;90d&quot;, &quot;actions&quot;: &#123; &quot;delete&quot;: &#123;&#125; &#125; &#125; &#125; &#125;&#125;这里定义的一个policy意思是：如果一个index的大小超过50GB，那么自动rollover如果一个index日期已在30天前创建索引后，那么自动rollover如果一个index的文档数超过10000，那么也会自动rollover当一个index创建的时间超过90天，那么也自动删除其实这个我们也可以通过Kibana帮我们来实现。请按照如下的步骤：紧接着点击“Index Lifecycle Policies”：再点击“Create Policy”:最后点“Save as new Policy”及可以在我们的Kibana中同过如下的命令可以查看到：1GET _ilm/policy/logs_policy显示结果：设置Index template我们可以通过如下的方法来建立template:1234567891011PUT _template/datastream_template&#123; &quot;index_patterns&quot;: [&quot;logs*&quot;], &quot;settings&quot;: &#123; &quot;number_of_shards&quot;: 1, &quot;number_of_replicas&quot;: 1, &quot;index.lifecycle.name&quot;: &quot;logs_policy&quot;, &quot;index.routing.allocation.require.data&quot;: &quot;hot&quot;, &quot;index.lifecycle.rollover_alias&quot;: &quot;logs&quot; &#125;&#125;这里的意思是所有以logs开头的index都需要遵循这个规律。这里定义了rollover的alias为“logs ”。这在我们下面来定义。同时也需要注意的是”index.routing.allocation.require.data”: “hot”。这个定义了我们需要indexing的node的属性是hot。请看一下我们上面的policy里定义的有一个叫做phases里的，它定义的是”hot”。在这里我们把所有的logs*索引都置于hot属性的node里。在实际的使用中，hot属性的index一般用作indexing。我们其实还可以定义一些其它phase，比如warm，这样可以把我们的用作搜索的index置于warm的节点中。这里就不一一描述了。定义Index alias我们可以通过如下的方法来定义：12345678PUT logs-000001&#123; &quot;aliases&quot;: &#123; &quot;logs&quot;: &#123; &quot;is_write_index&quot;: true &#125; &#125;&#125;在这里定义了一个叫做logs的alias，它指向logs-00001索引。注意这里的is_write_index为true。如果有rollover发生时，这个alias会自动指向最新rollover的index。生产数据在这里，我们使用之前我们已经导入的测试数据kibana_sample_data_logs，我们可以通过如下的方法来写入数据：123456789POST _reindex?requests_per_second=500&#123; &quot;source&quot;: &#123; &quot;index&quot;: &quot;kibana_sample_data_logs&quot; &#125;, &quot;dest&quot;: &#123; &quot;index&quot;: &quot;logs&quot; &#125;&#125;上面的意思是每秒按照500个文档从kibana_sample_data_logs索引reindex文档到logs别名所指向的index。我们运行后，通过如下的命令来查看最后的结果：1GET logs*/_count显示如下：我们可以看到有14074个文档被reindex到logs*索引中。通过如下的命令来查看：1GET _cat/shards/logs*我们可以看到logs-000002已经生产，并且所有的索引都在node1上面。我们可以通过如下的命令：1GET _cat/indices/logs?v我们可以看到logs-000001索引中有10000个文档，而logs-000002中含有4074个文档。由于我们已经设定了policy，那么所有的这些logs*索引的生命周期只有90天。90天过后（从索引被创建时算起），索引会自动被删除掉。]]></content>
      <categories>
        <category>elk</category>
      </categories>
      <tags>
        <tag>elk</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch：hanlp 中文分词器]]></title>
    <url>%2F2020%2F01%2F02%2FElasticsearch%EF%BC%9Ahanlp%20%E4%B8%AD%E6%96%87%E5%88%86%E8%AF%8D%E5%99%A8%2F</url>
    <content type="text"><![CDATA[HanLP 中文分词器是一个开源的分词器，是专为Elasticsearch而设计的。它是基于HanLP，并提供了HanLP中大部分的分词方式。它的源码位于：https://github.com/KennFalcon/elasticsearch-analysis-hanl从Elasticsearch 5.2.2开始，一直有跟随Elasticsearch的不同发行版而更新。安装1） 方式一：a. 下载对应的release安装包，最新release包可从baidu盘下载（链接:https://pan.baidu.com/s/1mFPNJXgiTPzZeqEjH_zifw 密码:i0o7）b. 执行如下命令安装，其中PATH为插件包绝对路径：1./bin/elasticsearch-plugin install file://$&#123;PATH&#125;2）方式二：a. 使用elasticsearch插件脚本安装command如下：1./bin/elasticsearch-plugin install https://github.com/KennFalcon/elasticsearch-analysis-hanlp/releases/download/v7.4.2/elasticsearch-analysis-hanlp-7.4.2.zip安装完后，我们可以使用如下的方式来验证我们的安装是否成功：12$ ./bin/elasticsearch-plugin listanalysis-hanlp如果我们安装时成功的话，我们可以看到上面的输出。安装数据包release包中存放的为HanLP源码中默认的分词数据，若要下载完整版数据包，请查看HanLP Release。数据包目录：ES_HOME/plugins/analysis-hanlp注：因原版数据包自定义词典部分文件名为中文，这里的hanlp.properties中已修改为英文，请对应修改文件名重启Elasticsearch注：上述说明中的ES_HOME为自己的ES安装路径，需要绝对路径。这一步非常重要。如果我们不重新启动，新安装的分词器将不会工作。热更新在本版本中，增加了词典热更新，修改步骤如下：a. 在ES_HOME/plugins/analysis-hanlp/data/dictionary/custom目录中新增自定义词典b. 修改hanlp.properties，修改CustomDictionaryPath，增加自定义词典配置c. 等待1分钟后，词典自动加载注：每个节点都需要做上述更改提供的分词方式说明hanlp: hanlp默认分词hanlp_standard: 标准分词hanlp_index: 索引分词hanlp_nlp: NLP分词hanlp_n_short: N-最短路分词hanlp_dijkstra: 最短路分词hanlp_crf: CRF分词（已有最新方式）hanlp_speed: 极速词典分词我们来做一个简单的例子：12345GET _analyze&#123; &quot;text&quot;: &quot;美国阿拉斯加州发生8.0级地震&quot;, &quot;tokenizer&quot;: &quot;hanlp&quot;&#125;那么显示的结果为：12345678910111213141516171819202122232425262728293031323334353637383940414243444546&#123; &quot;tokens&quot; : [ &#123; &quot;token&quot; : &quot;美国&quot;, &quot;start_offset&quot; : 0, &quot;end_offset&quot; : 2, &quot;type&quot; : &quot;nsf&quot;, &quot;position&quot; : 0 &#125;, &#123; &quot;token&quot; : &quot;阿拉斯加州&quot;, &quot;start_offset&quot; : 2, &quot;end_offset&quot; : 7, &quot;type&quot; : &quot;nsf&quot;, &quot;position&quot; : 1 &#125;, &#123; &quot;token&quot; : &quot;发生&quot;, &quot;start_offset&quot; : 7, &quot;end_offset&quot; : 9, &quot;type&quot; : &quot;v&quot;, &quot;position&quot; : 2 &#125;, &#123; &quot;token&quot; : &quot;8.0&quot;, &quot;start_offset&quot; : 9, &quot;end_offset&quot; : 12, &quot;type&quot; : &quot;m&quot;, &quot;position&quot; : 3 &#125;, &#123; &quot;token&quot; : &quot;级&quot;, &quot;start_offset&quot; : 12, &quot;end_offset&quot; : 13, &quot;type&quot; : &quot;q&quot;, &quot;position&quot; : 4 &#125;, &#123; &quot;token&quot; : &quot;地震&quot;, &quot;start_offset&quot; : 13, &quot;end_offset&quot; : 15, &quot;type&quot; : &quot;n&quot;, &quot;position&quot; : 5 &#125; ]&#125;更多详细阅读，请参阅链接https://github.com/KennFalcon/elasticsearch-analysis-hanlp]]></content>
      <categories>
        <category>elk</category>
      </categories>
      <tags>
        <tag>elk</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch：fielddata 介绍]]></title>
    <url>%2F2020%2F01%2F02%2FElasticsearch%EF%BC%9Afielddata%20%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[默认情况下，大多数字段都已编入索引，这使它们可搜索。 但是，脚本中的排序，聚合和访问字段值需要与搜索不同的访问模式。搜索需要回答“哪个文档包含该术语？”这个问题，而排序和汇总则需要回答一个不同的问题：“此字段对该文档的值是什么？”。大多数字段可以将索引时生产的磁盘doc_values(https://www.elastic.co/guide/en/elasticsearch/reference/current/doc-values.html)用于此数据访问模式，但是文本（text）字段不支持doc_values。替代的方案，文本（text）字段使用查询时内存中的数据结构，称为fielddata。 当我们首次将该字段用于聚合，排序或在脚本中使用时，将按需构建此数据结构。 它是通过从磁盘读取每个段的整个反向索引，反转术语↔︎文档关系并将结果存储在JVM堆中的内存中来构建的。Fielddata针对text字段在默认时是禁用的Fielddata会占用大量堆空间，尤其是在加载大量的文本字段时。 一旦将字段数据加载到堆中，它在该段的生命周期内将一直保留在那里。 同样，加载字段数据是一个昂贵的过程，可能导致用户遇到延迟的情况。 这就是默认情况下禁用字段数据的原因。假如我们创建一个如下的myindex的索引：123456789101112131415PUT myindex&#123; &quot;mappings&quot;: &#123; &quot;properties&quot;: &#123; &quot;address&quot;: &#123; &quot;type&quot;: &quot;text&quot; &#125; &#125; &#125;&#125; PUT myindex/_doc/1&#123; &quot;address&quot;: &quot;New York&quot; &#125;如果您尝试对文本字段中的脚本进行排序，汇总或访问值:123456789101112GET myindex/_search&#123; &quot;size&quot;: 20, &quot;aggs&quot;: &#123; &quot;aggr_mame&quot;: &#123; &quot;terms&quot;: &#123; &quot;field&quot;: &quot;address&quot;, &quot;size&quot;: 5 &#125; &#125; &#125;&#125;则会看到以下异常：显然，我们不能对text字段进行聚合处理。那么我们该如何处理这个问题呢？我们的一种方法就是在配置mapping的时候加入”fielddata”=true这个选项。我们来重新对我们的myindex的mapping进行配置：12345678910111213141516171819202122232425262728293031DELETE myindex PUT myindex&#123; &quot;mappings&quot;: &#123; &quot;properties&quot;: &#123; &quot;address&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;fielddata&quot;: true &#125; &#125; &#125;&#125; PUT myindex/_doc/1&#123; &quot;address&quot;: &quot;New York&quot; &#125; GET myindex/_search&#123; &quot;size&quot;: 0, &quot;aggs&quot;: &#123; &quot;aggr_mame&quot;: &#123; &quot;terms&quot;: &#123; &quot;field&quot;: &quot;address&quot;, &quot;size&quot;: 5 &#125; &#125; &#125;&#125;在这里，我们尽管还是把address这个字段设置为text，但是由于我们加入了”fielddata”=true，那么我们，我们就可以对这个项进行统计了。与简单的搜索操作不同，排序和聚合需要能够发现在特定文档的特定字段中可以找到哪些术语。 对于这些任务和其他任务，必须具有与Elasticsearch（反向）索引相反的数据结构。 这就是fielddata的目的。细心的开发者，如果这个时候去Kibana创建一个以myindex为索引的index pattern，我们可以发现：我们的address字段变为aggregatable，也就是说我们可以对它进行做聚合分析尽管它没有doc_values。在启动fielddata之前在启用fielddata之前，请考虑为什么将文本字段用于聚合，排序或在脚本中使用。 这样做通常没有任何意义。在索引之前会分析文本字段，以便可以通过搜索new或york来找到类似New York的值。 当您可能想要一个名为New York的存储桶时，此字段上的术语汇总将返回一个叫做new存储桶和一个叫做york存储桶。相反，您应该有一个用于全文搜索的文本字段，以及一个为聚合启用doc_values的未分析的keyword字段，如下所示：123456789101112131415161718DELETE myindex PUT myindex&#123; &quot;mappings&quot;: &#123; &quot;properties&quot;: &#123; &quot;address&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;fields&quot;: &#123; &quot;keyword&quot;: &#123; &quot;type&quot;: &quot;keyword&quot;, &quot;ignore_above&quot;: 256 &#125; &#125; &#125; &#125; &#125;&#125;这样，我们可以使用address来做全文的搜索，而address.keyword被用来做aggregations, sorting 及在脚本中使用。参考：【1】https://www.elastic.co/guide/en/elasticsearch/reference/current/fielddata.html【2】https://qbox.io/blog/field-data-elasticsearch-cluster-instability]]></content>
      <categories>
        <category>elk</category>
      </categories>
      <tags>
        <tag>elk</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch：Elasticsearch中的refresh和flush操作指南]]></title>
    <url>%2F2020%2F01%2F02%2FElasticsearch%EF%BC%9AElasticsearch%E4%B8%AD%E7%9A%84refresh%E5%92%8Cflush%E6%93%8D%E4%BD%9C%E6%8C%87%E5%8D%97%2F</url>
    <content type="text"><![CDATA[在今天的文章里，我们来主要介绍一下Elasticsearch的refresh及flush两种操作的区别。如果我们从字面的意思上讲，好像都是刷新的意思。但是在Elasticsearch中，这两种操作是有非常大的区别的。本指南将有效解决两者之间的差异。 我们还将介绍Lucene功能的基础知识，例如重新打开（reopen)和提交(commit)，这有助于理解refresh和flush操作。Refresh及Flush乍一看，Refresh和Flush操作的通用目的似乎是相同的。 两者都用于使文档在索引操作后立即可供搜索。 在Elasticsearch中添加新文档时，我们可以对索引调用_refresh或_flush操作，以使新文档可用于搜索。 要了解这些操作的工作方式，您必须熟悉Lucene中的Segments，Reopen和Commits。Apache Lucene是Elasticsearch中的基础查询引擎。Lucene中的Segments在Elasticsearch中，最基本的数据存储单位是shard。 但是，通过Lucene镜头看，情况会有所不同。 在这里，每个Elasticsearch分片都是一个Lucene索引(index)，每个Lucene索引都包含几个Lucene segments。 一个Segment包含映射到文档里的所有术语（terms)一个反向索引(inverted index)。下图显示了段的概念及其如何应用于Elasticsearch索引及其分片：这种分Segment的概念是，每当创建新文档时，它们就会被写入新的Segment中。 每当创建新文档时，它们都属于一个新的Segment，并且无需修改前一个Segment。 如果必须删除文档，则在其原始Segment中将其标记为已删除。 这意味着它永远不会从Segement中物理删除。与更新相同：文档的先前版本在上一个Segment中被标记为已删除，更新后的版本保留在当前Segment中的同一文档ID下。Lucene中的Reopen当调用Lucene Reopen时，将使累积的数据可用于搜索。 尽管可以搜索最新数据，但这不能保证数据的持久性或未将其写入磁盘。 我们可以调用n次重新打开功能，并使最新数据可搜索，但不能确定磁盘上是否存在数据。Lucene中的CommitsLucene提交使数据安全。 对于每次提交，来自不同段的数据将合并并推送到磁盘，从而使数据持久化。 尽管提交是持久保存数据的理想方法，但问题是每个提交操作都占用大量资源。 每个提交操作都有其自己的内部 I/O 操作以及与其相关的读/写周期。 这就是为什么我们希望在基于Lucene的系统中一次又一次地重新使用重新打开功能以使新数据可搜索的确切原因。Elasticsearch中的TranslogElasticsearch采用另一种方法来解决持久性问题。 它在每个分片中引入一个事务日志（transaction log）。 已建立索引的新文档将传递到此事务日志和内存缓冲区中。 下图显示了此过程：Elasticsearch中的refresh当我们把一条数据写入到Elasticsearch中后，它并不能马上被用于搜索。新增的索引必须写入到Segment后才能被搜索到，因此我们把数据写入到内存缓冲区之后并不能被搜索到。新增了一条记录时，Elasticsearch会把数据写到translog和in-memory buffer(内存缓存区)中,如下图所示:如果希望该文档能立刻被搜索，需要手动调用refresh操作。在Elasticsearch中，默认情况下_refresh操作设置为每秒执行一次。 在此操作期间，内存中缓冲区的内容将复制到内存中新创建的Segment中，如下图所示。 结果，新数据可用于搜索。这个refresh的时间间隔可以由index设置中index.refresh_interval来定义。执行完refresh后的结果如下：我们可以看出来，在In-meomory buffer中，现在所有的东西都是空的，但是Translog里还是有东西的。refresh的开销比较大,我在自己环境上测试10W条记录的场景下refresh一次大概要14ms,因此在批量构建索引时可以把refresh间隔设置成-1来临时关闭refresh,等到索引都提交完成之后再打开refresh,可以通过如下接口修改这个参数:12345curl -XPUT &apos;localhost:9200/test/_settings&apos; -d &apos;&#123; &quot;index&quot; : &#123; &quot;refresh_interval&quot; : &quot;-1&quot; &#125;&#125;&apos;另外当你在做批量索引时,可以考虑把副本数设置成0，因为document从主分片(primary shard)复制到从分片(replica shard)时,从分片也要执行相同的分析、索引和合并过程,这样的开销比较大，你可以在构建索引之后再开启副本，这样只需要把数据从主分片拷贝到从分片:12345curl -XPUT &apos;localhost:9200/my_index/_settings&apos; -d &apos; &#123; &quot;index&quot; : &#123; &quot;number_of_replicas&quot; : 0 &#125;&#125;&apos;执行完批量索引之后,把刷新间隔改回来:12345curl -XPUT &apos;localhost:9200/my_index/_settings&apos; -d &apos;&#123; &quot;index&quot; : &#123; &quot;refresh_interval&quot; : &quot;1s&quot; &#125; &#125;&apos;你还可以强制执行一次refresh以及索引分段的合并:12curl -XPOST &apos;localhost:9200/my_index/_refresh&apos;curl -XPOST &apos;localhost:9200/my_index/_forcemerge?max_num_segments=5&apos;Translog及持久化存储但是，translog如何解决持久性问题？ 每个Shard中都存在一个translog，这意味着它与物理磁盘内存有关。 它是同步且安全的，因此即使对于尚未提交的文档，您也可以获得持久性和持久性。 如果发生问题，可以还原事务日志。 同样，在每个设置的时间间隔内，或在成功完成请求（索引，批量，删除或更新）后，将事务日志提交到磁盘。Elasticsearch中的FlushFlush实质上意味着将内存缓冲区中的所有文档都写入新的Lucene Segment，如下面的图所示。 这些连同所有现有的内存段一起被提交到磁盘，该磁盘清除事务日志（参见图4）。 此提交本质上是Lucene提交（commit）。Flush会定期触发，也可以在Translog达到特定大小时触发。 这些设置可以防止Lucene提交带来的不必要的费用。结论在本指南中，我们探索了两个紧密相关的Elasticsearch操作，_flush和_refresh显示了它们之间的共性和差异。 我们还介绍了Lucene的基础架构组件-重新打开（reopen)并提交(commits)-这有助于掌握Elasticsearch中_refresh和_flush操作的要点。简而言之，_refresh用于使新文档可见以进行搜索。 而_flush用于将内存中的段保留在硬盘上。 _flush不会影响Elasticsearch中文档的可见性，因为搜索是在内存段中进行的，而不是_refresh会影响其可见性。参考：【1】https://qbox.io/blog/refresh-flush-operations-elasticsearch-guide【2】https://www.ezlippi.com/blog/2018/04/elasticsearch-translog.html]]></content>
      <categories>
        <category>elk</category>
      </categories>
      <tags>
        <tag>elk</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch：Dynamic mapping]]></title>
    <url>%2F2020%2F01%2F02%2FElasticsearch%EF%BC%9ADynamic%20mapping%2F</url>
    <content type="text"><![CDATA[Elasticsearch最重要的功能之一是它试图摆脱你的方式，让你尽快开始探索你的数据。 要索引文档，您不必首先创建索引，定义映射类型和定义字段 - 您只需索引文档，那么index，type和field将自动生效。比如：PUT data/_doc/1 { &quot;count&quot;: 5 } 上面的命令将自动帮我们生成一个叫做data的index，并同时生成一个叫做_doc的type及一个叫做count的field。count的数据类型是long。这个非常方便，我们不想传统的RDMS那样，先要创建一个数据库，让后一个table，然后才可以向table里写入数据。自动检测和添加新字段称为动态映射。 动态映射规则可以根据您的目的进行定制：动态字段映射：管理动态field检测的规则动态模板：用于配置动态添加字段的映射的自定义规则在今天的这篇文章中，我们来分别介绍这两个方面的内容。动态模板假设您有包含大量字段的文档或者在映射定义时未知的动态字段名称的文档和nested的key/value对不是一个很好的解决方案使用动态模板，您可以基于定义字段的映射字段的数据类型, 使用match_mapping_type字段的名字，使用match and unmatch 或match_pattern.或者字段的路径，使用path_match 及 path_unmatch.动态模板由命名对象的数组来定义的：123456789&quot;dynamic_templates&quot;: [ &#123; &quot;my_template_name&quot;: &#123; (1) ... match conditions ... (2) &quot;mapping&quot;: &#123; ... &#125; (3) &#125; &#125;, ... ]template的名字可以是任何一个字符串匹配的条件可以是这里的任何一种match_mapping_type, match, match_pattern, unmatch, path_match, path_unmatch被匹配的字段的mapping例如，如果我们想要将所有整数字段映射为整数而不是long，并将所有字符串字段映射为text和keyword，我们可以使用以下模板：PUT my_index { &quot;mappings&quot;: { &quot;dynamic_templates&quot;: [ { &quot;integers&quot;: { &quot;match_mapping_type&quot;: &quot;long&quot;, &quot;mapping&quot;: { &quot;type&quot;: &quot;integer&quot; } } }, { &quot;strings&quot;: { &quot;match_mapping_type&quot;: &quot;string&quot;, &quot;mapping&quot;: { &quot;type&quot;: &quot;text&quot;, &quot;fields&quot;: { &quot;raw&quot;: { &quot;type&quot;: &quot;keyword&quot;, &quot;ignore_above&quot;: 256 } } } } } ] } } PUT my_index/_doc/1 { &quot;my_integer&quot;: 5, &quot;my_string&quot;: &quot;Some string&quot; } 通过上面的动态template映射，我们可以看到my_integer被映射为integer而不是long。my_string是一个multi_field字段。假设您希望任何未映射的字符串字段默认情况下映射为“keyword”类型，那么我们可以这么定义：PUT test2 { &quot;mappings&quot;: { &quot;dynamic_templates&quot;: [ { &quot;my_string_fields&quot;: { &quot;match_mapping_type&quot;: &quot;string&quot;, &quot;mapping&quot;: { &quot;type&quot;: &quot;keyword&quot; } } } ] } } match及unmatchmatch参数使用模式匹配字段名称，而unmatch使用模式排除匹配匹配的字段。以下示例匹配名称以long_开头的所有字符串字段（以_text结尾的字符串除外）并将它们映射为长字段：1234567891011121314151617181920212223PUT my_index&#123; &quot;mappings&quot;: &#123; &quot;dynamic_templates&quot;: [ &#123; &quot;longs_as_strings&quot;: &#123; &quot;match_mapping_type&quot;: &quot;string&quot;, &quot;match&quot;: &quot;long_*&quot;, &quot;unmatch&quot;: &quot;*_text&quot;, &quot;mapping&quot;: &#123; &quot;type&quot;: &quot;long&quot; &#125; &#125; &#125; ] &#125;&#125; PUT my_index/_doc/1&#123; &quot;long_num&quot;: &quot;5&quot;, &quot;long_text&quot;: &quot;foo&quot; &#125;我们可以通过如下的命令来查看它们的数据类型：GET my_index/_mapping显示的结果为：1234567891011121314151617181920212223242526272829303132&#123; &quot;my_index&quot; : &#123; &quot;mappings&quot; : &#123; &quot;dynamic_templates&quot; : [ &#123; &quot;longs_as_strings&quot; : &#123; &quot;match&quot; : &quot;long_*&quot;, &quot;unmatch&quot; : &quot;*_text&quot;, &quot;match_mapping_type&quot; : &quot;string&quot;, &quot;mapping&quot; : &#123; &quot;type&quot; : &quot;long&quot; &#125; &#125; &#125; ], &quot;properties&quot; : &#123; &quot;long_num&quot; : &#123; &quot;type&quot; : &quot;long&quot; &#125;, &quot;long_text&quot; : &#123; &quot;type&quot; : &quot;text&quot;, &quot;fields&quot; : &#123; &quot;keyword&quot; : &#123; &quot;type&quot; : &quot;keyword&quot;, &quot;ignore_above&quot; : 256 &#125; &#125; &#125; &#125; &#125; &#125;&#125;从上面，我们可以看出来，long_num的数据类型是long，而long_text的数据类型是text。控制动态字段默认情况下，当在文档中找到以前未见过的字段时，Elasticsearch会将新字段添加到类型映射中。 通过将dynamic参数设置为false（忽略新字段）或strict（如果遇到未知字段则抛出异常），可以在文档和对象级别禁用此行为。在生产(product)环境中，你极有可能会创建你的mapping在索引你的数据之前，而且你极有可能不想你的mapping会被修改：1234POST blogs/_doc/2&#123;&quot;some_new_field&quot;: &quot;What should we do?&quot; &#125;在通常的情况下，上面的一个命令可能会自动帮我们在blogs索引里增加一个新的叫做some_new_field的字段。您可以使用“动态”属性（三个选项）控制添加到映射的新字段的效果：1234PUT blogs_example/_mapping&#123; &quot;dynamic&quot;: &quot;strict&quot;&#125;在上面我们在mapping中加入了dynamic，并且设置为strict，它表明如果现有的mapping里没有定义这个字段，那么就不index这个文档。1234PUT /blogs_example/_doc/1&#123; &quot;new_field&quot;: &quot;this is a new field&quot;&#125;如果new_field从来没有在mapping中定义过，那么，上面的命令会出现如下的错误：12345678910111213&#123; &quot;error&quot;: &#123; &quot;root_cause&quot;: [ &#123; &quot;type&quot;: &quot;strict_dynamic_mapping_exception&quot;, &quot;reason&quot;: &quot;mapping set to strict, dynamic introduction of [new_field] within [_doc] is not allowed&quot; &#125; ], &quot;type&quot;: &quot;strict_dynamic_mapping_exception&quot;, &quot;reason&quot;: &quot;mapping set to strict, dynamic introduction of [new_field] within [_doc] is not allowed&quot; &#125;, &quot;status&quot;: 400&#125;如果这个问题出现了，我们必须修改我们现有的index的mapping：12345678PUT blogs_example/_mapping&#123; &quot;properties&quot;: &#123; &quot;new_field&quot;: &#123; &quot;type&quot;: &quot;text&quot; &#125; &#125;&#125;那么重新运行之前的那个命令就可以了。settings以防止映射爆炸在索引中定义太多字段是一种可能导致映射爆炸的情况，这可能导致内存不足错误和难以恢复的情况。 这个问题可能比预期更常见。 例如，考虑插入的每个新文档引入新字段的情况。 这在动态映射中非常常见。 每次文档包含新字段时，这些字段最终都会出现在索引的映射中。 这并不需要担心少量数据，但随着映射的增加，它可能会成为一个问题。 以下设置允许您限制可手动或动态创建的字段映射的数量，以防止错误的文档导致映射爆炸：index.mapping.total_fields.limit索引中的最大字段数。 字段和对象映射以及字段别名都计入此限制。 默认值为1000index.mapping.depth.limit字段的最大深度，以内部对象的数量来衡量。 例如，如果所有字段都在根对象级别定义，则深度为1.如果有一个对象映射，则深度为2，等等。默认值为20。index.mapping.nested_fields.limit索引中不同nested映射的最大数量，默认为50。index.mapping.nested_objects.limit所有nested类型中单个文档中嵌套JSON对象的最大数量，默认为10000。index.mapping.field_name_length.limit设置字段名称的最大长度。 默认值为Long.MAX_VALUE（无限制）。 此设置实际上不是解决映射爆炸的问题，但如果要限制字段长度，则可能仍然有用。 通常不需要设置此设置。 默认是可以的，除非用户开始添加大量具有真正长名称的字段。上面的字段都可以在一个index的设置中进行设置，比如：12345678PUT test_blog &#123; &quot;settings&quot;: &#123; &quot;index.mapping.total_fields.limit&quot;: 2000, &quot;number_of_replicas&quot;: 0, &quot;number_of_shards&quot;: 1 &#125;&#125;]]></content>
      <categories>
        <category>elk</category>
      </categories>
      <tags>
        <tag>elk</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch：Cluster备份 Snapshot及Restore API]]></title>
    <url>%2F2020%2F01%2F02%2FElasticsearch%EF%BC%9ACluster%E5%A4%87%E4%BB%BD%20Snapshot%E5%8F%8ARestore%20API%2F</url>
    <content type="text"><![CDATA[Elasticsearch提供了replica解决方案，它可以帮我们解决了如果有一个或多个node失败了，那么我们的数据还是可以保证完整的情况，并且搜索还可以继续进行。但是，有一种情况是我们的所有的node，或者有一部分node失败，可能会造成我们的数据的丢失。也就是说replca不能提供一种灾难性的保护机制。我们需要一种完整的备份机制。Snapshot及Restore在Elastic里，我们提供了一个叫做snapshot及restore API的接口。使您可以使用数据和状态快照备份您的Elasticsearch索引和集群。 快照很重要，因为快照会在出现问题时提供您数据的副本。 如果需要回滚到旧版本的数据，则可以从存储库中还原快照。如上图所示，我们可以把当前index的状态及数据存入到一个repository里去。Repository为了能够做备份，我们首先必须创建一个repository，也就是一个仓库。你可以为一个cluster创建多个仓库。目前支持的仓库类型有：12345678# Elasticsearch支持仓库类型 Respository 配置类型Shared file system &quot;type&quot;: &quot;fs&quot;Read-only URL &quot;type&quot;: &quot;url&quot;S3 &quot;type&quot;: &quot;s3&quot;HDFS &quot;type&quot;: &quot;hdfs&quot;Azure &quot;type&quot;: &quot;azure&quot;Google Cloud Storage &quot;type&quot;: &quot;gcs&quot;这里需要注意的是： S3, HDFS, Azure and GCS 需要相应的插件进行安装才可以。注册仓库在一个snapshot可以被使用之前，我们必须注册一个仓库（repository)。使用_snapshot 终点文件夹必须对所有的node可以访问path.repo必须在所有的node上进行配置，针对一个fs的repository来说1234567PUT _snapshot/my_repo &#123; &quot;type&quot;: &quot;fs&quot;, &quot;settings&quot;: &#123; &quot;location&quot;: &quot;/mnt/my_repo_folder&quot; &#125; &#125;这里/mnt/my_repo_folder必须加进所有node的elasticsearch.yml文件中。fs resposity设置：12345678910PUT _snapshot/my_repo&#123; &quot;type&quot;: &quot;fs&quot;, &quot;settings&quot;: &#123; &quot;location&quot;: &quot;/mnt/my_repo_folder&quot;, &quot;compress&quot;: true, &quot;max_restore_bytes_per_sec&quot;: &quot;40mb&quot;, &quot;max_snapshot_bytes_per_sec&quot;: &quot;40mb&quot; &#125;&#125;这里，我们定义compress为true，表明我们希望压缩。通过max_restore_bytes_per_sec及max_snapshot_bytes_per_sec的定义，我们可以来限制数据的snapshot及恢复的数据速度。S3 repository设置为了能能使用S3仓库，我们必须使用如下的命令来进行安装插件：1./bin/elasticsearch-plugin install repository-s3注意，上面的命令必须是在Elasticsearch的安装目录下进行执行。我们可以通过如下的命令来进行配置：1234567PUT _snapshot/my_s3_repo&#123; &quot;type&quot;: &quot;s3&quot;, &quot;settings&quot;: &#123; &quot;bucket&quot;: &quot;my_s3_bucket_name&quot; &#125;&#125;这里的my_s3_bucket_name是我们在AWS上定义的S3 bucket。更多关于S3的配置可以参阅链接 Repository Settings(https://www.elastic.co/guide/en/elasticsearch/plugins/current/repository-s3-repository.html)。Snapshot所有的索引一旦我们的repository已经被配置好了，那么我们就可以利用_snapshot终点来进行snapshot。必须注意的是snapshot只拷贝在执行该命令时的所有的数据，而在之后的所有的数据将不被备份。snapshot是按照增量来进行备份的，也就是说它只拷贝从上次执行snapshot之后变化的部分。通常来说，每隔30分钟进行一次备份是足够的。snapshot命令：1PUT _snapshot/my_repo/my_snapshot_1这里必须注意的几点：my_repo是指的我们在上面定义的repository的名字my_snapshot_1指的是一个唯一的snapshot名字没有特定的索引名字被指出，那么它指的是所有的open索引如果我们想指定某个或某些特定的索引，那么我们可以使用如下的命令来执行备份（snapshot)123456PUT _snapshot/my_repo/my_logs_snapshot_1&#123; &quot;indices&quot;: &quot;logs-*&quot;, &quot;ignore_unavailable&quot;: true, &quot;include_global_state&quot;: true&#125;这里它表述我们相对所有以logs-为开头的索引进行备份。我们可以通过如下的命令来进行监测正在进行的snapshot的进度：1GET _snapshot/my_repo/my_snapshot_2/_status管理snapshots获取所有在repo中的snapshots:1GET _snapshot/my_repo/_all获取某个特定snapshot的信息1GET _snapshot/my_repo/my_snapshot_1删除一个snapshot1DELETE _snapshot/my_repo/my_snapshot_1恢复一个snapshot我们可以使用_restore终点来从一个snapshot恢复所有的索引：1POST _snapshot/my_repo/my_snapshot_2/_restore我们也可以通过如下的方法来恢复某个或某些特定的索引：123456POST _snapshot/my_repo/my_snapshot_2/_restore&#123; &quot;indices&quot;: &quot;logs-*&quot;, &quot;ignore_unavailable&quot;: true, &quot;include_global_state&quot;: false&#125;在很多的时候，我们想把snapshot中的索引恢复到一个不同名字的索引之中，从而不用覆盖现有的。我们可以通过rename_pattern及rename_replacement来进行配置：12345678POST _snapshot/my_repo/my_snapshot_2/_restore&#123; &quot;indices&quot;: &quot;logs-*&quot;, &quot;ignore_unavailable&quot;: true, &quot;include_global_state&quot;: false, &quot;rename_pattern&quot;: &quot;logs-(.+)&quot;, &quot;rename_replacement&quot;: &quot;restored-logs-$1&quot;&#125;在上面，我们把所有的以logs-为开头的索引恢复到以restored-logs-的开头的索引之中来。Restore到一个新的cluster针对这个情况，我们可以恢复从另外一个cluster中备份的snapshot到当前的cluster中来。我们必须在新的cluster中注册这个repository才可以进行下面的操作。从上面我们可以看出来，my_repo必须对两个cluster都是可见的才可以。动手实践准备数据：运行起来我们的Kibana:我们分别点击上面的1和2处：点击上面的“Add data”。这样我们就可以把我们的kibana_sample_data_logs索引加载到Elasticsearch中。1GET _cat/indices/kibana_sample_data_logs注册repository首先我们在我们的电脑上创建一个如下的目录：1/shared_folder/my_repo我们在termimal中打入如下的命令：1mkdir -p shared_folder/my_repo/1234$ pwd/Users/liuxg/shared_folderbogon:shared_folder liuxg$ ls -aldrwxr-xr-x 2 liuxg staff 64 Nov 13 13:23 my_repo将以下path.repo属性添加到我们运行的所有node的elasticsearch.yml文件中：1path.repo: /Users/liuxg/shared_folder/my_repo注意，针对你的情况，你需要改动这里的path路径。然后启动我们的Elasticsearch及Kibana。紧接着，我们在Kibana console中打入如下的命令：1234567PUT _snapshot/my_local_repo&#123; &quot;type&quot;: &quot;fs&quot;, &quot;settings&quot;: &#123; &quot;location&quot;: &quot;/Users/liuxg/shared_folder/my_repo&quot; &#125;&#125;注意这里的location是根据我自己的电脑的路径来设置的。你需要根据自己实际的路径进行修改。在这里my_local_repo是我们的repository名称。接下来，我们打入如下的命令来对我们的kibana_sample_data_logs索引进行snapshot:123456PUT _snapshot/my_local_repo/snapshot_1&#123; &quot;indices&quot;: &quot;kibana_sample_data_logs&quot;, &quot;ignore_unavailable&quot;: true, &quot;include_global_state&quot;: true&#125;我们可以通过如下的命令来查看snapshot:1GET _snapshot/my_local_repo/_all我们可以在右边看到snapshot_1出现在列表之中，说明我们已经成功地创建了这个snapshot。我们接下来可以通过如下的命令来删除kibana_sample_data_logs索引：1DELETE kibana_sample_data_logs这样我们彻底地删除了这个索引。那么我们该如何把之前备份的数据恢复回来呢？在Kibana中打入如下的命令：123456POST _snapshot/my_local_repo/snapshot_1/_restore&#123; &quot;indices&quot;: &quot;kibana_sample_data_logs&quot;, &quot;ignore_unavailable&quot;: true, &quot;include_global_state&quot;: false&#125;在执行完上面的命令后，我们可以通过如下的命令来查看恢复后的kibana_sample_data_logs索引：1GET kibana_sample_data_logs/_count显然我们已经成功地恢复了我们之前备份的数据。这个时候，如果我们去到我们的snapshot文件目录，我们可以看到：123456$ pwd/Users/liuxg/shared_folder/my_repobogon:my_repo liuxg$ lsindex-0 meta-TzygGpJ1SOK5yJdsmc1lng.datindex.latest snap-TzygGpJ1SOK5yJdsmc1lng.datindices显然在文件目录中，已经有新生产的文件了。]]></content>
      <categories>
        <category>elk</category>
      </categories>
      <tags>
        <tag>elk</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch：aggregation介绍]]></title>
    <url>%2F2020%2F01%2F02%2FElasticsearch%EF%BC%9Aaggregation%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[聚合(aggregation)功能集是整个Elasticsearch产品中最令人兴奋和有益的功能之一，主要是因为它提供了一个非常有吸引力对之前的facets的替代。在本教程中，我们将解释Elasticsearch中的聚合（aggregation）并逐步介绍一些示例。 我们比较了指标聚合和存储桶聚合，并展示了如何利用聚合嵌套（对于facets而言这是不可能的）。 欢迎您在本文中复制所有示例代码。关于Elastic Facets的一点背景如果您曾经使用过Elasticsearch的facets，那么您肯定了解它们的实用性。 经过丰富的经验，我们在这里告诉您Elasticsearch聚合（aggregations)甚至更好。 facets使您可以快速计算和汇总查询结果，并且可以将其用于各种任务，例如结果值的动态计数或创建分布直方图。 尽管facets非常强大，但它们在Elasticsearch核心中的实现存在一些限制。 由于facets仅执行一级深度的计算，因此将它们组合起来并不容易。聚合(Aggregation)API(https://www.elastic.co/guide/en/elasticsearch/client/java-api/7.4/java-aggs.html)解决了这些问题，并且还提供了一种简单的方法在查询时（在单个请求中）进行的非常精确的多级计算。 简而言之：Elasticsearch聚合是对facets的一个更加全面的提高的。准备数据为了完成我们今天的练习，我们先来准备一些数据。我们想创建一个叫做sports的索引。为此，我们先创建一个mapping：1234567891011121314151617181920212223PUT sports&#123; &quot;mappings&quot;: &#123; &quot;properties&quot;: &#123; &quot;birthdate&quot;: &#123; &quot;type&quot;: &quot;date&quot;, &quot;format&quot;: &quot;dateOptionalTime&quot; &#125;, &quot;location&quot;: &#123; &quot;type&quot;: &quot;geo_point&quot; &#125;, &quot;name&quot;: &#123; &quot;type&quot;: &quot;keyword&quot; &#125;, &quot;rating&quot;: &#123; &quot;type&quot;: &quot;integer&quot; &#125;, &quot;sport&quot;: &#123; &quot;type&quot;: &quot;keyword&quot; &#125; &#125; &#125;&#125;在上面，我们定义了一个sports索引的mapping。在下面，我们通过bulk API来把我们想要的数据导入到索引中。123456789101112131415161718192021222324252627282930313233343536373839404142434445POST _bulk/&#123;&quot;index&quot;:&#123;&quot;_index&quot;:&quot;sports&quot;&#125;&#125;&#123;&quot;name&quot;:&quot;Michael&quot;,&quot;birthdate&quot;:&quot;1989-10-1&quot;,&quot;sport&quot;:&quot;Baseball&quot;,&quot;rating&quot;:[&quot;5&quot;,&quot;4&quot;],&quot;location&quot;:&quot;46.22,-68.45&quot;&#125;&#123;&quot;index&quot;:&#123;&quot;_index&quot;:&quot;sports&quot;&#125;&#125;&#123;&quot;name&quot;:&quot;Bob&quot;,&quot;birthdate&quot;:&quot;1989-11-2&quot;,&quot;sport&quot;:&quot;Baseball&quot;,&quot;rating&quot;:[&quot;3&quot;,&quot;4&quot;],&quot;location&quot;:&quot;45.21,-68.35&quot;&#125;&#123;&quot;index&quot;:&#123;&quot;_index&quot;:&quot;sports&quot;&#125;&#125;&#123;&quot;name&quot;:&quot;Jim&quot;,&quot;birthdate&quot;:&quot;1988-10-3&quot;,&quot;sport&quot;:&quot;Baseball&quot;,&quot;rating&quot;:[&quot;3&quot;,&quot;2&quot;],&quot;location&quot;:&quot;45.16,-63.58&quot;&#125;&#123;&quot;index&quot;:&#123;&quot;_index&quot;:&quot;sports&quot;&#125;&#125;&#123;&quot;name&quot;:&quot;Joe&quot;,&quot;birthdate&quot;:&quot;1992-5-20&quot;,&quot;sport&quot;:&quot;Baseball&quot;,&quot;rating&quot;:[&quot;4&quot;,&quot;3&quot;],&quot;location&quot;:&quot;45.22,-68.53&quot;&#125;&#123;&quot;index&quot;:&#123;&quot;_index&quot;:&quot;sports&quot;&#125;&#125;&#123;&quot;name&quot;:&quot;Tim&quot;,&quot;birthdate&quot;:&quot;1992-2-28&quot;,&quot;sport&quot;:&quot;Baseball&quot;,&quot;rating&quot;:[&quot;3&quot;,&quot;3&quot;],&quot;location&quot;:&quot;46.22,-68.85&quot;&#125;&#123;&quot;index&quot;:&#123;&quot;_index&quot;:&quot;sports&quot;&#125;&#125;&#123;&quot;name&quot;:&quot;Alfred&quot;,&quot;birthdate&quot;:&quot;1990-9-9&quot;,&quot;sport&quot;:&quot;Baseball&quot;,&quot;rating&quot;:[&quot;2&quot;,&quot;2&quot;],&quot;location&quot;:&quot;45.12,-68.35&quot;&#125;&#123;&quot;index&quot;:&#123;&quot;_index&quot;:&quot;sports&quot;&#125;&#125;&#123;&quot;name&quot;:&quot;Jeff&quot;,&quot;birthdate&quot;:&quot;1990-4-1&quot;,&quot;sport&quot;:&quot;Baseball&quot;,&quot;rating&quot;:[&quot;2&quot;,&quot;3&quot;],&quot;location&quot;:&quot;46.12,-68.55&quot;&#125;&#123;&quot;index&quot;:&#123;&quot;_index&quot;:&quot;sports&quot;&#125;&#125;&#123;&quot;name&quot;:&quot;Will&quot;,&quot;birthdate&quot;:&quot;1988-3-1&quot;,&quot;sport&quot;:&quot;Baseball&quot;,&quot;rating&quot;:[&quot;4&quot;,&quot;4&quot;],&quot;location&quot;:&quot;46.25,-68.55&quot;&#125;&#123;&quot;index&quot;:&#123;&quot;_index&quot;:&quot;sports&quot;&#125;&#125;&#123;&quot;name&quot;:&quot;Mick&quot;,&quot;birthdate&quot;:&quot;1989-10-1&quot;,&quot;sport&quot;:&quot;Baseball&quot;,&quot;rating&quot;:[&quot;3&quot;,&quot;4&quot;],&quot;location&quot;:&quot;46.22,-68.45&quot;&#125;&#123;&quot;index&quot;:&#123;&quot;_index&quot;:&quot;sports&quot;&#125;&#125;&#123;&quot;name&quot;:&quot;Pong&quot;,&quot;birthdate&quot;:&quot;1989-11-2&quot;,&quot;sport&quot;:&quot;Baseball&quot;,&quot;rating&quot;:[&quot;1&quot;,&quot;3&quot;],&quot;location&quot;:&quot;45.21,-68.35&quot;&#125;&#123;&quot;index&quot;:&#123;&quot;_index&quot;:&quot;sports&quot;&#125;&#125;&#123;&quot;name&quot;:&quot;Ray&quot;,&quot;birthdate&quot;:&quot;1988-10-3&quot;,&quot;sport&quot;:&quot;Baseball&quot;,&quot;rating&quot;:[&quot;2&quot;,&quot;2&quot;],&quot;location&quot;:&quot;45.16,-63.58&quot;&#125;&#123;&quot;index&quot;:&#123;&quot;_index&quot;:&quot;sports&quot;&#125;&#125;&#123;&quot;name&quot;:&quot;Ping&quot;,&quot;birthdate&quot;:&quot;1992-5-20&quot;,&quot;sport&quot;:&quot;Baseball&quot;,&quot;rating&quot;:[&quot;4&quot;,&quot;3&quot;],&quot;location&quot;:&quot;45.22,-68.53&quot;&#125;&#123;&quot;index&quot;:&#123;&quot;_index&quot;:&quot;sports&quot;&#125;&#125;&#123;&quot;name&quot;:&quot;Duke&quot;,&quot;birthdate&quot;:&quot;1992-2-28&quot;,&quot;sport&quot;:&quot;Baseball&quot;,&quot;rating&quot;:[&quot;5&quot;,&quot;2&quot;],&quot;location&quot;:&quot;46.22,-68.85&quot;&#125;&#123;&quot;index&quot;:&#123;&quot;_index&quot;:&quot;sports&quot;&#125;&#125;&#123;&quot;name&quot;:&quot;Hal&quot;,&quot;birthdate&quot;:&quot;1990-9-9&quot;,&quot;sport&quot;:&quot;Baseball&quot;,&quot;rating&quot;:[&quot;4&quot;,&quot;2&quot;],&quot;location&quot;:&quot;45.12,-68.35&quot;&#125;&#123;&quot;index&quot;:&#123;&quot;_index&quot;:&quot;sports&quot;&#125;&#125;&#123;&quot;name&quot;:&quot;Charge&quot;,&quot;birthdate&quot;:&quot;1990-4-1&quot;,&quot;sport&quot;:&quot;Baseball&quot;,&quot;rating&quot;:[&quot;3&quot;,&quot;2&quot;],&quot;location&quot;:&quot;46.12,-68.55&quot;&#125;&#123;&quot;index&quot;:&#123;&quot;_index&quot;:&quot;sports&quot;&#125;&#125;&#123;&quot;name&quot;:&quot;Barry&quot;,&quot;birthdate&quot;:&quot;1988-3-1&quot;,&quot;sport&quot;:&quot;Baseball&quot;,&quot;rating&quot;:[&quot;5&quot;,&quot;2&quot;],&quot;location&quot;:&quot;46.25,-68.55&quot;&#125;&#123;&quot;index&quot;:&#123;&quot;_index&quot;:&quot;sports&quot;&#125;&#125;&#123;&quot;name&quot;:&quot;Bank&quot;,&quot;birthdate&quot;:&quot;1988-3-1&quot;,&quot;sport&quot;:&quot;Golf&quot;,&quot;rating&quot;:[&quot;6&quot;,&quot;4&quot;],&quot;location&quot;:&quot;46.25,-68.55&quot;&#125;&#123;&quot;index&quot;:&#123;&quot;_index&quot;:&quot;sports&quot;&#125;&#125;&#123;&quot;name&quot;:&quot;Bingo&quot;,&quot;birthdate&quot;:&quot;1988-3-1&quot;,&quot;sport&quot;:&quot;Golf&quot;,&quot;rating&quot;:[&quot;10&quot;,&quot;7&quot;],&quot;location&quot;:&quot;46.25,-68.55&quot;&#125;&#123;&quot;index&quot;:&#123;&quot;_index&quot;:&quot;sports&quot;&#125;&#125;&#123;&quot;name&quot;:&quot;James&quot;,&quot;birthdate&quot;:&quot;1988-3-1&quot;,&quot;sport&quot;:&quot;Basketball&quot;,&quot;rating&quot;:[&quot;10&quot;,&quot;8&quot;],&quot;location&quot;:&quot;46.25,-68.55&quot;&#125;&#123;&quot;index&quot;:&#123;&quot;_index&quot;:&quot;sports&quot;&#125;&#125;&#123;&quot;name&quot;:&quot;Wayne&quot;,&quot;birthdate&quot;:&quot;1988-3-1&quot;,&quot;sport&quot;:&quot;Hockey&quot;,&quot;rating&quot;:[&quot;10&quot;,&quot;10&quot;],&quot;location&quot;:&quot;46.25,-68.55&quot;&#125;&#123;&quot;index&quot;:&#123;&quot;_index&quot;:&quot;sports&quot;&#125;&#125;&#123;&quot;name&quot;:&quot;Brady&quot;,&quot;birthdate&quot;:&quot;1988-3-1&quot;,&quot;sport&quot;:&quot;Football&quot;,&quot;rating&quot;:[&quot;10&quot;,&quot;10&quot;],&quot;location&quot;:&quot;46.25,-68.55&quot;&#125;&#123;&quot;index&quot;:&#123;&quot;_index&quot;:&quot;sports&quot;&#125;&#125;&#123;&quot;name&quot;:&quot;Lewis&quot;,&quot;birthdate&quot;:&quot;1988-3-1&quot;,&quot;sport&quot;:&quot;Football&quot;,&quot;rating&quot;:[&quot;10&quot;,&quot;10&quot;],&quot;location&quot;:&quot;46.25,-68.55&quot;&#125;通过上面的bulk API接口，我们可以把我们想要的数据输入到sports的索引中。我们可以通过如下的接口来获得我多少条数据：1GET sports/_count显示结果：123456789&#123; &quot;count&quot; : 22, &quot;_shards&quot; : &#123; &quot;total&quot; : 1, &quot;successful&quot; : 1, &quot;skipped&quot; : 0, &quot;failed&quot; : 0 &#125;&#125;在这个数据库里，我们有可以看到有22条的数据。动手实践聚合的两个主要系列是指标聚合（metric aggregations）(https://www.elastic.co/guide/en/elasticsearch/reference/master/search-aggregations-metrics.html)和存储桶聚合（bucket aggregation）(https://www.elastic.co/guide/en/elasticsearch/reference/current/search-aggregations-bucket.html)。指标聚合计算一组文档中的某些值（例如平均值）； 存储桶聚合将文档分组到存储桶中。 在详细介绍之前，让我们看一下聚合请求的一般结构。除此之前，聚合还有Matrix(https://www.elastic.co/guide/en/elasticsearch/reference/current/search-aggregations-matrix.html)及Pipleline(https://www.elastic.co/guide/en/elasticsearch/reference/current/search-aggregations-pipeline.html聚合。Aggregation结构123456789&quot;aggregations&quot; : &#123; &quot;&lt;aggregation_name&gt;&quot; : &#123; &quot;&lt;aggregation_type&gt;&quot; : &#123; &lt;aggregation_body&gt; &#125;, [&quot;aggregations&quot; : &#123; [&lt;sub_aggregation&gt;]* &#125; ] &#125; [,&quot;&lt;aggregation_name_2&gt;&quot; : &#123; ... &#125; ]*&#125;请求json中的聚合（您也可以改用aggs）对象包含聚合名称，类型和主体。 &lt;aggregation_name&gt;是用户定义的名称（不带括号），该名称将唯一标识响应中的聚合名称/键。&lt;aggregation_type&gt;通常是聚合中的第一个键。 它可以是terms，stats或geo-distance聚合，但这是它的起点。 在我们的&lt;aggregation_type&gt;中，我们有一个&lt;aggregation_body&gt;。 在&lt;aggregation_body&gt;中，我们指定聚合所需的属性。 可用属性取决于聚合的类型。您可以选择提供子聚合，以将一个聚合元素的结果嵌套到另一个聚合元素中。 此外，您可以在查询中输入多个聚合（aggregation_name_2），以具有更多单独的顶级聚合。 尽管对嵌套级别没有限制，但是您不能将度量标准嵌套在度量标准聚合中，原因如下所述。 在研究可以聚合的不同类型的值之后，我们将了解桶聚合和度量聚合之间的区别。例子一些聚合使用从聚合文档中获取的值。 这些值可以从指定的文档字段（field）中获取，也可以从随每个文档生成值的脚本中获取。 下面的第一个示例在名称字段上提供了术语聚合(terms aggregation)，在子聚合rating_avg值上给出了顺序。 如您所见，我们使用嵌套的指标聚合对存储桶聚合的结果进行排序。尽管我们使用上面给出的索引，但是我们鼓励您运行此查询（以及下面的其他查询）。 您可以从工作中获得直接结果，然后对其进行修改以匹配您的数据集。另外，请仔细查看我们是否包含“ size”：0，因为我们的重点是聚合结果，而不是文档结果。这里设置为0，表示我们不想得到任何的文档。123456789101112131415161718192021GET sports/_search&#123; &quot;size&quot;: 0, &quot;aggregations&quot;: &#123; &quot;the_name&quot;: &#123; &quot;terms&quot;: &#123; &quot;field&quot;: &quot;name&quot;, &quot;order&quot;: &#123; &quot;rating_avg&quot;: &quot;desc&quot; &#125; &#125;, &quot;aggregations&quot;: &#123; &quot;rating_avg&quot;: &#123; &quot;avg&quot;: &#123; &quot;field&quot;: &quot;rating&quot; &#125; &#125; &#125; &#125; &#125;&#125;显示的结果为：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596&#123; &quot;took&quot; : 1, &quot;timed_out&quot; : false, &quot;_shards&quot; : &#123; &quot;total&quot; : 1, &quot;successful&quot; : 1, &quot;skipped&quot; : 0, &quot;failed&quot; : 0 &#125;, &quot;hits&quot; : &#123; &quot;total&quot; : &#123; &quot;value&quot; : 22, &quot;relation&quot; : &quot;eq&quot; &#125;, &quot;max_score&quot; : null, &quot;hits&quot; : [ ] &#125;, &quot;aggregations&quot; : &#123; &quot;the_name&quot; : &#123; &quot;doc_count_error_upper_bound&quot; : 0, &quot;sum_other_doc_count&quot; : 12, &quot;buckets&quot; : [ &#123; &quot;key&quot; : &quot;Brady&quot;, &quot;doc_count&quot; : 1, &quot;rating_avg&quot; : &#123; &quot;value&quot; : 10.0 &#125; &#125;, &#123; &quot;key&quot; : &quot;Lewis&quot;, &quot;doc_count&quot; : 1, &quot;rating_avg&quot; : &#123; &quot;value&quot; : 10.0 &#125; &#125;, &#123; &quot;key&quot; : &quot;Wayne&quot;, &quot;doc_count&quot; : 1, &quot;rating_avg&quot; : &#123; &quot;value&quot; : 10.0 &#125; &#125;, &#123; &quot;key&quot; : &quot;James&quot;, &quot;doc_count&quot; : 1, &quot;rating_avg&quot; : &#123; &quot;value&quot; : 9.0 &#125; &#125;, &#123; &quot;key&quot; : &quot;Bingo&quot;, &quot;doc_count&quot; : 1, &quot;rating_avg&quot; : &#123; &quot;value&quot; : 8.5 &#125; &#125;, &#123; &quot;key&quot; : &quot;Bank&quot;, &quot;doc_count&quot; : 1, &quot;rating_avg&quot; : &#123; &quot;value&quot; : 5.0 &#125; &#125;, &#123; &quot;key&quot; : &quot;Michael&quot;, &quot;doc_count&quot; : 1, &quot;rating_avg&quot; : &#123; &quot;value&quot; : 4.5 &#125; &#125;, &#123; &quot;key&quot; : &quot;Will&quot;, &quot;doc_count&quot; : 1, &quot;rating_avg&quot; : &#123; &quot;value&quot; : 4.0 &#125; &#125;, &#123; &quot;key&quot; : &quot;Barry&quot;, &quot;doc_count&quot; : 1, &quot;rating_avg&quot; : &#123; &quot;value&quot; : 3.5 &#125; &#125;, &#123; &quot;key&quot; : &quot;Bob&quot;, &quot;doc_count&quot; : 1, &quot;rating_avg&quot; : &#123; &quot;value&quot; : 3.5 &#125; &#125; ] &#125; &#125;&#125;上面的结果显示：我们得到了按照每个人来进行分类的聚合，而他们的顺序是按照rating_avg聚合所获得平均分数来排序的。我们还可以提供一个script脚本来生成聚合所使用的值:123456789101112131415161718192021222324252627GET sports/_search&#123; &quot;size&quot;: 0, &quot;aggs&quot;: &#123; &quot;age_range&quot;: &#123; &quot;range&quot;: &#123; &quot;script&quot;: &#123; &quot;source&quot;: &quot;&quot;&quot; ZonedDateTime dob = doc[&apos;birthdate&apos;].value; return params.now - dob.getYear() &quot;&quot;&quot; , &quot;params&quot;: &#123; &quot;now&quot;: 2019 &#125; &#125;, &quot;ranges&quot;: [ &#123; &quot;from&quot;: 30, &quot;to&quot;: 31 &#125; ] &#125; &#125; &#125;&#125;在上面，我们通过脚本生产value source，并对它做出统计。显示的结果是：123456789101112131415161718192021222324252627282930&#123; &quot;took&quot; : 0, &quot;timed_out&quot; : false, &quot;_shards&quot; : &#123; &quot;total&quot; : 1, &quot;successful&quot; : 1, &quot;skipped&quot; : 0, &quot;failed&quot; : 0 &#125;, &quot;hits&quot; : &#123; &quot;total&quot; : &#123; &quot;value&quot; : 22, &quot;relation&quot; : &quot;eq&quot; &#125;, &quot;max_score&quot; : null, &quot;hits&quot; : [ ] &#125;, &quot;aggregations&quot; : &#123; &quot;age_range&quot; : &#123; &quot;buckets&quot; : [ &#123; &quot;key&quot; : &quot;30.0-31.0&quot;, &quot;from&quot; : 30.0, &quot;to&quot; : 31.0, &quot;doc_count&quot; : 4 &#125; ] &#125; &#125;&#125;上面显示在30至31岁之间的有4个人。Metric Aggregations指标聚合类型用于计算整个文档集的指标。 有单值指标聚合（例如avg）和多值指标聚合（例如stats）。 指标聚合的一个简单示例是value_count聚合，它仅返回已为给定字段建立索引的值的总数。 要在运动员数据集中的“sport”字段中找到值的数量，我们可以使用以下查询：1234567891011GET sports/_search&#123; &quot;size&quot;: 0, &quot;aggs&quot;: &#123; &quot;sport_count&quot;: &#123; &quot;value_count&quot;: &#123; &quot;field&quot;: &quot;sport&quot; &#125; &#125; &#125;&#125;显示结果：1234567891011121314151617181920212223&#123; &quot;took&quot; : 2, &quot;timed_out&quot; : false, &quot;_shards&quot; : &#123; &quot;total&quot; : 1, &quot;successful&quot; : 1, &quot;skipped&quot; : 0, &quot;failed&quot; : 0 &#125;, &quot;hits&quot; : &#123; &quot;total&quot; : &#123; &quot;value&quot; : 22, &quot;relation&quot; : &quot;eq&quot; &#125;, &quot;max_score&quot; : null, &quot;hits&quot; : [ ] &#125;, &quot;aggregations&quot; : &#123; &quot;sport_count&quot; : &#123; &quot;value&quot; : 22 &#125; &#125;&#125;请注意，这将返回该字段的值总数，而不是唯一值的数目。 因此，在这种情况下（由于每个文档在“ sport”字段中都有一个单词值），结果仅等于索引中的文档数。Bucket Aggregations存储桶聚合是用于对文档进行分组的机制。 每种类型的存储桶聚合都有自己的分割文档集的方法。 也许最简单的类型是术语聚合。 这个功能非常像术语方面，返回给定字段索引的唯一术语以及匹配文档的数量。 如果我们想在数据集中的“sport”字段中找到所有值，则可以使用以下方法：123456789101112GET sports/_search&#123; &quot;size&quot;: 0, &quot;aggs&quot;: &#123; &quot;sport&quot;: &#123; &quot;terms&quot;: &#123; &quot;field&quot;: &quot;sport&quot;, &quot;size&quot;: 10 &#125; &#125; &#125;&#125;返回值：12345678910111213141516171819202122232425262728293031323334353637383940414243444546&#123; &quot;took&quot; : 0, &quot;timed_out&quot; : false, &quot;_shards&quot; : &#123; &quot;total&quot; : 1, &quot;successful&quot; : 1, &quot;skipped&quot; : 0, &quot;failed&quot; : 0 &#125;, &quot;hits&quot; : &#123; &quot;total&quot; : &#123; &quot;value&quot; : 22, &quot;relation&quot; : &quot;eq&quot; &#125;, &quot;max_score&quot; : null, &quot;hits&quot; : [ ] &#125;, &quot;aggregations&quot; : &#123; &quot;sport&quot; : &#123; &quot;doc_count_error_upper_bound&quot; : 0, &quot;sum_other_doc_count&quot; : 0, &quot;buckets&quot; : [ &#123; &quot;key&quot; : &quot;Baseball&quot;, &quot;doc_count&quot; : 16 &#125;, &#123; &quot;key&quot; : &quot;Football&quot;, &quot;doc_count&quot; : 2 &#125;, &#123; &quot;key&quot; : &quot;Golf&quot;, &quot;doc_count&quot; : 2 &#125;, &#123; &quot;key&quot; : &quot;Basketball&quot;, &quot;doc_count&quot; : 1 &#125;, &#123; &quot;key&quot; : &quot;Hockey&quot;, &quot;doc_count&quot; : 1 &#125; ] &#125; &#125;&#125;您可能会发现geo_distance聚合更具吸引力。 尽管它有许多选项，但在最简单的情况下，它取一个原点和一个距离范围，然后根据给定的geo_point字段计算圆中有多少文档。假设我们需要知道多少个运动员居住在距离地理位置“ 46.12，-68.55” 20英里范围内。 我们可以使用以下聚合：12345678910111213141516171819GET sports/_search&#123; &quot;size&quot;: 0, &quot;aggregations&quot;: &#123; &quot;baseball_player_ring&quot;: &#123; &quot;geo_distance&quot;: &#123; &quot;field&quot;: &quot;location&quot;, &quot;origin&quot;: &quot;46.12,-68.55&quot;, &quot;unit&quot;: &quot;mi&quot;, &quot;ranges&quot;: [ &#123; &quot;from&quot;: 0, &quot;to&quot;: 20 &#125; ] &#125; &#125; &#125;&#125;返回结果：123456789101112131415161718192021222324252627282930&#123; &quot;took&quot; : 4, &quot;timed_out&quot; : false, &quot;_shards&quot; : &#123; &quot;total&quot; : 1, &quot;successful&quot; : 1, &quot;skipped&quot; : 0, &quot;failed&quot; : 0 &#125;, &quot;hits&quot; : &#123; &quot;total&quot; : &#123; &quot;value&quot; : 22, &quot;relation&quot; : &quot;eq&quot; &#125;, &quot;max_score&quot; : null, &quot;hits&quot; : [ ] &#125;, &quot;aggregations&quot; : &#123; &quot;baseball_player_ring&quot; : &#123; &quot;buckets&quot; : [ &#123; &quot;key&quot; : &quot;*-20.0&quot;, &quot;from&quot; : 0.0, &quot;to&quot; : 20.0, &quot;doc_count&quot; : 14 &#125; ] &#125; &#125;&#125;内嵌 Bucket Aggregations许多开发人员会同意，桶聚合的最强大方面是嵌套它们的能力。 您可以定义顶级存储桶聚合，并在其内部定义对每个结果存储桶进行操作的第二级聚合。 此嵌套可以根据需要扩展到多个级别。继续我们的示例，我们可以使用按年龄划分的嵌套范围聚合（根据脚本的“出生日期”计算得出）来进一步细分geo_distance聚合的结果。 假设我们想知道属于两个年龄段的每个运动员中有多少运动员（他们生活在上一节中定义的圈子内）。 我们可以使用以下聚合来获取此信息：12345678910111213141516171819202122232425262728293031323334353637383940GET sports/_search&#123; &quot;size&quot;: 0, &quot;aggregations&quot;: &#123; &quot;baseball_player_ring&quot;: &#123; &quot;geo_distance&quot;: &#123; &quot;field&quot;: &quot;location&quot;, &quot;origin&quot;: &quot;46.12,-68.55&quot;, &quot;unit&quot;: &quot;mi&quot;, &quot;ranges&quot;: [ &#123; &quot;from&quot;: 0, &quot;to&quot;: 20 &#125; ] &#125;, &quot;aggregations&quot;: &#123; &quot;ring_age_ranges&quot;: &#123; &quot;range&quot;: &#123; &quot;script&quot;: &#123; &quot;source&quot;: &quot;&quot;&quot; ZonedDateTime dob = doc[&apos;birthdate&apos;].value; return params.now - dob.getYear() &quot;&quot;&quot; , &quot;params&quot;: &#123; &quot;now&quot;: 2019 &#125; &#125;, &quot;ranges&quot;: [ &#123; &quot;from&quot;: 30, &quot;to&quot;: 31 &#125;, &#123; &quot;from&quot;: 31, &quot;to&quot;: 32 &#125; ] &#125; &#125; &#125; &#125; &#125;&#125;显示的结果为：12345678910111213141516171819202122232425262728293031323334353637383940414243444546&#123; &quot;took&quot; : 0, &quot;timed_out&quot; : false, &quot;_shards&quot; : &#123; &quot;total&quot; : 1, &quot;successful&quot; : 1, &quot;skipped&quot; : 0, &quot;failed&quot; : 0 &#125;, &quot;hits&quot; : &#123; &quot;total&quot; : &#123; &quot;value&quot; : 22, &quot;relation&quot; : &quot;eq&quot; &#125;, &quot;max_score&quot; : null, &quot;hits&quot; : [ ] &#125;, &quot;aggregations&quot; : &#123; &quot;baseball_player_ring&quot; : &#123; &quot;buckets&quot; : [ &#123; &quot;key&quot; : &quot;*-20.0&quot;, &quot;from&quot; : 0.0, &quot;to&quot; : 20.0, &quot;doc_count&quot; : 14, &quot;ring_age_ranges&quot; : &#123; &quot;buckets&quot; : [ &#123; &quot;key&quot; : &quot;30.0-31.0&quot;, &quot;from&quot; : 30.0, &quot;to&quot; : 31.0, &quot;doc_count&quot; : 2 &#125;, &#123; &quot;key&quot; : &quot;31.0-32.0&quot;, &quot;from&quot; : 31.0, &quot;to&quot; : 32.0, &quot;doc_count&quot; : 8 &#125; ] &#125; &#125; ] &#125; &#125;&#125;现在，让我们使用stats（多值指标汇总器）来计算最内部结果的一些统计数据。 对于居住在我们圈子中的运动员以及两个年龄段的每个年龄段，我们现在都希望根据结果文档计算“rating”字段的统计信息：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647GET sports/_search&#123; &quot;size&quot;: 0, &quot;aggregations&quot;: &#123; &quot;baseball_player_ring&quot;: &#123; &quot;geo_distance&quot;: &#123; &quot;field&quot;: &quot;location&quot;, &quot;origin&quot;: &quot;46.12,-68.55&quot;, &quot;unit&quot;: &quot;mi&quot;, &quot;ranges&quot;: [ &#123; &quot;from&quot;: 0, &quot;to&quot;: 20 &#125; ] &#125;, &quot;aggregations&quot;: &#123; &quot;ring_age_ranges&quot;: &#123; &quot;range&quot;: &#123; &quot;script&quot;: &#123; &quot;source&quot;: &quot;&quot;&quot; ZonedDateTime dob = doc[&apos;birthdate&apos;].value; return params.now - dob.getYear() &quot;&quot;&quot; , &quot;params&quot;: &#123; &quot;now&quot;: 2019 &#125; &#125;, &quot;ranges&quot;: [ &#123; &quot;from&quot;: 30, &quot;to&quot;: 31 &#125;, &#123; &quot;from&quot;: 31, &quot;to&quot;: 32 &#125; ] &#125;, &quot;aggregations&quot;: &#123; &quot;rating_stats&quot;: &#123; &quot;stats&quot;: &#123; &quot;field&quot;: &quot;rating&quot; &#125; &#125; &#125; &#125; &#125; &#125; &#125;&#125;我们得到一个我们需要的统计信息的响应：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960&#123; &quot;took&quot; : 0, &quot;timed_out&quot; : false, &quot;_shards&quot; : &#123; &quot;total&quot; : 1, &quot;successful&quot; : 1, &quot;skipped&quot; : 0, &quot;failed&quot; : 0 &#125;, &quot;hits&quot; : &#123; &quot;total&quot; : &#123; &quot;value&quot; : 22, &quot;relation&quot; : &quot;eq&quot; &#125;, &quot;max_score&quot; : null, &quot;hits&quot; : [ ] &#125;, &quot;aggregations&quot; : &#123; &quot;baseball_player_ring&quot; : &#123; &quot;buckets&quot; : [ &#123; &quot;key&quot; : &quot;*-20.0&quot;, &quot;from&quot; : 0.0, &quot;to&quot; : 20.0, &quot;doc_count&quot; : 14, &quot;ring_age_ranges&quot; : &#123; &quot;buckets&quot; : [ &#123; &quot;key&quot; : &quot;30.0-31.0&quot;, &quot;from&quot; : 30.0, &quot;to&quot; : 31.0, &quot;doc_count&quot; : 2, &quot;rating_stats&quot; : &#123; &quot;count&quot; : 4, &quot;min&quot; : 3.0, &quot;max&quot; : 5.0, &quot;avg&quot; : 4.0, &quot;sum&quot; : 16.0 &#125; &#125;, &#123; &quot;key&quot; : &quot;31.0-32.0&quot;, &quot;from&quot; : 31.0, &quot;to&quot; : 32.0, &quot;doc_count&quot; : 8, &quot;rating_stats&quot; : &#123; &quot;count&quot; : 16, &quot;min&quot; : 2.0, &quot;max&quot; : 10.0, &quot;avg&quot; : 7.5, &quot;sum&quot; : 120.0 &#125; &#125; ] &#125; &#125; ] &#125; &#125;&#125;如您所见，您可以创建一个包含多个存储更多存储桶的大存储桶。 您还可以获取每个存储分区的指标（metrics)，以及不断提高的复杂性。 通过这些简单的构建块，您可以使用嵌套聚合从数据中获得深刻而复杂的见解。]]></content>
      <categories>
        <category>elk</category>
      </categories>
      <tags>
        <tag>elk</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch： Join数据类型]]></title>
    <url>%2F2020%2F01%2F02%2FElasticsearch%EF%BC%9A%20Join%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[在Elasticsearch中，Join可以让我们创建parent/child关系。Elasticsearch不是一个RDMS。通常join数据类型尽量不要使用，除非不得已。那么Elasticsearch为什么需要Join数据类型呢？在Elasticsearch中，更新一个object需要root object一个完整的reindex：即使是一个field的一个字符的改变即便是nested object也需要完整的reindex才可以实现搜索通常情况下，这是完全OK的，但是在有些场合下，如果我们有频繁的更新操作，这样可能对性能带来很大的影响。如果你的数据需要频繁的更新，并带来性能上的影响，这个时候，join数据类型可能是你的一个解决方案。join数据类型可以完全地把两个object分开，但是还是保持这两者之前的关系。parent及child是完全分开的两个文档parent可以单独更新而不需要重新reindex childchildren可以任意被添加/串改/删除而不影响parent及其它的children与 nested类型类似，父子关系也允许您将不同的实体关联在一起，但它们在实现和行为上有所不同。 与nested文档不同，它们不在同一文档中，而parent/child文档是完全独立的文档。 它们遵循一对多关系原则，允许您将一种类型定义为parent类型，将一种或多种类型定义为child类型即便join数据类型给我们带来了方便，但是，它也在搜索时给我带来额外的内存及计算方便的开销。注意：目前Kibana对nested及join数据类型有比较少的支持。如果你想使用Kibana来在dashboard里展示数据，这个方面的你需要考虑。在未来，这种情况可能会发生改变。join数据类型是一个特殊字段，用于在同一索引的文档中创建父/子关系。 关系部分定义文档中的一组可能关系，每个关系是父（parent)名称和子（child)名称。一个例子：12345678910111213PUT my_index&#123; &quot;mappings&quot;: &#123; &quot;properties&quot;: &#123; &quot;my_join_field&quot;: &#123; &quot;type&quot;: &quot;join&quot;, &quot;relations&quot;: &#123; &quot;question&quot;: &quot;answer&quot; &#125; &#125; &#125; &#125;&#125;在这里我们定义了一个叫做my_index的索引。在这个索引中，我们定义了一个field，它的名字是my_join_field。它的类型是join数据类型。同时我们定义了单个关系：question是answer的parent。要使用join来index文档，必须在source中提供关系的name和文档的可选parent。 例如，以下示例在question上下文中创建两个parent文档：123456789101112131415PUT my_index/_doc/1?refresh&#123; &quot;text&quot;: &quot;This is a question&quot;, &quot;my_join_field&quot;: &#123; &quot;name&quot;: &quot;question&quot; &#125;&#125; PUT my_index/_doc/2?refresh&#123; &quot;text&quot;: &quot;This is another question&quot;, &quot;my_join_field&quot;: &#123; &quot;name&quot;: &quot;question&quot; &#125;&#125;这里采用refresh来强制进行索引，以便接下来的搜索。在这里name标识question，说明这个文档时一个question文档。索引parent文档时，您可以选择仅将关系的名称指定为快捷方式，而不是将其封装在普通对象表示法中：1234567891011PUT my_index/_doc/1?refresh&#123; &quot;text&quot;: &quot;This is a question&quot;, &quot;my_join_field&quot;: &quot;question&quot; &#125; PUT my_index/_doc/2?refresh&#123; &quot;text&quot;: &quot;This is another question&quot;, &quot;my_join_field&quot;: &quot;question&quot;&#125;这种方法和前面的是一样的，只是这里我们只使用了question, 而不是一个像第一种方法那样，使用如下的一个对象来表达：123&quot;my_join_field&quot;: &#123; &quot;name&quot;: &quot;question&quot; &#125;在实际的使用中，你可以根据自己的喜好来使用。索引child项时，必须在_source中添加关系的名称以及文档的parent id。注意：需要在同一分片中索引父级的谱系，必须使用其parent的id来确保这个child和parent是在一个shard中。每个文档分配在那个shard之中在默认的情况下是按照文档的id进行一些hash来分配的，当然也可以通过routing来进行。针对child，我们使用其parent的id，这样就可以保证。否则在我们join数据的时候，跨shard是非常大的一个消费。例如，以下示例显示如何索引两个child文档：1234567891011121314151617PUT my_index/_doc/3?routing=1?refresh (1)&#123; &quot;text&quot;: &quot;This is an answer&quot;, &quot;my_join_field&quot;: &#123; &quot;name&quot;: &quot;answer&quot;, (2) &quot;parent&quot;: &quot;1&quot; (3) &#125;&#125; PUT my_index/_doc/4?routing=1?refresh&#123; &quot;text&quot;: &quot;This is another answer&quot;, &quot;my_join_field&quot;: &#123; &quot;name&quot;: &quot;answer&quot;, &quot;parent&quot;: &quot;1&quot; &#125;&#125;在上面的（1）处，我们必须使用routing，这样能确保parent和child是在同一个shard里。我们这里routing为1，这是因为parent的id 为1，在（3）处定义。(2) 处定义了该文档join的名称。parent-join及其性能join字段不应像关系数据库中的连接一样使用。 在Elasticsearch中，良好性能的关键是将数据去规范化为文档。 每个连接字段has_child或has_parent查询都会对查询性能产生重大影响。join字段有意义的唯一情况是，如果您的数据包含一对多关系，其中一个实体明显超过另一个实体。 这种情况的一个例子是产品的用例和这些产品的报价。 如果提供的产品数量明显多于产品数量，则将产品建模为父文档并将产品建模为子文档是有意义的。parent-join的限制对于每个index来说，只能有一个join字段parent及child文档，必须是在一个shard里建立索引。这也意味着，同样的routing值必须应用于getting, deleting或updating一个child文档。一个元素可以有多个children，但是只能有一个parent.可以对已有的join项添加新的关系也可以将child添加到现有元素，但仅当元素已经是parent时才可以。针对parent-join的搜索parent-join创建一个字段来索引文档中关系的名称（my_parent，my_child，…）。它还为每个parent/child关系创建一个字段。 此字段的名称是join字段的名称，后跟＃和关系中parent的名称。 因此，例如对于my_parent⇒[my_child，another_child]关系，join字段会创建一个名为my_join_field＃my_parent的附加字段。如果文档是子文件（my_child或another_child），则此字段包含文档链接到的parent_id，如果文档是parent文件（my_parent），则包含文档的_id。搜索包含join字段的索引时，始终在搜索响应中返回这两个字段：上面的描述比较绕口，我们还是以一个例子来说说明吧：1234567GET my_index/_search&#123; &quot;query&quot;: &#123; &quot;match_all&quot;: &#123;&#125; &#125;, &quot;sort&quot;: [&quot;_id&quot;]&#125;这里我们搜索所有的文档，并以_id进行排序：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879&#123; &quot;took&quot; : 2, &quot;timed_out&quot; : false, &quot;_shards&quot; : &#123; &quot;total&quot; : 1, &quot;successful&quot; : 1, &quot;skipped&quot; : 0, &quot;failed&quot; : 0 &#125;, &quot;hits&quot; : &#123; &quot;total&quot; : &#123; &quot;value&quot; : 4, &quot;relation&quot; : &quot;eq&quot; &#125;, &quot;max_score&quot; : null, &quot;hits&quot; : [ &#123; &quot;_index&quot; : &quot;my_index&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;1&quot;, &quot;_score&quot; : null, &quot;_source&quot; : &#123; &quot;text&quot; : &quot;This is a question&quot;, &quot;my_join_field&quot; : &quot;question&quot; (1) &#125;, &quot;sort&quot; : [ &quot;1&quot; ] &#125;, &#123; &quot;_index&quot; : &quot;my_index&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;2&quot;, &quot;_score&quot; : null, &quot;_source&quot; : &#123; &quot;text&quot; : &quot;This is another question&quot;, &quot;my_join_field&quot; : &quot;question&quot; (2) &#125;, &quot;sort&quot; : [ &quot;2&quot; ] &#125;, &#123; &quot;_index&quot; : &quot;my_index&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;3&quot;, &quot;_score&quot; : null, &quot;_routing&quot; : &quot;1&quot;, &quot;_source&quot; : &#123; &quot;text&quot; : &quot;This is an answer&quot;, &quot;my_join_field&quot; : &#123; &quot;name&quot; : &quot;answer&quot;, (3) &quot;parent&quot; : &quot;1&quot; (4) &#125; &#125;, &quot;sort&quot; : [ &quot;3&quot; ] &#125;, &#123; &quot;_index&quot; : &quot;my_index&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;4&quot;, &quot;_score&quot; : null, &quot;_routing&quot; : &quot;1&quot;, &quot;_source&quot; : &#123; &quot;text&quot; : &quot;This is another answer&quot;, &quot;my_join_field&quot; : &#123; &quot;name&quot; : &quot;answer&quot;, &quot;parent&quot; : &quot;1&quot; &#125; &#125;, &quot;sort&quot; : [ &quot;4&quot; ] &#125; ] &#125;&#125;在这里，我们可以看到4个文档：(1)表明这个文档是一个question join(2)表明这个文档是一个question join(3)表明这个文档是一个answer join(4)表明这个文档的parent是id为1的文档Parent-join 查询及aggregation可以在aggregation和script中访问join字段的值，并可以使用parent_id查询进行查询：123456789GET my_index/_search&#123; &quot;query&quot;: &#123; &quot;parent_id&quot;: &#123; &quot;type&quot;: &quot;answer&quot;, &quot;id&quot;: &quot;1&quot; &#125; &#125;&#125;我们通过查询parent_id，返回所有parent_id为1的所有answer类型的文档：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647&#123; &quot;took&quot; : 0, &quot;timed_out&quot; : false, &quot;_shards&quot; : &#123; &quot;total&quot; : 1, &quot;successful&quot; : 1, &quot;skipped&quot; : 0, &quot;failed&quot; : 0 &#125;, &quot;hits&quot; : &#123; &quot;total&quot; : &#123; &quot;value&quot; : 2, &quot;relation&quot; : &quot;eq&quot; &#125;, &quot;max_score&quot; : 0.35667494, &quot;hits&quot; : [ &#123; &quot;_index&quot; : &quot;my_index&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;4&quot;, &quot;_score&quot; : 0.35667494, &quot;_routing&quot; : &quot;1&quot;, &quot;_source&quot; : &#123; &quot;text&quot; : &quot;This is another answer&quot;, &quot;my_join_field&quot; : &#123; &quot;name&quot; : &quot;answer&quot;, &quot;parent&quot; : &quot;1&quot; &#125; &#125; &#125;, &#123; &quot;_index&quot; : &quot;my_index&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;3&quot;, &quot;_score&quot; : 0.35667494, &quot;_routing&quot; : &quot;1&quot;, &quot;_source&quot; : &#123; &quot;text&quot; : &quot;This is an answer&quot;, &quot;my_join_field&quot; : &#123; &quot;name&quot; : &quot;answer&quot;, &quot;parent&quot; : &quot;1&quot; &#125; &#125; &#125; ] &#125;&#125;在这里，我们可以看到返回id为3和4的文档。我们也可以对这些文档进行aggregation:123456789101112131415161718192021222324GET my_index/_search&#123; &quot;query&quot;: &#123; &quot;parent_id&quot;: &#123; &quot;type&quot;: &quot;answer&quot;, &quot;id&quot;: &quot;1&quot; &#125; &#125;, &quot;aggs&quot;: &#123; &quot;parents&quot;: &#123; &quot;terms&quot;: &#123; &quot;field&quot;: &quot;my_join_field#question&quot;, &quot;size&quot;: 10 &#125; &#125; &#125;, &quot;script_fields&quot;: &#123; &quot;parent&quot;: &#123; &quot;script&quot;: &#123; &quot;source&quot;: &quot;doc[&apos;my_join_field#question&apos;]&quot; &#125; &#125; &#125;&#125;就像我们在上一节中介绍的那样， 在我们的应用实例中，在index时，它也创建一个额外的一个字段，虽然在source里我们看不到。这个字段就是my_join_filed#question，这个字段含有parent _id。在上面的查询中，我们首先查询所有的parent_id为1的所有的answer类型的文档。接下来对所有的文档以parent_id进行聚合：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455&#123; &quot;took&quot; : 0, &quot;timed_out&quot; : false, &quot;_shards&quot; : &#123; &quot;total&quot; : 1, &quot;successful&quot; : 1, &quot;skipped&quot; : 0, &quot;failed&quot; : 0 &#125;, &quot;hits&quot; : &#123; &quot;total&quot; : &#123; &quot;value&quot; : 2, &quot;relation&quot; : &quot;eq&quot; &#125;, &quot;max_score&quot; : 0.35667494, &quot;hits&quot; : [ &#123; &quot;_index&quot; : &quot;my_index&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;4&quot;, &quot;_score&quot; : 0.35667494, &quot;_routing&quot; : &quot;1&quot;, &quot;fields&quot; : &#123; &quot;parent&quot; : [ &quot;1&quot; ] &#125; &#125;, &#123; &quot;_index&quot; : &quot;my_index&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;3&quot;, &quot;_score&quot; : 0.35667494, &quot;_routing&quot; : &quot;1&quot;, &quot;fields&quot; : &#123; &quot;parent&quot; : [ &quot;1&quot; ] &#125; &#125; ] &#125;, &quot;aggregations&quot; : &#123; &quot;parents&quot; : &#123; &quot;doc_count_error_upper_bound&quot; : 0, &quot;sum_other_doc_count&quot; : 0, &quot;buckets&quot; : [ &#123; &quot;key&quot; : &quot;1&quot;, &quot;doc_count&quot; : 2 &#125; ] &#125; &#125;&#125;一个parent对应多个child对于一个parent来说，我们可以定义多个child，比如：12345678910111213PUT my_index&#123; &quot;mappings&quot;: &#123; &quot;properties&quot;: &#123; &quot;my_join_field&quot;: &#123; &quot;type&quot;: &quot;join&quot;, &quot;relations&quot;: &#123; &quot;question&quot;: [&quot;answer&quot;, &quot;comment&quot;] &#125; &#125; &#125; &#125;&#125;在这里，question是answer及comment的parent。多层的parent join虽然这个不建议，这样做可能会可能在query时带来更多的内存及计算方面的开销：1234567891011121314PUT my_index&#123; &quot;mappings&quot;: &#123; &quot;properties&quot;: &#123; &quot;my_join_field&quot;: &#123; &quot;type&quot;: &quot;join&quot;, &quot;relations&quot;: &#123; &quot;question&quot;: [&quot;answer&quot;, &quot;comment&quot;], &quot;answer&quot;: &quot;vote&quot; &#125; &#125; &#125; &#125;&#125;这里question是answer及comment的parent，同时answer也是vote的parent。它表明了如下的关系：索引grandchild文档需routing值等于grand-parent（谱系里的更大parent）：12345678PUT my_index/_doc/3?routing=1&amp;refresh &#123; &quot;text&quot;: &quot;This is a vote&quot;, &quot;my_join_field&quot;: &#123; &quot;name&quot;: &quot;vote&quot;, &quot;parent&quot;: &quot;2&quot; &#125;&#125;这个child文档必须是和他的grand-parent在一个shard里。在这里它使用了1，也即question的id。同时，对于vote来说，它的parent必须是它的parent，也即answer的id。更多参考：https://www.elastic.co/guide/en/elasticsearch/reference/7.3/parent-join.html]]></content>
      <categories>
        <category>elk</category>
      </categories>
      <tags>
        <tag>elk</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch 使用URI Search]]></title>
    <url>%2F2020%2F01%2F02%2FElasticsearch%20%E4%BD%BF%E7%94%A8URI%20Search%2F</url>
    <content type="text"><![CDATA[在Elasticsearch中，我们可以使用_search终端进行搜索。这个在我之前的文章 “开始使用Elasticsearch （2）” 中有很多的描述。针对这种搜索，我们可以使用强大的DSL进行搜索。在Elasticsearch中，还有一类是基于URI的搜索。对于这种它可以很方便地直接在浏览器中的地址栏或命令行中直接使用。 使用此模式执行搜索时，并非所有搜索选项都公开，但是对于快速的“curl tests”来说，它可能很方便。在今天的文章中，我们来做一个简单的描述。同时我需要指出来的是，这里的语法和Kibana中的Search Bar搜索语法是一样的。安装Elastic Stack准备好数据为了说明问题的方便，我们首先在Kibana中使用如下的bulk指令来创建我们的twitter索引。12345678910111213POST _bulk&#123; &quot;index&quot; : &#123; &quot;_index&quot; : &quot;twitter&quot;, &quot;_id&quot;: 1&#125; &#125;&#123;&quot;user&quot;:&quot;张三&quot;,&quot;message&quot;:&quot;今儿天气不错啊，出去转转去&quot;,&quot;uid&quot;:2,&quot;age&quot;:20,&quot;city&quot;:&quot;北京&quot;,&quot;province&quot;:&quot;北京&quot;,&quot;country&quot;:&quot;中国&quot;,&quot;address&quot;:&quot;中国北京市海淀区&quot;,&quot;location&quot;:&#123;&quot;lat&quot;:&quot;39.970718&quot;,&quot;lon&quot;:&quot;116.325747&quot;&#125;, &quot;DOB&quot;:&quot;1980-12-01&quot;&#125;&#123; &quot;index&quot; : &#123; &quot;_index&quot; : &quot;twitter&quot;, &quot;_id&quot;: 2 &#125;&#125;&#123;&quot;user&quot;:&quot;老刘&quot;,&quot;message&quot;:&quot;出发，下一站云南！&quot;,&quot;uid&quot;:3,&quot;age&quot;:30,&quot;city&quot;:&quot;北京&quot;,&quot;province&quot;:&quot;北京&quot;,&quot;country&quot;:&quot;中国&quot;,&quot;address&quot;:&quot;中国北京市东城区台基厂三条3号&quot;,&quot;location&quot;:&#123;&quot;lat&quot;:&quot;39.904313&quot;,&quot;lon&quot;:&quot;116.412754&quot;&#125;, &quot;DOB&quot;:&quot;1981-12-01&quot;&#125;&#123; &quot;index&quot; : &#123; &quot;_index&quot; : &quot;twitter&quot;, &quot;_id&quot;: 3&#125; &#125;&#123;&quot;user&quot;:&quot;李四&quot;,&quot;message&quot;:&quot;happy birthday!&quot;,&quot;uid&quot;:4,&quot;age&quot;:30,&quot;city&quot;:&quot;北京&quot;,&quot;province&quot;:&quot;北京&quot;,&quot;country&quot;:&quot;中国&quot;,&quot;address&quot;:&quot;中国北京市东城区&quot;,&quot;location&quot;:&#123;&quot;lat&quot;:&quot;39.893801&quot;,&quot;lon&quot;:&quot;116.408986&quot;&#125;, &quot;DOB&quot;:&quot;1982-12-01&quot;&#125;&#123; &quot;index&quot; : &#123; &quot;_index&quot; : &quot;twitter&quot;, &quot;_id&quot;: 4&#125; &#125;&#123;&quot;user&quot;:&quot;老贾&quot;,&quot;message&quot;:&quot;123,gogogo&quot;,&quot;uid&quot;:5,&quot;age&quot;:35,&quot;city&quot;:&quot;北京&quot;,&quot;province&quot;:&quot;北京&quot;,&quot;country&quot;:&quot;中国&quot;,&quot;address&quot;:&quot;中国北京市朝阳区建国门&quot;,&quot;location&quot;:&#123;&quot;lat&quot;:&quot;39.718256&quot;,&quot;lon&quot;:&quot;116.367910&quot;&#125;, &quot;DOB&quot;:&quot;1983-12-01&quot;&#125;&#123; &quot;index&quot; : &#123; &quot;_index&quot; : &quot;twitter&quot;, &quot;_id&quot;: 5&#125; &#125;&#123;&quot;user&quot;:&quot;老王&quot;,&quot;message&quot;:&quot;Happy BirthDay My Friend!&quot;,&quot;uid&quot;:6,&quot;age&quot;:50,&quot;city&quot;:&quot;北京&quot;,&quot;province&quot;:&quot;北京&quot;,&quot;country&quot;:&quot;中国&quot;,&quot;address&quot;:&quot;中国北京市朝阳区国贸&quot;,&quot;location&quot;:&#123;&quot;lat&quot;:&quot;39.918256&quot;,&quot;lon&quot;:&quot;116.467910&quot;&#125;, &quot;DOB&quot;:&quot;1984-12-01&quot;&#125;&#123; &quot;index&quot; : &#123; &quot;_index&quot; : &quot;twitter&quot;, &quot;_id&quot;: 6&#125; &#125;&#123;&quot;user&quot;:&quot;老吴&quot;,&quot;message&quot;:&quot;好友来了都今天我生日，好友来了,什么 birthday happy 就成!&quot;,&quot;uid&quot;:7,&quot;age&quot;:90,&quot;city&quot;:&quot;上海&quot;,&quot;province&quot;:&quot;上海&quot;,&quot;country&quot;:&quot;中国&quot;,&quot;address&quot;:&quot;中国上海市闵行区&quot;,&quot;location&quot;:&#123;&quot;lat&quot;:&quot;31.175927&quot;,&quot;lon&quot;:&quot;121.383328&quot;&#125;, &quot;DOB&quot;:&quot;1985-12-01&quot;&#125;这里总共有6条数据。下面，我们来进行一些查询的动作。搜索数据首先，我们做一个简单的搜索，我们可以在浏览器中打入如下的命令：1GET twitter/_search?q=user:张三我们通过 “q=user:张三” 查询到我们所需要的文档。在有的时候这是一种非常快的查询方式。我们也可以在浏览器中直接打入一个这样的URI:1http://localhost:9200/_search?q=user:%E5%BC%A0%E4%B8%89&amp;pretty或者在命令行中：下面，我们将使用Kibana来展示使用URI搜索的一些最基本的特点。URI查询使用语法根据运算符（例如OR，AND或NOT）解析和拆分提供的查询字符串我们想使用sort来对数据进行排序：1GET twitter/_search?q=city:&quot;北京&quot;&amp;sort=DOB:desc上面显示了所有来自北京的用户，并按照出生年月降序排列。假如我们只想在_source里显示年龄，DOB及城市信息，我们可以这么做：1GET twitter/_search?q=city:&quot;北京&quot;&amp;sort=DOB:desc&amp;_source=city,age,DOB从上面的显示可以看出来，我们只看到有三个字段显示出来。加入我们想分页，每个页只有2个文档，那么我们可以这么做：1GET twitter/_search?q=city:&quot;北京&quot;&amp;sort=DOB:desc&amp;_source=city,age,DOB&amp;size=2从上面的显示上我们可以看出来，只有两个文档被显示出来尽管总共有5个文档满足条件。假如这个时候，我们想对city为“上海”和“北京”的所有用户都来统计一下，那么我们可以使用如下的语句：1GET twitter/_search?q=city:(&quot;北京&quot; or &quot;上海&quot;) &amp;sort=DOB:desc&amp;_source=city,age,DOB&amp;size=2显然这个时候，我们得到了6条数据。上海和北京的所有用户都被搜索出来了。假如我们想查询来自“北京”并且名字叫做“张三”的文档，那么我们可以这么查询：1GET twitter/_search?q=city:&quot;北京&quot; AND user:&quot;张三从上面可以看出来就只有一条数据。假如我们想得到来除了上海以外地区的所有的用户，那么我们可以使用如下的方法来得到：1GET twitter/_search?q=NOT city:&quot;上海&quot;我们看到了5个数据。我们也可以对某些想进行加权，以使得它们能够排在更前面，比如：上面的查询是寻找年龄是20岁的，或者是来自上海的人。从搜索的结果来看，我们可以看到上海的老吴是排在前面。如果我们想对年龄为20岁的人需要有更多的关注，那么我们可以对它们的搜索结果进行加权，这样会使得它们的分数更高。我们可以采用如下的方法来做：1GET twitter/_search?q=(age:20^5 OR city:&quot;上海&quot;)在上面，我们显然对age为20的这个选项进行了加权。那么搜索后的结果为：我们可以看到现在age为20岁的张三排到了搜索结果的前面。假如我们不指定任何的field的话，那么这个搜索将对所有的field都进行：1GET twitter/_search?q=张三当然我们也可以进行fuzzy搜索：上面标明有一个edit错误也可以被搜索出来。对于中文的检索，这个依赖于分词器。在我们的实验中没有使用具体的分词器。这个和实际的使用可能会有区别。我们也可以对一下范围进行搜索：1GET twitter/_search?q=age:[20 TO 30]上面搜索的结果是从20岁到30岁的所有的结果，并且都包含在里面。我们如果不想包含30岁的话，那么可以写成这样的格式：我们使用[20 TO 30}， 如果我们想搜索在30岁一下的所有文档，那么我们可以使用如下的搜索方式：在这里，我们使用[* TO 30}，这里不包含30。好了今天就讲到这里。这里的所有的语法也适用于在Kibana中的Search Bar。如果我们熟练地掌握了这些，也可以很方便地让我们熟练地操作Kibana中搜索。参考：【1】https://www.elastic.co/guide/en/elasticsearch/reference/current/query-filter-context.html【2】https://www.elastic.co/guide/en/elasticsearch/reference/7.4/search-uri-request.html]]></content>
      <categories>
        <category>elk</category>
      </categories>
      <tags>
        <tag>elk</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch rollover API]]></title>
    <url>%2F2020%2F01%2F02%2FElasticsearch%20rollover%20API%2F</url>
    <content type="text"><![CDATA[rollover使您可以根据索引大小，文档数或使用期限自动过渡到新索引。 当rollover触发后，将创建新索引，写别名（write alias)将更新为指向新索引，所有后续更新都将写入新索引。对于基于时间的rollover来说，基于大小，文档数或使用期限过渡至新索引是比较适合的。 在任意时间rollover通常会导致许多小的索引，这可能会对性能和资源使用产生负面影响。Rollover历史数据在大多数情况下，无限期保留历史数据是不可行的时间序列数据随着时间的流逝而失去价值，我们最终不得不将其删除但是其中一些数据对于分析仍然非常有用Elasticsearch 6.3引入了一项新的rollover功能，该功能以紧凑的聚合格式保存旧数据仅保存您感兴趣的数据就像上面的图片看到的那样，我们定义了一个叫做logs-alias的alias，对于写操作来说，它总是会自动指向最新的可以用于写入index的一个索引。针对我们上面的情况，它指向logs-000002。如果新的rollover发生后，新的logs-000003将被生成，并对于写操作来说，它自动指向最新生产的logs-000003索引。而对于读写操作来说，它将同时指向最先的logs-1，logs-000002及logs-000003。在这里我们需要注意的是：在我们最早设定index名字时，最后的一个字符必须是数字，比如我们上面显示的logs-1。否则，自动生产index将会失败。rollover例子我们还是先拿一个rollover的例子来说明，这样比较清楚。首先我们定义一个log-alias的alias:12345678PUT /%3Clogs-%7Bnow%2Fd%7D-1%3E&#123; &quot;aliases&quot;: &#123; &quot;log_alias&quot;: &#123; &quot;is_write_index&quot;: true &#125; &#125;&#125;如果大家对于上面的字符串“%3Clogs-%7Bnow%2Fd%7D-1%3E”比较陌生的话，可以参考网站https://www.urlencoder.io/。实际上它就是字符串“&lt;logs-{now/d}-1&gt;”的url编码形式。请注意上面的is_write_index必须设置为true。运行上面的结果是：12345&#123; &quot;acknowledged&quot; : true, &quot;shards_acknowledged&quot; : true, &quot;index&quot; : &quot;logs-2019.10.21-1&quot;&#125;显然，它帮我们生产了一个叫做logs-2019.10.21-1的index。接下来，我们先使用我们的Kibana来准备一下我们的index数据。我们运行起来我们的Kibana:我们分别点击上面的1和2处：点击上面的“Add data”。这样我们就可以把我们的kibana_sample_data_logs索引加载到Elasticsearch中。我们可以通过如下的命令进行查看：1GET _cat/indices/kibana_sample_data_logs命令显示结果为：它显示kibana_sample_data_logs具有11.1M的数据，并且它有14074个文档：我们接下来运行如下的命令：123456789POST _reindex&#123; &quot;source&quot;: &#123; &quot;index&quot;: &quot;kibana_sample_data_logs&quot; &#125;, &quot;dest&quot;: &#123; &quot;index&quot;: &quot;log_alias&quot; &#125;&#125;这个命令的作用是把kibana_sample_data_logs里的数据reindex到log_alias所指向的index。也就是把kibana_sample_data_logs的文档复制一份到我们上面显示的logs-2019.10.21-1索引里。我们做如下的操作查看一下结果：1GET logs-2019.10.21-1/_count显示的结果是：123456789&#123; &quot;count&quot; : 14074, &quot;_shards&quot; : &#123; &quot;total&quot; : 1, &quot;successful&quot; : 1, &quot;skipped&quot; : 0, &quot;failed&quot; : 0 &#125;&#125;显然，我们已经复制到所有的数据。那么接下来，我们来运行如下的一个指令：12345678POST /log_alias/_rollover?dry_run&#123; &quot;conditions&quot;: &#123; &quot;max_age&quot;: &quot;7d&quot;, &quot;max_docs&quot;: 14000, &quot;max_size&quot;: &quot;5gb&quot; &#125;&#125;在这里，我们定义了三个条件：如果时间超过7天，那么自动rollover，也就是使用新的index如果文档的数目超过14000个，那么自动rollover如果index的大小超过5G，那么自动rollover在上面我们使用了dry_run参数，表明就是运行时看看，但不是真正地实施。显示的结果是：12345678910111213&#123; &quot;acknowledged&quot; : false, &quot;shards_acknowledged&quot; : false, &quot;old_index&quot; : &quot;logs-2019.10.21-1&quot;, &quot;new_index&quot; : &quot;logs-2019.10.21-000002&quot;, &quot;rolled_over&quot; : false, &quot;dry_run&quot; : true, &quot;conditions&quot; : &#123; &quot;[max_docs: 1400]&quot; : true, &quot;[max_size: 5gb]&quot; : false, &quot;[max_age: 7d]&quot; : false &#125;&#125;根据目前我们的条件，我们的logs-2019.10.21-1文档数已经超过14000个了，所以会生产新的索引logs-2019.10.21-000002。因为我使用了dry_run，也就是演习，所以显示的rolled_over是false。为了能真正地rollover，我们运行如下的命令：12345678POST /log_alias/_rollover&#123; &quot;conditions&quot;: &#123; &quot;max_age&quot;: &quot;7d&quot;, &quot;max_docs&quot;: 1400, &quot;max_size&quot;: &quot;5gb&quot; &#125;&#125;显示的结果是：12345678910111213&#123; &quot;acknowledged&quot; : true, &quot;shards_acknowledged&quot; : true, &quot;old_index&quot; : &quot;logs-2019.10.21-1&quot;, &quot;new_index&quot; : &quot;logs-2019.10.21-000002&quot;, &quot;rolled_over&quot; : true, &quot;dry_run&quot; : false, &quot;conditions&quot; : &#123; &quot;[max_docs: 1400]&quot; : true, &quot;[max_size: 5gb]&quot; : false, &quot;[max_age: 7d]&quot; : false &#125;&#125;说明它已经rolled_ovder了。我们可以通过如下写的命令来检查：1GET _cat/indices/logs-2019*显示的结果为：我们现在可以看到有两个以logs-2019.10.21为头的index，并且第二文档logs-2019.10.21-000002文档数为0。如果我们这个时候直接再想log_alias写入文档的话：1234567891011121314151617181920212223242526272829303132333435363738POST log_alias/_doc&#123; &quot;agent&quot;: &quot;Mozilla/5.0 (X11; Linux x86_64; rv:6.0a1) Gecko/20110421 Firefox/6.0a1&quot;, &quot;bytes&quot;: 6219, &quot;clientip&quot;: &quot;223.87.60.27&quot;, &quot;extension&quot;: &quot;deb&quot;, &quot;geo&quot;: &#123; &quot;srcdest&quot;: &quot;IN:US&quot;, &quot;src&quot;: &quot;IN&quot;, &quot;dest&quot;: &quot;US&quot;, &quot;coordinates&quot;: &#123; &quot;lat&quot;: 39.41042861, &quot;lon&quot;: -88.8454325 &#125; &#125;, &quot;host&quot;: &quot;artifacts.elastic.co&quot;, &quot;index&quot;: &quot;kibana_sample_data_logs&quot;, &quot;ip&quot;: &quot;223.87.60.27&quot;, &quot;machine&quot;: &#123; &quot;ram&quot;: 8589934592, &quot;os&quot;: &quot;win 8&quot; &#125;, &quot;memory&quot;: null, &quot;message&quot;: &quot;&quot;&quot; 223.87.60.27 - - [2018-07-22T00:39:02.912Z] &quot;GET /elasticsearch/elasticsearch-6.3.2.deb_1 HTTP/1.1&quot; 200 6219 &quot;-&quot; &quot;Mozilla/5.0 (X11; Linux x86_64; rv:6.0a1) Gecko/20110421 Firefox/6.0a1&quot; &quot;&quot;&quot;, &quot;phpmemory&quot;: null, &quot;referer&quot;: &quot;http://twitter.com/success/wendy-lawrence&quot;, &quot;request&quot;: &quot;/elasticsearch/elasticsearch-6.3.2.deb&quot;, &quot;response&quot;: 200, &quot;tags&quot;: [ &quot;success&quot;, &quot;info&quot; ], &quot;timestamp&quot;: &quot;2019-10-13T00:39:02.912Z&quot;, &quot;url&quot;: &quot;https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-6.3.2.deb_1&quot;, &quot;utc_time&quot;: &quot;2019-10-13T00:39:02.912Z&quot;&#125;显示的结果：1234567891011121314&#123; &quot;_index&quot; : &quot;logs-2019.10.21-000002&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;xPyQ7m0BsjOKp1OsjsP8&quot;, &quot;_version&quot; : 1, &quot;result&quot; : &quot;created&quot;, &quot;_shards&quot; : &#123; &quot;total&quot; : 1, &quot;successful&quot; : 1, &quot;failed&quot; : 0 &#125;, &quot;_seq_no&quot; : 1, &quot;_primary_term&quot; : 1&#125;显然它写入的是logs-2019.10.21-000002索引。我们再次查询log_alias的总共文档数：1GET log_alias/_count显示的结果是：123456789&#123; &quot;count&quot; : 14075, &quot;_shards&quot; : &#123; &quot;total&quot; : 2, &quot;successful&quot; : 2, &quot;skipped&quot; : 0, &quot;failed&quot; : 0 &#125;&#125;显然它和之前的14074个文档多增加了一个文档，也就是说log_alias是同时指向logs-2019.10.21-1及logs-2019.10.21-000002。总结：在今天的文档里，我们讲述了如何使用rollover API来自动管理我们的index。利用rollover API，它可以很方便地帮我们自动根据我们设定的条件帮我们把我们的Index过度到新的index。在未来的文章里，我们将讲述如何使用Index life cycle policy来帮我们管理我们的index。]]></content>
      <categories>
        <category>elk</category>
      </categories>
      <tags>
        <tag>elk</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch Reindex接口]]></title>
    <url>%2F2020%2F01%2F02%2FElasticsearch%20Reindex%E6%8E%A5%E5%8F%A3%2F</url>
    <content type="text"><![CDATA[在我们开发的过程中，我们有很多时候需要用到Reindex接口。它可以帮我们把数据从一个index到另外一个index进行重新reindex。这个对于特别适用于我们在修改我们数据的mapping后，需要重新把数据从现有的index转到新的index建立新的索引，这是因为我们不能修改现有的index的mapping一旦已经定下来了。在接下来的介绍中，我们将学习如何使用reindex接口。为了能够使用reindex接口，我们必须满足一下的条件：_source选项对所有的源index文档是启动的，也即源index的source是被存储的reindex不是帮我们尝试设置好目的地index。它不拷贝源index的设置到目的地的index里去。你应该在做reindex之前把目的地的源的index设置好，这其中包括mapping, shard数目，replica等下面，我们来一个具体的例子，比如建立一个blogs的index。123456789101112131415PUT twitter2/_doc/1&#123; &quot;user&quot; : &quot;双榆树-张三&quot;, &quot;message&quot; : &quot;今儿天气不错啊，出去转转去&quot;, &quot;uid&quot; : 2, &quot;age&quot; : 20, &quot;city&quot; : &quot;北京&quot;, &quot;province&quot; : &quot;北京&quot;, &quot;country&quot; : &quot;中国&quot;, &quot;address&quot; : &quot;中国北京市海淀区&quot;, &quot;location&quot; : &#123; &quot;lat&quot; : &quot;39.970718&quot;, &quot;lon&quot; : &quot;116.325747&quot; &#125;&#125;上面的命令让我们建立了一个叫做twitter2的index，并同时帮我们生产了一个如下的mapping:1GET /twitter2/_mapping显示的结果是：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990&#123; &quot;twitter2&quot; : &#123; &quot;mappings&quot; : &#123; &quot;properties&quot; : &#123; &quot;address&quot; : &#123; &quot;type&quot; : &quot;text&quot;, &quot;fields&quot; : &#123; &quot;keyword&quot; : &#123; &quot;type&quot; : &quot;keyword&quot;, &quot;ignore_above&quot; : 256 &#125; &#125; &#125;, &quot;age&quot; : &#123; &quot;type&quot; : &quot;long&quot; &#125;, &quot;city&quot; : &#123; &quot;type&quot; : &quot;text&quot;, &quot;fields&quot; : &#123; &quot;keyword&quot; : &#123; &quot;type&quot; : &quot;keyword&quot;, &quot;ignore_above&quot; : 256 &#125; &#125; &#125;, &quot;country&quot; : &#123; &quot;type&quot; : &quot;text&quot;, &quot;fields&quot; : &#123; &quot;keyword&quot; : &#123; &quot;type&quot; : &quot;keyword&quot;, &quot;ignore_above&quot; : 256 &#125; &#125; &#125;, &quot;location&quot; : &#123; &quot;properties&quot; : &#123; &quot;lat&quot; : &#123; &quot;type&quot; : &quot;text&quot;, &quot;fields&quot; : &#123; &quot;keyword&quot; : &#123; &quot;type&quot; : &quot;keyword&quot;, &quot;ignore_above&quot; : 256 &#125; &#125; &#125;, &quot;lon&quot; : &#123; &quot;type&quot; : &quot;text&quot;, &quot;fields&quot; : &#123; &quot;keyword&quot; : &#123; &quot;type&quot; : &quot;keyword&quot;, &quot;ignore_above&quot; : 256 &#125; &#125; &#125; &#125; &#125;, &quot;message&quot; : &#123; &quot;type&quot; : &quot;text&quot;, &quot;fields&quot; : &#123; &quot;keyword&quot; : &#123; &quot;type&quot; : &quot;keyword&quot;, &quot;ignore_above&quot; : 256 &#125; &#125; &#125;, &quot;province&quot; : &#123; &quot;type&quot; : &quot;text&quot;, &quot;fields&quot; : &#123; &quot;keyword&quot; : &#123; &quot;type&quot; : &quot;keyword&quot;, &quot;ignore_above&quot; : 256 &#125; &#125; &#125;, &quot;uid&quot; : &#123; &quot;type&quot; : &quot;long&quot; &#125;, &quot;user&quot; : &#123; &quot;type&quot; : &quot;text&quot;, &quot;fields&quot; : &#123; &quot;keyword&quot; : &#123; &quot;type&quot; : &quot;keyword&quot;, &quot;ignore_above&quot; : 256 &#125; &#125; &#125; &#125; &#125; &#125;&#125;显然系统帮我们生产的location数据类型是不对的，我们必须进行修改。一种办法是删除现有的twitter2索引，让后修改它的mapping，再重新索引所有的数据。这对于一个两个文档还是可以的，但是如果已经有很多的数据了，这个方法并不可取。另外一种方式，是建立一个完全新的index，使用新的mapping进行reindex。下面我们展示如何使用这种方法。创建一个新的twitter3的index，使用如下的mapping:1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950PUT twitter3&#123; &quot;settings&quot;: &#123; &quot;number_of_shards&quot;: 1, &quot;number_of_replicas&quot;: 1 &#125;, &quot;mappings&quot;: &#123; &quot;properties&quot;: &#123; &quot;address&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;fields&quot;: &#123; &quot;keyword&quot;: &#123; &quot;type&quot;: &quot;keyword&quot;, &quot;ignore_above&quot;: 256 &#125; &#125; &#125;, &quot;age&quot;: &#123; &quot;type&quot;: &quot;long&quot; &#125;, &quot;city&quot;: &#123; &quot;type&quot;: &quot;text&quot; &#125;, &quot;country&quot;: &#123; &quot;type&quot;: &quot;text&quot; &#125;, &quot;location&quot;: &#123; &quot;type&quot;: &quot;geo_point&quot; &#125;, &quot;message&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;fields&quot;: &#123; &quot;keyword&quot;: &#123; &quot;type&quot;: &quot;keyword&quot;, &quot;ignore_above&quot;: 256 &#125; &#125; &#125;, &quot;province&quot;: &#123; &quot;type&quot;: &quot;text&quot; &#125;, &quot;uid&quot;: &#123; &quot;type&quot;: &quot;long&quot; &#125;, &quot;user&quot;: &#123; &quot;type&quot;: &quot;text&quot; &#125; &#125; &#125;&#125;这里我们我们修改了location及其它的一些数据项的数据类型。运行上面的指令，我们就可以创建一个完全新的twitter3的index。我们可以通过如下的命令来进行reindex：123456789POST _reindex&#123; &quot;source&quot;: &#123; &quot;index&quot;: &quot;twitter2&quot; &#125;, &quot;dest&quot;: &#123; &quot;index&quot;: &quot;twitter3&quot; &#125;&#125;显示的结果是：12345678910111213141516171819&#123; &quot;took&quot; : 52, &quot;timed_out&quot; : false, &quot;total&quot; : 1, &quot;updated&quot; : 0, &quot;created&quot; : 1, &quot;deleted&quot; : 0, &quot;batches&quot; : 1, &quot;version_conflicts&quot; : 0, &quot;noops&quot; : 0, &quot;retries&quot; : &#123; &quot;bulk&quot; : 0, &quot;search&quot; : 0 &#125;, &quot;throttled_millis&quot; : 0, &quot;requests_per_second&quot; : -1.0, &quot;throttled_until_millis&quot; : 0, &quot;failures&quot; : [ ]&#125;我们可以通过如下的命令来检查我们的twitter3是否已经有新的数据：1GET /twitter3/_search显示的结果是：123456789101112131415161718192021222324252627282930313233343536373839&#123; &quot;took&quot; : 100, &quot;timed_out&quot; : false, &quot;_shards&quot; : &#123; &quot;total&quot; : 1, &quot;successful&quot; : 1, &quot;skipped&quot; : 0, &quot;failed&quot; : 0 &#125;, &quot;hits&quot; : &#123; &quot;total&quot; : &#123; &quot;value&quot; : 1, &quot;relation&quot; : &quot;eq&quot; &#125;, &quot;max_score&quot; : 1.0, &quot;hits&quot; : [ &#123; &quot;_index&quot; : &quot;twitter3&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;1&quot;, &quot;_score&quot; : 1.0, &quot;_source&quot; : &#123; &quot;user&quot; : &quot;双榆树-张三&quot;, &quot;message&quot; : &quot;今儿天气不错啊，出去转转去&quot;, &quot;uid&quot; : 2, &quot;age&quot; : 20, &quot;city&quot; : &quot;北京&quot;, &quot;province&quot; : &quot;北京&quot;, &quot;country&quot; : &quot;中国&quot;, &quot;address&quot; : &quot;中国北京市海淀区&quot;, &quot;location&quot; : &#123; &quot;lat&quot; : &quot;39.970718&quot;, &quot;lon&quot; : &quot;116.325747&quot; &#125; &#125; &#125; ] &#125;&#125;显然我们的数据已经从twitter2到twitter3，并且它的数据类型已经是完全符合我们要求的数据类型。Reindex执行Reindex是一个时间点的副本就像上面返回的结果显示的那样，它是以batch（批量）的方式来执行的。默认的批量大小为1000你也可以只拷贝源index其中的一部分数据通过加入query到source中通过定义max_docs参数比如：123456789101112131415POST _reindex&#123; &quot;max_docs&quot;: 100, &quot;source&quot;: &#123; &quot;index&quot;: &quot;twitter2&quot;, &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;city&quot;: &quot;北京&quot; &#125; &#125; &#125;, &quot;dest&quot;: &#123; &quot;index&quot;: &quot;twitter3&quot; &#125;&#125;这里，我们定义最多不超过100个文档，同时，我们只拷贝来自“北京”的twitter记录。设置op_type to create将导致_reindex仅在目标索引中创建缺少的文档。 所有现有文档都会导致版本冲突，比如：12345678910POST _reindex&#123; &quot;source&quot;: &#123; &quot;index&quot;: &quot;twitter2&quot; &#125;, &quot;dest&quot;: &#123; &quot;index&quot;: &quot;twitter3&quot;, &quot;op_type&quot;: &quot;create&quot; &#125;&#125;如果我们之前已经做过reindex，那么我们可以看到如下的结果：123456789101112131415161718192021222324252627282930313233&#123; &quot;took&quot;: 2, &quot;timed_out&quot;: false, &quot;total&quot;: 1, &quot;updated&quot;: 0, &quot;created&quot;: 0, &quot;deleted&quot;: 0, &quot;batches&quot;: 1, &quot;version_conflicts&quot;: 1, &quot;noops&quot;: 0, &quot;retries&quot;: &#123; &quot;bulk&quot;: 0, &quot;search&quot;: 0 &#125;, &quot;throttled_millis&quot;: 0, &quot;requests_per_second&quot;: -1, &quot;throttled_until_millis&quot;: 0, &quot;failures&quot;: [ &#123; &quot;index&quot;: &quot;twitter3&quot;, &quot;type&quot;: &quot;_doc&quot;, &quot;id&quot;: &quot;1&quot;, &quot;cause&quot;: &#123; &quot;type&quot;: &quot;version_conflict_engine_exception&quot;, &quot;reason&quot;: &quot;[1]: version conflict, document already exists (current version [5])&quot;, &quot;index_uuid&quot;: &quot;ffz2LNIIQqqDx211R5f4fQ&quot;, &quot;shard&quot;: &quot;0&quot;, &quot;index&quot;: &quot;twitter3&quot; &#125;, &quot;status&quot;: 409 &#125; ]&#125;它表明我们之前的文档id为1的有版本上的冲突。默认情况下，版本冲突会中止_reindex进程。 “conflict”请求body参数可用于指示_reindex继续处理版本冲突的下一个文档。 请务必注意，其他错误类型的处理不受“conflict”参数的影响。 当“conflict”：在请求正文中设置“proceed”时，_reindex进程将继续发生版本冲突并返回遇到的版本冲突计数：1234567891011POST _reindex&#123; &quot;conflicts&quot;: &quot;proceed&quot;, &quot;source&quot;: &#123; &quot;index&quot;: &quot;twitter&quot; &#125;, &quot;dest&quot;: &#123; &quot;index&quot;: &quot;new_twitter&quot;, &quot;op_type&quot;: &quot;create&quot; &#125;&#125;Throttling重新索引大量文档可能会使您的群集泛滥甚至崩溃。requests_per_second限制索引操作速率。12345678910POST _reindex?requests_per_second=500 &#123; &quot;source&quot;: &#123; &quot;index&quot;: &quot;blogs&quot;, &quot;size&quot;: 500 &#125;, &quot;dest&quot;: &#123; &quot;index&quot;: &quot;blogs_fixed&quot; &#125;&#125;运用index别名来进行reindex我们可以通过如下的方法来实现从一个index到另外一个index的数据转移：123456789PUT test PUT test_2 POST /_aliases&#123; &quot;actions&quot; : [ &#123; &quot;add&quot;: &#123; &quot;index&quot;: &quot;test_2&quot;, &quot;alias&quot;: &quot;test&quot; &#125; &#125;, &#123; &quot;remove_index&quot;: &#123; &quot;index&quot;: &quot;test&quot; &#125; &#125; ]&#125;在上面的例子中，假如我们地添加了一个叫做test的index，而test_2是我们想要的。我们直接可以通过上面的方法吧test中的数据交换到test_2中，并同时把test索引删除。从远处进行reindex_reindex也支持从一个远处的Elasticsearch的服务器进行reindex，它的语法为：1234567891011121314POST _reindex&#123; &quot;source&quot;: &#123; &quot;index&quot;: &quot;blogs&quot;, &quot;remote&quot;: &#123; &quot;host&quot;: &quot;http://remote_cluster_node1:9200&quot;, &quot;username&quot;: &quot;USERNAME&quot;, &quot;password&quot;: &quot;PASSWORD&quot; &#125; &#125;, &quot;dest&quot;: &#123; &quot;index&quot;: &quot;blogs&quot; &#125;&#125;这里显示它从一个在http://remote_cluster_node1:9200的服务器来拷贝文件从一个index到另外一个index。参考：【1】https://www.elastic.co/guide/en/elasticsearch/reference/7.3/docs-reindex.html]]></content>
      <categories>
        <category>elk</category>
      </categories>
      <tags>
        <tag>elk</tag>
      </tags>
  </entry>
  <entry>
    <title></title>
    <url>%2F2020%2F01%2F02%2FElasticsearch%20Painless%20script%E7%BC%96%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[我们之前看见了在Elasticsearch里的ingest node里，我们可以通过以下processor的处理帮我们处理我们的一些数据。它们的功能是非常具体而明确的。那么在Elasticsearch里，有没有一种更加灵活的方式可供我们来进行编程处理呢？如果有，它使用的语言是什么呢？在Elasticsearc中，它使用了一个叫做Painless的语言。它是专门为Elasticsearch而建立的。Painless是一种简单，安全的脚本语言，专为与Elasticsearch一起使用而设计。 它是Elasticsearch的默认脚本语言，可以安全地用于inline和stored脚本。它具有像Groovy那样的语法。自Elasticsearch 6.0以后的版本不再支持Groovy，Javascript及Python语言。如何使用脚本脚本的语法为:12345&quot;script&quot;: &#123; &quot;lang&quot;: &quot;...&quot;, &quot;source&quot; | &quot;id&quot;: &quot;...&quot;, &quot;params&quot;: &#123; ... &#125; &#125;这里lang默认的值为”painless”。在实际的使用中可以不设置，除非有第二种语言供使用source可以为inline脚本，或者是一个id，那么这个id对应于一个stored脚本任何有名字的参数，可以被用于脚本的输入参数Painless的简单使用例子inline 脚本首先我们来创建一个简单的文档：123456789101112131415PUT twitter/_doc/1&#123; &quot;user&quot; : &quot;双榆树-张三&quot;, &quot;message&quot; : &quot;今儿天气不错啊，出去转转去&quot;, &quot;uid&quot; : 2, &quot;age&quot; : 20, &quot;city&quot; : &quot;北京&quot;, &quot;province&quot; : &quot;北京&quot;, &quot;country&quot; : &quot;中国&quot;, &quot;address&quot; : &quot;中国北京市海淀区&quot;, &quot;location&quot; : &#123; &quot;lat&quot; : &quot;39.970718&quot;, &quot;lon&quot; : &quot;116.325747&quot; &#125;&#125;在这个文档里，我们现在想把age修改为30，那么一种办法就是把所有的文档内容都读出来，让修改其中的age想为30，再重新用同样的方法写进去。首先这里需要有几个动作：先读出数据，然后修改，再次写入数据。显然这样比较麻烦。在这里我们可以直接使用Painless语言直接进行修改：123456POST twitter/_update/1&#123; &quot;script&quot;: &#123; &quot;source&quot;: &quot;ctx._source.age = 30&quot; &#125;&#125;这里的source表明是我们的Painless代码。这里我们只写了很少的代码在DSL之中。这种代码称之为inline。在这里我们直接通过ctx._source.age来访问 _souce里的age。这样我们通过编程的办法直接对年龄进行了修改。运行的结果是：1234567891011121314151617181920212223&#123; &quot;_index&quot; : &quot;twitter&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;1&quot;, &quot;_version&quot; : 16, &quot;_seq_no&quot; : 20, &quot;_primary_term&quot; : 1, &quot;found&quot; : true, &quot;_source&quot; : &#123; &quot;user&quot; : &quot;双榆树-张三&quot;, &quot;message&quot; : &quot;今儿天气不错啊，出去转转去&quot;, &quot;uid&quot; : 2, &quot;age&quot; : 30, &quot;city&quot; : &quot;北京&quot;, &quot;province&quot; : &quot;北京&quot;, &quot;country&quot; : &quot;中国&quot;, &quot;address&quot; : &quot;中国北京市海淀区&quot;, &quot;location&quot; : &#123; &quot;lat&quot; : &quot;39.970718&quot;, &quot;lon&quot; : &quot;116.325747&quot; &#125; &#125;&#125;显然这个age已经改变为30。上面的方法固然好，但是每次执行scripts都是需要重新进行编译的。编译好的script可以cache并供以后使用。上面的script如果是改变年龄的话，需要重新进行编译。一种更好的方法是改为这样的：123456789POST twitter/_update/1&#123; &quot;script&quot;: &#123; &quot;source&quot;: &quot;ctx._source.age = params.value&quot;, &quot;params&quot;: &#123; &quot;value&quot;: 34 &#125; &#125;&#125;这样，我们的script的source是不用改变的，只需要编译一次。下次调用的时候，只需要修改params里的参数即可。在Elasticsearch里：123&quot;script&quot;: &#123; &quot;source&quot;: &quot;ctx._source.num_of_views += 2&quot;&#125;和123&quot;script&quot;: &#123; &quot;source&quot;: &quot;ctx._source.num_of_views += 3&quot;&#125;被视为两个不同的脚本，需要分别进行编译，所以最好的办法是使用params来传入参数。存储的脚本 (stored script)在这种情况下，scripts可以被存放于一个集群的状态中。它之后可以通过ID进行调用：1234567PUT _scripts/add_age&#123; &quot;script&quot;: &#123; &quot;lang&quot;: &quot;painless&quot;, &quot;source&quot;: &quot;ctx._source.age += params.value&quot; &#125;&#125;在这里，我们定义了一个叫做add_age的script。它的作用就是帮我们把source里的age加上一个数值。我们可以在之后调用它：123456789POST twitter/_update/1&#123; &quot;script&quot;: &#123; &quot;id&quot;: &quot;add_age&quot;, &quot;params&quot;: &#123; &quot;value&quot;: 2 &#125; &#125;&#125;通过上面的执行，我们可以看到，age将会被加上2。访问source里的字段Painless中用于访问字段值的语法取决于上下文。在Elasticsearch中，有许多不同的Plainless上下文。就像那个链接显示的那样，Plainless上下文包括：ingest processor, update, update by query, sort，filter等等。Context 访问字段Ingest node: 访问字段使用ctx ctx.field_nameUpdates: 使用_source 字段 ctx._source.field_name这里的updates包括_update，_reindex以及update_by_query。这里，我们对于context（上下文的理解）非常重要。它的意思是针对不同的API，在使用中ctx所包含的字段是不一样的。在下面的例子中，我们针对一些情况来做具体的分析。Painless脚本例子首先我们创建一个叫做add_field_c的pipeline。关于如何创建一个pipleline，大家可以参考我之前写过的一个文章“如何在Elasticsearch中使用pipeline API来对事件进行处理”。例子11234567891011121314PUT _ingest/pipeline/add_field_c&#123; &quot;processors&quot;: [ &#123; &quot;script&quot;: &#123; &quot;lang&quot;: &quot;painless&quot;, &quot;source&quot;: &quot;ctx.field_c = (ctx.field_a + ctx.field_b) * params.value&quot;, &quot;params&quot;: &#123; &quot;value&quot;: 2 &#125; &#125; &#125; ]&#125;这个pipepline的作用是创建一个新的field：field_c。它的结果是field_a及field_b的和，并乘以2。那么我们创建一个如下的文档：12345PUT test_script/_doc/1?pipeline=add_field_c&#123; &quot;field_a&quot;: 10, &quot;field_b&quot;: 20&#125;在这里，我们使用了pipleline add_field_c。执行后的结果是：123456789101112131415161718192021222324252627282930&#123; &quot;took&quot; : 147, &quot;timed_out&quot; : false, &quot;_shards&quot; : &#123; &quot;total&quot; : 1, &quot;successful&quot; : 1, &quot;skipped&quot; : 0, &quot;failed&quot; : 0 &#125;, &quot;hits&quot; : &#123; &quot;total&quot; : &#123; &quot;value&quot; : 1, &quot;relation&quot; : &quot;eq&quot; &#125;, &quot;max_score&quot; : 1.0, &quot;hits&quot; : [ &#123; &quot;_index&quot; : &quot;test_script&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;1&quot;, &quot;_score&quot; : 1.0, &quot;_source&quot; : &#123; &quot;field_c&quot; : 60, &quot;field_a&quot; : 10, &quot;field_b&quot; : 20 &#125; &#125; ] &#125;&#125;显然，我们可以看到field_c被成功创建了。例子2在ingest过程中，可以使用脚本处理器来处理metadata，如_index和_type。 下面是一个Ingest Pipeline的示例，无论原始索引请求中提供了什么，它都会将索引和类型重命名为my_index：1234567891011121314PUT _ingest/pipeline/my_index&#123; &quot;description&quot;: &quot;use index:my_index and type:_doc&quot;, &quot;processors&quot;: [ &#123; &quot;script&quot;: &#123; &quot;source&quot;: &quot;&quot;&quot; ctx._index = &apos;my_index&apos;; ctx._type = &apos;_doc&apos;; &quot;&quot;&quot; &#125; &#125; ]&#125;使用上面的pipeline，我们可以尝试index一个文档到any_index：1234PUT any_index/_doc/1?pipeline=my_index&#123; &quot;message&quot;: &quot;text&quot;&#125;显示的结果是：1234567891011121314&#123; &quot;_index&quot;: &quot;my_index&quot;, &quot;_type&quot;: &quot;_doc&quot;, &quot;_id&quot;: &quot;1&quot;, &quot;_version&quot;: 1, &quot;result&quot;: &quot;created&quot;, &quot;_shards&quot;: &#123; &quot;total&quot;: 2, &quot;successful&quot;: 1, &quot;failed&quot;: 0 &#125;, &quot;_seq_no&quot;: 89, &quot;_primary_term&quot;: 1,&#125;也就是说真正的文档时存到my_index之中，而不是any_index。例子31234567891011121314PUT _ingest/pipeline/blogs_pipeline&#123; &quot;processors&quot;: [ &#123; &quot;script&quot;: &#123; &quot;source&quot;: &quot;&quot;&quot; if (ctx.category == &quot;&quot;) &#123; ctx.category = &quot;None&quot; &#125; &quot;&quot;&quot; &#125; &#125; ]&#125;我们上面定义了一个pipeline，它可以帮我们检查如果 category字段是否为空，如果是，就修改为“None”。还是以之前的那个test_script索引为例：12345678PUT test_script/_doc/2?pipeline=blogs_pipeline&#123; &quot;field_a&quot;: 5, &quot;field_b&quot;: 10, &quot;category&quot;: &quot;&quot;&#125; GET test_script/_doc/2显示的结果是：1234567891011121314&#123; &quot;_index&quot; : &quot;test_script&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;2&quot;, &quot;_version&quot; : 2, &quot;_seq_no&quot; : 6, &quot;_primary_term&quot; : 1, &quot;found&quot; : true, &quot;_source&quot; : &#123; &quot;field_a&quot; : 5, &quot;field_b&quot; : 10, &quot;category&quot; : &quot;None&quot; &#125;&#125;显然，它把category为“”的字段变为“None”了。例子412345678910111213141516POST _reindex&#123; &quot;source&quot;: &#123; &quot;index&quot;: &quot;blogs&quot; &#125;, &quot;dest&quot;: &#123; &quot;index&quot;: &quot;blogs_fixed&quot; &#125;, &quot;script&quot;: &#123; &quot;source&quot;: &quot;&quot;&quot; if (ctx._source.category == &quot;&quot;) &#123; ctx._source.category = &quot;None&quot; &#125;&quot;&quot;&quot; &#125;&#125;上面的这个例子在reindex时，如果category为空时，写入“None”。我们可以从上面的两个例子中看出来，针对pipeline，我们可以直接对cxt.field进行操作，而针对update来说，我们可以对cxt._source下的字段进行操作。这也是之前提到的上下文的区别。例子512345PUT test/_doc/1&#123; &quot;counter&quot; : 1, &quot;tags&quot; : [&quot;red&quot;]&#125;您可以使用和update脚本将tag添加到tags列表（这只是一个列表，因此即使存在标记也会添加）：12345678910POST test/_update/1&#123; &quot;script&quot; : &#123; &quot;source&quot;: &quot;ctx._source.tags.add(params.tag)&quot;, &quot;lang&quot;: &quot;painless&quot;, &quot;params&quot; : &#123; &quot;tag&quot; : &quot;blue&quot; &#125; &#125;&#125;显示结果：123456789101112131415161718GET test/_doc/1 &#123; &quot;_index&quot; : &quot;test&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;1&quot;, &quot;_version&quot; : 4, &quot;_seq_no&quot; : 3, &quot;_primary_term&quot; : 11, &quot;found&quot; : true, &quot;_source&quot; : &#123; &quot;counter&quot; : 1, &quot;tags&quot; : [ &quot;red&quot;, &quot;blue&quot; ] &#125; &#125;显示“blue”，已经被成功加入到tags列表之中了。您还可以从tags列表中删除tag。 删除tag的Painless函数采用要删除的元素的数组索引。 为避免可能的运行时错误，首先需要确保tag存在。 如果列表包含tag的重复项，则此脚本只删除一个匹配项。123456789101112POST test/_update/1&#123; &quot;script&quot;: &#123; &quot;source&quot;: &quot;if (ctx._source.tags.contains(params.tag)) &#123; ctx._source.tags.remove(ctx._source.tags.indexOf(params.tag)) &#125;&quot;, &quot;lang&quot;: &quot;painless&quot;, &quot;params&quot;: &#123; &quot;tag&quot;: &quot;blue&quot; &#125; &#125;&#125; GET test/_doc/1显示结果：123456789101112131415&#123; &quot;_index&quot; : &quot;test&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;1&quot;, &quot;_version&quot; : 5, &quot;_seq_no&quot; : 4, &quot;_primary_term&quot; : 11, &quot;found&quot; : true, &quot;_source&quot; : &#123; &quot;counter&quot; : 1, &quot;tags&quot; : [ &quot;red&quot; ] &#125;&#125;“blue”显然已经被删除了。Painless脚本简单的操练为了说明Painless的工作原理，让我们将一些曲棍球统计数据加载到Elasticsearch索引中：1234567891011121314151617181920212223PUT hockey/_bulk?refresh&#123;&quot;index&quot;:&#123;&quot;_id&quot;:1&#125;&#125;&#123;&quot;first&quot;:&quot;johnny&quot;,&quot;last&quot;:&quot;gaudreau&quot;,&quot;goals&quot;:[9,27,1],&quot;assists&quot;:[17,46,0],&quot;gp&quot;:[26,82,1],&quot;born&quot;:&quot;1993/08/13&quot;&#125;&#123;&quot;index&quot;:&#123;&quot;_id&quot;:2&#125;&#125;&#123;&quot;first&quot;:&quot;sean&quot;,&quot;last&quot;:&quot;monohan&quot;,&quot;goals&quot;:[7,54,26],&quot;assists&quot;:[11,26,13],&quot;gp&quot;:[26,82,82],&quot;born&quot;:&quot;1994/10/12&quot;&#125;&#123;&quot;index&quot;:&#123;&quot;_id&quot;:3&#125;&#125;&#123;&quot;first&quot;:&quot;jiri&quot;,&quot;last&quot;:&quot;hudler&quot;,&quot;goals&quot;:[5,34,36],&quot;assists&quot;:[11,62,42],&quot;gp&quot;:[24,80,79],&quot;born&quot;:&quot;1984/01/04&quot;&#125;&#123;&quot;index&quot;:&#123;&quot;_id&quot;:4&#125;&#125;&#123;&quot;first&quot;:&quot;micheal&quot;,&quot;last&quot;:&quot;frolik&quot;,&quot;goals&quot;:[4,6,15],&quot;assists&quot;:[8,23,15],&quot;gp&quot;:[26,82,82],&quot;born&quot;:&quot;1988/02/17&quot;&#125;&#123;&quot;index&quot;:&#123;&quot;_id&quot;:5&#125;&#125;&#123;&quot;first&quot;:&quot;sam&quot;,&quot;last&quot;:&quot;bennett&quot;,&quot;goals&quot;:[5,0,0],&quot;assists&quot;:[8,1,0],&quot;gp&quot;:[26,1,0],&quot;born&quot;:&quot;1996/06/20&quot;&#125;&#123;&quot;index&quot;:&#123;&quot;_id&quot;:6&#125;&#125;&#123;&quot;first&quot;:&quot;dennis&quot;,&quot;last&quot;:&quot;wideman&quot;,&quot;goals&quot;:[0,26,15],&quot;assists&quot;:[11,30,24],&quot;gp&quot;:[26,81,82],&quot;born&quot;:&quot;1983/03/20&quot;&#125;&#123;&quot;index&quot;:&#123;&quot;_id&quot;:7&#125;&#125;&#123;&quot;first&quot;:&quot;david&quot;,&quot;last&quot;:&quot;jones&quot;,&quot;goals&quot;:[7,19,5],&quot;assists&quot;:[3,17,4],&quot;gp&quot;:[26,45,34],&quot;born&quot;:&quot;1984/08/10&quot;&#125;&#123;&quot;index&quot;:&#123;&quot;_id&quot;:8&#125;&#125;&#123;&quot;first&quot;:&quot;tj&quot;,&quot;last&quot;:&quot;brodie&quot;,&quot;goals&quot;:[2,14,7],&quot;assists&quot;:[8,42,30],&quot;gp&quot;:[26,82,82],&quot;born&quot;:&quot;1990/06/07&quot;&#125;&#123;&quot;index&quot;:&#123;&quot;_id&quot;:39&#125;&#125;&#123;&quot;first&quot;:&quot;mark&quot;,&quot;last&quot;:&quot;giordano&quot;,&quot;goals&quot;:[6,30,15],&quot;assists&quot;:[3,30,24],&quot;gp&quot;:[26,60,63],&quot;born&quot;:&quot;1983/10/03&quot;&#125;&#123;&quot;index&quot;:&#123;&quot;_id&quot;:10&#125;&#125;&#123;&quot;first&quot;:&quot;mikael&quot;,&quot;last&quot;:&quot;backlund&quot;,&quot;goals&quot;:[3,15,13],&quot;assists&quot;:[6,24,18],&quot;gp&quot;:[26,82,82],&quot;born&quot;:&quot;1989/03/17&quot;&#125;&#123;&quot;index&quot;:&#123;&quot;_id&quot;:11&#125;&#125;&#123;&quot;first&quot;:&quot;joe&quot;,&quot;last&quot;:&quot;colborne&quot;,&quot;goals&quot;:[3,18,13],&quot;assists&quot;:[6,20,24],&quot;gp&quot;:[26,67,82],&quot;born&quot;:&quot;1990/01/30&quot;&#125;使用Painless访问Doc里的值文档里的值可以通过一个叫做doc的Map值来访问。例如，以下脚本计算玩家的总进球数。 此示例使用类型int和for循环。12345678910111213141516171819GET hockey/_search&#123; &quot;query&quot;: &#123; &quot;function_score&quot;: &#123; &quot;script_score&quot;: &#123; &quot;script&quot;: &#123; &quot;lang&quot;: &quot;painless&quot;, &quot;source&quot;: &quot;&quot;&quot; int total = 0; for (int i = 0; i &lt; doc[&apos;goals&apos;].length; ++i) &#123; total += doc[&apos;goals&apos;][i]; &#125; return total; &quot;&quot;&quot; &#125; &#125; &#125; &#125;&#125;这里我们通过script来计算每个文档的_score。通过script把每个运动员的goal都加起来，并形成最终的_score。这里我们通过doc[‘goals’]这个Map类型来访问我们的字段值。显示的结果为：12345678910111213141516171819202122232425262728293031323334353637383940414243&#123; &quot;took&quot; : 25, &quot;timed_out&quot; : false, &quot;_shards&quot; : &#123; &quot;total&quot; : 1, &quot;successful&quot; : 1, &quot;skipped&quot; : 0, &quot;failed&quot; : 0 &#125;, &quot;hits&quot; : &#123; &quot;total&quot; : &#123; &quot;value&quot; : 11, &quot;relation&quot; : &quot;eq&quot; &#125;, &quot;max_score&quot; : 87.0, &quot;hits&quot; : [ &#123; &quot;_index&quot; : &quot;hockey&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;2&quot;, &quot;_score&quot; : 87.0, &quot;_source&quot; : &#123; &quot;first&quot; : &quot;sean&quot;, &quot;last&quot; : &quot;monohan&quot;, &quot;goals&quot; : [ 7, 54, 26 ], &quot;assists&quot; : [ 11, 26, 13 ], &quot;gp&quot; : [ 26, 82, 82 ], &quot;born&quot; : &quot;1994/10/12&quot; &#125; &#125;,...或者，您可以使用script_fields而不是function_score执行相同的操作：1234567891011121314151617181920GET hockey/_search&#123; &quot;query&quot;: &#123; &quot;match_all&quot;: &#123;&#125; &#125;, &quot;script_fields&quot;: &#123; &quot;total_goals&quot;: &#123; &quot;script&quot;: &#123; &quot;lang&quot;: &quot;painless&quot;, &quot;source&quot;: &quot;&quot;&quot; int total = 0; for (int i = 0; i &lt; doc[&apos;goals&apos;].length; ++i) &#123; total += doc[&apos;goals&apos;][i]; &#125; return total; &quot;&quot;&quot; &#125; &#125; &#125;&#125;显示的结果为：123456789101112131415161718192021222324252627282930313233343536373839&#123; &quot;took&quot; : 5, &quot;timed_out&quot; : false, &quot;_shards&quot; : &#123; &quot;total&quot; : 1, &quot;successful&quot; : 1, &quot;skipped&quot; : 0, &quot;failed&quot; : 0 &#125;, &quot;hits&quot; : &#123; &quot;total&quot; : &#123; &quot;value&quot; : 11, &quot;relation&quot; : &quot;eq&quot; &#125;, &quot;max_score&quot; : 1.0, &quot;hits&quot; : [ &#123; &quot;_index&quot; : &quot;hockey&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;1&quot;, &quot;_score&quot; : 1.0, &quot;fields&quot; : &#123; &quot;total_goals&quot; : [ 37 ] &#125; &#125;, &#123; &quot;_index&quot; : &quot;hockey&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;2&quot;, &quot;_score&quot; : 1.0, &quot;fields&quot; : &#123; &quot;total_goals&quot; : [ 87 ] &#125; &#125;,...以下示例使用Painless脚本按其组合的名字和姓氏对玩家进行排序。 使用doc [‘first’]。value和doc [‘last’]。value访问名称。12345678910111213141516GET hockey/_search&#123; &quot;query&quot;: &#123; &quot;match_all&quot;: &#123;&#125; &#125;, &quot;sort&quot;: &#123; &quot;_script&quot;: &#123; &quot;type&quot;: &quot;string&quot;, &quot;order&quot;: &quot;asc&quot;, &quot;script&quot;: &#123; &quot;lang&quot;: &quot;painless&quot;, &quot;source&quot;: &quot;doc[&apos;first.keyword&apos;].value + &apos; &apos; + doc[&apos;last.keyword&apos;].value&quot; &#125; &#125; &#125;&#125;检查缺失项doc [‘field’].value。如果文档中缺少该字段，则抛出异常。要检查文档是否缺少值，可以调用doc [&#39;field&#39;] .size（）== 0。使用Painless更新字段您还可以轻松更新字段。 您可以使用ctx._source.&lt;field-name&gt;访问字段的原始源。首先，让我们通过提交以下请求来查看玩家的源数据：12345678910111213GET hockey/_search&#123; &quot;stored_fields&quot;: [ &quot;_id&quot;, &quot;_source&quot; ], &quot;query&quot;: &#123; &quot;term&quot;: &#123; &quot;_id&quot;: 1 &#125; &#125;&#125;显示的结果为：123456789101112131415161718192021222324252627282930313233343536373839404142434445&#123; &quot;took&quot; : 0, &quot;timed_out&quot; : false, &quot;_shards&quot; : &#123; &quot;total&quot; : 1, &quot;successful&quot; : 1, &quot;skipped&quot; : 0, &quot;failed&quot; : 0 &#125;, &quot;hits&quot; : &#123; &quot;total&quot; : &#123; &quot;value&quot; : 1, &quot;relation&quot; : &quot;eq&quot; &#125;, &quot;max_score&quot; : 1.0, &quot;hits&quot; : [ &#123; &quot;_index&quot; : &quot;hockey&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;1&quot;, &quot;_score&quot; : 1.0, &quot;_source&quot; : &#123; &quot;first&quot; : &quot;johnny&quot;, &quot;last&quot; : &quot;gaudreau&quot;, &quot;goals&quot; : [ 9, 27, 1 ], &quot;assists&quot; : [ 17, 46, 0 ], &quot;gp&quot; : [ 26, 82, 1 ], &quot;born&quot; : &quot;1993/08/13&quot; &#125; &#125; ] &#125;&#125;要将玩家1的姓氏更改为hockey，只需将ctx._source.last设置为新值：12345678910POST hockey/_update/1&#123; &quot;script&quot;: &#123; &quot;lang&quot;: &quot;painless&quot;, &quot;source&quot;: &quot;ctx._source.last = params.last&quot;, &quot;params&quot;: &#123; &quot;last&quot;: &quot;hockey&quot; &#125; &#125;&#125;您还可以向文档添加字段。 例如，此脚本添加一个包含玩家nickname为hockey的新字段。1234567891011121314POST hockey/_update/1&#123; &quot;script&quot;: &#123; &quot;lang&quot;: &quot;painless&quot;, &quot;source&quot;: &quot;&quot;&quot; ctx._source.last = params.last; ctx._source.nick = params.nick &quot;&quot;&quot;, &quot;params&quot;: &#123; &quot;last&quot;: &quot;gaudreau&quot;, &quot;nick&quot;: &quot;hockey&quot; &#125; &#125;&#125;显示的结果为：12345678910111213141516171819202122232425262728293031GET hockey/_doc/1 &#123; &quot;_index&quot; : &quot;hockey&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;1&quot;, &quot;_version&quot; : 2, &quot;_seq_no&quot; : 11, &quot;_primary_term&quot; : 1, &quot;found&quot; : true, &quot;_source&quot; : &#123; &quot;first&quot; : &quot;johnny&quot;, &quot;last&quot; : &quot;gaudreau&quot;, &quot;goals&quot; : [ 9, 27, 1 ], &quot;assists&quot; : [ 17, 46, 0 ], &quot;gp&quot; : [ 26, 82, 1 ], &quot;born&quot; : &quot;1993/08/13&quot;, &quot;nick&quot; : &quot;hockey&quot; &#125; &#125;有一个叫做 “nick”的新字段被加入了。我们甚至可以对日期类型来进行操作从而得到年月等信息：12345678910GET hockey/_search&#123; &quot;script_fields&quot;: &#123; &quot;birth_year&quot;: &#123; &quot;script&quot;: &#123; &quot;source&quot;: &quot;doc.born.value.year&quot; &#125; &#125; &#125;&#125;显示结果：12345678910111213141516171819202122232425262728&#123; &quot;took&quot; : 0, &quot;timed_out&quot; : false, &quot;_shards&quot; : &#123; &quot;total&quot; : 1, &quot;successful&quot; : 1, &quot;skipped&quot; : 0, &quot;failed&quot; : 0 &#125;, &quot;hits&quot; : &#123; &quot;total&quot; : &#123; &quot;value&quot; : 11, &quot;relation&quot; : &quot;eq&quot; &#125;, &quot;max_score&quot; : 1.0, &quot;hits&quot; : [ &#123; &quot;_index&quot; : &quot;hockey&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;2&quot;, &quot;_score&quot; : 1.0, &quot;fields&quot; : &#123; &quot;birth_year&quot; : [ 1994 ] &#125; &#125;,...Script CachingElasticsearch第一次看到一个新脚本，它会编译它并将编译后的版本存储在缓存中。无论是inline或是stored脚本都存储在缓存中。新脚本可以驱逐缓存的脚本。默认的情况下是可以存储100个脚本。我们可以通过设置script.cache.max_size来改变其大小，或者通过script.cache.expire来设置过期的时间。这些设置需要在config/elasticsearch.yml里设置。Script 调试不能调试的脚本是非常难的。有一个好的调试手段无疑对我们的脚本编程是非常有用的。Debug.explainPainless没有REPL，虽然有一天它很好，但它不会告诉你关于调试Elasticsearch中嵌入的Painless脚本的全部故事，因为脚本可以访问的数据或“上下文” 是如此重要。 目前，调试嵌入式脚本的最佳方法是在选择位置抛出异常。 虽然您可以抛出自己的异常（throw new exception(‘whatever’），但Painless的沙箱会阻止您访问有用的信息，如对象的类型。 所以Painless有一个实用工具方法Debug.explain，它会为你抛出异常。 例如，您可以使用_explain来探索script query可用的上下文。PUT /hockey/_doc/1?refresh {&quot;first&quot;:&quot;johnny&quot;,&quot;last&quot;:&quot;gaudreau&quot;,&quot;goals&quot;:[9,27,1],&quot;assists&quot;:[17,46,0],&quot;gp&quot;:[26,82,1]} POST /hockey/_explain/1 { &quot;query&quot;: { &quot;script&quot;: { &quot;script&quot;: &quot;Debug.explain(doc.goals)&quot; } } } 这表明doc.goals类是org.elasticsearch.index.fielddata.ScriptDocValues.Longs通过响应：{ &quot;error&quot;: { &quot;root_cause&quot;: [ { &quot;type&quot;: &quot;script_exception&quot;, &quot;reason&quot;: &quot;runtime error&quot;, &quot;painless_class&quot;: &quot;org.elasticsearch.index.fielddata.ScriptDocValues.Longs&quot;, &quot;to_string&quot;: &quot;[1, 9, 27]&quot;, &quot;java_class&quot;: &quot;org.elasticsearch.index.fielddata.ScriptDocValues$Longs&quot;, &quot;script_stack&quot;: [ &quot;Debug.explain(doc.goals)&quot;, &quot; ^---- HERE&quot; ], &quot;script&quot;: &quot;Debug.explain(doc.goals)&quot;, &quot;lang&quot;: &quot;painless&quot; } ], &quot;type&quot;: &quot;script_exception&quot;, &quot;reason&quot;: &quot;runtime error&quot;, &quot;painless_class&quot;: &quot;org.elasticsearch.index.fielddata.ScriptDocValues.Longs&quot;, &quot;to_string&quot;: &quot;[1, 9, 27]&quot;, &quot;java_class&quot;: &quot;org.elasticsearch.index.fielddata.ScriptDocValues$Longs&quot;, &quot;script_stack&quot;: [ &quot;Debug.explain(doc.goals)&quot;, &quot; ^---- HERE&quot; ], &quot;script&quot;: &quot;Debug.explain(doc.goals)&quot;, &quot;lang&quot;: &quot;painless&quot;, &quot;caused_by&quot;: { &quot;type&quot;: &quot;painless_explain_error&quot;, &quot;reason&quot;: null } }, &quot;status&quot;: 400 } 您可以使用相同的技巧来查看_source是_update API中的LinkedHashMap：1234POST /hockey/_update/1&#123; &quot;script&quot;: &quot;Debug.explain(ctx._source)&quot;&#125;显示的结果是：123456789101112131415161718192021222324252627282930&#123; &quot;error&quot;: &#123; &quot;root_cause&quot;: [ &#123; &quot;type&quot;: &quot;remote_transport_exception&quot;, &quot;reason&quot;: &quot;[localhost][127.0.0.1:9300][indices:data/write/update[s]]&quot; &#125; ], &quot;type&quot;: &quot;illegal_argument_exception&quot;, &quot;reason&quot;: &quot;failed to execute script&quot;, &quot;caused_by&quot;: &#123; &quot;type&quot;: &quot;script_exception&quot;, &quot;reason&quot;: &quot;runtime error&quot;, &quot;painless_class&quot;: &quot;java.util.LinkedHashMap&quot;, &quot;to_string&quot;: &quot;&#123;first=johnny, last=gaudreau, goals=[9, 27, 1], assists=[17, 46, 0], gp=[26, 82, 1], born=1993/08/13, nick=hockey&#125;&quot;, &quot;java_class&quot;: &quot;java.util.LinkedHashMap&quot;, &quot;script_stack&quot;: [ &quot;Debug.explain(ctx._source)&quot;, &quot; ^---- HERE&quot; ], &quot;script&quot;: &quot;Debug.explain(ctx._source)&quot;, &quot;lang&quot;: &quot;painless&quot;, &quot;caused_by&quot;: &#123; &quot;type&quot;: &quot;painless_explain_error&quot;, &quot;reason&quot;: null &#125; &#125; &#125;, &quot;status&quot;: 400&#125;参考：【1】https://www.elastic.co/guide/en/elasticsearch/painless/current/painless-walkthrough.html【2】https://www.elastic.co/guide/en/elasticsearch/painless/current/painless-debugging.html]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2020%2F01%2F02%2FElasticsearch%20Ngrams%2C%20edge%20ngrams%2C%20and%20shingles%2F</url>
    <content type="text"><![CDATA[Ngrams和edge ngrams是在Elasticsearch中标记文本的两种更独特的方式。 Ngrams是一种将一个标记分成一个单词的每个部分的多个子字符的方法。 ngram和edge ngram过滤器都允许您指定min_gram以及max_gram设置。 这些设置控制单词被分割成的标记的大小。 这可能令人困惑，让我们看一个例子。 假设你想用ngram分析仪分析“spaghetti”这个词，让我们从最简单的情况开始，1-gams（也称为unigrams）。在实际的搜索例子中，比如谷歌搜索：每当我们打入前面的几个字母时，就会出现相应的很多的候选名单。这个就是autocomplete功能。在Elasticsearch中，我们可以通过Edge ngram来实现这个目的。1-grams“spaghetti”的1-grams是s，p，a，g，h，e，t，t，i。 根据ngram的大小将字符串拆分为较小的token。 在这种情况下，每个token都是一个字符，因为我们谈论的是unigrams。Bigrams如果你要将字符串拆分为双字母组（这意味着大小为2），你将获得以下较小的token：sp，pa，ag，gh，he，et，tt，ti。Trigrams再说一次，如果你要使用三个大小，你将得到token为spa，pag，agh，ghe，het，ett，tti。设置min_gram和max_gram使用此分析器时，需要设置两种不同的大小：一种指定要生成的最小ngrams（min_gram设置），另一种指定要生成的最大ngrams。 使用前面的示例，如果您指定min_gram为2且max_gram为3，则您将获得前两个示例中的组合标记：1sp, spa, pa, pag, ag, agh, gh, ghe, he, het, et, ett, tt, tti, ti如果你要将min_gram设置为1并将max_gram设置为3，那么你将得到更多的标记，从s，sp，spa，p，pa，pag，a，….开始。以这种方式分析文本具有一个有趣的优点。 当你查询文本时，你的查询将以相同的方式被分割成文本，所以说你正在寻找拼写错误的单词“spaghety”。搜索这个的一种方法是做一个fuzzy query，它允许你 指定单词的编辑距离以检查匹配。 但是你可以通过使用ngrams来获得类似的行为。 让我们将原始单词（“spaghetti”）生成的bigrams与拼写错误的单词（“spaghety”）进行比较：“spaghetti”的bigrams：sp，pa，ag，gh，he，et，tt，ti“spaghety”的bigrams：sp，pa，ag，gh，he，et，ty您可以看到六个token重叠，因此当查询包含“spaghety”时，其中带有“spaghetti”的单词仍然匹配。请记住，这意味着您可能不打算使用的原始“spaghetti”单词更多的单词 ，所以请务必测试您的查询相关性！ngrams做的另一个有用的事情是允许您在事先不了解语言时或者当您使用与其他欧洲语言不同的方式组合单词的语言时分析文本。 这还有一个优点，即能够使用单个分析器处理多种语言，而不必指定。Edge ngrams常规ngram拆分的变体称为edge ngrams，仅从前沿构建ngram。 在“spaghetti”示例中，如果将min_gram设置为2并将max_gram设置为6，则会获得以下标记：1sp, spa, spag, spagh, spaghe您可以看到每个标记都是从边缘构建的。 这有助于搜索共享相同前缀的单词而无需实际执行前缀查询。 如果你需要从一个单词的后面构建ngrams，你可以使用side属性从后面而不是默认前面获取边缘。Ngram 设置当你不知道语言是什么时，Ngrams是分析文本的好方法，因为它们可以分析单词之间没有空格的语言。 使用min和max grams配置edge ngram analyzer的示例如下所示:1234567891011121314151617181920212223PUT my_index&#123; &quot;settings&quot;: &#123; &quot;analysis&quot;: &#123; &quot;analyzer&quot;: &#123; &quot;my_analyzer&quot;: &#123; &quot;tokenizer&quot;: &quot;my_tokenizer&quot; &#125; &#125;, &quot;tokenizer&quot;: &#123; &quot;my_tokenizer&quot;: &#123; &quot;type&quot;: &quot;edge_ngram&quot;, &quot;min_gram&quot;: 2, &quot;max_gram&quot;: 10, &quot;token_chars&quot;: [ &quot;letter&quot;, &quot;digit&quot; ] &#125; &#125; &#125; &#125;&#125;我们可以用刚才创建的my_tokenizer来分析我们的字符串：12345POST my_index/_analyze&#123; &quot;analyzer&quot;: &quot;my_analyzer&quot;, &quot;text&quot;: &quot;2 Quick Foxes.&quot;&#125;显示的结果是：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960&#123; &quot;tokens&quot; : [ &#123; &quot;token&quot; : &quot;Qu&quot;, &quot;start_offset&quot; : 2, &quot;end_offset&quot; : 4, &quot;type&quot; : &quot;word&quot;, &quot;position&quot; : 0 &#125;, &#123; &quot;token&quot; : &quot;Qui&quot;, &quot;start_offset&quot; : 2, &quot;end_offset&quot; : 5, &quot;type&quot; : &quot;word&quot;, &quot;position&quot; : 1 &#125;, &#123; &quot;token&quot; : &quot;Quic&quot;, &quot;start_offset&quot; : 2, &quot;end_offset&quot; : 6, &quot;type&quot; : &quot;word&quot;, &quot;position&quot; : 2 &#125;, &#123; &quot;token&quot; : &quot;Quick&quot;, &quot;start_offset&quot; : 2, &quot;end_offset&quot; : 7, &quot;type&quot; : &quot;word&quot;, &quot;position&quot; : 3 &#125;, &#123; &quot;token&quot; : &quot;Fo&quot;, &quot;start_offset&quot; : 8, &quot;end_offset&quot; : 10, &quot;type&quot; : &quot;word&quot;, &quot;position&quot; : 4 &#125;, &#123; &quot;token&quot; : &quot;Fox&quot;, &quot;start_offset&quot; : 8, &quot;end_offset&quot; : 11, &quot;type&quot; : &quot;word&quot;, &quot;position&quot; : 5 &#125;, &#123; &quot;token&quot; : &quot;Foxe&quot;, &quot;start_offset&quot; : 8, &quot;end_offset&quot; : 12, &quot;type&quot; : &quot;word&quot;, &quot;position&quot; : 6 &#125;, &#123; &quot;token&quot; : &quot;Foxes&quot;, &quot;start_offset&quot; : 8, &quot;end_offset&quot; : 13, &quot;type&quot; : &quot;word&quot;, &quot;position&quot; : 7 &#125; ]&#125;因为我们定义的min_gram是2，所以生成的token的长度是从2开始的。通常我们建议在索引时和搜索时使用相同的分析器。 在edge_ngram tokenizer的情况下，建议是不同的。 仅在索引时使用edge_ngram标记生成器才有意义，以确保部分单词可用于索引中的匹配。 在搜索时，只需搜索用户输入的术语，例如：Quick Fo。下面是如何为搜索类型设置字段的示例：12345678910111213141516171819202122232425262728293031323334353637PUT my_index&#123; &quot;settings&quot;: &#123; &quot;analysis&quot;: &#123; &quot;analyzer&quot;: &#123; &quot;autocomplete&quot;: &#123; &quot;tokenizer&quot;: &quot;autocomplete&quot;, &quot;filter&quot;: [ &quot;lowercase&quot; ] &#125;, &quot;autocomplete_search&quot;: &#123; &quot;tokenizer&quot;: &quot;lowercase&quot; &#125; &#125;, &quot;tokenizer&quot;: &#123; &quot;autocomplete&quot;: &#123; &quot;type&quot;: &quot;edge_ngram&quot;, &quot;min_gram&quot;: 2, &quot;max_gram&quot;: 10, &quot;token_chars&quot;: [ &quot;letter&quot; ] &#125; &#125; &#125; &#125;, &quot;mappings&quot;: &#123; &quot;properties&quot;: &#123; &quot;title&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;analyzer&quot;: &quot;autocomplete&quot;, &quot;search_analyzer&quot;: &quot;autocomplete_search&quot; &#125; &#125; &#125;&#125;在我们的例子中，我们索引时和搜索时时用了两个不同的analyzer：autocomplete及autocomplete_search。123456PUT my_index/_doc/1&#123; &quot;title&quot;: &quot;Quick Foxes&quot; &#125; POST my_index/_refresh上面我们加入一个文档。下面我们来进行搜索：1234567891011GET my_index/_search&#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;title&quot;: &#123; &quot;query&quot;: &quot;Quick Fo&quot;, &quot;operator&quot;: &quot;and&quot; &#125; &#125; &#125;&#125;显示结果：12345678910111213141516171819202122232425262728&#123; &quot;took&quot; : 3, &quot;timed_out&quot; : false, &quot;_shards&quot; : &#123; &quot;total&quot; : 1, &quot;successful&quot; : 1, &quot;skipped&quot; : 0, &quot;failed&quot; : 0 &#125;, &quot;hits&quot; : &#123; &quot;total&quot; : &#123; &quot;value&quot; : 1, &quot;relation&quot; : &quot;eq&quot; &#125;, &quot;max_score&quot; : 0.5753642, &quot;hits&quot; : [ &#123; &quot;_index&quot; : &quot;my_index&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;1&quot;, &quot;_score&quot; : 0.5753642, &quot;_source&quot; : &#123; &quot;title&quot; : &quot;Quick Foxes&quot; &#125; &#125; ] &#125;&#125;在这里autocomplete analyzer可以把字符串“Quick Foxes”分解为[qu, qui, quic, quick, fo, fox, foxe, foxes]。而自autocomplete_search analyzer搜索条目[quick，fo]，两者都出现在索引中。当然我们也可以做如下的搜索：12345678910GET my_index/_search&#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;title&quot;: &#123; &quot;query&quot;: &quot;Fo&quot; &#125; &#125; &#125;&#125;显示的和上面一样的结果。Shingles与ngrams和edge ngrams一样，有一个称为shingle的过滤器（不，不是疾病的那个shingle！）。 Shingle token过滤器基本上是token级别的ngrams而不是字符级别。想想我们最喜欢的单词“spaghetti”。使用最小和最大设置为1和3的ngrams，Elasticsearch将生成标记s，sp，spa，p，pa，pag，a，ag等。 一个shingle过滤器在token级别执行此操作，因此如果您有文本“foo bar baz”并再次使用in_shingle_size为2且max_shingle_size为3，则您将生成以下token：1foo, foo bar, foo bar baz, bar, bar baz, baz为什么仍然包含单token输出？ 这是因为默认情况下，shingle过滤器包含原始token，因此原始标记生成令牌foo，bar和baz，然后将其传递给shingle token过滤器，生成标记foo bar，foo bar baz和bar baz。 所有这些token组合在一起形成最终token流。 您可以通过将output_unigrams选项设置为false来禁用此行为，也即不需要最原始的token：foo, bar及baz下一个清单显示了shingle token过滤器的示例; 请注意，min_shingle_size选项必须大于或等于2。123456789101112131415161718192021222324PUT my_index&#123; &quot;settings&quot;: &#123; &quot;analysis&quot;: &#123; &quot;analyzer&quot;: &#123; &quot;shingle&quot;: &#123; &quot;type&quot;: &quot;custom&quot;, &quot;tokenizer&quot;: &quot;standard&quot;, &quot;filter&quot;: [ &quot;shingle-filter&quot; ] &#125; &#125;, &quot;filter&quot;: &#123; &quot;shingle-filter&quot;: &#123; &quot;type&quot;: &quot;shingle&quot;, &quot;min_shingle_size&quot;: 2, &quot;max_shingle_size&quot;: 3, &quot;output_unigrams&quot;: false &#125; &#125; &#125; &#125;&#125;在这里，我们定义了一个叫做shingle-filter的过滤器。最小的shangle大小是2，最大的shingle大小是3。同时我们设置output_unigrams为false，这样最初的那些token将不被包含在最终的结果之中。下面我们来做一个例子，看看显示的结果：12345GET /my_index/_analyze&#123; &quot;text&quot;: &quot;foo bar baz&quot;, &quot;analyzer&quot;: &quot;shingle&quot;&#125;显示的结果为：1234567891011121314151617181920212223242526&#123; &quot;tokens&quot; : [ &#123; &quot;token&quot; : &quot;foo bar&quot;, &quot;start_offset&quot; : 0, &quot;end_offset&quot; : 7, &quot;type&quot; : &quot;shingle&quot;, &quot;position&quot; : 0 &#125;, &#123; &quot;token&quot; : &quot;foo bar baz&quot;, &quot;start_offset&quot; : 0, &quot;end_offset&quot; : 11, &quot;type&quot; : &quot;shingle&quot;, &quot;position&quot; : 0, &quot;positionLength&quot; : 2 &#125;, &#123; &quot;token&quot; : &quot;bar baz&quot;, &quot;start_offset&quot; : 4, &quot;end_offset&quot; : 11, &quot;type&quot; : &quot;shingle&quot;, &quot;position&quot; : 1 &#125; ]&#125;参考：【1】 https://www.elastic.co/guide/en/elasticsearch/reference/7.3/analysis-edgengram-tokenizer.html]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2020%2F01%2F02%2FElasticsearch%20nested%E5%AF%B9%E8%B1%A1%2F</url>
    <content type="text"><![CDATA[在处理大量数据时，关系数据库存在很多问题。 无论是速度，高效处理，有效并行化，可扩展性还是成本，当数据量开始增长时，关系数据库都会失败。该关系数据库的另一个挑战是必须预先定义关系和模式。Elasticsearch也是一个NoSQL文档数据存储。 但是，尽管是一个NoSQL数据存储，Elasticsearch在一定程度上提供了很多帮助来管理关系数据。 它支持类似SQL的连接，并且在嵌套和相关的数据实体上工作得非常棒。比如，对于一个像下面的blog形式的文档：一个blog可能对应于很多的comments，或者一个员工对应于很多的经验。这种数据就是关系数据。使用Elasticsearch，您可以通过保留轻松地工作与不同实体的关联以及强大的全文搜索和分析。 Elasticsearch通过引入两种类型的文档关系模型使这成为可能：nested 关系: 在这种关系中，这种一对多的关系存在于同一个文档之中parent-child 关系：在这种关系中，它们存在于不同的文档之中。这两种关系在同一个模式下工作，即一对多个的关系。一个root或parent可以有一个及多个子object。如上图所示，在嵌套关系中，有一个根对象，它是我们拥有的主文档，它包含一个称为嵌套文档的子文档数组。 根对象内的文档嵌套级别没有限制。 例如，查看以下JSON以进行多级嵌套：1234567891011121314151617 &#123; &quot;location_id&quot;: &quot;axdbyu&quot;, &quot;location_name&quot;: &quot;gurgaon&quot;, &quot;company&quot;: [ &#123; &quot;name&quot;: &quot;honda&quot;, &quot;modelName&quot;: [ &#123; &quot;name&quot;: &quot;honda cr-v&quot;, &quot;price&quot;: &quot;2 million&quot; &#125; ]&#125;, &#123; &quot;name&quot;: &quot;bmw&quot;, &quot;modelName&quot;: [ &#123; &quot;name&quot;: &quot;BMW 3 Series&quot;, &quot;price&quot;: &quot;2 million&quot;&#125;, &#123; &quot;name&quot;: &quot;BMW 1 Series&quot;, &quot;price&quot;: &quot;3 million&quot; &#125; ] &#125; ]&#125;下面，我们来做一个例子来展示一下为什么nested对象可以解决我们的问题。Object数据类型我们首先创建一个叫做developer的index，并输入如下的两个数据：12345678910111213141516171819202122232425POST developer/_doc/101&#123; &quot;name&quot;: &quot;zhang san&quot;, &quot;skills&quot;: [ &#123; &quot;language&quot;: &quot;ruby&quot;, &quot;level&quot;: &quot;expert&quot; &#125;, &#123; &quot;language&quot;: &quot;javascript&quot;, &quot;level&quot;: &quot;beginner&quot; &#125; ]&#125; POST developer/_doc/102&#123; &quot;name&quot;: &quot;li si&quot;, &quot;skills&quot;: [ &#123; &quot;language&quot;: &quot;ruby&quot;, &quot;level&quot;: &quot;beginner&quot; &#125; ]&#125;上面显示是一对多的一个index。Object Query这个时候我们想搜一个skills: language是ruby，并且level是biginner的文档。我们可能想到的方法是：12345678910111213141516171819GET developer/_search&#123; &quot;query&quot;: &#123; &quot;bool&quot;: &#123; &quot;filter&quot;: [ &#123; &quot;match&quot;: &#123; &quot;skills.language&quot;: &quot;ruby&quot; &#125; &#125;, &#123; &quot;match&quot;: &#123; &quot;skills.level&quot;: &quot;beginner&quot; &#125; &#125; ] &#125; &#125;&#125;通过上面的搜寻，我们得到的结果是：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253&#123; &quot;took&quot; : 0, &quot;timed_out&quot; : false, &quot;_shards&quot; : &#123; &quot;total&quot; : 1, &quot;successful&quot; : 1, &quot;skipped&quot; : 0, &quot;failed&quot; : 0 &#125;, &quot;hits&quot; : &#123; &quot;total&quot; : &#123; &quot;value&quot; : 2, &quot;relation&quot; : &quot;eq&quot; &#125;, &quot;max_score&quot; : 0.0, &quot;hits&quot; : [ &#123; &quot;_index&quot; : &quot;developer&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;101&quot;, &quot;_score&quot; : 0.0, &quot;_source&quot; : &#123; &quot;name&quot; : &quot;zhang san&quot;, &quot;skills&quot; : [ &#123; &quot;language&quot; : &quot;ruby&quot;, &quot;level&quot; : &quot;expert&quot; &#125;, &#123; &quot;language&quot; : &quot;javascript&quot;, &quot;level&quot; : &quot;beginner&quot; &#125; ] &#125; &#125;, &#123; &quot;_index&quot; : &quot;developer&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;102&quot;, &quot;_score&quot; : 0.0, &quot;_source&quot; : &#123; &quot;name&quot; : &quot;li si&quot;, &quot;skills&quot; : [ &#123; &quot;language&quot; : &quot;ruby&quot;, &quot;level&quot; : &quot;beginner&quot; &#125; ] &#125; &#125; ] &#125;&#125;我们可以看到，我们得到两个结果。但是我们仔细查看一下发现得到的结果并不是我们想得到的。从我们的原意来说，我们想得到的是li si，因为只有li si这个人的language是ruby，并且他的level是biginner。zhang san这个文档，应该不在搜寻之列。这是为什么呢？原来，langauge及level是skills的JSON内部数组项。当JSON对象被Lucene扁平化后，我们失去了language和level之间的对应关系。取而代之的是如下的这种关系：12345&#123; &quot;name&quot;: &quot;zhang san&quot;, &quot;skills.language&quot; :[&quot;ruby&quot;, &quot;javascript&quot;], &quot;skills.level&quot;: [&quot;expert&quot;, &quot;beginner&quot;]&#125;如上所示，我们看到的是一个扁平化的数组。之前的那种language和level之间的那种对应关系已经不存在了。Object aggregation同样的问题也存在于aggregation中，比如我们想做一下的aggregation:12345678910111213141516GET developer/_search&#123; &quot;size&quot;: 0, &quot;aggs&quot;: &#123; &quot;languages&quot;: &#123; &quot;terms&quot;: &#123; &quot;field&quot;: &quot;skills.language.keyword&quot; &#125;, &quot;aggs&quot;: &#123; &quot;level&quot;: &#123; &quot;terms&quot;: &#123;&quot;field&quot;: &quot;skills.level.keyword&quot;&#125; &#125; &#125; &#125; &#125;&#125;显示的结果是：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162&#123; &quot;took&quot; : 2, &quot;timed_out&quot; : false, &quot;_shards&quot; : &#123; &quot;total&quot; : 1, &quot;successful&quot; : 1, &quot;skipped&quot; : 0, &quot;failed&quot; : 0 &#125;, &quot;hits&quot; : &#123; &quot;total&quot; : &#123; &quot;value&quot; : 2, &quot;relation&quot; : &quot;eq&quot; &#125;, &quot;max_score&quot; : null, &quot;hits&quot; : [ ] &#125;, &quot;aggregations&quot; : &#123; &quot;languages&quot; : &#123; &quot;doc_count_error_upper_bound&quot; : 0, &quot;sum_other_doc_count&quot; : 0, &quot;buckets&quot; : [ &#123; &quot;key&quot; : &quot;ruby&quot;, &quot;doc_count&quot; : 2, &quot;level&quot; : &#123; &quot;doc_count_error_upper_bound&quot; : 0, &quot;sum_other_doc_count&quot; : 0, &quot;buckets&quot; : [ &#123; &quot;key&quot; : &quot;beginner&quot;, &quot;doc_count&quot; : 2 &#125;, &#123; &quot;key&quot; : &quot;expert&quot;, &quot;doc_count&quot; : 1 &#125; ] &#125; &#125;, &#123; &quot;key&quot; : &quot;javascript&quot;, &quot;doc_count&quot; : 1, &quot;level&quot; : &#123; &quot;doc_count_error_upper_bound&quot; : 0, &quot;sum_other_doc_count&quot; : 0, &quot;buckets&quot; : [ &#123; &quot;key&quot; : &quot;beginner&quot;, &quot;doc_count&quot; : 1 &#125;, &#123; &quot;key&quot; : &quot;expert&quot;, &quot;doc_count&quot; : 1 &#125; ] &#125; &#125; ] &#125; &#125;&#125;显然，对于key javascript来说，它并没有expert对应的level，但是在我们的aggregation里显示出来了。这个结果显然是错误的。nested 数据类型nested数据类型能够让我们对object数组建立索引，并且分别进行查询。如果需要维护数组中每个对象的关系，请使用nested数据类型为了能够把我们的数据定义为nested，我们必须修改之前的索引mapping为：1234567891011121314151617181920212223DELETE developer PUT developer&#123; &quot;mappings&quot;: &#123; &quot;properties&quot;: &#123; &quot;name&quot;: &#123; &quot;type&quot;: &quot;text&quot; &#125;, &quot;skills&quot;: &#123; &quot;type&quot;: &quot;nested&quot;, &quot;properties&quot;: &#123; &quot;language&quot;: &#123; &quot;type&quot;: &quot;keyword&quot; &#125;, &quot;level&quot;: &#123; &quot;type&quot;: &quot;keyword&quot; &#125; &#125; &#125; &#125; &#125;&#125;经过这样的改造之后，重新把我们之前的数据输入到index里：12345678910111213141516171819202122232425POST developer/_doc/101&#123; &quot;name&quot;: &quot;zhang san&quot;, &quot;skills&quot;: [ &#123; &quot;language&quot;: &quot;ruby&quot;, &quot;level&quot;: &quot;expert&quot; &#125;, &#123; &quot;language&quot;: &quot;javascript&quot;, &quot;level&quot;: &quot;beginner&quot; &#125; ]&#125; POST developer/_doc/102&#123; &quot;name&quot;: &quot;li si&quot;, &quot;skills&quot;: [ &#123; &quot;language&quot;: &quot;ruby&quot;, &quot;level&quot;: &quot;beginner&quot; &#125; ]&#125;针对101，在Lucence中的数据结构变为：1234567891011&#123; &quot;name&quot;: &quot;zhang san&quot;, &#123; &quot;skills.language&quot;: &quot;ruby&quot;, &quot;skills.level&quot;: &quot;expert&quot; &#125;, &#123; &quot;skills.language&quot;: &quot;javascript&quot;, &quot;skills.level&quot;, &quot;beginner&quot; &#125;&#125;nested query我们来重新做我们之前的搜索：123456789101112131415161718192021222324GET developer/_search&#123; &quot;query&quot;: &#123; &quot;nested&quot;: &#123; &quot;path&quot;: &quot;skills&quot;, &quot;query&quot;: &#123; &quot;bool&quot;: &#123; &quot;filter&quot;: [ &#123; &quot;match&quot;: &#123; &quot;skills.language&quot;: &quot;ruby&quot; &#125; &#125;, &#123; &quot;match&quot;: &#123; &quot;skills.level&quot;: &quot;beginner&quot; &#125; &#125; ] &#125; &#125; &#125; &#125;&#125;注意上面的“nested”字段。显示的结果是：12345678910111213141516171819202122232425262728293031323334&#123; &quot;took&quot; : 5, &quot;timed_out&quot; : false, &quot;_shards&quot; : &#123; &quot;total&quot; : 1, &quot;successful&quot; : 1, &quot;skipped&quot; : 0, &quot;failed&quot; : 0 &#125;, &quot;hits&quot; : &#123; &quot;total&quot; : &#123; &quot;value&quot; : 1, &quot;relation&quot; : &quot;eq&quot; &#125;, &quot;max_score&quot; : 0.0, &quot;hits&quot; : [ &#123; &quot;_index&quot; : &quot;developer&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;102&quot;, &quot;_score&quot; : 0.0, &quot;_source&quot; : &#123; &quot;name&quot; : &quot;li si&quot;, &quot;skills&quot; : [ &#123; &quot;language&quot; : &quot;ruby&quot;, &quot;level&quot; : &quot;beginner&quot; &#125; ] &#125; &#125; ] &#125;&#125;显然，我们只得到了一个我们想要的结果。nested aggregation同样，我们可以对我们的index来做一个aggregation:12345678910111213141516171819202122232425GET developer/_search&#123; &quot;size&quot;: 0, &quot;aggs&quot;: &#123; &quot;nested_skills&quot;: &#123; &quot;nested&quot;: &#123; &quot;path&quot;: &quot;skills&quot; &#125;, &quot;aggs&quot;: &#123; &quot;languages&quot;: &#123; &quot;terms&quot;: &#123; &quot;field&quot;: &quot;skills.language&quot; &#125;, &quot;aggs&quot;: &#123; &quot;levels&quot;: &#123; &quot;terms&quot;: &#123; &quot;field&quot;: &quot;skills.level&quot; &#125; &#125; &#125; &#125; &#125; &#125; &#125;&#125;显示的结果是：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061&#123; &quot;took&quot; : 1, &quot;timed_out&quot; : false, &quot;_shards&quot; : &#123; &quot;total&quot; : 1, &quot;successful&quot; : 1, &quot;skipped&quot; : 0, &quot;failed&quot; : 0 &#125;, &quot;hits&quot; : &#123; &quot;total&quot; : &#123; &quot;value&quot; : 2, &quot;relation&quot; : &quot;eq&quot; &#125;, &quot;max_score&quot; : null, &quot;hits&quot; : [ ] &#125;, &quot;aggregations&quot; : &#123; &quot;nested_skills&quot; : &#123; &quot;doc_count&quot; : 3, &quot;languages&quot; : &#123; &quot;doc_count_error_upper_bound&quot; : 0, &quot;sum_other_doc_count&quot; : 0, &quot;buckets&quot; : [ &#123; &quot;key&quot; : &quot;ruby&quot;, &quot;doc_count&quot; : 2, &quot;levels&quot; : &#123; &quot;doc_count_error_upper_bound&quot; : 0, &quot;sum_other_doc_count&quot; : 0, &quot;buckets&quot; : [ &#123; &quot;key&quot; : &quot;beginner&quot;, &quot;doc_count&quot; : 1 &#125;, &#123; &quot;key&quot; : &quot;expert&quot;, &quot;doc_count&quot; : 1 &#125; ] &#125; &#125;, &#123; &quot;key&quot; : &quot;javascript&quot;, &quot;doc_count&quot; : 1, &quot;levels&quot; : &#123; &quot;doc_count_error_upper_bound&quot; : 0, &quot;sum_other_doc_count&quot; : 0, &quot;buckets&quot; : [ &#123; &quot;key&quot; : &quot;beginner&quot;, &quot;doc_count&quot; : 1 &#125; ] &#125; &#125; ] &#125; &#125; &#125;&#125;从上面显示的结果，可以看出来对于ruby来说，它分别对应于一个bigginer及一个expert。这个和我们之前的数据是一样的。对于javascript来说，它只有一个beginner的level。]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2020%2F01%2F02%2FElasticsearch%20Index%20template%2F</url>
    <content type="text"><![CDATA[Index template定义在创建新index时可以自动应用的settings和mappings。 Elasticsearch根据与index名称匹配的index模式将模板应用于新索引。这个对于我们想创建的一系列的Index具有同样的settings及mappings。比如我们希望每一天/月的日志的index都具有同样的设置。Index template仅在index创建期间应用。 对index template的更改不会影响现有索引。 create index API请求中指定的设置和映射会覆盖索引模板中指定的任何设置或映射。你可以在代码中加入像C语言那样的block注释。你可以把这个注释放在出来开头 “{”和结尾的“}”之间的任何地方。定义一个Index template我们可以使用如下的接口来定义一个index template：1PUT /_template/&lt;index-template&gt;我们可以使用_template这个终点来创建，删除，查看一个index template。下面，我们来举一个例子：12345678910111213141516PUT _template/logs_template&#123; &quot;index_patterns&quot;: &quot;logs-*&quot;, &quot;order&quot;: 1, &quot;settings&quot;: &#123; &quot;number_of_shards&quot;: 4, &quot;number_of_replicas&quot;: 1 &#125;, &quot;mappings&quot;: &#123; &quot;properties&quot;: &#123; &quot;@timestamp&quot;: &#123; &quot;type&quot;: &quot;date&quot; &#125; &#125; &#125;&#125;在上面，我们可以看到，我们定义了一个叫做logs_template的index template。它的index_patterns定义为“logs-*”，说明，任何以“logs-”为开头的任何一个index将具有在该template里具有的settings及mappings属性。这里的“order”的意思是：如果索引与多个模板匹配，则Elasticsearch应用此模板的顺序。该值为1，表明有最先合并，如果有更高order的template，这个settings或mappings有可能被其它的template所覆盖。下面，我们来测试一下我们刚定义的index template：1PUT logs-2019-03-01在这里，我们创建了一个叫做logs-2019-03-01的index。我们使用如下的命令来检查被创建的情况：1GET logs-2019-03-01显示的结果为：123456789101112131415161718192021222324&#123; &quot;logs-2019-03-01&quot; : &#123; &quot;aliases&quot; : &#123; &#125;, &quot;mappings&quot; : &#123; &quot;properties&quot; : &#123; &quot;@timestamp&quot; : &#123; &quot;type&quot; : &quot;date&quot; &#125; &#125; &#125;, &quot;settings&quot; : &#123; &quot;index&quot; : &#123; &quot;creation_date&quot; : &quot;1567652671032&quot;, &quot;number_of_shards&quot; : &quot;4&quot;, &quot;number_of_replicas&quot; : &quot;1&quot;, &quot;uuid&quot; : &quot;Dz5rqRS4SEyLM_gf5eEolQ&quot;, &quot;version&quot; : &#123; &quot;created&quot; : &quot;7030099&quot; &#125;, &quot;provided_name&quot; : &quot;logs-2019-03-01&quot; &#125; &#125; &#125;&#125;证如上所示，我们已经成功创建了一个我们想要的index，并且它具有我们之前定义的settings及mappings。Index template和alias我们甚至可以为我们的index template添加index alias：12345678910111213141516171819PUT _template/logs_template&#123; &quot;index_patterns&quot;: &quot;logs-*&quot;, &quot;order&quot;: 1, &quot;settings&quot;: &#123; &quot;number_of_shards&quot;: 4, &quot;number_of_replicas&quot;: 1 &#125;, &quot;mappings&quot;: &#123; &quot;properties&quot;: &#123; &quot;@timestamp&quot;: &#123; &quot;type&quot;: &quot;date&quot; &#125; &#125; &#125;, &quot;aliases&quot;: &#123; &quot;&#123;index&#125;-alias&quot; : &#123;&#125; &#125;&#125;在上面，我们已经创立了一个叫做{index}-alias的别名。这里的{index}就是实际生成index的文件名来代替。我们下面用一个例子来说明：1PUT logs-2019-04-01我们创建一个叫做logs-2019-04-01的index, 那么它同时生成了一个叫做logs-2019-04-01-alias的别名。我们可以通过如下的命令来检查：1GET logs-2019-04-01-alias显示的结果是：1234567891011121314151617181920212223242526&#123; &quot;logs-2019-04-01&quot; : &#123; &quot;aliases&quot; : &#123; &quot;logs-2019-04-01-alias&quot; : &#123; &#125; &#125;, &quot;mappings&quot; : &#123; &quot;properties&quot; : &#123; &quot;@timestamp&quot; : &#123; &quot;type&quot; : &quot;date&quot; &#125; &#125; &#125;, &quot;settings&quot; : &#123; &quot;index&quot; : &#123; &quot;creation_date&quot; : &quot;1567653644605&quot;, &quot;number_of_shards&quot; : &quot;4&quot;, &quot;number_of_replicas&quot; : &quot;1&quot;, &quot;uuid&quot; : &quot;iLf-j_G2T4CYcHCqwz32Ng&quot;, &quot;version&quot; : &#123; &quot;created&quot; : &quot;7030099&quot; &#125;, &quot;provided_name&quot; : &quot;logs-2019-04-01&quot; &#125; &#125; &#125;&#125;Index匹配多个template多个索引模板可能与索引匹配，在这种情况下，设置和映射都合并到索引的最终配置中。 可以使用order参数控制合并的顺序，首先应用较低的顺序，并且覆盖它们的较高顺序。 例如：1234567891011121314151617181920212223PUT /_template/template_1&#123; &quot;index_patterns&quot; : [&quot;*&quot;], &quot;order&quot; : 0, &quot;settings&quot; : &#123; &quot;number_of_shards&quot; : 1 &#125;, &quot;mappings&quot; : &#123; &quot;_source&quot; : &#123; &quot;enabled&quot; : false &#125; &#125;&#125; PUT /_template/template_2&#123; &quot;index_patterns&quot; : [&quot;te*&quot;], &quot;order&quot; : 1, &quot;settings&quot; : &#123; &quot;number_of_shards&quot; : 1 &#125;, &quot;mappings&quot; : &#123; &quot;_source&quot; : &#123; &quot;enabled&quot; : true &#125; &#125;&#125;以上的template_1将禁用存储_source，但对于以te *开头的索引，仍将启用_source。 注意，对于映射，合并是“深度”的，这意味着可以在高阶模板上轻松添加/覆盖特定的基于对象/属性的映射，而较低阶模板提供基础。我们可以来创建一个例子看看：123PUT test10 GET test10显示的结果是：123456789101112131415161718&#123; &quot;test10&quot; : &#123; &quot;aliases&quot; : &#123; &#125;, &quot;mappings&quot; : &#123; &#125;, &quot;settings&quot; : &#123; &quot;index&quot; : &#123; &quot;creation_date&quot; : &quot;1567654333181&quot;, &quot;number_of_shards&quot; : &quot;1&quot;, &quot;number_of_replicas&quot; : &quot;1&quot;, &quot;uuid&quot; : &quot;iEwaQFl9RAKyTt79PduN-Q&quot;, &quot;version&quot; : &#123; &quot;created&quot; : &quot;7030099&quot; &#125;, &quot;provided_name&quot; : &quot;test10&quot; &#125; &#125; &#125;&#125;如果我们创建另外一个不是以 “te”开头的index，我们可以看看如下的情况：12PUT my_test_indexGET my_test_index显示的结果是：12345678910111213141516171819202122&#123; &quot;my_test_index&quot; : &#123; &quot;aliases&quot; : &#123; &#125;, &quot;mappings&quot; : &#123; &quot;_source&quot; : &#123; &quot;enabled&quot; : false &#125; &#125;, &quot;settings&quot; : &#123; &quot;index&quot; : &#123; &quot;creation_date&quot; : &quot;1567654713059&quot;, &quot;number_of_shards&quot; : &quot;1&quot;, &quot;number_of_replicas&quot; : &quot;1&quot;, &quot;uuid&quot; : &quot;aSsIZMT2RyWKT44G2dF2zg&quot;, &quot;version&quot; : &#123; &quot;created&quot; : &quot;7030099&quot; &#125;, &quot;provided_name&quot; : &quot;my_test_index&quot; &#125; &#125; &#125;&#125;显然在mappings里显示source是被禁止的。如果对于两个templates来说，如果order是一样的话，我们可能陷于一种不可知论的合并状态。在实际的使用中必须避免。查询Index template接口我们可以通过如下的接口来查询已经被创建好的index template:1GET /_template/&lt;index-template&gt;比如：1GET _template/logs_template显示的结果是：123456789101112131415161718192021222324&#123; &quot;logs_template&quot; : &#123; &quot;order&quot; : 1, &quot;index_patterns&quot; : [ &quot;logs-*&quot; ], &quot;settings&quot; : &#123; &quot;index&quot; : &#123; &quot;number_of_shards&quot; : &quot;4&quot;, &quot;number_of_replicas&quot; : &quot;1&quot; &#125; &#125;, &quot;mappings&quot; : &#123; &quot;properties&quot; : &#123; &quot;@timestamp&quot; : &#123; &quot;type&quot; : &quot;date&quot; &#125; &#125; &#125;, &quot;aliases&quot; : &#123; &quot;&#123;index&#125;-alias&quot; : &#123; &#125; &#125; &#125;&#125;显示的内容就是我们之前已经创建的那个index template。你也可以通过如下的方式来同时查询多个template的情况：123GET /_template/template_1,template_2GET /_template/temp*GET /_template删除一个index template在之前的练习中，我们匹配“*”，也就是我们以后所有的创建的新的index将不存储source，这个显然不是我们所需要的。我们需要来把这个template进行删除。删除一个template的接口如下：1DELETE /_template/&lt;index-template&gt;那么针对我们的情况，我们可以使用如下的命令来删除我们不需要的template:12DELETE _template/template_1DELETE _template/template_2这样我们删除了我们刚才创建的两个templates。参考：【1】https://www.elastic.co/guide/en/elasticsearch/reference/7.4/indices-get-template.html【2】https://www.elastic.co/guide/en/elasticsearch/reference/7.4/indices-delete-template.html【3】https://www.elastic.co/guide/en/elasticsearch/reference/7.4/indices-templates.html]]></content>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch API响应的一些常用选项]]></title>
    <url>%2F2020%2F01%2F02%2FElasticsearch%20API%E5%93%8D%E5%BA%94%E7%9A%84%E4%B8%80%E4%BA%9B%E5%B8%B8%E7%94%A8%E9%80%89%E9%A1%B9%2F</url>
    <content type="text"><![CDATA[我们可以点击Elasticsearch API以获取所需的响应，但是如果要修改API响应，以便我们更改显示格式或过滤掉某些字段，然后我们可以将这些选项与查询一起应用。 有一些常见的选项可以适用于API，在下面我们来介绍一些常用的选项。准备数据我们首先使用Bulk API来把我们的文档导入到Elasticsearch中：12345678910111213POST _bulk&#123; &quot;index&quot; : &#123; &quot;_index&quot; : &quot;twitter&quot;, &quot;_id&quot;: 1&#125; &#125;&#123;&quot;user&quot;:&quot;张三&quot;,&quot;message&quot;:&quot;今儿天气不错啊，出去转转去&quot;,&quot;uid&quot;:2,&quot;city&quot;:&quot;北京&quot;,&quot;province&quot;:&quot;北京&quot;,&quot;country&quot;:&quot;中国&quot;,&quot;address&quot;:&quot;中国北京市海淀区&quot;,&quot;location&quot;:&#123;&quot;lat&quot;:&quot;39.970718&quot;,&quot;lon&quot;:&quot;116.325747&quot;&#125;, &quot;DOB&quot;:&quot;1980-12-01&quot;&#125;&#123; &quot;index&quot; : &#123; &quot;_index&quot; : &quot;twitter&quot;, &quot;_id&quot;: 2 &#125;&#125;&#123;&quot;user&quot;:&quot;老刘&quot;,&quot;message&quot;:&quot;出发，下一站云南！&quot;,&quot;uid&quot;:3,&quot;city&quot;:&quot;北京&quot;,&quot;province&quot;:&quot;北京&quot;,&quot;country&quot;:&quot;中国&quot;,&quot;address&quot;:&quot;中国北京市东城区台基厂三条3号&quot;,&quot;location&quot;:&#123;&quot;lat&quot;:&quot;39.904313&quot;,&quot;lon&quot;:&quot;116.412754&quot;&#125;, &quot;DOB&quot;:&quot;1981-12-01&quot;&#125;&#123; &quot;index&quot; : &#123; &quot;_index&quot; : &quot;twitter&quot;, &quot;_id&quot;: 3&#125; &#125;&#123;&quot;user&quot;:&quot;李四&quot;,&quot;message&quot;:&quot;happy birthday!&quot;,&quot;uid&quot;:4,&quot;city&quot;:&quot;北京&quot;,&quot;province&quot;:&quot;北京&quot;,&quot;country&quot;:&quot;中国&quot;,&quot;address&quot;:&quot;中国北京市东城区&quot;,&quot;location&quot;:&#123;&quot;lat&quot;:&quot;39.893801&quot;,&quot;lon&quot;:&quot;116.408986&quot;&#125;, &quot;DOB&quot;:&quot;1982-12-01&quot;&#125;&#123; &quot;index&quot; : &#123; &quot;_index&quot; : &quot;twitter&quot;, &quot;_id&quot;: 4&#125; &#125;&#123;&quot;user&quot;:&quot;老贾&quot;,&quot;message&quot;:&quot;123,gogogo&quot;,&quot;uid&quot;:5,&quot;city&quot;:&quot;北京&quot;,&quot;province&quot;:&quot;北京&quot;,&quot;country&quot;:&quot;中国&quot;,&quot;address&quot;:&quot;中国北京市朝阳区建国门&quot;,&quot;location&quot;:&#123;&quot;lat&quot;:&quot;39.718256&quot;,&quot;lon&quot;:&quot;116.367910&quot;&#125;, &quot;DOB&quot;:&quot;1983-12-01&quot;&#125;&#123; &quot;index&quot; : &#123; &quot;_index&quot; : &quot;twitter&quot;, &quot;_id&quot;: 5&#125; &#125;&#123;&quot;user&quot;:&quot;老王&quot;,&quot;message&quot;:&quot;Happy BirthDay My Friend!&quot;,&quot;uid&quot;:6,&quot;city&quot;:&quot;北京&quot;,&quot;province&quot;:&quot;北京&quot;,&quot;country&quot;:&quot;中国&quot;,&quot;address&quot;:&quot;中国北京市朝阳区国贸&quot;,&quot;location&quot;:&#123;&quot;lat&quot;:&quot;39.918256&quot;,&quot;lon&quot;:&quot;116.467910&quot;&#125;, &quot;DOB&quot;:&quot;1984-12-01&quot;&#125;&#123; &quot;index&quot; : &#123; &quot;_index&quot; : &quot;twitter&quot;, &quot;_id&quot;: 6&#125; &#125;&#123;&quot;user&quot;:&quot;老吴&quot;,&quot;message&quot;:&quot;好友来了都今天我生日，好友来了,什么 birthday happy 就成!&quot;,&quot;uid&quot;:7,&quot;city&quot;:&quot;上海&quot;,&quot;province&quot;:&quot;上海&quot;,&quot;country&quot;:&quot;中国&quot;,&quot;address&quot;:&quot;中国上海市闵行区&quot;,&quot;location&quot;:&#123;&quot;lat&quot;:&quot;31.175927&quot;,&quot;lon&quot;:&quot;121.383328&quot;&#125;, &quot;DOB&quot;:&quot;1985-12-01&quot;&#125;这样我们就有6个文档了。Pretty=true我们在我们的请求里加入?pretty=true可以使用这个选项使我们的显示格式更加漂亮。当我们调试我们的接口时，这个是推荐的用法。比如：1GET twitter/_doc/1?pretty=true显示结果：1234567891011121314151617181920212223&#123; &quot;_index&quot; : &quot;twitter&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;1&quot;, &quot;_version&quot; : 2, &quot;_seq_no&quot; : 6, &quot;_primary_term&quot; : 5, &quot;found&quot; : true, &quot;_source&quot; : &#123; &quot;user&quot; : &quot;张三&quot;, &quot;message&quot; : &quot;今儿天气不错啊，出去转转去&quot;, &quot;uid&quot; : 2, &quot;city&quot; : &quot;北京&quot;, &quot;province&quot; : &quot;北京&quot;, &quot;country&quot; : &quot;中国&quot;, &quot;address&quot; : &quot;中国北京市海淀区&quot;, &quot;location&quot; : &#123; &quot;lat&quot; : &quot;39.970718&quot;, &quot;lon&quot; : &quot;116.325747&quot; &#125;, &quot;DOB&quot; : &quot;1980-12-01&quot; &#125;&#125;在一般的情况下，在Kibana的Dev tools中，显示就是这样的结果。format在默认的情况先返回的结果都是以JSON格式的。对于有些情况来说，我们可能需要的结果是yml格式的，那么我们可以使用format-yaml格式来返回yaml格式的结果：1GET twitter/_doc/1?format=yaml返回结果：1234567891011121314151617181920---_index: &quot;twitter&quot;_type: &quot;_doc&quot;_id: &quot;1&quot;_version: 2_seq_no: 6_primary_term: 5found: true_source: user: &quot;张三&quot; message: &quot;今儿天气不错啊，出去转转去&quot; uid: 2 city: &quot;北京&quot; province: &quot;北京&quot; country: &quot;中国&quot; address: &quot;中国北京市海淀区&quot; location: lat: &quot;39.970718&quot; lon: &quot;116.325747&quot; DOB: &quot;1980-12-01&quot;显然这个是yaml格式的结果。human有一些人类可读的值将以某种方式返回结果让我们更容易理解。 例如，3,600,000毫秒令人困惑，但是1个小时是清楚的。 设置human=true可将结果转换为更易读的响应。假如我们想得到当前索引的配置：1GET twitter/_settings那么显示的结果是：12345678910111213141516&#123; &quot;twitter&quot; : &#123; &quot;settings&quot; : &#123; &quot;index&quot; : &#123; &quot;creation_date&quot; : &quot;1577087951094&quot;, &quot;number_of_shards&quot; : &quot;1&quot;, &quot;number_of_replicas&quot; : &quot;1&quot;, &quot;uuid&quot; : &quot;y3PqEnjBRnKPFHDPTrirkA&quot;, &quot;version&quot; : &#123; &quot;created&quot; : &quot;7050099&quot; &#125;, &quot;provided_name&quot; : &quot;twitter&quot; &#125; &#125; &#125;&#125;显然这里的create_date和created版本信息都是我们没法理解的。在这个时候如果我们加上human=true再来看看显示的结果：1GET twitter/_settings?human=true这次显示的结果是：123456789101112131415161718&#123; &quot;twitter&quot; : &#123; &quot;settings&quot; : &#123; &quot;index&quot; : &#123; &quot;creation_date_string&quot; : &quot;2019-12-23T07:59:11.094Z&quot;, &quot;number_of_shards&quot; : &quot;1&quot;, &quot;provided_name&quot; : &quot;twitter&quot;, &quot;creation_date&quot; : &quot;1577087951094&quot;, &quot;number_of_replicas&quot; : &quot;1&quot;, &quot;uuid&quot; : &quot;y3PqEnjBRnKPFHDPTrirkA&quot;, &quot;version&quot; : &#123; &quot;created_string&quot; : &quot;7.5.0&quot;, &quot;created&quot; : &quot;7050099&quot; &#125; &#125; &#125; &#125;&#125;这一次，我们可以清楚地看到creation_date_string及created_string的值了。filter_path在查询中使用filter_path参数，我们可以减少来自Elasticsearch。 它支持过滤器列表或通配符以匹配字段名称或部分字段名称。首先，我们来做一个正常的查询：1GET twitter/_search返回的结果是：123456789101112131415161718192021222324252627282930313233343536373839&#123; &quot;took&quot; : 0, &quot;timed_out&quot; : false, &quot;_shards&quot; : &#123; &quot;total&quot; : 1, &quot;successful&quot; : 1, &quot;skipped&quot; : 0, &quot;failed&quot; : 0 &#125;, &quot;hits&quot; : &#123; &quot;total&quot; : &#123; &quot;value&quot; : 6, &quot;relation&quot; : &quot;eq&quot; &#125;, &quot;max_score&quot; : 1.0, &quot;hits&quot; : [ &#123; &quot;_index&quot; : &quot;twitter&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;1&quot;, &quot;_score&quot; : 1.0, &quot;_source&quot; : &#123; &quot;user&quot; : &quot;张三&quot;, &quot;message&quot; : &quot;今儿天气不错啊，出去转转去&quot;, &quot;uid&quot; : 2, &quot;city&quot; : &quot;北京&quot;, &quot;province&quot; : &quot;北京&quot;, &quot;country&quot; : &quot;中国&quot;, &quot;address&quot; : &quot;中国北京市海淀区&quot;, &quot;location&quot; : &#123; &quot;lat&quot; : &quot;39.970718&quot;, &quot;lon&quot; : &quot;116.325747&quot; &#125;, &quot;DOB&quot; : &quot;1980-12-01&quot; &#125; &#125;, ... ]...假如我们只想返回我们想要的一些字段，那么怎么办？我们可以通过配置filter_path来进行选择，比如：1GET twitter/_search?filter_path=hits.hits._source.user, hits.hits._source.country在上面，我们通过filter_path来选择想要的user及country字段。返回的结果是：123456789101112131415161718192021222324252627282930313233343536373839404142&#123; &quot;hits&quot; : &#123; &quot;hits&quot; : [ &#123; &quot;_source&quot; : &#123; &quot;user&quot; : &quot;张三&quot;, &quot;country&quot; : &quot;中国&quot; &#125; &#125;, &#123; &quot;_source&quot; : &#123; &quot;user&quot; : &quot;老刘&quot;, &quot;country&quot; : &quot;中国&quot; &#125; &#125;, &#123; &quot;_source&quot; : &#123; &quot;user&quot; : &quot;李四&quot;, &quot;country&quot; : &quot;中国&quot; &#125; &#125;, &#123; &quot;_source&quot; : &#123; &quot;user&quot; : &quot;老贾&quot;, &quot;country&quot; : &quot;中国&quot; &#125; &#125;, &#123; &quot;_source&quot; : &#123; &quot;user&quot; : &quot;老王&quot;, &quot;country&quot; : &quot;中国&quot; &#125; &#125;, &#123; &quot;_source&quot; : &#123; &quot;user&quot; : &quot;老吴&quot;, &quot;country&quot; : &quot;中国&quot; &#125; &#125; ] &#125;&#125;flat_settings将flat_settings过滤器设置为true将以flat格式返回结果。 如果设置为假，它将以更易理解的格式返回结果，比如，正常情况下：1GET twitter/_settings返回的结果是:12345678910111213141516&#123; &quot;twitter&quot; : &#123; &quot;settings&quot; : &#123; &quot;index&quot; : &#123; &quot;creation_date&quot; : &quot;1577862721663&quot;, &quot;number_of_shards&quot; : &quot;1&quot;, &quot;number_of_replicas&quot; : &quot;1&quot;, &quot;uuid&quot; : &quot;4IwiEL23Roa8DIs2chxVTQ&quot;, &quot;version&quot; : &#123; &quot;created&quot; : &quot;7050099&quot; &#125;, &quot;provided_name&quot; : &quot;twitter&quot; &#125; &#125; &#125;&#125;如果我们设置flat_settings=true，那么：1GET twitter/_settings?flat_settings=true返回的结果是：123456789101112&#123; &quot;twitter&quot; : &#123; &quot;settings&quot; : &#123; &quot;index.creation_date&quot; : &quot;1577862721663&quot;, &quot;index.number_of_replicas&quot; : &quot;1&quot;, &quot;index.number_of_shards&quot; : &quot;1&quot;, &quot;index.provided_name&quot; : &quot;twitter&quot;, &quot;index.uuid&quot; : &quot;4IwiEL23Roa8DIs2chxVTQ&quot;, &quot;index.version.created&quot; : &quot;7050099&quot; &#125; &#125;&#125;————————————————版权声明：本文为CSDN博主「Elastic 中国社区官方博客」的原创文章，遵循 CC 4.0 BY-SA 版权协议，转载请附上原文出处链接及本声明。原文链接：https://blog.csdn.net/UbuntuTouch/article/details/103792780]]></content>
      <categories>
        <category>elk</category>
      </categories>
      <tags>
        <tag>elk</tag>
      </tags>
  </entry>
  <entry>
    <title></title>
    <url>%2F2020%2F01%2F02%2FElasticsearch%20analyzer%2F</url>
    <content type="text"><![CDATA[在今天的文章中，我们来进一步了解analyzer。 analyzer执行将输入字符流分解为token的过程，它一般发生在两个场合：在indexing的时候，也即在建立索引的时候在searching的时候，也即在搜索时，分析需要搜索的词语什么是analysis?分析是Elasticsearch在文档发送之前对文档正文执行的过程，以添加到反向索引中（inverted index）。 在将文档添加到索引之前，Elasticsearch会为每个分析的字段执行许多步骤：Character filtering (字符过滤器): 使用字符过滤器转换字符Breaking text into tokens (把文字转化为标记): 将文本分成一组一个或多个标记Token filtering：使用标记过滤器转换每个标记Token indexing：把这些标记存于index中接下来我们将更详细地讨论每个步骤，但首先让我们看一下图表中总结的整个过程。 图5.1显示了“share your experience with NoSql &amp; big data technologies”为分析的标记：share, your, experience, with, nosql, big, data，tools,及 technologies。上面所展示的是一个由character过滤器，标准的tokenizer及Token filter组成的定制analyzer。上面的这个图非常好，它很简洁地描述一个analyzer的基本组成部分，以及每个部分所需要表述的东西。每当一个文档被ingest节点纳入，它需要经历如下的步骤，才能最终把文档写入到Elasticsearch的数据库中：上面中间的那部分就叫做analyzer，即分析器。它有三个部分组成：Char Filters, Tokenizer及 Token Filter。它们的作用分别如下：Char Filter: 字符过滤器的工作是执行清除任务，例如剥离HTML标记，还有上面的把“&amp;”转换为“and”字符串Tokenizer: 下一步是将文本拆分为称为标记的术语。 这是由tokenizer完成的。 可以基于任何规则（例如空格）来完成拆分。 有关tokennizer的更多详细信息，请访问以下URL：https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-tokenizers.html。Token filter: 一旦创建了token，它们就会被传递给token filter，这些过滤器会对token进行规范化。 Token filter可以更改token，删除术语或向token添加术语。Elasticsearch已经提供了比较丰富的开箱即用analyzer。我们可以自己创建自己的token analyzer，甚至可以利用已经有的char filter，tokenizer及token filter来重新组合成一个新的analyzer，并可以对文档中的每一个字段分别定义自己的analyzer。如果大家对analyzer比较感兴趣的话，请参阅我们的网址https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-analyzers.html。在默认的情况下，standard analyzer是Elasticsearch的缺省分析器(https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-standard-analyzer.html)：没有 Char Filter使用standard tokonizer把字符串变为小写，同时有选择地删除一些stop words等。默认的情况下stop words为none，也即不过滤任何stop words。总体说来一个analyzer可以分为如下的几个部分：0个或1个以上的character filter1个tokenizer0个或1个以上的token filterAnalyze API1234GET /_analyzePOST /_analyzeGET /&lt;index&gt;/_analyzePOST /&lt;index&gt;/_analyze使用_analyze API来测试analyzer如何解析我们的字符串的，比如：12345GET /_analyze&#123; &quot;analyzer&quot;: &quot;standard&quot;, &quot;text&quot;: &quot;Quick Brown Foxes!&quot;&#125;返回结果：123456789101112131415161718192021222324 &quot;tokens&quot; : [ &#123; &quot;token&quot; : &quot;quick&quot;, &quot;start_offset&quot; : 0, &quot;end_offset&quot; : 5, &quot;type&quot; : &quot;&lt;ALPHANUM&gt;&quot;, &quot;position&quot; : 0 &#125;, &#123; &quot;token&quot; : &quot;brown&quot;, &quot;start_offset&quot; : 6, &quot;end_offset&quot; : 11, &quot;type&quot; : &quot;&lt;ALPHANUM&gt;&quot;, &quot;position&quot; : 1 &#125;, &#123; &quot;token&quot; : &quot;foxes&quot;, &quot;start_offset&quot; : 12, &quot;end_offset&quot; : 17, &quot;type&quot; : &quot;&lt;ALPHANUM&gt;&quot;, &quot;position&quot; : 2 &#125; ]&#125;在这里我们使用了standard分析器，它把我们的字符串分解为三个token，并显示它们分别的位置信息。Multi-field字符字段我们可以针对这个使用多个不同的anaylzer来提高我们的搜索：使用不同的分析器来分析同样的一个字符串，用不同的方式。我们可以使用现有的分析器俩设置一个定制的分析器。比如我们定义如下的一个mapping:1234567891011121314151617PUT multifield&#123; &quot;mappings&quot;: &#123; &quot;properties&quot;: &#123; &quot;content&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;analyzer&quot;: &quot;standard&quot;, &quot;fields&quot;: &#123; &quot;english&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;analyzer&quot;: &quot;english&quot; &#125; &#125; &#125; &#125; &#125;&#125;在这里我们定义了一个叫做multifield的index，我们可以对这个index进行分析。我们对整个field定义了一个standard分析器，同时为叫做english的字段定义了一个english的分析器，这样有利于我们删除一些stop words及运用一些同根词。我们首先来为multifield来建立一个文档：1234PUT multifield/_doc/1&#123; &quot;content&quot;: &quot;We are excited to introduce the world to X-Pack&quot;&#125;那么我们可以通过如下的方法来进行搜索：12345678GET /multifield/_search&#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;content&quot;: &quot;the&quot; &#125; &#125;&#125;我们可以看到搜索的结果：12345678910111213141516171819202122232425262728&#123; &quot;took&quot; : 0, &quot;timed_out&quot; : false, &quot;_shards&quot; : &#123; &quot;total&quot; : 1, &quot;successful&quot; : 1, &quot;skipped&quot; : 0, &quot;failed&quot; : 0 &#125;, &quot;hits&quot; : &#123; &quot;total&quot; : &#123; &quot;value&quot; : 1, &quot;relation&quot; : &quot;eq&quot; &#125;, &quot;max_score&quot; : 0.2876821, &quot;hits&quot; : [ &#123; &quot;_index&quot; : &quot;multifield&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;1&quot;, &quot;_score&quot; : 0.2876821, &quot;_source&quot; : &#123; &quot;content&quot; : &quot;We are excited to introduce the world to X-Pack&quot; &#125; &#125; ] &#125;&#125;我们可以看到搜寻的结果，但是如果我们使用如下的方法：12345678GET /multifield/_search&#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;content.english&quot;: &quot;the&quot; &#125; &#125;&#125;我们啥也看不到，这是因为“the”在english analyzer里“the”被认为是stop word，而被忽略。如何定义一个定制的分析器在这里我们主要运用现有的plugin来完成定制的分析器。对于需要开发自己的plugin的需求，不在这篇文章的范围。假如我们有一下的一个句子：12345GET _analyze&#123; &quot;text&quot;: &quot;I am so excited to go to the x-school&quot;, &quot;analyzer&quot;: &quot;standard&quot;&#125;我们可以看到这样的结果：1234567891011121314&#123; &quot;token&quot; : &quot;x&quot;, &quot;start_offset&quot; : 29, &quot;end_offset&quot; : 30, &quot;type&quot; : &quot;&lt;ALPHANUM&gt;&quot;, &quot;position&quot; : 8&#125;,&#123; &quot;token&quot; : &quot;school&quot;, &quot;start_offset&quot; : 31, &quot;end_offset&quot; : 37, &quot;type&quot; : &quot;&lt;ALPHANUM&gt;&quot;, &quot;position&quot; : 9&#125;x-school在这里被分为两个token：x 及 school。如果我们想把x-school当做一个该怎么办呢？我们可以通过设置特有的mapping来实现，比如我们有一个叫做blog的index：1234567891011121314151617181920212223242526272829303132333435PUT blogs&#123; &quot;settings&quot;: &#123; &quot;analysis&quot;: &#123; &quot;char_filter&quot;: &#123; &quot;xschool_filter&quot;: &#123; &quot;type&quot;: &quot;mapping&quot;, &quot;mappings&quot;: [ &quot;X-School =&gt; XSchool&quot; ] &#125; &#125;, &quot;analyzer&quot;: &#123; &quot;my_content_analyzer&quot;: &#123; &quot;type&quot;: &quot;custom&quot;, &quot;char_filter&quot;: [ &quot;xschool_filter&quot; ], &quot;tokenizer&quot;: &quot;standard&quot;, &quot;filter&quot;: [ &quot;lowercase&quot; ] &#125; &#125; &#125; &#125;, &quot;mappings&quot;: &#123; &quot;properties&quot;: &#123; &quot;content&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;analyzer&quot;: &quot;my_content_analyzer&quot; &#125; &#125; &#125; &#125;大家请注意在settings里的“analysis”部分，我们定义了一个称之为xschool_filter的char_filter，它可以帮我们把“x-school”转化为“XSchool”。紧接着，我们利用xschool_filter定义了一个叫做“my_content_analyzer”。它是一个定制的类型。我们定义它的char_filter， tokenizer及filter。现在我们可以利用我们刚才定义my_content_analyzer来分析我们的字符串。我们在mappings里可以看到：12345678&quot;mappings&quot;: &#123; &quot;properties&quot;: &#123; &quot;content&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;analyzer&quot;: &quot;my_content_analyzer&quot; &#125; &#125;&#125;在这里，我们使用了我们刚才在analysis里定义的my_content_analyzer分析器。我们可以通过如下的方法来测试它是否工作：12345POST blogs/_analyze&#123; &quot;text&quot;: &quot;I am so excited to go to the X-School&quot;, &quot;analyzer&quot;: &quot;my_content_analyzer&quot;&#125;我们可以看到如下的结果：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667&#123; &quot;tokens&quot; : [ &#123; &quot;token&quot; : &quot;i&quot;, &quot;start_offset&quot; : 0, &quot;end_offset&quot; : 1, &quot;type&quot; : &quot;&lt;ALPHANUM&gt;&quot;, &quot;position&quot; : 0 &#125;, &#123; &quot;token&quot; : &quot;am&quot;, &quot;start_offset&quot; : 2, &quot;end_offset&quot; : 4, &quot;type&quot; : &quot;&lt;ALPHANUM&gt;&quot;, &quot;position&quot; : 1 &#125;, &#123; &quot;token&quot; : &quot;so&quot;, &quot;start_offset&quot; : 5, &quot;end_offset&quot; : 7, &quot;type&quot; : &quot;&lt;ALPHANUM&gt;&quot;, &quot;position&quot; : 2 &#125;, &#123; &quot;token&quot; : &quot;excited&quot;, &quot;start_offset&quot; : 8, &quot;end_offset&quot; : 15, &quot;type&quot; : &quot;&lt;ALPHANUM&gt;&quot;, &quot;position&quot; : 3 &#125;, &#123; &quot;token&quot; : &quot;to&quot;, &quot;start_offset&quot; : 16, &quot;end_offset&quot; : 18, &quot;type&quot; : &quot;&lt;ALPHANUM&gt;&quot;, &quot;position&quot; : 4 &#125;, &#123; &quot;token&quot; : &quot;go&quot;, &quot;start_offset&quot; : 19, &quot;end_offset&quot; : 21, &quot;type&quot; : &quot;&lt;ALPHANUM&gt;&quot;, &quot;position&quot; : 5 &#125;, &#123; &quot;token&quot; : &quot;to&quot;, &quot;start_offset&quot; : 22, &quot;end_offset&quot; : 24, &quot;type&quot; : &quot;&lt;ALPHANUM&gt;&quot;, &quot;position&quot; : 6 &#125;, &#123; &quot;token&quot; : &quot;the&quot;, &quot;start_offset&quot; : 25, &quot;end_offset&quot; : 28, &quot;type&quot; : &quot;&lt;ALPHANUM&gt;&quot;, &quot;position&quot; : 7 &#125;, &#123; &quot;token&quot; : &quot;xschool&quot;, &quot;start_offset&quot; : 29, &quot;end_offset&quot; : 37, &quot;type&quot; : &quot;&lt;ALPHANUM&gt;&quot;, &quot;position&quot; : 8 &#125; ]&#125;在这里，我们可以看到“xschool”这个token。从上面的返回的结果来看，我们还是可以看到“the”，“to”这样的token。如果我们想去掉这些token的话，我们可以做做如下的设置：1234567891011121314151617181920212223242526272829303132333435363738394041424344DELETE blogs PUT blogs&#123; &quot;settings&quot;: &#123; &quot;analysis&quot;: &#123; &quot;char_filter&quot;: &#123; &quot;xschool_filter&quot;: &#123; &quot;type&quot;: &quot;mapping&quot;, &quot;mappings&quot;: [ &quot;X-School =&gt; XSchool&quot; ] &#125; &#125;, &quot;analyzer&quot;: &#123; &quot;my_content_analyzer&quot;: &#123; &quot;type&quot;: &quot;custom&quot;, &quot;char_filter&quot;: [ &quot;xschool_filter&quot; ], &quot;tokenizer&quot;: &quot;standard&quot;, &quot;filter&quot;: [ &quot;lowercase&quot;, &quot;my_stop&quot; ] &#125; &#125;, &quot;filter&quot;: &#123; &quot;my_stop&quot;: &#123; &quot;type&quot;: &quot;stop&quot;, &quot;stopwords&quot;: [&quot;so&quot;, &quot;to&quot;, &quot;the&quot;] &#125; &#125; &#125; &#125;, &quot;mappings&quot;: &#123; &quot;properties&quot;: &#123; &quot;content&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;analyzer&quot;: &quot;my_content_analyzer&quot; &#125; &#125; &#125;&#125;在这里，我们重新加入了一个叫做my_stop的过滤器：123456&quot;filter&quot;: &#123; &quot;my_stop&quot;: &#123; &quot;type&quot;: &quot;stop&quot;, &quot;stopwords&quot;: [&quot;so&quot;, &quot;to&quot;, &quot;the&quot;] &#125;&#125;我们在我们自己定制的分析器中也加入了my_stop。重新运行我们的分析：12345POST blogs/_analyze&#123; &quot;text&quot;: &quot;I am so excited to go to the X-School&quot;, &quot;analyzer&quot;: &quot;my_content_analyzer&quot;&#125;在上面我们把so, to及the作为stop words去掉了。重新运行我们的分析：12345POST blogs/_analyze&#123; &quot;text&quot;: &quot;I am so excited to go to the X-School&quot;, &quot;analyzer&quot;: &quot;my_content_analyzer&quot;&#125;显示的结果为：123456789101112131415161718192021222324252627282930313233343536373839&#123; &quot;tokens&quot; : [ &#123; &quot;token&quot; : &quot;i&quot;, &quot;start_offset&quot; : 0, &quot;end_offset&quot; : 1, &quot;type&quot; : &quot;&lt;ALPHANUM&gt;&quot;, &quot;position&quot; : 0 &#125;, &#123; &quot;token&quot; : &quot;am&quot;, &quot;start_offset&quot; : 2, &quot;end_offset&quot; : 4, &quot;type&quot; : &quot;&lt;ALPHANUM&gt;&quot;, &quot;position&quot; : 1 &#125;, &#123; &quot;token&quot; : &quot;excited&quot;, &quot;start_offset&quot; : 8, &quot;end_offset&quot; : 15, &quot;type&quot; : &quot;&lt;ALPHANUM&gt;&quot;, &quot;position&quot; : 3 &#125;, &#123; &quot;token&quot; : &quot;go&quot;, &quot;start_offset&quot; : 19, &quot;end_offset&quot; : 21, &quot;type&quot; : &quot;&lt;ALPHANUM&gt;&quot;, &quot;position&quot; : 5 &#125;, &#123; &quot;token&quot; : &quot;xschool&quot;, &quot;start_offset&quot; : 29, &quot;end_offset&quot; : 37, &quot;type&quot; : &quot;&lt;ALPHANUM&gt;&quot;, &quot;position&quot; : 8 &#125; ]&#125;我们可以看到so, the及to都被过滤掉了。Filter的顺序也很重要我们来试一下下面的一个例子：123456789GET _analyze&#123; &quot;tokenizer&quot;: &quot;whitespace&quot;, &quot;filter&quot;: [ &quot;lowercase&quot;, &quot;stop&quot; ], &quot;text&quot;: &quot;To Be Or Not To Be&quot;&#125;在这里我们先进行lowercase的过滤器，先变成小写字母，再进行stop过滤器，那么返回的结果是[]，也即没有。相反，如果我们使用如下的顺序：123456789GET _analyze&#123; &quot;tokenizer&quot;: &quot;whitespace&quot;, &quot;filter&quot;: [ &quot;stop&quot;, &quot;lowercase&quot; ], &quot;text&quot;: &quot;To Be Or Not To Be&quot;&#125;这里先进行stop过滤器，因为这里的词有些是大写字母，所以不被认为是stop词，那么没有被过滤掉。之后进行lowercase，显示的结果是to, be, or, not, to, be这些token。search_analyzer也许大家已经看出来了，每当一个文档在被录入到Elasticsearch中时，需要一个叫做index的过程。在Index的过程中，它会为该字符串进行分词，并最终形成一个一个的token，并存于数据库。但是，每当我们搜索一个字符串时，在搜索时，我们同样也要对该字符串进行分词，也会建立token。当然这些token不会被存放于数据库中。比如：12345678GET /chinese/_search&#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;content&quot;: &quot;Happy a birthday&quot; &#125; &#125;&#125;对于这个搜索来说，我们在默认的情况下，会把”Happy a birthday”使用同样的analyzer进行分词。如果我们的analyzer里含有stop过滤器，它极有可能把字母“a”过滤掉，那么直剩下“happy”及“birthday”这两个词，而“a”将不进入搜索之中。在实际的使用中，我们也可以通过如下的方法对搜索进行制定具体的search_analyzer。12345678910111213141516171819202122232425262728293031323334353637383940414243PUT blogs&#123; &quot;settings&quot;: &#123; &quot;analysis&quot;: &#123; &quot;char_filter&quot;: &#123; &quot;xschool_filter&quot;: &#123; &quot;type&quot;: &quot;mapping&quot;, &quot;mappings&quot;: [ &quot;X-School =&gt; XSchool&quot; ] &#125; &#125;, &quot;analyzer&quot;: &#123; &quot;my_content_analyzer&quot;: &#123; &quot;type&quot;: &quot;custom&quot;, &quot;char_filter&quot;: [ &quot;xschool_filter&quot; ], &quot;tokenizer&quot;: &quot;standard&quot;, &quot;filter&quot;: [ &quot;lowercase&quot;, &quot;my_stop&quot; ] &#125; &#125;, &quot;filter&quot;: &#123; &quot;my_stop&quot;: &#123; &quot;type&quot;: &quot;stop&quot;, &quot;stopwords&quot;: [&quot;so&quot;, &quot;to&quot;, &quot;the&quot;] &#125; &#125; &#125; &#125;, &quot;mappings&quot;: &#123; &quot;properties&quot;: &#123; &quot;content&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;analyzer&quot;: &quot;my_content_analyzer&quot;, &quot;search_analyzer&quot;: &quot;standard&quot; &#125; &#125; &#125;&#125;在上面，我们可以看到，我们分别定义了不用的analyzer：在录入文档时，我们使用了my_content_analyzer分析器，而在搜索时，我们使用了standard分析器。]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2020%2F01%2F02%2FElasticsearch%20alias%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[就像其他的很多语言一样，我们可以给已有的变量取一个别名（alias）。即便是对高级语言一样，比如我们定义不同的指针变量，指向同一个内存空间。这个有些类似别名的概念。在Elasticsearch中，我们也可以为index中的一个字段（field）取一个另外的名字：它可以用来代替搜索请求中的目标（target）字段以及其它的被选定的API中通常alias可以用来帮助我们重新命名一个字段，并让这个字段的名称符合我们的命名规则。我们可以参考ECS。通过alias的使用，可以使得我们的字段根据符合ECS标准。一个字段的alias只能有一个目标字段。在使用alias时，字段别名的目标有一些限制：它必须是一个具体的字段（不是一个对象或者是另外一个alias）它必须在alias被创建时已经存在如果是一个nested的对象，那么alias必须具有和它的目标具有同样的nested scope一个alias的应用例子下面，我们来用一个具体的例子来说说明。我们首先来定义一个index的mapping如下：1234567891011121314151617PUT trips&#123; &quot;mappings&quot;: &#123; &quot;properties&quot;: &#123; &quot;distance&quot;: &#123; &quot;type&quot;: &quot;long&quot; &#125;, &quot;route_length_miles&quot;: &#123; &quot;type&quot;: &quot;alias&quot;, &quot;path&quot;: &quot;distance&quot; &#125;, &quot;transit_mode&quot;: &#123; &quot;type&quot;: &quot;keyword&quot; &#125; &#125; &#125;&#125;现在我们输入一下的两个文档，并搜索：12345678910111213141516171819202122PUT trips/_doc/1&#123; &quot;distance&quot;: 100, &quot;transit_mode&quot;: &quot;mode1&quot;&#125; PUT trips/_doc/2&#123; &quot;distance&quot;: 50, &quot;transit_mode&quot;: &quot;mode2&quot;&#125; GET _search&#123; &quot;query&quot;: &#123; &quot;range&quot; : &#123; &quot;route_length_miles&quot; : &#123; &quot;gte&quot; : 60 &#125; &#125; &#125;&#125;显示的结果是：1234567891011121314151617181920212223242526272829&#123; &quot;took&quot; : 346, &quot;timed_out&quot; : false, &quot;_shards&quot; : &#123; &quot;total&quot; : 20, &quot;successful&quot; : 20, &quot;skipped&quot; : 0, &quot;failed&quot; : 0 &#125;, &quot;hits&quot; : &#123; &quot;total&quot; : &#123; &quot;value&quot; : 1, &quot;relation&quot; : &quot;eq&quot; &#125;, &quot;max_score&quot; : 1.0, &quot;hits&quot; : [ &#123; &quot;_index&quot; : &quot;trips&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;1&quot;, &quot;_score&quot; : 1.0, &quot;_source&quot; : &#123; &quot;distance&quot; : 100, &quot;transit_mode&quot; : &quot;mode1&quot; &#125; &#125; ] &#125;&#125;从上面可以看出来，虽然我们没有使用在source中的distance，但是我们使用它的别名route_length_miles，我们可以照样把我们的统计数据搜索出来。不被支持的API不支持写入字段别名：尝试在索引或更新请求中使用别名将导致失败。 同样，别名不能用作copy_to的目标或多字段。由于文档源中不存在别名，因此在执行源过滤时不能使用别名。 例如，以下请求将返回_source的空结果：不支持写入字段别名：尝试在索引或更新请求中使用别名将导致失败。 同样，别名不能用作copy_to的目标或多字段。由于文档源中不存在别名，因此在执行源过滤时不能使用别名。 例如，以下请求将返回_source的空结果：]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2020%2F01%2F02%2FElastic%EF%BC%9AElastic%E9%83%A8%E7%BD%B2%E6%9E%B6%E6%9E%84%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[Elastic Stack是一套完整的从数据采集，解析，分析，丰富，到搜索，检索，数据程序等一套完整的软件栈。在具体的实践中，我们应该如何搭建我们的系统呢？下图描述了常用的Elastic Stack的部署架构：该图描述了三种可能的体系结构：将操作指标直接发送到Elasticsearch：如上图所示，您将在要从其发送操作指标/日志的边缘服务器上安装各种类型的Beats，例如Metricbeat，Filebeat，Packetbeat等。 如果不需要进一步处理，那么可以将生成的事件直接传送到Elasticsearch集群。 一旦数据出现在Elasticsearch中，就可以使用Kibana对其进行可视化/分析。 在这种体系结构中，事件流将是Beats→Elasticsearch→Kibana。当然如果您需要做进一步的处理，您也可以通过ingest node的pipleline帮助实现。将操作指标发送到Logstash：Beats捕获并安装在边缘服务器上的操作指标/日志将发送到Logstash进行进一步处理，例如解析日志或丰富日志事件。 然后，已解析/丰富的事件被推送到Elasticsearch。 为了提高处理能力，您可以扩展Logstash实例，例如，通过配置一组Beats将数据发送到Logstash实例1，并配置另一组Beats将数据发送到Logstash实例2，依此类推。 在这种架构中，事件流将是Beats→Logstash→Elasticsearch→Kibana。将操作指标发送到弹性队列：如果生成的事件发生率很高，并且Logstash停机时Logstash无法应付负载或防止数据/事件丢失，则可以使用诸如以下的弹性队列 Apache Kafka，以便将事件排队。 然后，Logstash可以以自己的速度处理它们，从而避免丢失Beats捕获的操作指标/日志。 在这种体系结构中，事件流将是Beats→Kafka→Logstash→Elasticsearch→Kibana。提示：从Logstash 5.x开始，您可以使用Logstash的持久队列设置，也可以将其用作队列。 但是，它不像Kafka一样提供高度的弹性。也有一些应用场景是这样部署的：同样的，在这里，我们可以通过radis或Kafaka来提供一个弹性队列来缓冲高发生率事件。在实际的使用中，如果我们不把Elasticsearch当做唯一的数据库来存储的话，那么，我们可以采用如下的方案：在这种架构中，您有两个数据存储，必须找到一种使它们保持同步的方法。 根据您的主要数据存储区和数据布局方式，您可以部署Elasticsearch插件以使两个实体保持同步。如下的是另外一中有外部数据，物联网等的一种架构：或者一个更加全面的架构图：在Elastic的官方文档中，有更多关于部署架构的描述。详细文档：https://www.elastic.co/assets/blt2614227bb99b9878/architecture-best-practices.pdf]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2020%2F01%2F02%2FElastic%20%E5%BA%94%E7%94%A8%E7%A8%8B%E5%BA%8F%E6%80%A7%E8%83%BD%E7%9B%91%E6%8E%A7%20%E7%AE%A1%E7%90%86(APM)%E5%AE%9E%E8%B7%B5%2F</url>
    <content type="text"><![CDATA[在今天的文章里，我们将介绍Elastic的一个重要的应用：应用程序性能管理（Application Performance Monitoring/Management)，简称APM。那么到底什么是APM呢？随着时代的发展，我们的IT架构越来越复杂，比如：我们系统的服务器越来越多，而且更多的设备都部署在云端。复杂的系统甚至有成千个微服务及架构所组成，那么我们的业务请求可能需要一个或更多的服务共同来完成。那么现在的问题是，如果我们的请求变得很慢，我们想知道到底是哪个环节出现问题了呢？经验丰富的程序员或者系统设计者，可能从一些log里找到答案。可是当我们的log变得非常大的时候，而且我们的接口也越来越多时，这个时候，也许我们也无能为力。当我们在设计页面或者请求时，经常会遇到上面的这种等待的情况。可能有个别的工具能有效地解决部分的问题，但是如何能从整个的系统里来完成这种问题的定位及分析。Elastic推出的APM解决方案可以完美地解决这些问题。为我们的系统设计者或程序员提供了一个快速定位的方法。APM 到底是什么呢？我们先来看一下如下的这个图：如上图所示，在不同时刻我们请求时，我们发现为什么在17:36:30发生的一个请求需要花将近8秒的时间，而另外在17:36:30分发生的一个请求却返回一个错误的代码？Elastic APM方案是世界上第一个开源的APM 解决方案：APM记录数据库查询，外部HTTP请求以及对应用程序的请求期间发生的其他缓慢操作的跟踪很容易让程序员看到应用在运行时各个部分所花的时间它收集未处理的错误和异常很容让程序员调试错误在客户面对性能瓶颈和错误之前先定位到问题所在提高开发团队的生产力APM适用于Elastic Stack的位置APM如何把数据存于Elasticsearch中，并提供分析呢？我们看一下如下的架构图：如上图所示，我们看到一个最典型的APM架构图：我们需要架设一台专门的APM服务器，虽然也可以和Elastic Stack的其它服务器处于同一台服务器中AMP agent专门收集数据并发送数据到APM服务器中。这里的APM agents包含：APM服务器把数据发送到Elasticsearch中，并进行数据分析Kibana可以帮我们把数据进行展示并显示在Dashboard之中总体来说，APM数据仅仅是另外Elasticsearch索引。在Kibana中已经有一个现成的APM应用可以被我们所使用。我们也可以根据需求自己定制自己的Dashboard。APM可以完美地结合机器学习和告警。APM术语Service: 在apm agent配置中进行设置，以将特定的apm agent组标识为单个服务，这是一种逻辑上标识一组事务的方法Transaction: 组成一个服务的请求和响应，例如 登录api调用，每个调用由单独的span组成。Span: 事务中的单个事件，例如方法调用，数据库查询或缓存插入或检索，即需要花费时间才能完成的任何事件。Erorrs：具有匹配的异常或日志消息的异常组它们之间的关系可以用如下的图来表示：分布式tracing：例子在今天的练习中，我们将以Java Spring boot为例来展示如何使用Elastic APM。下载Spring boot代码首先，我们在terminal中打入如下的命令：1git clone https://github.com/liu-xiao-guo/elastic-apm-demo上面的一个例子是一个简单的Spring boot应用。它有一下的几个特点：它可以REST接口访问MySQL的数据库进行添加数据，请求数据它可以通过REST接口进行访问百度天气接口来获得天气数据下面是它的部分代码：123456789101112131415161718192021222324252627282930@PostMapping(path=&quot;/add&quot;) // Map ONLY POST Requestspublic @ResponseBody String addNewUser (@RequestParam String name , @RequestParam String email) &#123; // @ResponseBody means the returned String is the response, not a view name // @RequestParam means it is a parameter from the GET or POST request User n = new User(); n.setName(name); n.setEmail(email); userRepository.save(n); return &quot;Saved&quot;;&#125;@GetMapping(path=&quot;/all&quot;)public @ResponseBody Iterable&lt;User&gt; getAllUsers() &#123; // This returns a JSON or XML with the users return userRepository.findAll();&#125;@GetMapping(path=&quot;/weather&quot;)public @ResponseBody String getBaiduWeather() throws InterruptedException &#123; // Add some random delays before getting the info double delay = Math.random() * 10; System.out.println(&quot;delay: &quot; + delay); TimeUnit.SECONDS.sleep((long)delay); String weather = getWeatherInform(&quot;北京&quot;); return weather;&#125;在获得天气（weather）的接口中，我故意加入了一下随机数的延迟，这样来模拟每一次请求的时间是不同的。我们可以在应用的根目录下打入如下的命令：1./mvnw clean package这样它将会在当前目录下的target子目录下生产一个叫做accessing-data-mysql-0.0.1-SNAPSHOT.jar的文件。12$ ls ./target/accessing-data-mysql-0.0.1-SNAPSHOT.jar./target/accessing-data-mysql-0.0.1-SNAPSHOT.jar我们可以把这个文件拷入到我们想要的任何一个目录中。针对我的情况，我把它拷入到我的home目录下的data/apm目录中。1234$ pwd/Users/liuxg/data/apmliuxg-2:apm liuxg$ ls accessing-data-mysql-0.0.1-SNAPSHOT.jaraccessing-data-mysql-0.0.1-SNAPSHOT.jar安装MySQL我们可以按照文档的需求来安装我们的MySQL。我们在一个terminal中打入如下的命令：1mysql -uroot -p我们打入root用户的密码进入到MySQL之中。为了创建一个数据库，我们在MySQL的prompt中打入如下的命令：123mysql&gt; create database db_example; -- Creates the new databasemysql&gt; create user &apos;springuser&apos;@&apos;%&apos; identified by &apos;ThePassword&apos;; -- Creates the usermysql&gt; grant all on db_example.* to &apos;springuser&apos;@&apos;%&apos;; -- Gives all privileges to the new user on the newly created database上面的命令创建了一个叫做db_example的数据库。同时，它也创建了一个叫做springuser的用户及其密码ThePassword。我们可以通过Navicat工具来查看:运行Elastic Stack安装及运行我们的Elasticsearch及Kibana。我们打开我们的Kibana界面，并点击左上角的部分：然后，我们按照上面的步骤一步一步地进行安装：上面的步骤非常详细。对于APM agent的选择来讲，因为我们是Java应用，所以我们选择Java agent。我们下载相应的agent jar文件，并存放于我们上面放置spring boot的jar文件所处的文件夹。针对我的情况是home目录下的data/apm。1234$ pwd/Users/liuxg/data/apmliuxg-2:apm liuxg$ ls elastic-apm-agent-1.10.0.jar elastic-apm-agent-1.10.0.jar在这个时候，我们可以开始运行我们的Spring Java应用了。我们可以通过如下的命令来运行：123456java -javaagent:./elastic-apm-agent-1.10.0.jar \ -Delastic.apm.service_name=sample_apm \ -Delastic.apm.server_url=http://localhost:8200 \ -Delastic.apm.secret_token= \ -Delastic.apm.application_packages=accessing-data-mysql \ -jar accessing-data-mysql-0.0.1-SNAPSHOT.jar注意：这里的sample_apm是我给取的一个服务名称。你可以根据自己的需求取一个独特的名字。如果你不想这么麻烦，你可以在当前的目录下生产一个叫做elasticapm.properties的文件。它的内容如下：123service_name=sample_apmapplication_packages=accessing-data-mysqlserver_url=http://localhost:8200那么我们可以通过如下的命令来运行：123java -javaagent:./elastic-apm-agent-1.10.0.jar \ -Delastic.apm.secret_token= \ -jar accessing-data-mysql-0.0.1-SNAPSHOT.jar等我们的Spring Boot应用完全起来后，我们点击Kibana中的“Check agent status”按钮。这个时候可能显示没有任何的数据。我们可以打开我们的浏览器，并在浏览器的地址栏中输入如下的地址：我们可以看到我们得到了一下天气的数据信息。那么这个时候我们可以在Agent status中看到信息：启动APM应用如果你已经运行到这里，那么你基本上已经把整个的环境运行起来了。我们可以在terminal中打入如下的命令：1curl localhost:8080/demo/add -d name=First -d email=someemail@someemailprovider.com上面的应用是向我们的数据中写入一条记录。1curl &apos;localhost:8080/demo/all&apos;运行上面的命令可以展示已经输入的所有的记录1curl &apos;localhost:8080/demo/weather&apos;运行上面的命令可以获得百度天气API接口所带给我们的天气信息。上面的所有的信息我们都可以在浏览器中的地址栏中输入。点击Kibana中的APM应用图标：在上面我们可以看到应用的四个接口的统计情况。我们在这个APM应用的dashboard上可以看到我们所有的API的调用情况。比如：因为在我的应用中，我故意加入了一些延迟，所以导致我们的整个getBaiduWeather的请求时间为9.157秒才完成，而api.map.baidu.com的时间只有149ms。到这里我的讲解就完成了。剩下的留给大家自己去挖掘哈！参考：【1】Accessing data with MySQL(https://spring.io/guides/gs/accessing-data-mysql/)]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2020%2F01%2F02%2FBeats%EF%BC%9A%E5%A6%82%E4%BD%95%E5%88%9B%E5%BB%BA%E4%B8%80%E4%B8%AA%E5%AE%9A%E5%88%B6%E7%9A%84Elastic%20Beat%2F</url>
    <content type="text"><![CDATA[Beats作为Elastic Stack家族中重要的部分。它可以和方便地让我们把我们的数据发送到Elasticsearch或Logstash之中。如果我们想要生成自己的Beat，请使用GitHub的beats仓库中提供的Beat生成器。在今天的文章中，我们将详细介绍如何一步一步地来创建一个我们自己想要的beat。设置自己的开发环境安装go环境Beats实际上是go程序。我们可以参照链接“Go get started”(https://golang.org/doc/install)来安装自己的golang语言开发环境。等我们安装好我们的go后，我们可以在terminal中打入如下的命令：12$ which go/usr/local/go/bin/go那么我们需要在我们的环境中设置如下的变量：123export GOROOT=/usr/local/goexport PATH=$GOPATH/bin:$GOROOT/bin:$PATHexport GOPATH=$HOME/go/beats在这里，我也设置了以GOPATH。你可以设置自己的路径。针对我的情况，我在我的home目录下创建了一个go目录，并在go目录下生产一个叫做beats的目录。在一下，我们会在这个目录里生成我们的定制的beat。下载Elastic beats源码在这一步我们下载Elastic beats的源码。在termnial中打入如下的命令：12mkdir -p $&#123;GOPATH&#125;/src/github.com/elasticgit clone https://github.com/elastic/beats $&#123;GOPATH&#125;/src/github.com/elastic/beats安装Python目前generator只对Python2适用，所以，我们需要安装Python2。我们可以参照页面https://www.python.org/downloads/进行安装我们的python2。安装virtualenv我们必须安装virtualenv才能使得generator正常工作。可以参照链接https://virtualenv.pypa.io/en/latest/installation/来进行安装。如果自己的电脑上同时已经安装了python3，那么我们需要同时设置如写变量：12345export PYTHON_EXE=&apos;python2.7&apos;export VIRTUALENV_PARAMS=&apos;-p python2.7&apos;export VIRTUALENV_PYTHON=&apos;/usr/bin/python2.7&apos; export VIRTUALENV_PYTHON=&apos;/usr/local/bin/python&apos; (for Mac)请注意：这里的python是2.x版本的python，而不是python3。我们需要保证VIRTUALENV_PYTHON指向我们的Python2的执行文件。安装mage我们需要在地址https://github.com/magefile/mage下载这个源码，并编译：123go get -u -d github.com/magefile/magecd $GOPATH/src/github.com/magefile/magego run bootstrap.go等上面的命令执行完后，我们可以在如下的目录中找到编译好的执行文件mage:12liuxg-2:bin liuxg$ ls $GOPATH/binmage创建定制beat首先创建一个目录在$GOPATH下，并进入该目录。12mkdir $&#123;GOPATH&#125;/src/github.com/&#123;user&#125;cd $&#123;GOPATH&#125;/src/github.com/&#123;user&#125;注意这里的user指的是自己在github上的用户名。比如针对我的情况是liu-xiao-guo。我打入如下写的命令：12mkdir $&#123;GOPATH&#125;/src/github.com/liu-xiao-guocd $GOPATH/src/github.com/elastic/beats/接下来，我们运行如下的命令：1mage GenerateCustomBeat执行结果：12345678910111213141516171819$ mage GenerateCustomBeat2019/11/13 15:24:01 Found Elastic Beats dir at /Users/liuxg/go/beats/src/github.com/elastic/beatsEnter the beat name [examplebeat]: CountbeatEnter your github name [your-github-name]: liu-xiao-guoEnter the beat path [github.com/liu-xiao-guo/countbeat]: Enter your full name [Firstname Lastname]: Xiaoguo LiuEnter the beat type [beat]: DEPRECATION: Python 2.7 will reach the end of its life on January 1st, 2020. Please upgrade your Python as Python 2.7 won&apos;t be maintained after that date. A future version of pip will drop support for Python 2.7. More details about Python 2 support in pip, can be found at https://pip.pypa.io/en/latest/development/release-process/#python-2-supportDEPRECATION: Python 2.7 will reach the end of its life on January 1st, 2020. Please upgrade your Python as Python 2.7 won&apos;t be maintained after that date. A future version of pip will drop support for Python 2.7. More details about Python 2 support in pip, can be found at https://pip.pypa.io/en/latest/development/release-process/#python-2-support WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by &apos;ReadTimeoutError(&quot;HTTPSConnectionPool(host=&apos;pypi.tuna.tsinghua.edu.cn&apos;, port=443): Read timed out. (read timeout=15)&quot;,)&apos;: /simple/semver/2019/11/13 15:25:50 Found Elastic Beats dir at /Users/liuxg/go/beats/src/github.com/liu-xiao-guo/countbeat/vendor/github.com/elastic/beatsGenerated fields.yml for countbeat to /Users/liuxg/go/beats/src/github.com/liu-xiao-guo/countbeat/fields.yml2019/11/13 15:25:52 Found Elastic Beats dir at /Users/liuxg/go/beats/src/github.com/liu-xiao-guo/countbeat/vendor/github.com/elastic/beatsAuto packing the repository in background for optimum performance.See &quot;git help gc&quot; for manual housekeeping.=======================Your custom beat is now available as /Users/liuxg/go/beats/src/github.com/liu-xiao-guo/countbeat=======================这样，我们基本上就生产了一个最基本的beat的框架。接下来，我们进入到我们的beat目录里，并进行编译：1cd $&#123;GOPATH&#125;/src/github.com/&#123;user&#125;/countbeat针对我的情况：1cd $&#123;GOPATH&#125;/src/github.com/liu-xiao-guo/countbeat我们可以看一下里面最基本的文件：1234567891011$ pwd/Users/liuxg/go/beats/src/github.com/liu-xiao-guo/countbeatliuxg-2:countbeat liuxg$ lsCONTRIBUTING.md cmd magefile.goLICENSE.txt config main.goMakefile countbeat.docker.yml main_test.goNOTICE.txt countbeat.reference.yml make.batREADME.md countbeat.yml tests_meta docs vendorbeater fields.ymlbuild include这里有最基本的框架文件。里面含有一个叫做countbeat.yml的配置文件及一些标准的模板文件。我们在命令行中直接打入如下的指令：1234make $ make go build -i -ldflags &quot;-X github.com/liu-xiao-guo/countbeat/vendor/github.com/elastic/beats/libbeat/version.buildTime=2019-11-13T07:33:25Z -X github.com/liu-xiao-guo/countbeat/vendor/github.com/elastic/beats/libbeat/version.commit=501bd87da668346f78398676c78b4a39394a3640&quot;经过上面的编译，我们可以发现在当前的目录下，有一个已经编译好的countbeat可执行文件：我们在当前的目录下直接运行这个可执行的文件：1./countbeat -e -d &quot;*&quot;我们可以在terminal中看到：那么在我们的Kibana中也可以看到如下信息：显然数据已经被成功上传到Elasticsearch中了。每一个文档的内容如下：1234567891011121314151617181920212223242526272829&#123; &quot;@timestamp&quot;: &quot;2019-11-13T07:38:57.095Z&quot;, &quot;agent&quot;: &#123; &quot;version&quot;: &quot;8.0.0&quot;, &quot;type&quot;: &quot;countbeat&quot;, &quot;ephemeral_id&quot;: &quot;d3f0638e-ee58-45ff-92cc-74f188fd66a4&quot;, &quot;hostname&quot;: &quot;liuxg-2.local&quot;, &quot;id&quot;: &quot;1d35220e-7f75-442a-88eb-43ec1e97f0d0&quot; &#125;, &quot;counter&quot;: 5, &quot;ecs&quot;: &#123; &quot;version&quot;: &quot;1.2.0&quot; &#125;, &quot;host&quot;: &#123; &quot;hostname&quot;: &quot;liuxg-2.local&quot;, &quot;architecture&quot;: &quot;x86_64&quot;, &quot;os&quot;: &#123; &quot;build&quot;: &quot;19B88&quot;, &quot;platform&quot;: &quot;darwin&quot;, &quot;version&quot;: &quot;10.15.1&quot;, &quot;family&quot;: &quot;darwin&quot;, &quot;name&quot;: &quot;Mac OS X&quot;, &quot;kernel&quot;: &quot;19.0.0&quot; &#125;, &quot;id&quot;: &quot;E51545F1-4BDC-5890-B194-83D23620325A&quot;, &quot;name&quot;: &quot;liuxg-2.local&quot; &#125;, &quot;type&quot;: &quot;liuxg-2.local&quot;&#125;它里面含有一个counter的整数值。所有关于beat的设计上的代码可以在目录${GOPATH}/src/github.com/liu-xiao-guo/countbeat下的/beater/CountBeat.go文件里实现的。设计比较直接。大家可以看一下代码应该可以明白。读取JSON文件beat在上面我们已经熟悉了如何去创建一个template的beat。它是一个最基本的beat，并没有什么特别的功能。在这节里，我们接着如法炮制来创建一个稍微有一点用途的beat。我们的这个beat叫做readjson beat。它的源码可以按照如下的方法得到：1git clone https://github.com/liu-xiao-guo/beats-readjson首先，我们可以准备一个我们想要的json文件，比如：123456789101112131415161718192021222324users.json &#123; &quot;users&quot;: [ &#123; &quot;name&quot;: &quot;Elliot&quot;, &quot;type&quot;: &quot;Reader&quot;, &quot;age&quot;: 23, &quot;social&quot;: &#123; &quot;facebook&quot;: &quot;https://facebook.com&quot;, &quot;twitter&quot;: &quot;https://twitter.com&quot; &#125; &#125;, &#123; &quot;name&quot;: &quot;Fraser&quot;, &quot;type&quot;: &quot;Author&quot;, &quot;age&quot;: 17, &quot;social&quot;: &#123; &quot;facebook&quot;: &quot;https://facebook.com&quot;, &quot;twitter&quot;: &quot;https://twitter.com&quot; &#125; &#125; ] &#125;我们可以把这个文件放入到我们如何喜欢的位置。针对我的情况，我把它置于我的电脑的如下位置：1/Users/liuxg/data/beats/users.json我们可以在readjson.yml文件中进行配置：1readjson.yml我们的readjson.go设计也相当简单：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135readjson.go package beater import ( &quot;fmt&quot; &quot;os&quot; &quot;io/ioutil&quot; &quot;encoding/json&quot; &quot;strconv&quot; &quot;time&quot; &quot;os/signal&quot; &quot;syscall&quot; &quot;github.com/elastic/beats/libbeat/beat&quot; &quot;github.com/elastic/beats/libbeat/common&quot; &quot;github.com/elastic/beats/libbeat/logp&quot; &quot;github.com/liu-xiao-guo/readjson/config&quot; ) type Users struct &#123; Users []User `json:&quot;users&quot;` &#125; // User struct which contains a name // a type and a list of social links type User struct &#123; Name string `json:&quot;name&quot;` Type string `json:&quot;type&quot;` Age int `json:&quot;Age&quot;` Social Social `json:&quot;social&quot;` &#125; // Social struct which contains a // list of links type Social struct &#123; Facebook string `json:&quot;facebook&quot;` Twitter string `json:&quot;twitter&quot;` &#125; // readjson configuration. type readjson struct &#123; done chan struct&#123;&#125; config config.Config client beat.Client &#125; // New creates an instance of readjson. func New(b *beat.Beat, cfg *common.Config) (beat.Beater, error) &#123; c := config.DefaultConfig if err := cfg.Unpack(&amp;c); err != nil &#123; return nil, fmt.Errorf(&quot;Error reading config file: %v&quot;, err) &#125; bt := &amp;readjson&#123; done: make(chan struct&#123;&#125;), config: c, &#125; return bt, nil &#125; // Run starts readjson. func (bt *readjson) Run(b *beat.Beat) error &#123; logp.Info(&quot;readjson is running! Hit CTRL-C to stop it.&quot;) var err error bt.client, err = b.Publisher.Connect() if err != nil &#123; return err &#125; fmt.Println(&quot;Path: &quot;, bt.config.Path) fmt.Println(&quot;Period: &quot;, bt.config.Period) // Open our jsonFile jsonFile, err := os.Open(bt.config.Path) // if we os.Open returns an error then handle it if err != nil &#123; fmt.Println(err) &#125; fmt.Println(&quot;Successfully Opened users.json&quot;) // defer the closing of our jsonFile so that we can parse it later on defer jsonFile.Close() byteValue, _ := ioutil.ReadAll(jsonFile) // we initialize our Users array var users Users json.Unmarshal(byteValue, &amp;users) // we iterate through every user within our users array and // print out the user Type, their name, and their facebook url // as just an example for i := 0; i &lt; len(users.Users); i++ &#123; fmt.Println(&quot;User Type: &quot; + users.Users[i].Type) fmt.Println(&quot;User Age: &quot; + strconv.Itoa(users.Users[i].Age)) fmt.Println(&quot;User Name: &quot; + users.Users[i].Name) fmt.Println(&quot;Facebook Url: &quot; + users.Users[i].Social.Facebook) event := beat.Event&#123; Timestamp: time.Now(), Fields: common.MapStr &#123; &quot;ostype&quot;: b.Info.Name, &quot;name&quot;: users.Users[i].Name, &quot;type&quot;: users.Users[i].Type, &quot;age&quot;: users.Users[i].Age, &quot;social&quot;: users.Users[i].Social, &#125;, &#125; bt.client.Publish(event) &#125; c := make(chan os.Signal) signal.Notify(c, os.Interrupt, syscall.SIGTERM) go func() &#123; &lt;-c os.Exit(1) &#125;() for &#123; fmt.Println(&quot;sleeping...&quot;) time.Sleep(10 * time.Second) &#125; &#125; // Stop stops readjson. func (bt *readjson) Stop() &#123; bt.client.Close() close(bt.done) &#125;它在run method里把json文件读入，并把它们分别发送出去到我们的Elasticsearch中。我们按照上面的步骤进行编译，并最终运行我们的readjson beat。1./readjson -e我们可以在Kibana中看到我们已经发送上来的beat信息：参考：【1】https://www.elastic.co/guide/en/beats/devguide/7.5/newbeat-generate.html]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2020%2F01%2F02%2FBeats%EF%BC%9ABeats%E5%9C%A8Kibana%E4%B8%AD%E7%9A%84%E9%9B%86%E4%B8%AD%E7%AE%A1%E7%90%86%2F</url>
    <content type="text"><![CDATA[我们可以通过在命令行中对我们的Beats进行管理，比如我们可以启动metric几个模块，我们可以通过如下的命令来执行：1./metricbeat modules enable apache mysql上面的命令启动apache mysql模块。我们也许觉得这个这样做很方便。但是如果我相对许多的机器（比如几千部机器）来做这样的管理，可能也很麻烦，这是因为我们需要到每一台机器上重复做同样的动作。如果我们需要有改动的话，那么需要对每一台机器再次执行同样的操作。那么有什么办法可以帮助我们减少这个工作量呢？Elastic在Kibana中做进去一个新的功能：集中管理。Beats中央管理使用一种称为配置标签的机制来对相关配置进行分组。 注册第一个Beat后，您可以在Kibana的中央管理UI中定义配置标签。Beats集中管理是6.5版带来的功能。 出于安全考虑，此功能在Elastic Gold许可证或使用我们的Elastic Cloud服务的Standard许可证下可用，以确保正确保护部署。 它包含Kibana中新的Beats中央管理UI，并利用Elasticsearch作为集中式配置存储。 在不久的将来，我们还计划公开一个API，以便更轻松地与外部工具和系统集成。下面我来展示如何使用Beats的集中管理。准备工作就像我上面提到的，我们必须购买Elastic Gold才可以拥有这样的功能。为了测试这个功能，我们可以接受30天尝试，这样我们就可以开始我们的测试了。我们首先点击Kibana中的Management，让后选择30天尝试。当我们接受完条件后，我们可以看到：大家一定可以看到左边的列表会多了一个叫做Beats的种类，并在其下面有一个叫做Central Management的项。我们点击Central Management：它显示我们的安全没有打开，也就是说，这个功能必须配合安全功能才能启用。我们参照我之前的文章“Elasticsearch：设置Elastic账户安全”来启动安全功能。我们使用elastic账号进行登录：我们可以看到一个对话框，提示我们Enroll Beat。点击这个按钮。目前我们看到有两个Beats：Filebeat及Metricbeat可以供我们来选择。我们来选择Metricbeat来做一些实验。同时在Platform中选择自己喜欢的平台：针对我们的情况，我选择MacOS。由于需要使用到Metricbeat，需要安装我们的Metribeat。同时在我们的Terminal中打入从Copy Command处拷贝来的命令：这个时候在我们的Kibana中会显示：在上面显示了我的hostname以及metricbeat的版本信息。我们接下来选择Continue按钮：我们可以选一个我们喜欢的Tag Name和自己喜欢的颜色。在上面我选择了Local表明我的这个Metricbeat是在本地运行的。这样以后我们能很容易地找到我们的这个机器的配置。我们点击Add configuration block按钮：我们可以选择我们的模块，并选择喜欢额module。最后选择Save按钮。再接着选择Save &amp; Continue按钮：最终我们完成了：在上面的画面中选择Done：我们可以看出来我们已经成功地配置好我们的Metricbeat模块了。上面显示Config Status是Offline状态。我们可以在我们的Terminal中打入如下的命令（在Metricbeat的安装目录中）：1./metricbeat run我们再重新刷新我们的Kibana界面：从上面我们可以看出来我们的metricbeat已经在成功运行了。当然我们也可以找到相应的index。按照同样的方法，我们可以对其它的模块来进行配置。我们接下来需要点击我们的Tags来添加或配置我们的Beats:我们可以点击Add configuration block来添加同一个Beat模块里的其它模块，或者增加一个输出到Elasticsearch：针对你的设置你需要修个这个hosts的地址。这样，我们的filebeat的输出就会发送到我们的Elasticsearch中了。我们也可以按照同样的方法来添加另外一个module。参考：【1】https://www.elastic.co/guide/en/beats/filebeat/current/how-central-managment-works.html【2】https://www.elastic.co/blog/introducing-beats-central-management-in-the-elastic-stack]]></content>
  </entry>
  <entry>
    <title><![CDATA[如何使用Elasticsearch中的copy_to来提高搜索效率]]></title>
    <url>%2F2020%2F01%2F02%2F%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8Elasticsearch%E4%B8%AD%E7%9A%84copy_to%E6%9D%A5%E6%8F%90%E9%AB%98%E6%90%9C%E7%B4%A2%E6%95%88%E7%8E%87%2F</url>
    <content type="text"><![CDATA[在今天的这个教程中，我们来着重讲解一下如何使用Elasticsearch中的copy来提高搜索的效率。比如在我们的搜索中，经常我们会遇到如下的文档：1234567891011121314&#123; &quot;user&quot; : &quot;双榆树-张三&quot;, &quot;message&quot; : &quot;今儿天气不错啊，出去转转去&quot;, &quot;uid&quot; : 2, &quot;age&quot; : 20, &quot;city&quot; : &quot;北京&quot;, &quot;province&quot; : &quot;北京&quot;, &quot;country&quot; : &quot;中国&quot;, &quot;address&quot; : &quot;中国北京市海淀区&quot;, &quot;location&quot; : &#123; &quot;lat&quot; : &quot;39.970718&quot;, &quot;lon&quot; : &quot;116.325747&quot; &#125;&#125;在这里，我们可以看到在这个文档中，我们有这样的几个字段：123&quot;city&quot; : &quot;北京&quot;,&quot;province&quot; : &quot;北京&quot;,&quot;country&quot; : &quot;中国&quot;,它们是非常相关的。我们在想是不是可以把它们综合成一个字段，这样可以方便我们的搜索。假如我们要经常对这三个字段进行搜索，那么一种方法我们可以在must子句中使用should子句运行bool查询。这种方法写起来比较麻烦。有没有一种更好的方法呢？我们其实可以使用Elasticsearch所提供的copy_to来提高我们的搜索效率。我们可以首先把我们的index的mapping设置成如下的项（这里假设我们使用的是一个叫做twitter的index)。1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859PUT twitter&#123; &quot;mappings&quot;: &#123; &quot;properties&quot;: &#123; &quot;address&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;fields&quot;: &#123; &quot;keyword&quot;: &#123; &quot;type&quot;: &quot;keyword&quot;, &quot;ignore_above&quot;: 256 &#125; &#125; &#125;, &quot;age&quot;: &#123; &quot;type&quot;: &quot;long&quot; &#125;, &quot;city&quot;: &#123; &quot;type&quot;: &quot;keyword&quot;, &quot;copy_to&quot;: &quot;region&quot; &#125;, &quot;country&quot;: &#123; &quot;type&quot;: &quot;keyword&quot;, &quot;copy_to&quot;: &quot;region&quot; &#125;, &quot;province&quot;: &#123; &quot;type&quot;: &quot;keyword&quot;, &quot;copy_to&quot;: &quot;region&quot; &#125;, &quot;region&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;store&quot;: true &#125;, &quot;location&quot;: &#123; &quot;type&quot;: &quot;geo_point&quot; &#125;, &quot;message&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;fields&quot;: &#123; &quot;keyword&quot;: &#123; &quot;type&quot;: &quot;keyword&quot;, &quot;ignore_above&quot;: 256 &#125; &#125; &#125;, &quot;uid&quot;: &#123; &quot;type&quot;: &quot;long&quot; &#125;, &quot;user&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;fields&quot;: &#123; &quot;keyword&quot;: &#123; &quot;type&quot;: &quot;keyword&quot;, &quot;ignore_above&quot;: 256 &#125; &#125; &#125; &#125; &#125;&#125;在这里，我们特别注意如下的这个部分：123456789101112131415&quot;city&quot;: &#123; &quot;type&quot;: &quot;keyword&quot;, &quot;copy_to&quot;: &quot;region&quot;&#125;,&quot;country&quot;: &#123; &quot;type&quot;: &quot;keyword&quot;, &quot;copy_to&quot;: &quot;region&quot; &#125;,&quot;province&quot;: &#123; &quot;type&quot;: &quot;keyword&quot;, &quot;copy_to&quot;: &quot;region&quot;&#125;,&quot;region&quot;: &#123; &quot;type&quot;: &quot;text&quot;&#125;我们把city, country及province三个项合并成为一个项region，但是这个region并不存在于我们文档的source里。当我们这么定义我们的mapping的话，在文档被索引之后，有一个新的region项可以供我们进行搜索。我们可以采用如下的数据来进行展示：12345678910111213POST _bulk&#123; &quot;index&quot; : &#123; &quot;_index&quot; : &quot;twitter&quot;, &quot;_id&quot;: 1&#125; &#125;&#123;&quot;user&quot;:&quot;双榆树-张三&quot;,&quot;message&quot;:&quot;今儿天气不错啊，出去转转去&quot;,&quot;uid&quot;:2,&quot;age&quot;:20,&quot;city&quot;:&quot;北京&quot;,&quot;province&quot;:&quot;北京&quot;,&quot;country&quot;:&quot;中国&quot;,&quot;address&quot;:&quot;中国北京市海淀区&quot;,&quot;location&quot;:&#123;&quot;lat&quot;:&quot;39.970718&quot;,&quot;lon&quot;:&quot;116.325747&quot;&#125;&#125;&#123; &quot;index&quot; : &#123; &quot;_index&quot; : &quot;twitter&quot;, &quot;_id&quot;: 2 &#125;&#125;&#123;&quot;user&quot;:&quot;东城区-老刘&quot;,&quot;message&quot;:&quot;出发，下一站云南！&quot;,&quot;uid&quot;:3,&quot;age&quot;:30,&quot;city&quot;:&quot;北京&quot;,&quot;province&quot;:&quot;北京&quot;,&quot;country&quot;:&quot;中国&quot;,&quot;address&quot;:&quot;中国北京市东城区台基厂三条3号&quot;,&quot;location&quot;:&#123;&quot;lat&quot;:&quot;39.904313&quot;,&quot;lon&quot;:&quot;116.412754&quot;&#125;&#125;&#123; &quot;index&quot; : &#123; &quot;_index&quot; : &quot;twitter&quot;, &quot;_id&quot;: 3&#125; &#125;&#123;&quot;user&quot;:&quot;东城区-李四&quot;,&quot;message&quot;:&quot;happy birthday!&quot;,&quot;uid&quot;:4,&quot;age&quot;:30,&quot;city&quot;:&quot;北京&quot;,&quot;province&quot;:&quot;北京&quot;,&quot;country&quot;:&quot;中国&quot;,&quot;address&quot;:&quot;中国北京市东城区&quot;,&quot;location&quot;:&#123;&quot;lat&quot;:&quot;39.893801&quot;,&quot;lon&quot;:&quot;116.408986&quot;&#125;&#125;&#123; &quot;index&quot; : &#123; &quot;_index&quot; : &quot;twitter&quot;, &quot;_id&quot;: 4&#125; &#125;&#123;&quot;user&quot;:&quot;朝阳区-老贾&quot;,&quot;message&quot;:&quot;123,gogogo&quot;,&quot;uid&quot;:5,&quot;age&quot;:35,&quot;city&quot;:&quot;北京&quot;,&quot;province&quot;:&quot;北京&quot;,&quot;country&quot;:&quot;中国&quot;,&quot;address&quot;:&quot;中国北京市朝阳区建国门&quot;,&quot;location&quot;:&#123;&quot;lat&quot;:&quot;39.718256&quot;,&quot;lon&quot;:&quot;116.367910&quot;&#125;&#125;&#123; &quot;index&quot; : &#123; &quot;_index&quot; : &quot;twitter&quot;, &quot;_id&quot;: 5&#125; &#125;&#123;&quot;user&quot;:&quot;朝阳区-老王&quot;,&quot;message&quot;:&quot;Happy BirthDay My Friend!&quot;,&quot;uid&quot;:6,&quot;age&quot;:50,&quot;city&quot;:&quot;北京&quot;,&quot;province&quot;:&quot;北京&quot;,&quot;country&quot;:&quot;中国&quot;,&quot;address&quot;:&quot;中国北京市朝阳区国贸&quot;,&quot;location&quot;:&#123;&quot;lat&quot;:&quot;39.918256&quot;,&quot;lon&quot;:&quot;116.467910&quot;&#125;&#125;&#123; &quot;index&quot; : &#123; &quot;_index&quot; : &quot;twitter&quot;, &quot;_id&quot;: 6&#125; &#125;&#123;&quot;user&quot;:&quot;虹桥-老吴&quot;,&quot;message&quot;:&quot;好友来了都今天我生日，好友来了,什么 birthday happy 就成!&quot;,&quot;uid&quot;:7,&quot;age&quot;:90,&quot;city&quot;:&quot;上海&quot;,&quot;province&quot;:&quot;上海&quot;,&quot;country&quot;:&quot;中国&quot;,&quot;address&quot;:&quot;中国上海市闵行区&quot;,&quot;location&quot;:&#123;&quot;lat&quot;:&quot;31.175927&quot;,&quot;lon&quot;:&quot;121.383328&quot;&#125;&#125;在Kibnana中执行上面的语句，它将为我们生产我们的twitter索引。同时我们可以通过如下的语句来查询我们的mapping:我们可以看到twitter的mapping中有一个新的被称作为region的项。它将为我们的搜索带来方便。那么假如我们想搜索country:中国，province:北京 这样的记录的话，我们可以只写如下的一条语句就可以了：1234567891011GET twitter/_search &#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;region&quot;: &#123; &quot;query&quot;: &quot;中国 北京&quot;, &quot;minimum_should_match&quot;: 4 &#125; &#125; &#125;&#125;下面显示的是搜索的结果：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119&#123; &quot;took&quot; : 0, &quot;timed_out&quot; : false, &quot;_shards&quot; : &#123; &quot;total&quot; : 1, &quot;successful&quot; : 1, &quot;skipped&quot; : 0, &quot;failed&quot; : 0 &#125;, &quot;hits&quot; : &#123; &quot;total&quot; : &#123; &quot;value&quot; : 5, &quot;relation&quot; : &quot;eq&quot; &#125;, &quot;max_score&quot; : 0.8114117, &quot;hits&quot; : [ &#123; &quot;_index&quot; : &quot;twitter&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;1&quot;, &quot;_score&quot; : 0.8114117, &quot;_source&quot; : &#123; &quot;user&quot; : &quot;双榆树-张三&quot;, &quot;message&quot; : &quot;今儿天气不错啊，出去转转去&quot;, &quot;uid&quot; : 2, &quot;age&quot; : 20, &quot;city&quot; : &quot;北京&quot;, &quot;province&quot; : &quot;北京&quot;, &quot;country&quot; : &quot;中国&quot;, &quot;address&quot; : &quot;中国北京市海淀区&quot;, &quot;location&quot; : &#123; &quot;lat&quot; : &quot;39.970718&quot;, &quot;lon&quot; : &quot;116.325747&quot; &#125; &#125; &#125;, &#123; &quot;_index&quot; : &quot;twitter&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;2&quot;, &quot;_score&quot; : 0.8114117, &quot;_source&quot; : &#123; &quot;user&quot; : &quot;东城区-老刘&quot;, &quot;message&quot; : &quot;出发，下一站云南！&quot;, &quot;uid&quot; : 3, &quot;age&quot; : 30, &quot;city&quot; : &quot;北京&quot;, &quot;province&quot; : &quot;北京&quot;, &quot;country&quot; : &quot;中国&quot;, &quot;address&quot; : &quot;中国北京市东城区台基厂三条3号&quot;, &quot;location&quot; : &#123; &quot;lat&quot; : &quot;39.904313&quot;, &quot;lon&quot; : &quot;116.412754&quot; &#125; &#125; &#125;, &#123; &quot;_index&quot; : &quot;twitter&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;3&quot;, &quot;_score&quot; : 0.8114117, &quot;_source&quot; : &#123; &quot;user&quot; : &quot;东城区-李四&quot;, &quot;message&quot; : &quot;happy birthday!&quot;, &quot;uid&quot; : 4, &quot;age&quot; : 30, &quot;city&quot; : &quot;北京&quot;, &quot;province&quot; : &quot;北京&quot;, &quot;country&quot; : &quot;中国&quot;, &quot;address&quot; : &quot;中国北京市东城区&quot;, &quot;location&quot; : &#123; &quot;lat&quot; : &quot;39.893801&quot;, &quot;lon&quot; : &quot;116.408986&quot; &#125; &#125; &#125;, &#123; &quot;_index&quot; : &quot;twitter&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;4&quot;, &quot;_score&quot; : 0.8114117, &quot;_source&quot; : &#123; &quot;user&quot; : &quot;朝阳区-老贾&quot;, &quot;message&quot; : &quot;123,gogogo&quot;, &quot;uid&quot; : 5, &quot;age&quot; : 35, &quot;city&quot; : &quot;北京&quot;, &quot;province&quot; : &quot;北京&quot;, &quot;country&quot; : &quot;中国&quot;, &quot;address&quot; : &quot;中国北京市朝阳区建国门&quot;, &quot;location&quot; : &#123; &quot;lat&quot; : &quot;39.718256&quot;, &quot;lon&quot; : &quot;116.367910&quot; &#125; &#125; &#125;, &#123; &quot;_index&quot; : &quot;twitter&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;5&quot;, &quot;_score&quot; : 0.8114117, &quot;_source&quot; : &#123; &quot;user&quot; : &quot;朝阳区-老王&quot;, &quot;message&quot; : &quot;Happy BirthDay My Friend!&quot;, &quot;uid&quot; : 6, &quot;age&quot; : 50, &quot;city&quot; : &quot;北京&quot;, &quot;province&quot; : &quot;北京&quot;, &quot;country&quot; : &quot;中国&quot;, &quot;address&quot; : &quot;中国北京市朝阳区国贸&quot;, &quot;location&quot; : &#123; &quot;lat&quot; : &quot;39.918256&quot;, &quot;lon&quot; : &quot;116.467910&quot; &#125; &#125; &#125; ] &#125;&#125;这样我们只对一个region进行操作就可以了，否则我们需要针对country, city及province分别进行搜索。如何查看copy_to的内容在之前的mapping中，我们对region字段加入了如下的一个属性：1234&quot;region&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;store&quot;: true&#125;这里的store属性为true，那么我们可以通过如下的命令来查看文档的region的内容：1GET twitter/_doc/1?stored_fields=region那么它显示的内容如下：12345678910111213141516&#123; &quot;_index&quot; : &quot;twitter&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;1&quot;, &quot;_version&quot; : 1, &quot;_seq_no&quot; : 0, &quot;_primary_term&quot; : 1, &quot;found&quot; : true, &quot;fields&quot; : &#123; &quot;region&quot; : [ &quot;北京&quot;, &quot;北京&quot;, &quot;中国&quot; ] &#125;&#125;如果你想了解更多关于Elastic Stack，请参阅文章“Elasticsearch简介”]]></content>
      <categories>
        <category>elk</category>
      </categories>
      <tags>
        <tag>elk</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ES集群SSL相关]]></title>
    <url>%2F2019%2F12%2F31%2FES%E9%9B%86%E7%BE%A4SSL%E7%9B%B8%E5%85%B3%2F</url>
    <content type="text"><![CDATA[为您的每个Elasticsearch节点生成一个私钥和X.509证书在Elasticsearch加密通讯前提条件：确认xpack.security.enabled设置为true生成节点证书为您的Elasticsearch集群创建一个证书颁发机构bin/elasticsearch-certutil ca您可以将群集配置为信任具有此CA签名的证书的所有节点。该命令输出单个文件，默认名称为elastic-stack-ca.p12。此文件是PKCS＃12密钥库，其中包含CA的公共证书和用于对每个节点的证书签名的私钥。该elasticsearch-certutil命令还会提示您输入密码以保护文件和密钥。如果您打算将来将更多节点添加到群集中，请保留该文件的副本并记住其密码。为集群中的每个节点生成证书和私钥bin/elasticsearch-certutil cert --ca elastic-stack-ca.p12输出是单个PKCS＃12密钥库，其中包括节点证书，节点密钥和CA证书。还提示您输入密码。您可以输入证书和密钥的密码，也可以按Enter键将密码保留为空白。默认情况下，elasticsearch-certutil生成的证书中没有主机名信息（即，它们没有任何“使用者备用名称”字段）。这意味着您可以对群集中的每个节点使用证书，但是必须关闭主机名验证，如下面的配置所示。如果你想用你的集群中的主机名的验证，运行 elasticsearch-certutil cert命令一次，每个节点和提供的–name，–dns和–ip选项。将节点证书复制到适当的位置将适用的.p12文件复制到每个节点上的Elasticsearch配置目录内的目录中。例如，/home/es/config/certs。无需将CA文件复制到此目录。对于要配置的每个其他Elastic产品，将证书复制到相关的配置目录。加密集群中的节点之间的通信启用TLS并指定访问节点证书所需的信息如果签名证书为PKCS＃12格式，则将以下信息添加到elasticsearch.yml每个节点上的 文件中：1234xpack.security.transport.ssl.enabled: truexpack.security.transport.ssl.verification_mode: certificate xpack.security.transport.ssl.keystore.path: certs/elastic-certificates.p12 xpack.security.transport.ssl.truststore.path: certs/elastic-certificates.p12如果在命令中使用–dns或–ip选项，elasticsearch-certutil cert并且要启用严格的主机名检查，请将验证模式设置为 full。有关xpack.security.transport.ssl.verification_mode这些值的描述，请参见。如果为每个节点创建了单独的证书，则可能需要在每个节点上自定义此路径。如果文件名与节点名称匹配，则可以使用certs/${node.name}.p12例如格式。所述elasticsearch-certutil输出PKCS＃12密钥库，其包括CA证书作为信任证书的条目。这允许密钥库也用作信任库。在这种情况下，路径值应与该keystore.path值匹配。但是请注意，这不是一般规则。有些密钥库不能用作信任库，只有经过特殊设计的密钥库才能使用如果证书为PEM格式，则将以下信息添加到elasticsearch.yml每个节点上的 文件中：12345xpack.security.transport.ssl.enabled: truexpack.security.transport.ssl.verification_mode: certificate xpack.security.transport.ssl.key: /home/es/config/node01.key xpack.security.transport.ssl.certificate: /home/es/config/node01.crt xpack.security.transport.ssl.certificate_authorities: [ &quot;/home/es/config/ca.crt&quot; ]如果在命令中使用–dns或–ip选项，elasticsearch-certutil cert并且要启用严格的主机名检查，请将验证模式设置为 full。有关xpack.security.transport.ssl.verification_mode这些值的描述，请参见。节点密钥文件的完整路径。该位置必须在Elasticsearch配置目录中。节点证书的完整路径。该位置必须在Elasticsearch配置目录中。应当信任的CA证书路径的数组。这些路径必须是Elasticsearch配置目录中的位置。如果您使用密码保护了节点证书的安全，请将密码添加到您的Elasticsearch密钥库中：3.1 如果签名证书为PKCS＃12格式，请使用以下命令：12bin/elasticsearch-keystore add xpack.security.transport.ssl.keystore.secure_passwordbin/elasticsearch-keystore add xpack.security.transport.ssl.truststore.secure_password3.2 如果证书为PEM格式，请使用以下命令：1bin/elasticsearch-keystore add xpack.security.transport.ssl.secure_key_passphrase重新启动Elasticsearch您必须执行完全集群重启。配置为使用TLS的节点无法与使用未加密网络的节点通信（反之亦然）。启用TLS之后，您必须重新启动所有节点，以维护整个群集之间的通信。Elasticsearch监视配置为TLS相关节点设置值的所有文件，例如证书，密钥，密钥库或信任库。如果您更新了这些文件中的任何一个（例如，当您的主机名更改或您的证书到期时），Elasticsearch将重新加载它们。以全局Elasticsearch resource.reload.interval.high 设置（默认为5秒）确定的频率轮询文件是否有更改加密HTTP客户端通信启用TLS并指定访问节点证书所需的信息如果证书采用PKCS＃12格式，则将以下信息添加到elasticsearch.yml每个节点上的 文件中：123xpack.security.http.ssl.enabled: truexpack.security.http.ssl.keystore.path: certs/elastic-certificates.p12 xpack.security.http.ssl.truststore.path: certs/elastic-certificates.p12如果为每个节点创建了单独的证书，则可能需要在每个节点上自定义此路径。如果文件名与节点名称匹配，则可以使用certs/${node.name}.p12例如格式。该elasticsearch-certutil输出包括PKCS＃12密钥库内部的CA证书，因此密钥库也可以被用作信任库。此名称应与keystore.path值匹配。如果证书为PEM格式，则将以下信息添加到elasticsearch.yml每个节点上的 文件中：1234xpack.security.http.ssl.enabled: truexpack.security.http.ssl.key: /home/es/config/node01.key xpack.security.http.ssl.certificate: /home/es/config/node01.crt xpack.security.http.ssl.certificate_authorities: [ &quot;/home/es/config/ca.crt&quot; ]节点密钥文件的完整路径。该位置必须在Elasticsearch配置目录中。节点证书的完整路径。该位置必须在Elasticsearch配置目录中。应当信任的CA证书路径的数组。这些路径必须是Elasticsearch配置目录中的位置。如果您使用密码保护了节点证书的安全，请将密码添加到您的Elasticsearch密钥库中：3.1 如果签名证书为PKCS＃12格式，请使用以下命令：12bin/elasticsearch-keystore add xpack.security.http.ssl.keystore.secure_passwordbin/elasticsearch-keystore add xpack.security.http.ssl.truststore.secure_password3.2 如果证书为PEM格式，请使用以下命令：1bin/elasticsearch-keystore add xpack.security.http.ssl.secure_key_passphrase重新启动ElasticsearchElasticsearch监视配置为TLS相关节点设置值的所有文件，例如证书，密钥，密钥库或信任库。如果您更新了这些文件中的任何一个（例如，当您的主机名更改或您的证书到期时），Elasticsearch将重新加载它们。以全局Elasticsearch resource.reload.interval.high 设置（默认为5秒）确定的频率轮询文件是否有更改配置监视功能以使用加密的连接Elastic Stack监视功能由两个组件组成：您在每个Elasticsearch和Logstash节点上安装的代理，以及Kibana中的Monitoring UI。监视代理程序从节点收集指标并为其建立索引，您可以通过Kibana中的“监视”仪表板可视化数据。代理可以为同一Elasticsearch集群上的数据建立索引，或将其发送到外部监视集群。要在启用安全性功能的情况下使用监视功能，您需要 设置Kibana以使用安全性功能， 并为监视UI创建至少一个用户。如果使用的是外部监视群集，则还需要为监视代理程序配置用户，并配置代理程序以在与监视群集通信时使用适当的凭据。在Kibana配置安全当集群上启用了X-Pack安全性时，Kibana用户必须登录。您可以为Kibana用户配置X-Pack安全角色，以控制这些用户可以访问哪些数据。通过Kibana向Elasticsearch发出的大多数请求都使用登录用户的凭据进行身份验证。但是，Kibana服务器需要向Elasticsearch集群提出一些内部请求。因此，您必须为Kibana服务器配置凭据以用于那些请求。启用X-Pack安全性后，如果加载Kibana仪表板来访问未经授权查看的索引中的数据，则会出现错误，指示该索引不存在。X-Pack安全性当前不提供控制哪些用户可以加载哪些仪表板的方法。在Elasticsearch配置安全1.1 验证xpack.security.enabled设置是否true在群集中的每个节点上。如果您使用基本或试用许可证，则默认值为false。1.2 配置用于节点间通信的传输层安全性（TLS / SSL）1.3 设置所有内置用户的密码Elasticsearch安全功能提供 内置用户来帮助您启动和运行。该elasticsearch-setup-passwords命令是首次设置内置用户密码的最简单方法。1bin/elasticsearch-setup-passwords interactive该elasticsearch-setup-passwords命令使用瞬态引导密码，该密码在命令成功运行后将不再有效。您不能elasticsearch-setup-passwords再次运行该命令。相反，您可以从Kibana中的“ 管理”&gt;“用户” UI 更新密码，或使用安全用户API。配置Kibana以使用适当的内置用户更新kibana.yml配置文件中的以下设置：12elasticsearch.username: &quot;kibana&quot;elasticsearch.password: &quot;kibanapassword&quot;Kibana服务器以该用户身份提交请求，以访问集群监视API和.kibana索引。该服务器并没有需要访问用户索引。内置kibana用户的密码通常是在Elasticsearch上的X-Pack安全配置过程中设置的。xpack.security.encryptionKey在kibana.yml 配置文件中设置属性。您可以使用32个字符或更长的任何文本字符串作为加密密钥。1xpack.security.encryptionKey: &quot;something_at_least_32_characters&quot;可选：更改默认会话持续时间。默认情况下，会话保持活动状态，直到关闭浏览器。要更改持续时间，请xpack.security.sessionTimeout在kibana.yml配置文件中设置 属性。超时以毫秒为单位。例如，将超时设置为600000以使会话在10分钟后过期：1xpack.security.sessionTimeout: 600000可选：配置Kibana以加密通信。Kibana支持客户端请求的传输层安全性（TLS / SSL）加密。如果您正在使用X-Pack安全性或为Elasticsearch提供HTTPS端点的代理，则可以配置Kibana通过HTTPS访问Elasticsearch。因此，Kibana和Elasticsearch之间的通信也被加密。5.1 配置Kibana以加密浏览器和Kibana服务器之间的通信：您无需为这种类型的加密启用X-Pack安全性。为Kibana生成服务器证书。a. 您必须将证书的 subjectAltName名称设置为Kibana服务器的主机名，标准域名（FQDN）或IP地址，或者将CN设置为Kibana服务器的主机名或FQDN。将服务器的IP地址用作CN无效。b. 设置server.ssl.enabled，server.ssl.key以及server.ssl.certificate 在性能kibana.yml：123server.ssl.enabled: trueserver.ssl.key: /path/to/your/server.keyserver.ssl.certificate: /path/to/your/server.crt进行这些更改之后，您必须始终通过HTTPS访问Kibana。例如， https：// localhost：5601。5.2 配置Kibana以通过HTTPS连接到Elasticsearch要执行此步骤，您必须 启用Elasticsearch安全功能，或者必须具有为Elasticsearch提供HTTPS端点的代理。a. elasticsearch.hosts在Kibana配置文件的设置中指定HTTPS协议kibana.yml：1elasticsearch.hosts: [&quot;https://&lt;your_elasticsearch_host&gt;.com:9200&quot;]b. 如果您使用自己的CA为Elasticsearch签名证书，请在中进行 elasticsearch.ssl.certificateAuthorities设置kibana.yml以指定PEM文件的位置。1elasticsearch.ssl.certificateAuthorities: /path/to/your/cacert.pem设置certificateAuthorities属性可以使您使用默认 verificationMode选项full。5.3 （可选）如果启用了弹性监视功能，请配置Kibana以通过HTTPS连接到Elasticsearch监视集群：要执行此步骤，您必须启用Elasticsearch安全功能，或者必须具有为Elasticsearch提供HTTPS端点的代理。a. xpack.monitoring.elasticsearch.hosts在Kibana配置文件的设置中指定HTTPS URL ，kibana.yml1xpack.monitoring.elasticsearch.hosts: [&quot;https://&lt;your_monitoring_cluster&gt;:9200&quot;]b. xpack.monitoring.elasticsearch.ssl.*在kibana.yml文件中指定设置 。例如，如果您使用自己的证书颁发机构来签署证书，请在文件中指定PEM文件的位置kibana.yml：1xpack.monitoring.elasticsearch.ssl.certificateAuthorities: /path/to/your/cacert.pem重新启动Kibana选择一种身份验证机制，并向用户授予使用Kibana所需的特权可以在Kibana 的“ 管理/安全性/角色”页面上管理特权,授予用户访问将在Kibana中使用的索引的权限在Elasticsearch中配置监视在Kibana中配置监视配置Logstash节点的监视]]></content>
      <categories>
        <category>elk</category>
      </categories>
      <tags>
        <tag>elk</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[elastic stack 7.2技术栈【一】Elasticsearch 7.2 技术栈安装与配置概要的说明]]></title>
    <url>%2F2019%2F12%2F31%2Felastic%20stack%207.2%E6%8A%80%E6%9C%AF%E6%A0%88%E3%80%90%E4%B8%80%E3%80%91%E2%80%94%E2%80%94%20%E9%9C%80%E8%A6%81%E6%8E%8C%E6%8F%A1%E7%9A%84%E4%B8%80%E4%BA%9Belastic%E9%87%8D%E8%A6%81%E6%A6%82%E5%BF%B5%E4%B8%8E%E9%85%8D%E7%BD%AE%E5%8F%82%E6%95%B0%2F</url>
    <content type="text"><![CDATA[Elasticsearch 7.2 技术栈安装与配置概要的说明我们将大体上按以下步骤逐步安装和配置出一套满足生产环境运行要求和信息安全管理要求的三节点es服务集群。在节点1上面先行安装一套es+kinaba+beats的单节点服务详细步骤可以参考官网这个教程：get-started-elastic-stack继续在单节点的结构下启用和配置出es安全管理功能详细步骤可以参考官网这个教程：security-getting-started进行面向支持多节点的加密通信改造的相关配置，同时将节点2、3加入es集群中详细步骤可以参考官网这个教程：encrypting-internode-communications我们之所以没有选择按一步到位的方法部署整套es集群，是因为在启用和配置xpack的部分很容易遇到问题。参照上面的三个步骤，可以依次部署、配置并对结果进行验证，在前一步骤部署成功的基础上继续做更多内容的部署，这样处理的成功率会比较高。版本及许可的说明操作系统使用centos7 minimual，升级到最新小版本。elasticsearch技术栈均采用 7.2版本，默认使用basic许可，可免费使用xpack部分安全管理服务。使用官网下载的tar.gz安装包进行部署。一些elastic重要概念与配置参数ES 是在 lucene 的基础上进行研发的，隐藏了 lucene 的复杂性，提供简单易用的 RESTful Api接口。ES 的分片相当于 lucene 的索引。Node 节点的几种部署实例实例一: 只用于数据存储和数据查询，降低其资源消耗率12node.master: falsenode.data: true实例二: 来协调各种创建索引请求或者查询请求，但不存储任何索引数据12node.master: truenode.data: false实例三: 主要用 于查询负载均衡， 并请求分发到多个指定的node服务器，并对各个node服务器返回的结果进行一个汇总处理，最终返回给客户端12node.master: falsenode.data: false实例四: 即有成为主节点的资格，又存储数据12node.master: truenode.data: true在只有3个节点的部署方案中，建议设置3个节点均有成为master节点的资格，且存储索引数据。数据目录配置与物理磁盘的使用一般来说，是这样配置：123path: logs: /var/log/elasticsearch data: /var/data/elasticsearch数据目录可以支持使用多个：12345path: data: - /mnt/elasticsearch_1 - /mnt/elasticsearch_2 - /mnt/elasticsearch_3物理磁盘的使用：由于es已经提供了数据副本的冗余，所以建议使用raid0，不通过raid提供额外的数据保护；当有多块数据盘时，通过path.data配置把数据条带化分配到多块盘上是可行的，但建议是通过设置raid0将多块物理磁盘整合为一块逻辑盘使用，以确保每个分片都是被放入的同样的目录；集群名称配置1cluster.name: logging-prodnode节点名称默认为使用主机名，也可以在elasticsearch.yml中指定。在一个主机上同时跑多个es实例时，这个配置项就会很有帮助了。1node.name: prod-data-2网络地址配置默认将服务绑定到loopback接口，这需要按实际情况调整。1network.host: 10.20.0.11注：变更服务绑定接口后，会被认为是作为生产环境使用，会触发es的环境检查操作。当有不符要求的系统或集群配置参数时，es服务会无法启动。节点发现和cluster初始化参数单播主机列表通过discovery.zen.ping.unicast.hosts来配置。这个配置在 elasticsearch.yml 文件中：1discovery.zen.ping.unicast.hosts: [&quot;host1&quot;, &quot;host2:port&quot;]具体的值是一个主机数组或逗号分隔的字符串。每个值应采用host：port或host的形式（其中port默认为设置transport.profiles.default.port，如果未设置则返回transport.tcp.port）。请注意，必须将IPv6主机置于括号内。此设置的默认值为127.0.0.1，[:: 1]。使用单播，你可以为 Elasticsearch 提供一些它应该去尝试连接的节点列表。当一个节点联系到单播列表中的成员时，它就会得到整个集群所有节点的状态，然后它会联系 master 节点，并加入集群。12345678discovery.seed_hosts: - 192.168.1.10:9300 - 192.168.1.11 - seeds.mydomain.comcluster.initial_master_nodes: - master-node-a - master-node-b - master-node-c提供了seed.hosts参数的三种赋值方式initial_master_nodes参数只能使用节点的node.name参数值，一般来说是主机名Zen Discovery 是 ES 默认内建发现机制。它提供单播和多播的发现方式，并且可以扩展为通过插件支持云环境和其他形式的发现。Elasticsearch 官方推荐我们使用 单播 代替 组播。而且 Elasticsearch 默认被配置为使用 单播 发现，以防止节点无意中加入集群。设置JVM heap size通过jvm.options文件设置jvm缓存参数，过大或过小都不好，过大的缓存也会让垃圾回收变慢。当jvm缓存设置大于26GB时，需要评估zero-based compressed oops限制，参见下面的说明：https://www.elastic.co/guide/en/elasticsearch/reference/current/heap-size.html由于ES构建基于lucene, 而lucene设计强大之处在于lucene能够很好的利用操作系统内存来缓存索引数据，以提供快速的查询性能。lucene的索引文件segements是存储在单文件中的，并且不可变，对于OS来说，能够很友好地将索引文件保持在cache中，以便快速访问；因此，我们很有必要将一半的物理内存留给lucene ; 另一半的物理内存留给ES（JVM heap )。所以， 在ES内存设置方面，可以遵循以下原则：当机器内存小于64G时，遵循通用的原则，50%给ES，50%留给lucene。当机器内存大于64G时，遵循以下原则：a. 如果主要的使用场景是全文检索, 那么建议给ES Heap分配 4~32G的内存即可；其它内存留给操作系统, 供lucene使用（segments cache), 以提供更快的查询性能。b. 如果主要的使用场景是聚合或排序， 并且大多数是numerics, dates, geo_points 以及not_analyzed的字符类型， 建议分配给ES Heap分配 4~32G的内存即可，其它内存留给操作系统，供lucene使用(doc values cache)，提供快速的基于文档的聚类、排序性能。c. 如果使用场景是聚合或排序，并且都是基于analyzed 字符数据，这时需要更多的 heap size, 建议机器上运行多ES实例，每个实例保持不超过50%的ES heap设置(但不超过32G，堆内存设置32G以下时，JVM使用对象指标压缩技巧节省空间)，50%以上留给lucene。禁止swap，一旦允许内存与磁盘的交换，会引起致命的性能问题。 通过： 在elasticsearch.yml 中 bootstrap.memory_lock: true， 以保持JVM锁定内存，保证ES的性能。操作系统通过交换（swap）将内存的分页写入磁盘，es在内存中保留了很多运行时必需的数据和缓存，所以消耗磁盘的操作将严重影响正在运行的集群。关闭es交换最彻底的方法是，在elasticsearch.yml文件中将bootstrap.mlockall设置为true 。GC设置原则：a. 保持GC的现有设置，默认设置为：Concurrent-Mark and Sweep (CMS)，别换成G1GC，因为目前G1还有很多BUG。b. 保持线程池的现有设置，目前ES的线程池较1.X有了较多优化设置，保持现状即可；默认线程池大小等于CPU核心数。如果一定要改，按公式（（CPU核心数* 3）/ 2）+ 1 设置；不能超过CPU核心数的2倍；但是不建议修改默认配置，否则会对CPU造成硬伤。Temp directory配置在使用.tar.gz方式部署es服务时，建议指定一个安全的临时文件目录，避免因为默认使用的/tmp下的临时目录被操作系统定期删除，造成服务故障。通过环境变量 $ES_TMPDIR 来设置。分片分配的感知分配感知（allocation awareness）是管理在哪里放置数据的副本。https://www.elastic.co/guide/en/elasticsearch/reference/7.2/allocation-awareness.html1. 基于分片的分配分配感知允许用户使用自定义的参数来配置分片的分配。通过定义一组键，然后在合适的节点上设置这个键，就可以开启分配感知。elasticsearch.yml1cluster.routing.allocation.awareness.attributes: rack_id注：支持赋多个值同时用作感知属性，如cluster.routing.allocation.awareness.attributes: rack, group, zone针对每个es节点，用户可以修改elasticsearch.yml，按期待的网络配置来设置该值。ES允许用户在节点上设置元数据，这些元数据的键将成为我们要使用的分配感知参数。1node.attr.rack_id: rack_one当有多个es节点可用时，es会尽量把分片与副本均衡到rack_id值不同的节点上去。但如果只剩一个可用的es数据节点了，es也会选择把一个索引的分片和副本全部部署在同一个节点上面。常见的使用场景是按照地点、机架或是虚拟机等来划分集群的拓扑。2. 强制性的分配感知在用户事先规则好分片分组信息，且希望限制每个分组的副本分片数量时，强制分配感知是适用的解决方法。在这种情况下，即便因为部分分组的数据节点不可用，导致es服务可用性风险，es也不会把索引的分片与副本都部署在相同的分组节点上面。例如，用户想在区域级别使用强制分配。可以先指定一个zone的属性，然后为该分组添加多个维度。如下所示：12cluster.routing.allocation.awareness.attributes: zonecluster.routing.allocation.force.zone.values: us-east, us-west此时，我们在东部地区启用了一批节点，这些节点的配置都是node.attr.zone: us-east ，在创建索引时由于以上限制，副本分片只会被均衡到没有相应zone值的节点上去。3. 动态设置分片感知可以通过集群设置API在运行时进行修改，这个修改的效果可以自行选择是持久的，还是临时性的。123456curl -XPUT localhost:9200/_cluster/settings -d &apos;&#123; &quot;persistent&quot;: &#123; &quot;cluster.routing.allocation.awareness.attributes&quot;: zone &quot;cluster.routing.allocation.force.zone.values&quot;: us-east, us-west &#125;&#125;&apos;]]></content>
      <categories>
        <category>elk</category>
      </categories>
      <tags>
        <tag>elk</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[elastic stack 7.2技术栈【五】向es集群中增加更多节点并配置节点间的加密通信]]></title>
    <url>%2F2019%2F12%2F31%2Felastic%20stack%207.2%E6%8A%80%E6%9C%AF%E6%A0%88%E3%80%90%E4%BA%94%E3%80%91%E2%80%94%E2%80%94%20%E5%90%91es%E9%9B%86%E7%BE%A4%E4%B8%AD%E5%A2%9E%E5%8A%A0%E6%9B%B4%E5%A4%9A%E8%8A%82%E7%82%B9%E5%B9%B6%E9%85%8D%E7%BD%AE%E8%8A%82%E7%82%B9%E9%97%B4%E7%9A%84%E5%8A%A0%E5%AF%86%E9%80%9A%E4%BF%A1%2F</url>
    <content type="text"><![CDATA[本章节的配置内容，是建立在成功部署和配置了前面几个章节的基础之上。制作数字证书在一个安全的集群中，Elasticsearch节点在与其他节点通信时会使用证书来标识自己。群集必须验证这些证书的真实性。 建议的方法是信任特定的证书颁发机构（CA）。 因此，当节点添加到群集时，需要他们使用由同一CA签名的证书。制作ca证书：1./bin/elasticsearch-certutil ca使用默认输出文件名elastic-stack-ca.p12，并为证书设置访问口令：my-elastic123。如果是你的生产集群，请注意做好证书和口令的保护。创建一个目录用于存放证书：12cd $&#123;ES_HOME&#125;/configmkdir certs为节点1制作证书和密钥：1./bin/elasticsearch-certutil cert --ca elastic-stack-ca.p12 --ip 10.20.0.11 --out config/certs/es-node1.p12输入ca的口令，并为节点1的证书设置口令为：es-node123elasticsearch-certutil工具提供了更多复杂的证书管理功能，如果有使用需求可以参见：https://www.elastic.co/guide/en/elasticsearch/reference/7.2/certutil.html配置集群节点间的加密通信编辑elasticsearch.yml文件，修改以下参数。1）禁用single-node discovery：1discovery.type: single-node我们之前为了调试单节点的es功能而启用的这个参数，现在直接从配置文件中清除这一配置项，使用默认值即可。2）设置这次启动中有资格成为master节点的主机列表：1cluster.initial_master_nodes: [&quot;es-node1&quot;]因为此时我们还只有一个节点。如果是已经有多个具备master node资格的节点时，则这里需要把它们都维护在列表中。这一参数仅在集群初次建立时有用。3）为传输（节点间）通信启用传输层安全性（TLS / SSL）：ES_PATH_CONF/elasticsearch.yml1234xpack.security.enabled: truexpack.security.transport.ssl.enabled: truexpack.security.transport.ssl.keystore.path: certs/$&#123;node.name&#125;.p12xpack.security.transport.ssl.truststore.path: certs/$&#123;node.name&#125;.p12将PKCS＃12文件的密码存储在Elasticsearch密钥库中。123./bin/elasticsearch-keystore create./bin/elasticsearch-keystore add xpack.security.transport.ssl.keystore.secure_password./bin/elasticsearch-keystore add xpack.security.transport.ssl.truststore.secure_password如果elasticsearch keystore文件已经存在了，则可以跳过创建命令系统将提示你提供为es-node1.p12文件创建的密码。 我们将此文件用于传输TLS密钥库和信任库，因此为这两个设置提供相同的密码。1234[elastic@es-node1 elasticsearch-7.2.0]$ ./bin/elasticsearch-keystore listkeystore.seedxpack.security.transport.ssl.keystore.secure_passwordxpack.security.transport.ssl.truststore.secure_password重新启动es服务：1./bin/elasticsearch以及kibana服务：1./bin/kibana向es集群中添加更多的节点es可以使用的节点有很多类型，详见：https://www.elastic.co/guide/en/elasticsearch/reference/7.2/modules-node.html 。我们在这里向集群中添加两个节点，每个节点都既作为master-eligible node（node.master: true），也作为data node（node.data： true）。1）我们先配置好另外的两个主机节点参照本文第一部分的内容对系统进行初始化配置；参照本文第二部分，第1章节的内容部署elasticsearch程序，暂不启动es服务；2）在节点1上为新增的两个节点制作证书1./bin/elasticsearch-certutil cert --ca elastic-stack-ca.p12 --multiple依次为es-node2, es-node3生成证书，得到名为certificate-bundle.zip的输出文件；为简单起见，统一将两个新节点证书的密码也设置为和节点1相同的 es-node123 ；将解压缩后得到证书文件，参照节点1的存放路径（/opt/elasticsearch-7.2.0/config/certs），分别部署到节点2和节点3上去。3）编辑3个节点的ES_PATH_CONF/elasticsearch.yml文件节点es-node1：12345678910cluster.name: my-elasticnode.name: es-node1bootstrap.memory_lock: truenetwork.host: 10.20.0.11cluster.initial_master_nodes: [&quot;es-node1&quot;,&quot;es-node2&quot;,&quot;es-node3&quot;]discovery.seed_hosts: [&quot;10.20.0.11&quot;,&quot;10.20.0.12&quot;,&quot;10.20.0.13&quot;]xpack.security.enabled: truexpack.security.transport.ssl.enabled: truexpack.security.transport.ssl.keystore.path: certs/$&#123;node.name&#125;.p12xpack.security.transport.ssl.truststore.path: certs/$&#123;node.name&#125;.p12节点es-node2：12345678910cluster.name: my-elasticnode.name: es-node2bootstrap.memory_lock: truenetwork.host: 10.20.0.12cluster.initial_master_nodes: [&quot;es-node1&quot;,&quot;es-node2&quot;,&quot;es-node3&quot;]discovery.seed_hosts: [&quot;10.20.0.11&quot;,&quot;10.20.0.12&quot;,&quot;10.20.0.13&quot;]xpack.security.enabled: truexpack.security.transport.ssl.enabled: truexpack.security.transport.ssl.keystore.path: certs/$&#123;node.name&#125;.p12xpack.security.transport.ssl.truststore.path: certs/$&#123;node.name&#125;.p12节点es-node3：123456789cluster.name: my-elasticnode.name: es-node3bootstrap.memory_lock: truenetwork.host: 10.20.0.13cluster.initial_master_nodes: [&quot;es-node1&quot;,&quot;es-node2&quot;,&quot;es-node3&quot;]discovery.seed_hosts: [&quot;10.20.0.11&quot;,&quot;10.20.0.12&quot;,&quot;10.20.0.13&quot;]xpack.security.enabled: truexpack.security.transport.ssl.enabled: truexpack.security.transport.ssl.keystore.path: certs/$&#123;node.name&#125;.p12xpack.security.transport.ssl.truststore.path: certs/$&#123;node.name&#125;.p124）将两个新节点的PKCS#12 证书口令存储在Elasticsearch Keystore中123./bin/elasticsearch-keystore create./bin/elasticsearch-keystore add xpack.security.transport.ssl.keystore.secure_password./bin/elasticsearch-keystore add xpack.security.transport.ssl.truststore.secure_password根据提示，将节点证书的密码信息es-node123保存到es密钥库中5）依次启动三个节点上的es服务1./bin/elasticsearch注意观察日志输出启动kibana，在dev tools中执行GET _cluster/health 查看集群健康状态。执行 GET _cat/nodes?v 查看master node角色分布：]]></content>
      <categories>
        <category>elk</category>
      </categories>
      <tags>
        <tag>elk</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[elastic stack 7.2技术栈【四】在单机版es技术栈之上启用和配置es安全管理功能]]></title>
    <url>%2F2019%2F12%2F31%2Felastic%20stack%207.2%E6%8A%80%E6%9C%AF%E6%A0%88%E3%80%90%E5%9B%9B%E3%80%91%E2%80%94%E2%80%94%20%E5%9C%A8%E5%8D%95%E6%9C%BA%E7%89%88es%E6%8A%80%E6%9C%AF%E6%A0%88%E4%B9%8B%E4%B8%8A%E5%90%AF%E7%94%A8%E5%92%8C%E9%85%8D%E7%BD%AEes%E5%AE%89%E5%85%A8%E7%AE%A1%E7%90%86%E5%8A%9F%E8%83%BD%2F</url>
    <content type="text"><![CDATA[Elasticsearch安全功能使得可以轻松保护群集。 可以对数据进行基础的密码保护，并实施更高级的安全措施，例如加密通信，基于角色的访问控制，IP过滤和审计。在最新发行的es和kinaba版本中，basic级别的许可中已经开放了部分安全管理的功能特性，可以免费使用。详细信息参见：https://www.elastic.co/subscriptions依次停止Metricbeat、kibana和elasticsearch服务进程编辑ES_PATH_CONF/elasticsearch.yml文件，启用xpack服务添加以下内容：1xpack.security.enabled: true编辑ES_PATH_CONF/elasticsearch.yml文件，启用 single-node discovery功能1discovery.type: single-node通过将discovery.type设置为single-node， 在这种情况下，节点将选择自己作为主节点，并且不会加入任何其他节点的集群。启用Elasticsearch安全功能后，默认情况下会启用基本身份验证。 要与群集通信，必须指定用户名和密码。 除非启用了匿名访问，否则所有不包含用户名和密码的请求都将被拒绝。为内置用户创建密码有一些管理用途的集群内建用户，如apm_system, beats_system, elastic, kibana, logstash_system, and remote_monitoring_user，我们需要为其设置密码。这些内建管理用户的说明信息参见： https://www.elastic.co/guide/en/elastic-stack-overview/7.2/built-in-users.html启动es服务：1./bin/elasticsearch执行下面命令设置内建管理用户的密码：1./bin/elasticsearch-setup-passwords interactive这里设置的账号密码，在后续各种服务集成配置中会使用到，所以务必做好记录。该命令仅可以执行一次。123456elastic/elastic123apm_system/apm_system123kibana/kibana123logstash_system/logstash_system123beats_system/beats_system123remote_monitoring_user/remote_monitoring_user123账号与密码信息的管理这里又有两种方式进行配置，一个是直接将账号/密码维护在配置文件中。另一个方法是，把账号/密码存储到keystore密钥库中。后一种的安全性更高些。编辑 KIBANA_HOME/config/kibana.yml ：12elasticsearch.username: &quot;kibana&quot;elasticsearch.password: &quot;kibana123&quot;或者存储在keystore中：12345678# 放置历史操作记录被查看到set +o historyexport LOGSTASH_KEYSTORE_PASS=mypassword set -o history./bin/kibana-keystore create./bin/kibana-keystore add elasticsearch.username./bin/kibana-keystore add elasticsearch.password在提示输入时，根据提示输入kibana/kibana123的用户名和密码信息。该账号将被用于kibana访问es服务时使用。启动kibana服务：1./bin/kibana现在我们已经设置好了内置用户，需要决定如何管理所有其他用户。Elastic Stack对用户进行身份验证以确保它们有效。 身份验证过程由realm进行处理。 可以使用一个或多个内置realm，例如native，file，LDAP，PKI，Active Directory，SAML或Kerberos realm。 也可以创建自己的自定义realm。 在本教程中，我们将使用本机native realm，这也是basic许可所允许免费使用的用户身份验证方式之一。通常，可以通过在elasticsearch.yml文件中添加xpack.security.authc.realm设置来配置realm。 但是，如果未配置其他域，则默认情况下为使用本机native realm。 因此，无需在本教程中执行任何额外的配置步骤，就可以直接跳转到创建用户了！创建用户我们创建两个基于native realm的用户。使用浏览器打开kibana服务地址，会发现此时已经需要登录才能使用kibana服务了。1）使用elastic/elastic123账号，登录进入kibana。2）转到Management / Security / Users页面。3）点击Create new user，创建一个账号gqtest/gqtest123，但暂时先不设置Roles，该账号将作为查看kibana的个人账号使用。4）再创建一个logstash_internal/logstash_internal123用户，用于logstash向es写入数据时使用。5）配置角色授权每个角色定义一组特定的操作（如读取，创建或删除），这些操作可以在特定的安全资源（例如索引，别名，文档，字段或集群）上执行。 es已经提供了很多内置角色可以直接使用。打开Management / Security / Roles页面，查看系统内置的各种Roles。点击某个角色的名称，可以查看该角色都被授予了哪些权限。我们将kibana_user角色分配给你的用户。 返回Management / Security / Users页面并选择你的用户。 添加kibana_user角色并保存更改。该角色将提供kibana的所有使用权限。在实际使用中，我们可能需要为特定的用户创建专用的kibana账号，仅显示部分kibana功能菜单，同时控制各菜单项的读、写权限。这些可以通过在kibana上自定义Role角色来实现。详情配置方法参见：https://www.elastic.co/guide/en/kibana/7.2/kibana-role-management.html在前面配置步骤中，我们在Elasticsearch中存储Metricbeat数据。 让我们创建两个角色，授予对该数据的不同级别的访问权限。转到Management / Security / Roles页面，然后单击Create role。创建一个metricbeat_reader角色，该角色对metricbeat- 索引具有read和view_index_metadata特权：创建一个metricbert_writer角色，该角色具有manage_index_templates的权限并监视集群特权，以及对metricbeat- 索引的write, delete, create_index, manage的特权：Role metricbert_writer：现在返回Management / Security / Users页面并将这些角色分配给适当的用户。 将metricbeat_reader角色分配给你的个人用户。 将metricbeat_writer角色分配给logstash_internal用户。如果需要了解更多的授权和角色管理知识，请参见：https://www.elastic.co/guide/en/elastic-stack-overview/7.2/authorization.html6）配置Logstash或Metricbeat使用es账号Logstash的配置方法参见：https://www.elastic.co/guide/en/elastic-stack-overview/7.2/get-started-logstash-user.htmlMetricbeat配置使用es账号创建一个keystore密钥库：1./metricbeat keystore create将敏感的密码信息存放在密钥库里：1./metricbeat keystore add ES_PWD使用账号metricbeat_internal/metricbeat_internal123，将其密码信息保存于keystore中的ES_PWD变量中。metricbeat_internal是我们创建的一个用户，授予了beats_system, kibana_user，metricbert_writer这3个角色权限。查看与删除的方法：12./metricbeat keystore list./metricbeat keystore remove ES_PWD编辑metricbeat.yml配置文件：12345678910111213setup.kibana: host: &quot;es-node1:5601&quot; username: &quot;metricbeat_internal&quot; password: &quot;$&#123;ES_PWD&#125;&quot;#-------------------------- Elasticsearch output ------------------------------output.elasticsearch: # Array of hosts to connect to. hosts: [&quot;localhost:9200&quot;] # Optional protocol and basic auth credentials. #protocol: &quot;https&quot; username: &quot;beats_system&quot; password: &quot;$&#123;ES_PWD&#125;&quot;启动Metricbeat服务：1./metricbeat -e回到kibana的web页面，使用之前创建的gqtest/gqtest123账号登录并查看系统指标数据，该账号具有metricbeat_reader 和 kibana_user的授权。可以看到Metricbeat采集的系统指标数据正常写入elasticsearch，且在kibana上正常展示，则可以继续配置下面的步骤。]]></content>
      <categories>
        <category>elk</category>
      </categories>
      <tags>
        <tag>elk</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[elastic stack 7.2技术栈【三】部署单机版本的elasticsearch技术栈]]></title>
    <url>%2F2019%2F12%2F31%2Felastic%20stack%207.2%E6%8A%80%E6%9C%AF%E6%A0%88%E3%80%90%E4%B8%89%E3%80%91%E2%80%94%E2%80%94%20%E9%83%A8%E7%BD%B2%E5%8D%95%E6%9C%BA%E7%89%88%E6%9C%AC%E7%9A%84elasticsearch%E6%8A%80%E6%9C%AF%E6%A0%88%2F</url>
    <content type="text"><![CDATA[在节点1上面安装单节点es12curl -L -O https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.2.0-linux-x86_64.tar.gztar -xzvf elasticsearch-7.2.0-linux-x86_64.tar.gz对elasticsearch.yml几个重要配置参数进行设置：1234cluster.name: my-elasticnode.name: es-node1# network.host: 10.20.0.11bootstrap.memory_lock: true暂不能启用network.host参数，否则会触发es的强调集群配置检查，因为es集群配置还不完整，所以这会导致启动失败启动es服务：12cd elasticsearch-7.2.0./bin/elasticsearch检测下服务是否正常启动：1curl http://10.20.0.11:9200在节点1上安装kibana123curl -L -O https://artifacts.elastic.co/downloads/kibana/kibana-7.2.0-linux-x86_64.tar.gztar xzvf kibana-7.2.0-linux-x86_64.tar.gzcd kibana-7.2.0-linux-x86_64/编辑config/kibana.yml文件：12elasticsearch.hosts: [&quot;http://10.20.0.11:9200&quot;]server.host: &quot;es-node1&quot;启动：1./bin/kibana调整防火墙放行规则：12firewall-cmd --permanent --zone=public --add-port=5601/tcpfirewall-cmd --reload检测服务是否正常启动：1curl http://10.20.0.11:5601在节点1上安装BeatsBeats针对不同的使用场景，分别提供了不同的工具实现。这里我们使用Metricbeat对节点主机的系统进行监控。Metricbeat提供预构建的模块，可以使用它们快速实施和部署系统监控解决方案。12curl -L -O https://artifacts.elastic.co/downloads/beats/metricbeat/metricbeat-7.2.0-linux-x86_64.tar.gztar xzvf metricbeat-7.2.0-linux-x86_64.tar.gz在这里，我们将运行Metricbeat的system模块以从服务器上运行的操作系统和服务收集指标。system模块收集系统级指标，例如CPU使用率，内存，文件系统，磁盘IO和网络IO统计信息，以及系统上运行的每个进程的类似顶级的统计信息。启用system模块：1./metricbeat modules enable system初始化数据：1./metricbeat setup -e如果前一步中kibana服务绑定的网卡变更为对外服务的网卡了，则在执行这个命令前请修改metricbeat.yml文件中kibana服务地址的定义。启动Metricbeat服务：1./metricbeat -e要看可视化系统指标，请打开浏览器并导航到Metricbeat系统概述仪表板：http://10.20.0.11:5601/app/kibana#/dashboard/Metricbeat-system-overview-ecs至此，单机版本的部署完成，在部署结果通过验收后，进入下一步骤，启用和配置出es安全管理功能。]]></content>
      <categories>
        <category>elk</category>
      </categories>
      <tags>
        <tag>elk</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[elastic stack 7.2技术栈【七】启用Metricbeat的安全性配置]]></title>
    <url>%2F2019%2F12%2F31%2Felastic%20stack%207.2%E6%8A%80%E6%9C%AF%E6%A0%88%E3%80%90%E4%B8%83%E3%80%91%E2%80%94%E2%80%94%E5%90%AF%E7%94%A8Metricbeat%E7%9A%84%E5%AE%89%E5%85%A8%E6%80%A7%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[我们使用Metricbeat采集es主机节点的系统监控指标数据，以及监控es集群中索引等服务。对每个es服务节点编辑elasticsearch.yml，以启用监控数据采集1xpack.monitoring.collection.enabled: true在每个节点的metricbeat部署目录下，启用elasticsearch-xpack模块1./metricbeat modules enable elasticsearch-xpack进入elasticsearch部署目录，为每个节点上的Metricbeat制作一个数字证书123./bin/elasticsearch-certutil cert --ca elastic-stack-ca.p12 --name es-node1 --ip 10.20.0.11 --pem./bin/elasticsearch-certutil cert --ca elastic-stack-ca.p12 --name es-node2 --ip 10.20.0.12 --pem./bin/elasticsearch-certutil cert --ca elastic-stack-ca.p12 --name es-node3 --ip 10.20.0.13 --pem执行以上命令，将输出结果分别保存为es-node1.zip es-node2.zip es-node3.zip将压缩包解压后的证书文件，分别部署到每个主机节点上Metricbeat下的certs子目录中，文件属主elastic.elastic，访问权限600因为我们使用的是自签CA证书，所以还需要把CA证书公钥文件cacert.pem，同样在certs目录中放一份编辑modules.d/elasticsearch-xpack.yml文件123456789101112131415161718- module: elasticsearch metricsets: - ccr - cluster_stats - index - index_recovery - index_summary - ml_job - node_stats - shard period: 10s hosts: [&quot;https://10.20.0.11:9200&quot;] username: &quot;remote_monitoring_user&quot; password: &quot;remote_monitoring_user123&quot; ssl.certificate_authorities: [&quot;certs/cacert.pem&quot;] ssl.certificate: &quot;certs/es-node1.crt&quot; ssl.key: &quot;certs/es-node1.key&quot; xpack.enabled: true这里使用了一个es提供的内建管理账号remote_monitoring_user编辑metricbeat.yml文件在节点1上面：12345678910111213setup.kibana: host: &quot;https://es-node1:5601&quot; username: &quot;metricbeat_internal&quot; password: &quot;$&#123;ES_PWD&#125;&quot;output.elasticsearch: hosts: [&quot;10.20.0.11:9200&quot;] protocol: &quot;https&quot; ssl.certificate_authorities: [&quot;certs/cacert.pem&quot;] username: &quot;metricbeat_internal&quot; password: &quot;$&#123;ES_PWD&#125;&quot; ssl.certificate: &quot;certs/es-node1.crt&quot; ssl.key: &quot;certs/es-node1.key&quot;在这里我们使用前面章节中创建的账号metricbeat_internal同时作为metricbeat访问kibana和elasticsearch时的授权账号；需要注意的是，在启用了es安全特性后，metricbeat采集和向es索引写入监控指标数据时需要拥有适当的角色授权，请使用elastic账号登录kibana并为metricbeat_internal用户增加remote_monitoring_collector、remote_monitoring_agent两个角色的授权。参照上面的说明，对节点2和节点3上的Metricbeat配置文件metricbeat.yml进行修改，注意要引用本节点的密钥文件名。启动Metricbeat服务1./metricbeat -e确认日志输出中没有需要引起注意的warning或error信息登录kibana并查看Stack Monitor页面，如果能正常看到以下内容则表示监控数据采集和展示是正确的]]></content>
      <categories>
        <category>elk</category>
      </categories>
      <tags>
        <tag>elk</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[elastic stack 7.2技术栈【六】启用Kibana中的安全性配置]]></title>
    <url>%2F2019%2F12%2F31%2Felastic%20stack%207.2%E6%8A%80%E6%9C%AF%E6%A0%88%E3%80%90%E5%85%AD%E3%80%91%E2%80%94%E2%80%94%E5%90%AF%E7%94%A8Kibana%E4%B8%AD%E7%9A%84%E5%AE%89%E5%85%A8%E6%80%A7%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[本章节主要包括配置kibana的会话安全和加密通信两项内容。配置xpack.security.encryptionKey属性用于加密cookie中的凭据的任意字符串，长度不超过32个字符。 至关重要的是，这个密钥不会暴露给Kibana的用户。 默认情况下，会在内存中自动生成一个值。 如果使用该默认行为，则在Kibana重新启动时，所有会话都将失效。设置会话超时时间为30min。在kibana.yml配置文件中：12xpack.security.encryptionKey: &quot;something_at_least_32_characters&quot;xpack.security.sessionTimeout: 1800000配置kibana使用https访问es服务在节点1上es家目录下，为kibana服务制作数字证书：1./bin/elasticsearch-certutil cert --ca elastic-stack-ca.p12 --name es-node1 --ip 10.20.0.11 --pem将得到的证书文件存放到kibana部署路径的配置文件目录下certs子目录中：1234[elastic@es-node1 certs]$ pwd/opt/kibana-7.2.0-linux-x86_64/config/certs[elastic@es-node1 certs]$ lses-node1.crt es-node1.key在kibana.yml文件中为kibana配置ssl的相关参数123server.ssl.enabled: trueserver.ssl.key: config/certs/es-node1.keyserver.ssl.certificate: config/certs/es-node1.crt此时，启动kibana服务，验证通过 https://10.20.0.11:5601 是否可以成功访问网站。至此，我们在访问kibana服务时已经实现的加密通信。接下来，继续将kibana访问elasticsearch服务的过程切换到https加密通信。配置kibana通过https访问elasticsearch因为我们使用的是自签名证书，所以需要为kibana提供ca证书。我们先在节点1上，将pkcs12格式的ca证书转换为kibana适用的pem格式，其中cacert.pem是证书公钥文件。12openssl pkcs12 -nocerts -nodes -in elastic-stack-ca.p12 -out private.pemopenssl pkcs12 -clcerts -nokeys -in elastic-stack-ca.p12 -out cacert.pem在kibana.yml文件中配置elasticsearch.hosts参数，并指定ca证书文件位置：：12elasticsearch.hosts: [&quot;https://10.20.0.11:9200&quot;]elasticsearch.ssl.certificateAuthorities: [ &quot;config/certs/cacert.pem&quot; ]将Kibana配置为通过HTTPS连接到Elasticsearch监控集群，在这里我们使用的是同一个es集群：12xpack.monitoring.elasticsearch.hosts: [&quot;https://10.20.0.11:9200&quot;]xpack.monitoring.elasticsearch.ssl.certificateAuthorities: config/certs/cacert.pem配置三个节点上的elasticsearch服务，在elasticsearch.yml文件中补充以下http.ssl服务使用的配置参数：123xpack.security.http.ssl.enabled: truexpack.security.http.ssl.keystore.path: certs/$&#123;node.name&#125;.p12xpack.security.http.ssl.truststore.path: certs/$&#123;node.name&#125;.p12将http.ssl访问证书时使用的密码信息存储在密钥库中：12bin/elasticsearch-keystore add xpack.security.http.ssl.keystore.secure_passwordbin/elasticsearch-keystore add xpack.security.http.ssl.truststore.secure_password在前面章节中我们为节点证书设置的密码是：es-node123三个es节点上均需要设置重启服务进程并验证配置结果重启三节点上的es服务进程：bin/elasticsearch重启节点1上的kibana服务进程：bin/kibana看到下面这样的日志输出时，可以确认配置成功了：12log [18:46:19.967] [info][listening] Server running at https://es-node1:5601log [18:46:19.997] [info][status][plugin:spaces@7.2.0] Status changed from yellow to green - Read]]></content>
      <categories>
        <category>elk</category>
      </categories>
      <tags>
        <tag>elk</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[elastic stack 7.2技术栈【二】操作系统的初始化配置]]></title>
    <url>%2F2019%2F12%2F31%2Felastic%20stack%207.2%E6%8A%80%E6%9C%AF%E6%A0%88%E3%80%90%E4%BA%8C%E3%80%91%E2%80%94%E2%80%94%20%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E7%9A%84%E5%88%9D%E5%A7%8B%E5%8C%96%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[操作系统使用centos7 minimual版本。elasticsearch技术栈均采用 7.2版本，默认使用basic许可，可免费使用xpack部分安全管理服务。部署使用的主机资源规划使用3个主机节点10.20.0.11 es-node110.20.0.12 es-node210.20.0.13 es-node3升级至最新的系统小版本1yum -y update &amp;&amp; hostnamectl set-hostname es-node1在设置节点2，节点3时请注意变更为正确的主机名禁用swap因为es是在jvm中运行的，在内存使用上涉及不到使用swap。12swapoff -ased -i &apos;/swap/d&apos; /etc/fstab系统可用资源限制文件句柄与最大线程并发数量：12345678cat &lt;&lt; EOF &gt;&gt; /etc/security/limits.conf* soft nofile 65535* hard nofile 65535* soft nproc 4096* hard nproc 4096* soft memlock unlimited* hard memlock unlimitedEOF虚拟内存Elasticsearch uses a mmapfs directory by default to store its indices.12345cat &lt;&lt; EOF &gt;&gt; /etc/sysctl.confvm.max_map_count=262144EOFsysctl -pMMap FS类型通过将文件映射到内存（mmap）来将分片索引存储在文件系统上（映射到Lucene MMapDirectory）。 内存映射使用进程中虚拟内存地址空间的一部分等于要映射的文件的大小。 在使用此类之前，请确保您已经拥有足够的虚拟地址空间。创建es专用的系统用户useradd elastic配置firewalld防火墙规则123firewall-cmd --zone=public --permanent --add-rich-rule=&quot;rule family=&apos;ipv4&apos; source address=&apos;10.20.0.0/24&apos; accept&quot;firewall-cmd --reloadfirewall-cmd --list-all关闭selinux12setenforce 0sed -i &apos;/SELINUX/s/enforcing/disabled/&apos; /etc/selinux/config配置/etc/hosts12345cat &lt;&lt; EOF &gt;&gt; /etc/hosts10.20.0.11 es-node110.20.0.12 es-node210.20.0.13 es-node3EOF重启系统验证各项配置生效后，继续本系列文章的下面章节的配置。]]></content>
      <categories>
        <category>elk</category>
      </categories>
      <tags>
        <tag>elk</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[elastic stack 7.2技术栈【八】Filebeat与Logstash两个工具之间怎样配置SSL加密通信]]></title>
    <url>%2F2019%2F12%2F31%2Felastic%20stack%207.2%E6%8A%80%E6%9C%AF%E6%A0%88%E3%80%90%E5%85%AB%E3%80%91%E2%80%94%E2%80%94%20Filebeat%E4%B8%8ELogstash%E4%B8%A4%E4%B8%AA%E5%B7%A5%E5%85%B7%E4%B9%8B%E9%97%B4%E6%80%8E%E6%A0%B7%E9%85%8D%E7%BD%AESSL%E5%8A%A0%E5%AF%86%E9%80%9A%E4%BF%A1%2F</url>
    <content type="text"><![CDATA[在目前比较流行的技术实践方案里，Filebeat大多用作日志采集工具，虽然它可以直接将数据写入Elasticsearch中，但出于各方面的效率考虑，一般都会通过Logstash做一个日志数据的汇聚、转换和分发处理。为了保证应用日志数据的传输安全，我们可以使用SSL相互身份验证来保护Filebeat和Logstash之间的连接。 这可以确保Filebeat仅将加密数据发送到受信任的Logstash服务器，并确保Logstash服务器仅从受信任的Filebeat客户端接收数据。下面就讲述一下配置Filebeat与Logstash之间进行加密通信的方法。全文是在CentOS7上基于Elastic 7.2技术栈所验证的。我们需要一个自签的CA证书，以及使用该CA证书签署的两份数据证书。一份是给Logstash作为server端验证自己身份时使用，一份是提供给Filebeat客户端验证自己身份使用。在这里，我们是直接利用的Elasticsearch随安装包提供的数字证书工具elasticsearch-certutil来制作需要的证书。如果您需要对该工具做更多的了解，参考官网的这个资料：elasticsearch-certutil制作自签的CA证书在Linux下，进入到Elasticsearch程序的部署家目录中，执行以下命令可以生成一份自签的CA证书：1./bin/elasticsearch-certutil ca使用默认输出文件名elastic-stack-ca.p12，并为证书设置访问口令。根据证书文件导出一份CA公钥文件，用于后续各应用配置文件中引用CA公钥时使用：1openssl pkcs12 -clcerts -nokeys -in elastic-stack-ca.p12 -out cacert.pem制作Logstash使用的数字证书Logstash服务在启用SSL加密通信支持时，会有一个特殊的问题。因为Logstash在底层是通过集成了Netty来提供的对外服务端口，而Netty在支持数字证书这一功能上面，有一个局限性，即Netty仅支持使用PKCS#8的密钥格式。对于我们使用最多的PEM格式证书，Logstash会毫不留情地打印出以下异常信息：123456[2019-08-06T14:48:35,643][ERROR][logstash.inputs.beats ] Looks like you either have a bad certificate, an invalid key or your private key was not in PKCS8 format.[2019-08-06T14:48:35,643][WARN ][io.netty.channel.ChannelInitializer] Failed to initialize a channel. Closing: [id: 0x81e7ac55, L:/172.17.0.6:5044 - R:/100.200.106.60:32500]java.lang.IllegalArgumentException: File does not contain valid private key: /data/logstash/config/certs/logstash.key at io.netty.handler.ssl.SslContextBuilder.keyManager(SslContextBuilder.java:270) ~[netty-all-4.1.30.Final.jar:4.1.30.Final] at io.netty.handler.ssl.SslContextBuilder.forServer(SslContextBuilder.java:90) ~[netty-all-4.1.30.Final.jar:4.1.30.Final] at org.logstash.netty.SslSimpleBuilder.build(SslSimpleBuilder.java:112) ~[logstash-input-beats-6.0.0.jar:?]由于Elastic官网上对于Filebeat和Logstash之间配置SSL加密通信时的说明资料对制作Logstash使用的数字证书的操作一带而过，只是简单的说既可以使用elasticsearch自带的证书工具，也可以使用通用的openssl。所以，按照Elastic技术栈中处理其它工具配置SSL功能支持时的方法，制作和得到PEM格式的证书后，便会遇到Logstash抛出的上面的异常信息了。由于Logstash打印的错误信息比较多，分析了很长时间才定位到是由于未使用PKCS8密钥格式所引发的。有兴趣进一步了解Netty这方面配置特性的同学，可以参考这个链接：https://netty.io/wiki/sslcontextbuilder-and-private-key.html 。制作Logstash Server证书的正确方法1234./bin/elasticsearch-certutil cert --ca elastic-stack-ca.p12 --name logstash --dns testserver --ip 172.17.0.6 --pemunzip certificate-bundle.zipcd logstashopenssl pkcs8 -in logstash.key -topk8 -nocrypt -out logstash.p8经由命令1，我们使用自签的CA签署生成了一份名为logstash的数字证书；得到的数字证书是pem格式的，解压后会各有一个.key和.crt后缀的文件；命令3，使用openssl转换出一份PKCS#8格式的密钥文件，即logstash.p8；对于我们制作的logstash.crt的证书，可以使用以下命令查看证书中的信息：1openssl x509 -in logstash.crt -text将证书文件部署到Logstash配置目录下假定我们部署Logstash的路径为/data/logstash ，我们创建下面这样的证书存放目录，并把包括logstash证书和ca证书在内的文件部署于此。1234mkdir /data/logstash/config/certs$ ls /data/logstash/config/certscacert.pem logstash.crt logstash.key logstash.p8安全起见，将以上文件权限调整为600 。为Filebeat服务制作和配置数字证书回到刚才我们制作CA证书的地方，继续为Filebeat生成一份数字证书：1./bin/elasticsearch-certutil cert --ca elastic-stack-ca.p12 --name filebeat --dns test-filebeat --pem在生成证书时至少需要提供–dns参数的值，可以使用逗号分隔指定多个，简单处理的话可以直接指定为Filebeat工具所在主机的hostname即可。也可以使用–ip为证书绑定IP地址，或者二者同时使用。最终会得到一个zip文件，内含PEM格式的证书与密钥文件。请将得到的数字证书和密钥文件，以及ca证书文件，存放到Filebeat以下部署路径中：1234mkdir /data/filebeat/certsls /data/filebeat/certsfilebeat.crt filebeat.key cacert.pem安全起见，将以上文件权限调整为600 。配置Filebeat使用SSL编辑filebeat.yml文件，参照以下内容进行配置：12345output.logstash: hosts: [&quot;log.mytestserver.com:5044&quot;] ssl.certificate_authorities: [&quot;/data/filebeat/certs/cacert.pem&quot;] ssl.certificate: &quot;/data/filebeat/certs/filebeat.crt&quot; ssl.key: &quot;/data/filebeat/certs/filebeat.key&quot;配置Logstash在通过beats接收日志数据时使用SSL1234567891011121314input &#123; beats &#123; id =&gt; &quot;logstash-1&quot; port =&gt; 5044 codec =&gt; plain &#123; charset =&gt; &quot;UTF-8&quot; &#125; ssl =&gt; true ssl_certificate_authorities =&gt; [&quot;/data/logstash/config/certs/cacert.pem&quot;] ssl_certificate =&gt; &quot;/data/logstash/config/certs/logstash.crt&quot; ssl_key =&gt; &quot;/data/logstash/config/certs/logstash.p8&quot; ssl_verify_mode =&gt; &quot;force_peer&quot; &#125;&#125;如上所示，在ssl_key参数中，引用的是我们制作的PCKS#8格式的密钥文件。Logstash的filter和output插件配置不是本文的重点，这里直接省略掉了。启动Filebeat服务并观察日志观察日志输出，显示有类似以下信息时表示Filebeat正常连接到Logstash服务且SSL功能工作正常。122019-08-06T16:29:08.745+0800 INFO pipeline/output.go:95 Connecting to backoff(async(tcp://log.mytestserver.com:5044))2019-08-06T16:29:08.992+0800 INFO pipeline/output.go:105 Connection to backoff(async(tcp://log.mytestserver.com:5044)) established参考材料：https://www.elastic.co/guide/en/beats/filebeat/7.2/configuring-ssl-logstash.html]]></content>
      <categories>
        <category>elk</category>
      </categories>
      <tags>
        <tag>elk</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[网络_udp]]></title>
    <url>%2F2019%2F12%2F31%2F%E7%BD%91%E7%BB%9C_udp%2F</url>
    <content type="text"><![CDATA[网络通信概述什么是网络一种辅助双方或者多方能够连接在一起的工具使用网络的目的就是为了联通多方然后进行通信用的，即把数据从一方传递给另外一方,为了让在不同的电脑上运行的软件之间能够互相传递数据，就需要借助网络的功能小总结:使用网络能够把多方链接在一起，然后可以进行数据传递所谓的网络编程就是，让在不同的电脑上的软件能够进行数据传递，即进程之间的通信IP地址什么是IP地址用来在网络中标记一台电脑，比如192.168.1.1；在本地局域网上是唯一的。ip地址的作用ip地址的分类（了解）每一个IP地址包括两部分：网络地址和主机地址A类IP地址一个A类IP地址由1字节的网络地址和3字节主机地址组成，网络地址的最高位必须是“0”，地址范围1.0.0.1-126.255.255.254二进制表示为：00000001 00000000 00000000 00000001 - 01111110 11111111 11111111 11111110可用的A类网络有126个，每个网络能容纳1677214个主机B类IP地址一个B类IP地址由2个字节的网络地址和2个字节的主机地址组成，网络地址的最高位必须是“10”，地址范围128.1.0.1-191.255.255.254二进制表示为：10000000 00000001 00000000 00000001 - 10111111 11111111 11111111 11111110可用的B类网络有16384个，每个网络能容纳65534主机C类IP地址一个C类IP地址由3字节的网络地址和1字节的主机地址组成，网络地址的最高位必须是“110”范围192.0.1.1-223.255.255.254二进制表示为: 11000000 00000000 00000001 00000001 - 11011111 11111111 11111110 11111110C类网络可达2097152个，每个网络能容纳254个主机D类地址用于多点广播D类IP地址第一个字节以“1110”开始，它是一个专门保留的地址。它并不指向特定的网络，目前这一类地址被用在多点广播（Multicast）中多点广播地址用来一次寻址一组计算机 s 地址范围224.0.0.1-239.255.255.254E类IP地址以“1111”开始，为将来使用保留E类地址保留，仅作实验和开发用私有ip在这么多网络IP中，国际规定有一部分IP地址是用于我们的局域网使用，也就是属于私网IP，不在公网中使用的，它们的范围是：10.0.0.0～10.255.255.255172.16.0.0～172.31.255.255192.168.0.0～192.168.255.255注意IP地址127．0．0．1~127．255．255．255用于回路测试，如：127.0.0.1可以代表本机IP地址，用http://127.0.0.1就可以测试本机中配置的Web服务器。Linux命令(ping, ifconfig)查看或配置网卡信息：ifconfig敲：ifconfig，它会显示所有网卡的信息测试远程主机连通性：ping通常用ping来检测网络是否正常端口什么是端口端口就好一个房子的门，是出入这间房子的必经之路。如果一个程序需要收发网络数据，那么就需要有这样的端口.在linux系统中，端口可以有65536（2的16次方）个之多！既然有这么多，操作系统为了统一管理，所以进行了编号，这就是端口号。端口号端口是通过端口号来标记的，端口号只有整数，范围是从0到65535注意：端口数不一样的*nix系统不一样，还可以手动修改端口是怎样分配的端口号不是随意使用的，而是按照一定的规定进行分配。端口的分类标准有好几种，我们这里不做详细讲解，只介绍一下知名端口和动态端口知名端口（Well Known Ports）知名端口是众所周知的端口号，范围从0到102380端口分配给HTTP服务21端口分配给FTP服务可以理解为，一些常用的功能使用的号码是估计的，好比 电话号码110、10086、10010一样一般情况下，如果一个程序需要使用知名端口的需要有root权限。动态端口（Dynamic Ports）动态端口的范围是从1024到65535之所以称为动态端口，是因为它一般不固定分配某种服务，而是动态分配。动态分配是指当一个系统程序或应用程序程序需要网络通信时，它向主机申请一个端口，主机从可用的端口号中分配一个供它使用。当这个程序关闭时，同时也就释放了所占用的端口号。怎样查看端口 ？用“netstat －an”查看端口状态lsof -i [tcp/udp]:2425小总结端口有什么用呢 ？ 我们知道，一台拥有IP地址的主机可以提供许多服务，比如HTTP（万维网服务）、FTP（文件传输）、SMTP（电子邮件）等，这些服务完全可以通过1个IP地址来实现。那么，主机是怎样区分不同的网络服务呢？显然不能只靠IP地址，因为IP地址与网络服务的关系是一对多的关系。实际上是通过“IP地址+端口号”来区分不同的服务的。 需要注意的是，端口并不是一一对应的。比如你的电脑作为客户机访问一台WWW服务器时，WWW服务器使用“80”端口与你的电脑通信，但你的电脑则可能使用“3457”这样的端口。socket简介不同电脑上的进程之间如何通信首要解决的问题是如何唯一标识一个进程，否则通信无从谈起！在1台电脑上可以通过进程号（PID）来唯一标识一个进程，但是在网络中这是行不通的。其实TCP/IP协议族已经帮我们解决了这个问题，网络层的“ip地址”可以唯一标识网络中的主机，而传输层的“协议+端口”可以唯一标识主机中的应用进程（进程）。这样利用ip地址，协议，端口就可以标识网络的进程了，网络中的进程通信就可以利用这个标志与其它进程进行交互。注意:所谓进程指的是：运行的程序以及运行时用到的资源这个整体称之为进程（在讲解多任务编程时进行详细讲解）所谓进程间通信指的是：运行的程序之间的数据共享后面课程中会详细说到，像网络层等知识，不要着急什么是socketsocket(简称 套接字) 是进程间通信的一种方式，它与其他进程间通信的一个主要不同是：它能实现不同主机间的进程间通信，我们网络上各种各样的服务大多都是基于 Socket 来完成通信的例如我们每天浏览网页、QQ 聊天、收发 email 等等socket套接字走的是全双工创建socket在 Python 中 使用socket 模块的函数 socket 就可以完成：12import socketsocket.socket(AddressFamily, Type)说明：函数 socket.socket 创建一个 socket，该函数带有两个参数：Address Family：可以选择 AF_INET（用于 Internet 进程间通信） 或者 AF_UNIX（用于同一台机器进程间通信）,实际工作中常用AF_INETType：套接字类型，可以是 SOCK_STREAM（流式套接字，主要用于 TCP 协议）或者 SOCK_DGRAM（数据报套接字，主要用于 UDP 协议）创建一个tcp socket（tcp套接字）123456789import socket# 创建tcp的套接字s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)# ...这里是使用套接字的功能（省略）...# 不用的时候，关闭套接字s.close()创建一个udp socket（udp套接字）123456789import socket# 创建udp的套接字s = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)# ...这里是使用套接字的功能（省略）...# 不用的时候，关闭套接字s.close()说明套接字使用流程 与 文件的使用流程很类似创建套接字使用套接字收/发数据关闭套接字udp网络程序-发送、接收数据udp网络程序-发送数据创建一个基于udp的网络程序流程很简单，具体步骤如下：1. 创建客户端套接字 2. 发送/接收数据 3. 关闭套接字 代码如下：1234567891011121314151617181920#coding=utf-8from socket import *# 1. 创建udp套接字udp_socket = socket(AF_INET, SOCK_DGRAM)# 2. 准备接收方的地址# '192.168.1.103'表示目的ip地址# 8080表示目的端口dest_addr = ('192.168.1.103', 8080) # 注意 是元组，ip是字符串，端口是数字# 3. 从键盘获取数据send_data = input("请输入要发送的数据:")# 4. 发送数据到指定的电脑上的指定程序中udp_socket.sendto(send_data.encode('utf-8'), dest_addr)# 5. 关闭套接字udp_socket.close()运行现象：在Ubuntu中运行脚本：在windows中运行“网络调试助手”：udp网络程序-发送、接收数据12345678910111213141516171819202122232425262728#coding=utf-8from socket import *# 1. 创建udp套接字udp_socket = socket(AF_INET, SOCK_DGRAM)# 2. 准备接收方的地址dest_addr = ('192.168.236.129', 8080)# 3. 从键盘获取数据send_data = input("请输入要发送的数据:")# 4. 发送数据到指定的电脑上udp_socket.sendto(send_data.encode('utf-8'), dest_addr)# 5. 等待接收对方发送的数据recv_data = udp_socket.recvfrom(1024) # 1024表示本次接收的最大字节数# 6. 显示对方发送的数据# 接收到的数据recv_data是一个元组# 第1个元素是对方发送的数据# 第2个元素是对方的ip和端口print(recv_data[0].decode('gbk'))print(recv_data[1])# 7. 关闭套接字udp_socket.close()python脚本：网络调试助手截图：python3编码转换12str-&gt;bytes:encode编码bytes-&gt;str:decode解码字符串通过编码成为字节码，字节码通过解码成为字符串。12345678910111213141516171819&gt;&gt;&gt; text = '我是文本'&gt;&gt;&gt; text'我是文本'&gt;&gt;&gt; print(text)我是文本&gt;&gt;&gt; bytesText = text.encode()&gt;&gt;&gt; bytesTextb'\xe6\x88\x91\xe6\x98\xaf\xe6\x96\x87\xe6\x9c\xac'&gt;&gt;&gt; print(bytesText)b'\xe6\x88\x91\xe6\x98\xaf\xe6\x96\x87\xe6\x9c\xac'&gt;&gt;&gt; type(text)&lt;class 'str'&gt;&gt;&gt;&gt; type(bytesText)&lt;class 'bytes'&gt;&gt;&gt;&gt; textDecode = bytesText.decode()&gt;&gt;&gt; textDecode'我是文本'&gt;&gt;&gt; print(textDecode)我是文本其中decode()与encode()方法可以接受参数，其声明分别为:12bytes.decode(encoding="utf-8", errors="strict")str.encode(encoding="utf-8", errors="strict")其中的encoding是指在解码编码过程中使用的编码(此处指“编码方案”是名词)，errors是指错误的处理方案。udp绑定信息udp网络程序-端口问题会变的端口号重新运行多次脚本，然后在“网络调试助手”中，看到的现象如下：说明：每重新运行一次网络程序，上图中红圈中的数字，不一样的原因在于，这个数字标识这个网络程序，当重新运行时，如果没有确定到底用哪个，系统默认会随机分配记住一点：这个网络程序在运行的过程中，这个就唯一标识这个程序，所以如果其他电脑上的网络程序如果想要向此程序发送数据，那么就需要向这个数字（即端口）标识的程序发送即可udp绑定信息绑定信息一般情况下，在一台电脑上运行的网络程序有很多，为了不与其他的网络程序占用同一个端口号，往往在编程中，udp的端口号一般不绑定但是如果需要做成一个服务器端的程序的话，是需要绑定的，想想看这又是为什么呢？如果报警电话每天都在变，想必世界就会乱了，所以一般服务性的程序，往往需要一个固定的端口号，这就是所谓的端口绑定绑定示例12345678910111213141516171819#coding=utf-8from socket import *# 1. 创建套接字udp_socket = socket(AF_INET, SOCK_DGRAM)# 2. 绑定本地的相关信息，如果一个网络程序不绑定，则系统会随机分配local_addr = ('', 7788) # ip地址和端口号，ip一般不用写，表示本机的任何一个ipudp_socket.bind(local_addr)# 3. 等待接收对方发送的数据recv_data = udp_socket.recvfrom(1024) # 1024表示本次接收的最大字节数# 4. 显示接收到的数据print(recv_data[0].decode('gbk'))# 5. 关闭套接字udp_socket.close()运行结果：总结:一个udp网络程序，可以不绑定，此时操作系统会随机进行分配一个端口，如果重新运行此程序端口可能会发生变化一个udp网络程序，也可以绑定信息（ip地址，端口号），如果绑定成功，那么操作系统用这个端口号来进行区别收到的网络数据是否是此进程的网络通信过程(简单版)网络通信过程中，之所需要ip、port等，就是为了能够将一个复杂的通信过程进行任务划分，从而保证数据准确无误的传递应用：udp聊天器说明在一个电脑中编写1个程序，有2个功能1.获取键盘数据，并将其发送给对方2.接收数据并显示并且功能数据进行选择以上的2个功能调用参考代码:12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849#coding=utf-8import socketdef send_msg(udp_socket): """获取键盘数据，并将其发送给对方""" # 1. 从键盘输入数据 msg = input("\n请输入要发送的数据:") # 2. 输入对方的ip地址 dest_ip = input("\n请输入对方的ip地址:") # 3. 输入对方的port dest_port = int(input("\n请输入对方的port:")) # 4. 发送数据 udp_socket.sendto(msg.encode("utf-8"), (dest_ip, dest_port))def recv_msg(udp_socket): """接收数据并显示""" # 1. 接收数据 recv_msg = udp_socket.recvfrom(1024) # 2. 解码 recv_ip = recv_msg[1] recv_msg = recv_msg[0].decode("utf-8") # 3. 显示接收到的数据 print("&gt;&gt;&gt;%s:%s" % (str(recv_ip), recv_msg))def main(): # 1. 创建套接字 udp_socket = socket.socket(socket.AF_INET, socket.SOCK_DGRAM) # 2. 绑定本地信息 udp_socket.bind(("", 7890)) while True: # 3. 选择功能 print("="*30) print("1:发送消息") print("2:接收消息") print("="*30) op_num = input("请输入要操作的功能序号:") # 4. 根据选择调用相应的函数 if op_num == "1": send_msg(udp_socket) elif op_num == "2": recv_msg(udp_socket) else: print("输入有误，请重新输入...")if __name__ == "__main__": main()想一想以上的程序如果选择了接收数据功能，并且此时没有数据，程序会堵塞在这，那么怎样才能让这个程序收发数据一起进行呢？]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>udp</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[网络_tcp]]></title>
    <url>%2F2019%2F12%2F31%2F%E7%BD%91%E7%BB%9C_tcp%2F</url>
    <content type="text"><![CDATA[TCP简介TCP介绍TCP协议，传输控制协议（英语：Transmission Control Protocol，缩写为 TCP）是一种面向连接的、可靠的、基于字节流的传输层通信协议，由IETF的RFC 793定义。TCP通信需要经过创建连接、数据传送、终止连接三个步骤。TCP通信模型中，在通信开始之前，一定要先建立相关的链接，才能发送数据，类似于生活中，”打电话””TCP特点面向连接通信双方必须先建立连接才能进行数据的传输，双方都必须为该连接分配必要的系统内核资源，以管理连接的状态和连接上的传输。双方间的数据传输都可以通过这一个连接进行。完成数据交换后，双方必须断开此连接，以释放系统资源。这种连接是一对一的，因此TCP不适用于广播的应用程序，基于广播的应用程序请使用UDP协议。可靠传输TCP采用发送应答机制TCP发送的每个报文段都必须得到接收方的应答才认为这个TCP报文段传输成功超时重传发送端发出一个报文段之后就启动定时器，如果在定时时间内没有收到应答就重新发送这个报文段。TCP为了保证不发生丢包，就给每个包一个序号，同时序号也保证了传送到接收端实体的包的按序接收。然后接收端实体对已成功收到的包发回一个相应的确认（ACK）；如果发送端实体在合理的往返时延（RTT）内未收到确认，那么对应的数据包就被假设为已丢失将会被进行重传。错误校验TCP用一个校验和函数来检验数据是否有错误；在发送和接收时都要计算校验和。流量控制和阻塞管理流量控制用来避免主机发送得过快而使接收方来不及完全收下。TCP与UDP的不同点面向连接（确认有创建三方交握，连接已创建才作传输。）有序数据传输重发丢失的数据包舍弃重复的数据包无差错的数据传输阻塞/流量控制udp通信模型udp通信模型中，在通信开始之前，不需要建立相关的链接，只需要发送数据即可，类似于生活中，”写信””TCP通信模型tcp通信模型中，在通信开始之前，一定要先建立相关的链接，才能发送数据，类似于生活中，”打电话””tcp客户端所谓的服务器端：就是提供服务的一方，而客户端，就是需要被服务的一方tcp客户端构建流程tcp的客户端要比服务器端简单很多，如果说服务器端是需要自己买手机、查手机卡、设置铃声、等待别人打电话流程的话，那么客户端就只需要找一个电话亭，拿起电话拨打即可，流程要少很多示例代码：1234567891011121314151617181920212223from socket import *# 创建sockettcp_client_socket = socket(AF_INET, SOCK_STREAM)# 目的信息server_ip = input("请输入服务器ip:")server_port = int(input("请输入服务器port:"))# 链接服务器tcp_client_socket.connect((server_ip, server_port))# 提示用户输入数据send_data = input("请输入要发送的数据：")tcp_client_socket.send(send_data.encode("gbk"))# 接收对方发送过来的数据，最大接收1024个字节recvData = tcp_client_socket.recv(1024)print('接收到的数据为:', recvData.decode('gbk'))# 关闭套接字tcp_client_socket.close()运行流程tcp客户端1234请输入服务器ip:10.10.0.47请输入服务器port:8080请输入要发送的数据：你好啊接收到的数据为: 我很好，你呢网络调试助手tcp服务器在程序中，如果想要完成一个tcp服务器的功能，需要的流程如下：socket创建一个套接字bind绑定ip和portlisten使套接字变为可以被动链接accept等待客户端的链接recv/send接收发送数据一个很简单的tcp服务器如下：12345678910111213141516171819202122232425262728from socket import *# 创建sockettcp_server_socket = socket(AF_INET, SOCK_STREAM)# 本地信息address = ('', 7788)# 绑定tcp_server_socket.bind(address)# 使用socket创建的套接字默认的属性是主动的，使用listen将其变为被动的，这样就可以接收别人的链接了tcp_server_socket.listen(128)# 如果有新的客户端来链接服务器，那么就产生一个新的套接字专门为这个客户端服务# client_socket用来为这个客户端服务# tcp_server_socket就可以省下来专门等待其他新客户端的链接client_socket, clientAddr = tcp_server_socket.accept()# 接收对方发送过来的数据recv_data = client_socket.recv(1024) # 接收1024个字节print('接收到的数据为:', recv_data.decode('gbk'))# 发送一些数据到客户端client_socket.send("thank you !".encode('gbk'))# 关闭为这个客户端服务的套接字，只要关闭了，就意味着为不能再为这个客户端服务了，如果还需要服务，只能再次重新连接client_socket.close()运行流程tcp服务器1接收到的数据为: 你在么？网络调试助手tcp注意点tcp服务器一般情况下都需要绑定，否则客户端找不到这个服务器tcp客户端一般不绑定，因为是主动链接服务器，所以只要确定好服务器的ip、port等信息就好，本地客户端可以随机tcp服务器中通过listen可以将socket创建出来的主动套接字变为被动的，这是做tcp服务器时必须要做的当客户端需要链接服务器时，就需要使用connect进行链接，udp是不需要链接的而是直接发送，但是tcp必须先链接，只有链接成功才能通信当一个tcp客户端连接服务器时，服务器端会有1个新的套接字，这个套接字用来标记这个客户端，单独为这个客户端服务listen后的套接字是被动套接字，用来接收新的客户端的链接请求的，而accept返回的新套接字是标记这个新客户端的关闭listen后的套接字意味着被动套接字关闭了，会导致新的客户端不能够链接服务器，但是之前已经链接成功的客户端正常通信。关闭accept返回的套接字意味着这个客户端已经服务完毕当客户端的套接字调用close后，服务器端会recv解堵塞，并且返回的长度为0，因此服务器可以通过返回数据的长度来区别客户端是否已经下线案例:文件下载器服务器 参考代码如下:1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950from socket import *import sysdef get_file_content(file_name): """获取文件的内容""" try: with open(file_name, "rb") as f: content = f.read() return content except: print("没有下载的文件:%s" % file_name)def main(): if len(sys.argv) != 2: print("请按照如下方式运行：python3 xxx.py 7890") return else: # 运行方式为python3 xxx.py 7890 port = int(sys.argv[1]) # 创建socket tcp_server_socket = socket(AF_INET, SOCK_STREAM) # 本地信息 address = ('', port) # 绑定本地信息 tcp_server_socket.bind(address) # 将主动套接字变为被动套接字 tcp_server_socket.listen(128) while True: # 等待客户端的链接，即为这个客户端发送文件 client_socket, clientAddr = tcp_server_socket.accept() # 接收对方发送过来的数据 recv_data = client_socket.recv(1024) # 接收1024个字节 file_name = recv_data.decode("utf-8") print("对方请求下载的文件名为:%s" % file_name) file_content = get_file_content(file_name) # 发送文件的数据给客户端 # 因为获取打开文件时是以rb方式打开，所以file_content中的数据已经是二进制的格式，因此不需要encode编码 if file_content: client_socket.send(file_content) # 关闭这个套接字 client_socket.close() # 关闭监听套接字 tcp_server_socket.close()if __name__ == "__main__": main()客户端 参考代码如下:123456789101112131415161718192021222324252627282930313233from socket import *def main(): # 创建socket tcp_client_socket = socket(AF_INET, SOCK_STREAM) # 目的信息 server_ip = input("请输入服务器ip:") server_port = int(input("请输入服务器port:")) # 链接服务器 tcp_client_socket.connect((server_ip, server_port)) # 输入需要下载的文件名 file_name = input("请输入要下载的文件名：") # 发送文件下载请求 tcp_client_socket.send(file_name.encode("utf-8")) # 接收对方发送过来的数据，最大接收1024个字节（1K） recv_data = tcp_client_socket.recv(1024) # print('接收到的数据为:', recv_data.decode('utf-8')) # 如果接收到数据再创建文件，否则不创建 if recv_data: with open("[接收]"+file_name, "wb") as f: f.write(recv_data) # 关闭套接字 tcp_client_socket.close()if __name__ == "__main__": main()tcp的3次握手tcp的4次挥手tcp长连接和短连接TCP在真正的读写操作之前，server与client之间必须建立一个连接，当读写操作完成后，双方不再需要这个连接时它们可以释放这个连接，连接的建立通过三次握手，释放则需要四次握手，所以说每个连接的建立都是需要资源消耗和时间消耗的。TCP通信的整个过程，如下图:TCP短连接模拟一种TCP短连接的情况:1. client 向 server 发起连接请求 2. server 接到请求，双方建立连接 3. client 向 server 发送消息 4. server 回应 client 5. 一次读写完成，此时双方任何一个都可以发起 close 操作 在步骤5中，一般都是 client 先发起 close 操作。当然也不排除有特殊的情况。从上面的描述看，短连接一般只会在 client/server 间传递一次读写操作！TCP长连接再模拟一种长连接的情况:1. client 向 server 发起连接 2. server 接到请求，双方建立连接 3. client 向 server 发送消息 4. server 回应 client 5. 一次读写完成，连接不关闭 6. 后续读写操作... 7. 长时间操作之后client发起关闭请求 TCP长/短连接操作过程短连接的操作步骤是：建立连接——数据传输——关闭连接…建立连接——数据传输——关闭连接长连接的操作步骤是：建立连接——数据传输…（保持连接）…数据传输——关闭连接TCP长/短连接的优点和缺点长连接可以省去较多的TCP建立和关闭的操作，减少浪费，节约时间。对于频繁请求资源的客户来说，较适用长连接。client与server之间的连接如果一直不关闭的话，会存在一个问题，随着客户端连接越来越多，server早晚有扛不住的时候，这时候server端需要采取一些策略，如关闭一些长时间没有读写事件发生的连接，这样可以避免一些恶意连接导致server端服务受损；如果条件再允许就可以以客户端机器为颗粒度，限制每个客户端的最大长连接数，这样可以完全避免某个蛋疼的客户端连累后端服务。短连接对于服务器来说管理较为简单，存在的连接都是有用的连接，不需要额外的控制手段。但如果客户请求频繁，将在TCP的建立和关闭操作上浪费时间和带宽。TCP长/短连接的应用场景长连接多用于操作频繁，点对点的通讯，而且连接数不能太多情况。每个TCP连接都需要三次握手，这需要时间，如果每个操作都是先连接，再操作的话那么处理速度会降低很多，所以每个操作完后都不断开，再次处理时直接发送数据包就OK了，不用建立TCP连接。例如：数据库的连接用长连接，如果用短连接频繁的通信会造成socket错误，而且频繁的socket 创建也是对资源的浪费。而像WEB网站的http服务一般都用短链接，因为长连接对于服务端来说会耗费一定的资源，而像WEB网站这么频繁的成千上万甚至上亿客户端的连接用短连接会更省一些资源，如果用长连接，而且同时有成千上万的用户，如果每个用户都占用一个连接的话，那可想而知吧。所以并发量大，但每个用户无需频繁操作情况下需用短链接好。tcp-ip简介什么是协议为了解决不同种族人之间的语言沟通障碍，现规定国际通用语言是英语，这就是一个规定，这就是协议计算机网络沟通用什么不同的计算机只需要能够联网（有线无线都可以）那么就可以相互进行传递数据就像说不同语言的人沟通一样，只要有一种大家都认可都遵守的协议即可，那么这个计算机都遵守的网络通信协议叫做TCP/IP协议TCP/IP协议(族)为了把全世界的所有不同类型的计算机都连接起来，就必须规定一套全球通用的协议，为了实现互联网这个目标，互联网协议族（Internet Protocol Suite）就是通用协议标准。因为互联网协议包含了上百种协议标准，但是最重要的两个协议是TCP和IP协议，所以，大家把互联网的协议简称TCP/IP协议(族)常用的网络协议如下图所示：说明：网际层也称为：网络层网络接口层也称为：链路层另外一套标准:]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>tcp</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[爬取干货集中营的图片]]></title>
    <url>%2F2019%2F12%2F31%2F%E7%88%AC%E5%8F%96%E5%B9%B2%E8%B4%A7%E9%9B%86%E4%B8%AD%E8%90%A5%E7%9A%84%E5%9B%BE%E7%89%87%2F</url>
    <content type="text"><![CDATA[网站地址http://gank.io/ 分析网站网站地址:https://gank.io/, 要爬取的是网站首页上展示出来的图片 在网站首页底部左上角有API地址,点击进入到干货集中营 API 文档 其中有这样的内容显示,如下图： 尝试访问:https://gank.io/api/data/福利/10/1, 出来如下的json数据： 通过API文档描述,可以知道链接地址中的请求个数和请求页数,获取到所有的图片信息。 比如:https://gank.io/api/data/福利/700/1, 请求700条数据,获取第一页,发现返回的结果中总共有670条数据(截止到20190314),第二页没有数据. 再次请求:https://gank.io/api/data/福利/600/1, 请求600条数据,获取第一页,发现确实有600条数据, 然后再请求第二页(https://gank.io/api/data/福利/600/2),有70条数据。 结合以上分析,截止到今天(20190314),共有670条数据,避免使用翻页的情况,就直接使用如下网址获取全部数据:https://gank.io/api/data/福利/700/1 根据获取到的son数据,提取出所需要的图片链接,最后下载这些图片并保存. 实际代码#!/usr/bin/env python # -*- coding: utf-8 -*- import datetime import os from _md5 import md5 import requests from requests import RequestException def get_one_page(url): try: response = requests.get(url) if response.status_code == 200: return response.json() except: return None def utc_to_local(utc_time_str, utc_format=&apos;%Y-%m-%dT%H:%M:%S.%fZ&apos;): &quot;&quot;&quot; &quot;2015-06-05T03:54:29.403Z&quot;格式的时间转换成2015-06-05 11:54:29 &quot;&quot;&quot; local_format = &quot;%Y-%m-%d %H:%M:%S&quot; utc_dt = datetime.datetime.strptime(utc_time_str, utc_format) local_dt = utc_dt + datetime.timedelta(hours=8) time_str = local_dt.strftime(local_format) return time_str def https_to_http(url): &quot;&quot;&quot; 把图片链接是https的换成http &quot;&quot;&quot; if url[0:5] == &apos;https&apos;: url = url.replace(url[0:5], &apos;http&apos;) return url def parse_one_page(html): items = html[&apos;results&apos;] for item in items: yield { &apos;id&apos;: item[&apos;_id&apos;], &apos;publishedAt&apos;: utc_to_local(item[&apos;publishedAt&apos;]), &apos;url&apos;: https_to_http(item[&apos;url&apos;]) } # 请求图片url,获取图片二进制数据 def download_image(url): try: response = requests.get(url) if response.status_code == 200: save_image(response.content) # response.contenter二进制数据 response.text文本数据 return None except RequestException: print(&apos;请求图片出错&apos;, url) return None def save_image(content): &quot;&quot;&quot; 需要提前建好目录D:\\pachong\\gank1\\ &quot;&quot;&quot; file_path = &apos;D:\\pachong\\gank1\\{1}.{2}&apos;.format(os.getcwd(), md5(content).hexdigest(), &apos;jpg&apos;) if not os.path.exists(file_path): with open(file_path, &apos;wb&apos;) as f: f.write(content) def main(): url = &apos;http://gank.io/api/data/福利/700/1&apos; html = get_one_page(url) for item in parse_one_page(html): download_image(item[&apos;url&apos;]) if __name__ == &apos;__main__&apos;: main() 结果分析总共700条json数据,实际只有564条数据,通过分析获取到的图片url,发现有些图片url链接本身已失效,使用浏览器打开这些图片链接会报如下错误: 失效链接展示: 实际结果：]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>爬虫</tag>
        <tag>美女</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[爬取美图录网站图片]]></title>
    <url>%2F2019%2F12%2F31%2F%E7%88%AC%E5%8F%96%E7%BE%8E%E5%9B%BE%E5%BD%95%E7%BD%91%E7%AB%99%E5%9B%BE%E7%89%87%2F</url>
    <content type="text"><![CDATA[网站地址https://www.meitulu.com/ 分析该网站1. 打开网站地址后，查看网站右侧导航菜单，目标是提取出爬取网站图片所需的链接地址，这些链接地址最好是包含整个网站的链接地址。 2. 随便点开一个图集分类下的类别，比如&quot;女神&quot;，，进入到的链接地址是：https://www.meitulu.com/t/nvshen/. 3. 从中随便选一个图集点开，进入到该图集的详情页面。上面显示的该图集的相关信息，下面显示的是该图集的每张图片。 右上角显示的是该图集的当前位置。注意这个当前位置。 4. 再从图集类别中选一个进入到该图集的详情页面，发现右上角也有当前位置。 5. 点击导航菜单中的精选美女，进入到某一个图集详情页面，发现右上角也有当前位置。 6. 点击导航菜单中的日韩美女，进入到某一个图集详情页面，发现右上角也有当前位置。 7. 经过以上分析可知，该网站的图集分为如下三大类：日韩美女，港台美女和国产美女。图集分类中是每一个图集的标签汇总。 8. 提取出图集三大类的地址如下： 日韩美女：https://www.meitulu.com/rihan/, 港台美女：https://www.meitulu.com/gangtai/， 国产美女：https://www.meitulu.com/guochan/, 经查看这三个网址，只有最后的不一样，前面的网址等都是一样的，可以构造列表来遍历循环使用，[&apos;rihan&apos;,&apos;gangtai&apos;,&apos;guochan&apos;] 9. 假如进入国产美女里。通过分析页面信息，每一个图集信息都是在一个li标签里，可以使用正则表达式提取出这些li标签。 10.点击其中一个图集，其链接是：https://www.meitulu.com/item/16889.html, 进入到该图集的详情页面. 最上面显示该图集的图片共有96张，每页显示4张图片，拉到最后的第24页，24*4=96张。 10.查看该图集下的每一个图片链接，发现是在一个img标签里，第一个的4张图片地址依次是： https://mtl.ttsqgs.com/images/img/16889/1.jpg， https://mtl.ttsqgs.com/images/img/16889/2.jpg， https://mtl.ttsqgs.com/images/img/16889/3.jpg， https://mtl.ttsqgs.com/images/img/16889/4.jpg， 最后24页的4张图片地址链接是：https://www.meitulu.com/item/16889_24.html， 每一个图片的地址依次是： https://mtl.ttsqgs.com/images/img/16889/93.jpg， https://mtl.ttsqgs.com/images/img/16889/94.jpg， https://mtl.ttsqgs.com/images/img/16889/95.jpg， https://mtl.ttsqgs.com/images/img/16889/96.jpg 同时页面显示的有&quot;美图录提示：点击图片，查看原尺寸高清大图&quot;,js代码是: function() { window.open(&quot;/img.html?img=&quot; + this.src + &quot;&quot;) } 点击图片进入原尺寸高清大图，复制出网址如下：https://www.meitulu.com/img.html?img=https://mtl.ttsqgs.com/images/img/16889/1.jpg。 11. 分析第10步的图片链接，可以发现： (1)图集链接地址(https://www.meitulu.com/item/16889.html)中的数字16889跟该图集中的每一个张图片的链接地址(https://mtl.ttsqgs.com/images/img/16889/1.jpg)相关. (2)每一个图片的链接地址最后的数字是从1开始的，一直到该图集的总数第96 (2)图集详情页中的图片总数跟图集分页数有关，图片总数除以4，若有余数再加1，得到的数字就是该图集的分页数。 (3)该图集的链接地址也有规律，比如图集的第一页地址是：https://www.meitulu.com/item/16889.html， 第二页的是：https://www.meitulu.com/item/16889_2.html， 最后第24页的是：https://www.meitulu.com/item/16889_24.html。 (4)经过以上分析，优先采用(1)和(2)中得到的规律，提取出每一个图集的名称，图集的链接和图片总数，然后构造该图集下的每一个图片的链接。 (5)图集的链接也有规律，比如国产美女分类，第一页的地址是：https://www.meitulu.com/guochan/， 第二页的地址是：https://www.meitulu.com/guochan/2.html， 第164页的地址是：https://www.meitulu.com/guochan/164.html。 可以采用遍历的方式。 实际操作中的坑1. 使用requests的get方式请求每一个图片的链接，得到的图片是损坏的，向群里其他人请教得知，请求是需要加上headers头部信息，必须有Referer，且Referer参数值还有要求，比如说某张图片的图片地址是：https://mtl.ttsqgs.com/images/img/16889/13.jpg， 则Referer的值是https://www.meitulu.com/img.html?img=https://mtl.ttsqgs.com/images/img/16889/13.jpg， 注意查看这俩网址之间的关系。 2. 请求次数过多会报403 Forbidden，通过使用模块fake_useragent生成随机的User-Agent信息。可以解决一小部分情况，时间一长还是会再次报403 Forbidden，只能再次随机生成不同的User-Agent值。 3. 时间长的话会出现这样一个情况，往后的每个图集只能下载保存前9张图片，以后的图片访问请求均报403 Forbidden，这个估计是封IP了，所以还需要使用代理才行。 4. 在网上找到的一个能用的代理软件，地址是：https://github.com/chenjiandongx/async-proxy-pool， 若使用的redis版本低于3.0，则代理池代码可以直接运行使用，若高于3.0版本，则需要修改其中一个文件，具体如下：async_proxy_pool/database.py，修改其中的第45行，原先是self.redis.zadd(REDIS_KEY, proxy, score)，修改成：self.redis.zadd(REDIS_KEY, ｛proxy： score｝)。 5. 使用代理 (1)运行客户端，启动收集器和校验器：python3 client.py (2)运行服务器，启动 web 服务:python3 server_flask.py (3)获取代理地址信息 import requests proxy = requests.get(&apos;http://192.168.0.200:3289/pop&apos;) proxies = proxy.json() print(proxies) (4)爬虫代码中使用代理 跟(3)合二为一 import requests proxy = requests.get(&apos;http://192.168.0.200:3289/pop&apos;) requests.get(&quot;http://example.org&quot;, proxies=proxies) 实际代码#!/usr/bin/env python # -*- coding: utf-8 -*- import os import random import re import requests from fake_useragent import UserAgent from requests.exceptions import RequestException ​# 获取三大分类页面详细数据 def get_one_page(url): try: response = requests.get(url) if response.status_code == 200: response.encoding = &apos;utf8&apos; return response.text else: return None except RequestException: print(&apos;请求失败&apos;) return None ​# 解析三大分类页面数据，提取需要的数据 def parse_one_page(html): pattern = re.compile(&apos;&lt;li&gt;.*?&lt;p&gt;数量： (.*?) 张&lt;/p&gt;.*?&lt;p class=p_title&gt;&lt;a href=&quot;(.*?)&quot;.*?&gt;(.*?)&lt;/a&gt;&lt;/p&gt;.*?&lt;/li&gt;&apos;, re.S | re.M) items = re.findall(pattern, html) for i in range(len(items)): yield { &apos;num&apos;: int(items[i][0]), # 获取图集的图片总数 &apos;name&apos;: items[i][2], # 获取图集名称 &apos;url&apos;: items[i][1].split(&apos;/&apos;)[4][:-5], # 获取图集id } ​# 获取随机请求头 def GetUserAgent(): ua = UserAgent() return random.choice([ua.safari, ua.firefox, ua.chrome, ua.opera, ua.ie, ua.random]) ​# 获取随机代理地址,因爬取速度慢，暂不使用 def GetProxy(): proxy = requests.get(&apos;http://192.168.0.200:3289/pop&apos;) proxies = proxy.json() return proxies ​# 请求图片链接地址 def download_image(item): try: file_path = item[&apos;name&apos;] # 获取图集名称 id = item[&apos;url&apos;] # 获取图集id num = item[&apos;num&apos;] # 获取图集的图片总数 for i in range(1, num + 1): url = f&apos;https://mtl.ttsqgs.com/images/img/{id}/{i}.jpg&apos; # 使用f-string的方式拼接字符串 headers = { # 构造请求头 &apos;User-Agent&apos;: GetUserAgent(), &apos;Accept&apos;: &apos;text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8&apos;, &apos;Accept-Encoding&apos;: &apos;gzip, deflate, br&apos;, &apos;Accept-Language&apos;: &apos;zh-CN,en-US;q=0.8,zh;q=0.5,en;q=0.3&apos;, &apos;Referer&apos;: &apos;https://www.meitulu.com/img.html?img=%s&apos; % url, } yield { &apos;name&apos;: file_path, &apos;headers&apos;: headers, &apos;i&apos;: i, &apos;url&apos;: url, &apos;num&apos;: num } except RequestException: print(&apos;请求图片出错&apos;) return None ​# 保存图片，使用第三方错误重试模块，该模块需要导入 # @retry(stop_max_attempt_number=3) def save_image(path, x): name = x[&apos;name&apos;] i = x[&apos;i&apos;] headers = x[&apos;headers&apos;] url = x[&apos;url&apos;] num = x[&apos;num&apos;] base_path = f&apos;{os.getcwd()}{os.sep}{path}{os.sep}{name}&apos; if not os.path.exists(base_path): os.makedirs(base_path) save_path = f&apos;{base_path}{os.sep}{i}.jpg&apos; if not os.path.exists(save_path): try: requests.packages.urllib3.disable_warnings() response = requests.get(url, headers=headers, verify=False) except: print(f&apos;请求{name}图集的第{i}张图片链接地址失败,共{num}张,图片链接是{url}&apos;) else: if response.status_code == 200: try: with open(save_path, &apos;wb&apos;) as f: f.write(response.content) print(f&apos;保存{name}图集的第{i}张图片成功,共{num}张&apos;) except: print(f&apos;保存{name}图集的第{i}张图片失败,共{num}张,图片链接是{url}&apos;) else: print(f&apos;请求{name}图集的第{i}张图片链接地址状态不是200,共{num}张,图片链接是{url}&apos;) else: print(f&apos;{name}图集文件夹已存在,请求一下图集&apos;) # 遍历三大分类 def img_url(): img_ict = {&apos;rihan&apos;: 88, &apos;gangtai&apos;: 36, &apos;guochan&apos;: 165} for k, v in img_ict.items(): yield { &apos;name&apos;: k, &apos;num&apos;: v } # 新传分类名称 def main(): for img_u in img_url(): for i in range(1, img_u[&apos;num&apos;]): if i == 1: url = f&apos;https://www.meitulu.com/{img_u[&quot;name&quot;]}/&apos; else: url = f&apos;https://www.meitulu.com/{img_u[&quot;name&quot;]}/{i}.html&apos; html = get_one_page(url) for item in parse_one_page(html): for x in download_image(item): save_image(img_u[&apos;name&apos;], x) ​if name == ‘main‘:main()效果展示]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>爬虫</tag>
        <tag>美女</tag>
        <tag>requests</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[多任务_进程]]></title>
    <url>%2F2019%2F12%2F31%2F%E5%A4%9A%E4%BB%BB%E5%8A%A1_%E8%BF%9B%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[进程以及状态进程程序：例如xxx.py这是程序，是一个静态的进程：一个程序运行起来后，代码+用到的资源 称之为进程，它是操作系统分配资源的基本单元。不仅可以通过线程完成多任务，进程也是可以的进程的状态工作中，任务数往往大于cpu的核数，即一定有一些任务正在执行，而另外一些任务在等待cpu进行执行，因此导致了有了不同的状态就绪态：运行的条件都已经慢去，正在等在cpu执行执行态：cpu正在执行其功能等待态：等待某些条件满足，例如一个程序sleep了，此时就处于等待态进程的创建-multiprocessingmultiprocessing模块就是跨平台版本的多进程模块，提供了一个Process类来代表一个进程对象，这个对象可以理解为是一个独立的进程，可以执行另外的事情2个while循环一起执行1234567891011121314151617from multiprocessing import Processimport timedef run_proc(): """子进程要执行的代码""" while True: print("----2----") time.sleep(1)if __name__=='__main__': p = Process(target=run_proc) p.start() while True: print("----1----") time.sleep(1)说明创建子进程时，只需要传入一个执行函数和函数的参数，创建一个Process实例，用start()方法启动进程pid12345678910111213from multiprocessing import Processimport osimport timedef run_proc(): """子进程要执行的代码""" print('子进程运行中，pid=%d...' % os.getpid()) # os.getpid获取当前进程的进程号 print('子进程将要结束...')if __name__ == '__main__': print('父进程pid: %d' % os.getpid()) # os.getpid获取当前进程的进程号 p = Process(target=run_proc) p.start()Process语法结构Process([group [, target [, name [, args [, kwargs]]]]])target：如果传递了函数的引用，可以任务这个子进程就执行这里的代码args：给target指定的函数传递的参数，以元组的方式传递kwargs：给target指定的函数传递命名参数name：给进程设定一个名字，可以不设定group：指定进程组，大多数情况下用不到Process创建的实例对象的常用方法：start()：启动子进程实例（创建子进程）is_alive()：判断进程子进程是否还在活着join([timeout])：是否等待子进程执行结束，或等待多少秒terminate()：不管任务是否完成，立即终止子进程Process创建的实例对象的常用属性：name：当前进程的别名，默认为Process-N，N为从1开始递增的整数pid：当前进程的pid（进程号）给子进程指定的函数传递参数12345678910111213141516from multiprocessing import Processimport osfrom time import sleepdef run_proc(name, age, **kwargs): for i in range(10): print('子进程运行中，name= %s,age=%d ,pid=%d...' % (name, age, os.getpid())) print(kwargs) sleep(0.2)if __name__=='__main__': p = Process(target=run_proc, args=('test',18), kwargs=&#123;"m":20&#125;) p.start() sleep(1) # 1秒中之后，立即结束子进程 p.terminate() p.join()进程间不共享全局变量12345678910111213141516171819202122232425from multiprocessing import Processimport osimport timenums = [11, 22]def work1(): """子进程要执行的代码""" print("in process1 pid=%d ,nums=%s" % (os.getpid(), nums)) for i in range(3): nums.append(i) time.sleep(1) print("in process1 pid=%d ,nums=%s" % (os.getpid(), nums))def work2(): """子进程要执行的代码""" print("in process2 pid=%d ,nums=%s" % (os.getpid(), nums))if __name__ == '__main__': p1 = Process(target=work1) p1.start() p1.join() p2 = Process(target=work2) p2.start()运行结果:12345in process1 pid=11349 ,nums=[11, 22]in process1 pid=11349 ,nums=[11, 22, 0]in process1 pid=11349 ,nums=[11, 22, 0, 1]in process1 pid=11349 ,nums=[11, 22, 0, 1, 2]in process2 pid=11350 ,nums=[11, 22]进程、线程对比功能进程，能够完成多任务，比如 在一台电脑上能够同时运行多个QQ线程，能够完成多任务，比如 一个QQ中的多个聊天窗口定义的不同进程是系统进行资源分配和调度的一个独立单位.线程是进程的一个实体,是CPU调度和分派的基本单位,它是比进程更小的能独立运行的基本单位.线程自己基本上不拥有系统资源,只拥有一点在运行中必不可少的资源(如程序计数器,一组寄存器和栈),但是它可与同属一个进程的其他的线程共享进程所拥有的全部资源.区别一个程序至少有一个进程,一个进程至少有一个线程.线程的划分尺度小于进程(资源比进程少)，使得多线程程序的并发性高。进程在执行过程中拥有独立的内存单元，而多个线程共享内存，从而极大地提高了程序的运行效率![]/images_jincheng/002.png)线程不能够独立执行，必须依存在进程中可以将进程理解为工厂中的一条流水线，而其中的线程就是这个流水线上的工人优缺点线程和进程在使用上各有优缺点：线程执行开销小，但不利于资源的管理和保护；而进程正相反。进程间通信-QueueProcess之间有时需要通信，操作系统提供了很多机制来实现进程间的通信。Queue的使用可以使用multiprocessing模块的Queue实现多进程之间的数据传递，Queue本身是一个消息列队程序，首先用一个小实例来演示一下Queue的工作原理：123456789101112131415161718192021222324252627from multiprocessing import Queueq=Queue(3) #初始化一个Queue对象，最多可接收三条put消息q.put("消息1") q.put("消息2")print(q.full()) #Falseq.put("消息3")print(q.full()) #True#因为消息列队已满下面的try都会抛出异常，第一个try会等待2秒后再抛出异常，第二个Try会立刻抛出异常try: q.put("消息4",True,2)except: print("消息列队已满，现有消息数量:%s"%q.qsize())try: q.put_nowait("消息4")except: print("消息列队已满，现有消息数量:%s"%q.qsize())#推荐的方式，先判断消息列队是否已满，再写入if not q.full(): q.put_nowait("消息4")#读取消息时，先判断消息列队是否为空，再读取if not q.empty(): for i in range(q.qsize()): print(q.get_nowait())运行结果:1234567FalseTrue消息列队已满，现有消息数量:3消息列队已满，现有消息数量:3消息1消息2消息3说明初始化Queue()对象时（例如：q=Queue()），若括号中没有指定最大可接收的消息数量，或数量为负值，那么就代表可接受的消息数量没有上限（直到内存的尽头）；Queue.qsize()：返回当前队列包含的消息数量；Queue.empty()：如果队列为空，返回True，反之False ；Queue.full()：如果队列满了，返回True,反之False；Queue.get([block[, timeout]])：获取队列中的一条消息，然后将其从列队中移除，block默认值为True；1）如果block使用默认值，且没有设置timeout（单位秒），消息列队如果为空，此时程序将被阻塞（停在读取状态），直到从消息列队读到消息为止，如果设置了timeout，则会等待timeout秒，若还没读取到任何消息，则抛出”Queue.Empty”异常；2）如果block值为False，消息列队如果为空，则会立刻抛出”Queue.Empty”异常；Queue.get_nowait()：相当Queue.get(False)；Queue.put(item,[block[, timeout]])：将item消息写入队列，block默认值为True；1）如果block使用默认值，且没有设置timeout（单位秒），消息列队如果已经没有空间可写入，此时程序将被阻塞（停在写入状态），直到从消息列队腾出空间为止，如果设置了timeout，则会等待timeout秒，若还没空间，则抛出”Queue.Full”异常；2）如果block值为False，消息列队如果没有空间可写入，则会立刻抛出”Queue.Full”异常；Queue.put_nowait(item)：相当Queue.put(item, False)；Queue实例我们以Queue为例，在父进程中创建两个子进程，一个往Queue里写数据，一个从Queue里读数据：1234567891011121314151617181920212223242526272829303132333435from multiprocessing import Process, Queueimport os, time, random# 写数据进程执行的代码:def write(q): for value in ['A', 'B', 'C']: print('Put %s to queue...' % value) q.put(value) time.sleep(random.random())# 读数据进程执行的代码:def read(q): while True: if not q.empty(): value = q.get(True) print('Get %s from queue.' % value) time.sleep(random.random()) else: breakif __name__=='__main__': # 父进程创建Queue，并传给各个子进程： q = Queue() pw = Process(target=write, args=(q,)) pr = Process(target=read, args=(q,)) # 启动子进程pw，写入: pw.start() # 等待pw结束: pw.join() # 启动子进程pr，读取: pr.start() pr.join() # pr进程里是死循环，无法等待其结束，只能强行终止: print('') print('所有数据都写入并且读完')进程池Pool当需要创建的子进程数量不多时，可以直接利用multiprocessing中的Process动态成生多个进程，但如果是上百甚至上千个目标，手动的去创建进程的工作量巨大，此时就可以用到multiprocessing模块提供的Pool方法。初始化Pool时，可以指定一个最大进程数，当有新的请求提交到Pool中时，如果池还没有满，那么就会创建一个新的进程用来执行该请求；但如果池中的进程数已经达到指定的最大值，那么该请求就会等待，直到池中有进程结束，才会用之前的进程来执行新的任务，请看下面的实例：12345678910111213141516171819202122from multiprocessing import Poolimport os, time, randomdef worker(msg): t_start = time.time() print("%s开始执行,进程号为%d" % (msg,os.getpid())) # random.random()随机生成0~1之间的浮点数 time.sleep(random.random()*2) t_stop = time.time() print(msg,"执行完毕，耗时%0.2f" % (t_stop-t_start))if __name__ == '__main__': po = Pool(3) # 定义一个进程池，最大进程数3 for i in range(0, 10): # Pool().apply_async(要调用的目标,(传递给目标的参数元祖,)) # 每次循环将会用空闲出来的子进程去调用目标 po.apply_async(worker, (i,)) print("----start----") po.close() # 关闭进程池，关闭后po不再接收新的请求 po.join() # 等待po中所有子进程执行完成，必须放在close语句之后 print("-----end-----")运行结果:12345678910111213141516171819202122----start----0开始执行,进程号为214661开始执行,进程号为214682开始执行,进程号为214670 执行完毕，耗时1.013开始执行,进程号为214662 执行完毕，耗时1.244开始执行,进程号为214673 执行完毕，耗时0.565开始执行,进程号为214661 执行完毕，耗时1.686开始执行,进程号为214684 执行完毕，耗时0.677开始执行,进程号为214675 执行完毕，耗时0.838开始执行,进程号为214666 执行完毕，耗时0.759开始执行,进程号为214687 执行完毕，耗时1.038 执行完毕，耗时1.059 执行完毕，耗时1.69-----end-----multiprocessing.Pool常用函数解析：apply_async(func[, args[, kwds]]) ：使用非阻塞方式调用func（并行执行，堵塞方式必须等待上一个进程退出才能执行下一个进程），args为传递给func的参数列表，kwds为传递给func的关键字参数列表；close()：关闭Pool，使其不再接受新的任务；terminate()：不管任务是否完成，立即终止；join()：主进程阻塞，等待子进程的退出， 必须在close或terminate之后使用；进程池中的Queue如果要使用Pool创建进程，就需要使用multiprocessing.Manager()中的Queue()，而不是multiprocessing.Queue()，否则会得到一条如下的错误信息：RuntimeError: Queue objects should only be shared between processes through inheritance.下面的实例演示了进程池中的进程如何通信：1234567891011121314151617181920212223242526# 修改import中的Queue为Managerfrom multiprocessing import Manager,Poolimport os,time,randomdef reader(q): print("reader启动(%s),父进程为(%s)" % (os.getpid(), os.getppid())) for i in range(q.qsize()): print("reader从Queue获取到消息：%s" % q.get(True))def writer(q): print("writer启动(%s),父进程为(%s)" % (os.getpid(), os.getppid())) for i in "itcast": q.put(i)if __name__=="__main__": print("(%s) start" % os.getpid()) q = Manager().Queue() # 使用Manager中的Queue po = Pool() po.apply_async(writer, (q,)) time.sleep(1) # 先让上面的任务向Queue存入数据，然后再让下面的任务开始从中取数据 po.apply_async(reader, (q,)) po.close() po.join() print("(%s) End" % os.getpid())运行结果:12345678910(11095) startwriter启动(11097),父进程为(11095)reader启动(11098),父进程为(11095)reader从Queue获取到消息：ireader从Queue获取到消息：treader从Queue获取到消息：creader从Queue获取到消息：areader从Queue获取到消息：sreader从Queue获取到消息：t(11095) End应用：文件夹copy器（多进程版）1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768import multiprocessingimport osimport timeimport randomdef copy_file(queue, file_name,source_folder_name, dest_folder_name): """copy文件到指定的路径""" f_read = open(source_folder_name + "/" + file_name, "rb") f_write = open(dest_folder_name + "/" + file_name, "wb") while True: time.sleep(random.random()) content = f_read.read(1024) if content: f_write.write(content) else: break f_read.close() f_write.close() # 发送已经拷贝完毕的文件名字 queue.put(file_name)def main(): # 获取要复制的文件夹 source_folder_name = input("请输入要复制文件夹名字:") # 整理目标文件夹 dest_folder_name = source_folder_name + "[副本]" # 创建目标文件夹 try: os.mkdir(dest_folder_name) except: pass # 如果文件夹已经存在，那么创建会失败 # 获取这个文件夹中所有的普通文件名 file_names = os.listdir(source_folder_name) # 创建Queue queue = multiprocessing.Manager().Queue() # 创建进程池 pool = multiprocessing.Pool(3) for file_name in file_names: # 向进程池中添加任务 pool.apply_async(copy_file, args=(queue, file_name, source_folder_name, dest_folder_name)) # 主进程显示进度 pool.close() all_file_num = len(file_names) while True: file_name = queue.get() if file_name in file_names: file_names.remove(file_name) copy_rate = (all_file_num-len(file_names))*100/all_file_num print("\r%.2f...(%s)" % (copy_rate, file_name) + " "*50, end="") if copy_rate &gt;= 100: break print()if __name__ == "__main__": main()]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[多任务_线程]]></title>
    <url>%2F2019%2F12%2F31%2F%E5%A4%9A%E4%BB%BB%E5%8A%A1_%E7%BA%BF%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[多任务的概念简单地说，就是操作系统可以同时运行多个任务。现在，多核CPU已经非常普及了，但是，即使过去的单核CPU，也可以执行多任务。由于CPU执行代码都是顺序执行的，那么，单核CPU是怎么执行多任务的呢？答案就是操作系统轮流让各个任务交替执行，任务1执行0.01秒，切换到任务2，任务2执行0.01秒，再切换到任务3，执行0.01秒……这样反复执行下去。表面上看，每个任务都是交替执行的，但是，由于CPU的执行速度实在是太快了，我们感觉就像所有任务都在同时执行一样。真正的并行执行多任务只能在多核CPU上实现，但是，由于任务数量远远多于CPU的核心数量，所以，操作系统也会自动把很多任务轮流调度到每个核心上执行。注意：并发：指的是任务数多于cpu核数，通过操作系统的各种任务调度算法，实现用多个任务“一起”执行（实际上总有一些任务不在执行，因为切换任务的速度相当快，看上去一起执行而已）并行：指的是任务数小于等于cpu核数，即任务真的是一起执行的线程python的thread模块是比较底层的模块，python的threading模块是对thread做了一些包装的，可以更加方便的被使用使用threading模块单线程执行12345678910#coding=utf-8import timedef saySorry(): print("亲爱的，我错了，我能吃饭了吗？") time.sleep(1)if __name__ == "__main__": for i in range(5): saySorry()多线程执行123456789101112#coding=utf-8import threadingimport timedef saySorry(): print("亲爱的，我错了，我能吃饭了吗？") time.sleep(1)if __name__ == "__main__": for i in range(5): t = threading.Thread(target=saySorry) t.start() #启动线程，即让线程开始执行说明可以明显看出使用了多线程并发的操作，花费时间要短很多当调用start()时，才会真正的创建线程，并且开始执行主线程会等待所有的子线程结束后才结束12345678910111213141516171819202122232425#coding=utf-8import threadingfrom time import sleep,ctimedef sing(): for i in range(3): print("正在唱歌...%d"%i) sleep(1)def dance(): for i in range(3): print("正在跳舞...%d"%i) sleep(1)if __name__ == '__main__': print('---开始---:%s'%ctime()) t1 = threading.Thread(target=sing) t2 = threading.Thread(target=dance) t1.start() t2.start() #sleep(5) # 屏蔽此行代码，试试看，程序是否会立马结束？ print('---结束---:%s'%ctime())查看线程数量123456789101112131415161718192021222324252627282930#coding=utf-8import threadingfrom time import sleep,ctimedef sing(): for i in range(3): print("正在唱歌...%d"%i) sleep(1)def dance(): for i in range(3): print("正在跳舞...%d"%i) sleep(1)if __name__ == '__main__': print('---开始---:%s'%ctime()) t1 = threading.Thread(target=sing) t2 = threading.Thread(target=dance) t1.start() t2.start() while True: length = len(threading.enumerate()) print('当前运行的线程数为：%d'%length) if length&lt;=1: break sleep(0.5)线程-注意点线程执行代码的封装通过使用threading模块能完成多任务的程序开发，为了让每个线程的封装性更完美，所以使用threading模块时，往往会定义一个新的子类class，只要继承threading.Thread就可以了，然后重写run方法示例如下：123456789101112131415#coding=utf-8import threadingimport timeclass MyThread(threading.Thread): def run(self): for i in range(3): time.sleep(1) msg = "I'm "+self.name+' @ '+str(i) #name属性中保存的是当前线程的名字 print(msg)if __name__ == '__main__': t = MyThread() t.start()说明python的threading.Thread类有一个run方法，用于定义线程的功能函数，可以在自己的线程类中覆盖该方法。而创建自己的线程实例后，通过Thread类的start方法，可以启动该线程，交给python虚拟机进行调度，当该线程获得执行的机会时，就会调用run方法执行线程。线程的执行顺序12345678910111213141516#coding=utf-8import threadingimport timeclass MyThread(threading.Thread): def run(self): for i in range(3): time.sleep(1) msg = "I'm "+self.name+' @ '+str(i) print(msg)def test(): for i in range(5): t = MyThread() t.start()if __name__ == '__main__': test()说明从代码和执行结果我们可以看出，多线程程序的执行顺序是不确定的。当执行到sleep语句时，线程将被阻塞（Blocked），到sleep结束后，线程进入就绪（Runnable）状态，等待调度。而线程调度将自行选择一个线程执行。上面的代码中只能保证每个线程都运行完整个run函数，但是线程的启动顺序、run函数中每次循环的执行顺序都不能确定。修改上述代码,使之能按顺序执行12345678910111213141516171819202122#!/usr/bin/env python# -*- coding: utf-8 -*-import threadingimport timeclass MyThread(threading.Thread): def run(self): for i in range(3): # time.sleep(1) msg = "I'm " + self.name + ' @ ' + str(i) print(msg)def test(): for i in range(5): t = MyThread() t.start() time.sleep(1) # 延迟时间可适当延长if __name__ == '__main__': test()总结每个线程默认有一个名字，尽管上面的例子中没有指定线程对象的name，但是python会自动为线程指定一个名字。当线程的run()方法结束时该线程完成。无法控制线程调度程序，但可以通过别的方式来影响线程调度的方式。多线程-共享全局变量1234567891011121314151617181920212223242526272829303132#!/usr/bin/env python# -*- coding: utf-8 -*-from threading import Threadimport timeg_num = 100def work1(): global g_num for i in range(3): g_num += 1 print("----in work1, g_num is %d---" % g_num)def work2(): global g_num print("----in work2, g_num is %d---" % g_num)print("---线程创建之前g_num is %d---" % g_num)t1 = Thread(target=work1)t1.start()# 延时一会，保证t1线程中的事情做完time.sleep(1)t2 = Thread(target=work2)t2.start()列表当做实参传递到线程中12345678910111213141516171819202122#!/usr/bin/env python# -*- coding: utf-8 -*-from threading import Threadimport timedef work1(nums): nums.append(44) print("----in work1---", nums)def work2(nums): # 延时一会，保证t1线程中的事情做完 time.sleep(1) print("----in work2---", nums)g_nums = [11, 22, 33]t1 = Thread(target=work1, args=(g_nums,))t1.start()t2 = Thread(target=work2, args=(g_nums,))t2.start()总结在一个进程内的所有线程共享全局变量，很方便在多个线程间共享数据缺点就是，线程是对全局变量随意遂改可能造成多线程之间对全局变量的混乱（即线程非安全）多线程-共享全局变量问题多线程开发可能遇到的问题假设两个线程t1和t2都要对全局变量g_num(默认是0)进行加1运算，t1和t2都各对g_num加10次，g_num的最终的结果应该为20。但是由于是多线程同时操作，有可能出现下面情况：在g_num=0时，t1取得g_num=0。此时系统把t1调度为”sleeping”状态，把t2转换为”running”状态，t2也获得g_num=0然后t2对得到的值进行加1并赋给g_num，使得g_num=1然后系统又把t2调度为”sleeping”，把t1转为”running”。线程t1又把它之前得到的0加1后赋值给g_num。这样导致虽然t1和t2都对g_num加1，但结果仍然是g_num=1测试11234567891011121314151617181920212223242526272829303132333435#!/usr/bin/env python# -*- coding: utf-8 -*-import threadingimport timeg_num = 0def work1(num): global g_num for i in range(num): g_num += 1 print("----in work1, g_num is %d---" % g_num)def work2(num): global g_num for i in range(num): g_num += 1 print("----in work2, g_num is %d---" % g_num)print("---线程创建之前g_num is %d---" % g_num)t1 = threading.Thread(target=work1, args=(100,))t1.start()t2 = threading.Thread(target=work2, args=(100,))t2.start()while len(threading.enumerate()) != 1: time.sleep(1)print("2个线程对同一个全局变量操作之后的最终结果是:%s" % g_num)运行结果:1234---线程创建之前g_num is 0-------in work1, g_num is 100-------in work2, g_num is 200---2个线程对同一个全局变量操作之后的最终结果是:200测试212345678910111213141516171819202122232425262728293031323334353637#!/usr/bin/env python# -*- coding: utf-8 -*-import threadingimport timeg_num = 0def work1(num): global g_num for i in range(num): g_num += 1 print("----in work1, g_num is %d---" % g_num)def work2(num): global g_num for i in range(num): g_num += 1 print("----in work2, g_num is %d---" % g_num)print("---线程创建之前g_num is %d---" % g_num)t1 = threading.Thread(target=work1, args=(1000000,))t1.start()# time.sleep(5) # 可加延时确保t1先执行完t2 = threading.Thread(target=work2, args=(1000000,))t2.start()while len(threading.enumerate()) != 1: time.sleep(1)print("2个线程对同一个全局变量操作之后的最终结果是:%s" % g_num)运行结果:1234---线程创建之前g_num is 0-------in work2, g_num is 1306621-------in work1, g_num is 1329573---2个线程对同一个全局变量操作之后的最终结果是:1329573结论如果多个线程同时对同一个全局变量操作，会出现资源竞争问题，从而数据结果会不正确同步的概念同步就是协同步调，按预定的先后次序进行运行。如:你说完，我再说。“同”字从字面上容易理解为一起动作,其实不是，”同”字应是指协同、协助、互相配合。如进程、线程同步，可理解为进程或线程A和B一块配合，A执行到一定程度时要依靠B的某个结果，于是停下来，示意B运行;B执行，再将结果给A;A再继续操作。解决线程同时修改全局变量的方式可以通过线程同步来进行解决,思路如下:系统调用t1，然后获取到g_num的值为0，此时上一把锁，即不允许其他线程操作g_numt1对g_num的值进行+1t1解锁，此时g_num的值为1，其他的线程就可以使用g_num了，而且是g_num的值不是0而是1同理其他线程在对g_num进行修改时，都要先上锁，处理完后再解锁，在上锁的整个过程中不允许其他线程访问，就保证了数据的正确性互斥锁当多个线程几乎同时修改某一个共享数据的时候，需要进行同步控制线程同步能够保证多个线程安全访问竞争资源，最简单的同步机制是引入互斥锁。互斥锁为资源引入一个状态：锁定/非锁定某个线程要更改共享数据时，先将其锁定，此时资源的状态为“锁定”，其他线程不能更改；直到该线程释放资源，将资源的状态变成“非锁定”，其他的线程才能再次锁定该资源。互斥锁保证了每次只有一个线程进行写入操作，从而保证了多线程情况下数据的正确性。threading模块中定义了Lock类，可以方便的处理锁定：123456# 创建锁mutex = threading.Lock()# 锁定mutex.acquire()# 释放mutex.release()注意：如果这个锁之前是没有上锁的，那么acquire不会堵塞如果在调用acquire对这个锁上锁之前 它已经被 其他线程上了锁，那么此时acquire会堵塞，直到这个锁被解锁为止使用互斥锁完成2个线程对同一个全局变量各加100万次的操作12345678910111213141516171819202122232425262728293031323334353637383940414243444546#!/usr/bin/env python# -*- coding: utf-8 -*-import threadingimport timeg_num = 0def test1(num): global g_num for i in range(num): mutex.acquire() # 上锁 g_num += 1 mutex.release() # 解锁 print("---test1---g_num=%d" % g_num)def test2(num): global g_num for i in range(num): mutex.acquire() # 上锁 g_num += 1 mutex.release() # 解锁 print("---test2---g_num=%d" % g_num)# 创建一个互斥锁# 默认是未上锁的状态mutex = threading.Lock()# 创建2个线程，让他们各自对g_num加1000000次p1 = threading.Thread(target=test1, args=(1000000,))p1.start()p2 = threading.Thread(target=test2, args=(1000000,))p2.start()# 等待计算完成while len(threading.enumerate()) != 1: time.sleep(1)print("2个线程对同一个全局变量操作之后的最终结果是:%s" % g_num)运行结果：123---test1---g_num=1940444---test2---g_num=20000002个线程对同一个全局变量操作之后的最终结果是:2000000可以看到最后的结果，加入互斥锁后，其结果与预期相符。上锁解锁过程当一个线程调用锁的acquire()方法获得锁时，锁就进入“locked”状态。每次只有一个线程可以获得锁。如果此时另一个线程试图获得这个锁，该线程就会变为“blocked”状态，称为“阻塞”，直到拥有锁的线程调用锁的release()方法释放锁之后，锁进入“unlocked”状态。线程调度程序从处于同步阻塞状态的线程中选择一个来获得锁，并使得该线程进入运行（running）状态。总结锁的好处：确保了某段关键代码只能由一个线程从头到尾完整地执行锁的坏处：阻止了多线程并发执行，包含锁的某段代码实际上只能以单线程模式执行，效率就大大地下降了由于可以存在多个锁，不同的线程持有不同的锁，并试图获取对方持有的锁时，可能会造成死锁死锁在线程间共享多个资源的时候，如果两个线程分别占有一部分资源并且同时等待对方的资源，就会造成死锁。尽管死锁很少发生，但一旦发生就会造成应用的停止响应。下面看一个死锁的例子123456789101112131415161718192021222324252627282930313233343536373839404142434445import threadingimport timeclass MyThread1(threading.Thread): def run(self): # 对mutexA上锁 mutexA.acquire() # mutexA上锁后，延时1秒，等待另外那个线程 把mutexB上锁 print(self.name+'----do1---up----') time.sleep(1) # 此时会堵塞，因为这个mutexB已经被另外的线程抢先上锁了 mutexB.acquire() print(self.name+'----do1---down----') mutexB.release() # 对mutexA解锁 mutexA.release()class MyThread2(threading.Thread): def run(self): # 对mutexB上锁 mutexB.acquire() # mutexB上锁后，延时1秒，等待另外那个线程 把mutexA上锁 print(self.name+'----do2---up----') time.sleep(1) # 此时会堵塞，因为这个mutexA已经被另外的线程抢先上锁了 mutexA.acquire() print(self.name+'----do2---down----') mutexA.release() # 对mutexB解锁 mutexB.release()mutexA = threading.Lock()mutexB = threading.Lock()if __name__ == '__main__': t1 = MyThread1() t2 = MyThread2() t1.start() t2.start()运行此代码会发现此时已经进入到了死锁状态，可以使用ctrl-c退出避免死锁程序设计时要尽量避免（银行家算法）添加超时时间等附录-银行家算法[背景知识]一个银行家如何将一定数目的资金安全地借给若干个客户，使这些客户既能借到钱完成要干的事，同时银行家又能收回全部资金而不至于破产，这就是银行家问题。这个问题同操作系统中资源分配问题十分相似：银行家就像一个操作系统，客户就像运行的进程，银行家的资金就是系统的资源。[问题的描述]一个银行家拥有一定数量的资金，有若干个客户要贷款。每个客户须在一开始就声明他所需贷款的总额。若该客户贷款总额不超过银行家的资金总数，银行家可以接收客户的要求。客户贷款是以每次一个资金单位（如1万RMB等）的方式进行的，客户在借满所需的全部单位款额之前可能会等待，但银行家须保证这种等待是有限的，可完成的。例如：有三个客户C1，C2，C3，向银行家借款，该银行家的资金总额为10个资金单位，其中C1客户要借9各资金单位，C2客户要借3个资金单位，C3客户要借8个资金单位，总计20个资金单位。某一时刻的状态如图所示。对于a图的状态，按照安全序列的要求，我们选的第一个客户应满足该客户所需的贷款小于等于银行家当前所剩余的钱款，可以看出只有C2客户能被满足：C2客户需1个资金单位，小银行家手中的2个资金单位，于是银行家把1个资金单位借给C2客户，使之完成工作并归还所借的3个资金单位的钱，进入b图。同理，银行家把4个资金单位借给C3客户，使其完成工作，在c图中，只剩一个客户C1，它需7个资金单位，这时银行家有8个资金单位，所以C1也能顺利借到钱并完成工作。最后（见图d）银行家收回全部10个资金单位，保证不赔本。那麽客户序列{C1，C2，C3}就是个安全序列，按照这个序列贷款，银行家才是安全的。否则的话，若在图b状态时，银行家把手中的4个资金单位借给了C1，则出现不安全状态：这时C1，C3均不能完成工作，而银行家手中又没有钱了，系统陷入僵持局面，银行家也不能收回投资。综上所述，银行家算法是从当前状态出发，逐个按安全序列检查各客户谁能完成其工作，然后假定其完成工作且归还全部贷款，再进而检查下一个能完成工作的客户，……。如果所有客户都能完成工作，则找到一个安全序列，银行家才是安全的。案例：多任务版udp聊天器说明编写一个有2个线程的程序线程1用来接收数据然后显示线程2用来检测键盘数据然后通过udp发送数据参考代码:123456789101112131415161718192021222324252627282930313233343536373839404142import socketimport threadingdef send_msg(udp_socket): """获取键盘数据，并将其发送给对方""" while True: # 1. 从键盘输入数据 msg = input("\n请输入要发送的数据:") # 2. 输入对方的ip地址 dest_ip = input("\n请输入对方的ip地址:") # 3. 输入对方的port dest_port = int(input("\n请输入对方的port:")) # 4. 发送数据 udp_socket.sendto(msg.encode("utf-8"), (dest_ip, dest_port))def recv_msg(udp_socket): """接收数据并显示""" while True: # 1. 接收数据 recv_msg = udp_socket.recvfrom(1024) # 2. 解码 recv_ip = recv_msg[1] recv_msg = recv_msg[0].decode("utf-8") # 3. 显示接收到的数据 print("&gt;&gt;&gt;%s:%s" % (str(recv_ip), recv_msg))def main(): # 1. 创建套接字 udp_socket = socket.socket(socket.AF_INET, socket.SOCK_DGRAM) # 2. 绑定本地信息 udp_socket.bind(("", 7890)) # 3. 创建一个子线程用来接收数据 t = threading.Thread(target=recv_msg, args=(udp_socket,)) t.start() # 4. 让主线程用来检测键盘数据并且发送 send_msg(udp_socket)if __name__ == "__main__": main()]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[多任务_协程]]></title>
    <url>%2F2019%2F12%2F31%2F%E5%A4%9A%E4%BB%BB%E5%8A%A1_%E5%8D%8F%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[迭代器迭代是访问集合元素的一种方式。迭代器是一个可以记住遍历的位置的对象。迭代器对象从集合的第一个元素开始访问，直到所有的元素被访问完结束。迭代器只能往前不会后退。1. 可迭代对象我们已经知道可以对list、tuple、str等类型的数据使用for…in…的循环语法从其中依次拿到数据进行使用，我们把这样的过程称为遍历，也叫迭代。但是，是否所有的数据类型都可以放到for…in…的语句中，然后让for…in…每次从中取出一条数据供我们使用，即供我们迭代吗？12345678910111213141516171819202122232425262728&gt;&gt;&gt; for i in 100:... print(i)...Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt;TypeError: 'int' object is not iterable&gt;&gt;&gt;# int整型不是iterable，即int整型不是可以迭代的# 我们自定义一个容器MyList用来存放数据，可以通过add方法向其中添加数据&gt;&gt;&gt; class MyList(object):... def __init__(self):... self.container = []... def add(self, item):... self.container.append(item)...&gt;&gt;&gt; mylist = MyList()&gt;&gt;&gt; mylist.add(1)&gt;&gt;&gt; mylist.add(2)&gt;&gt;&gt; mylist.add(3)&gt;&gt;&gt; for num in mylist:... print(num)...Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt;TypeError: 'MyList' object is not iterable&gt;&gt;&gt;# MyList容器的对象也是不能迭代的我们自定义了一个容器类型MyList，在将一个存放了多个数据的MyList对象放到for…in…的语句中，发现for…in…并不能从中依次取出一条数据返回给我们，也就说我们随便封装了一个可以存放多条数据的类型却并不能被迭代使用。我们把可以通过for…in…这类语句迭代读取一条数据供我们使用的对象称之为可迭代对象（Iterable）**。2. 如何判断一个对象是否可以迭代可以使用 isinstance() 判断一个对象是否是 Iterable 对象：12345678910111213141516In [50]: from collections import IterableIn [51]: isinstance([], Iterable)Out[51]: TrueIn [52]: isinstance(&#123;&#125;, Iterable)Out[52]: TrueIn [53]: isinstance('abc', Iterable)Out[53]: TrueIn [54]: isinstance(mylist, Iterable)Out[54]: FalseIn [55]: isinstance(100, Iterable)Out[55]: False3. 可迭代对象的本质我们分析对可迭代对象进行迭代使用的过程，发现每迭代一次（即在for…in…中每循环一次）都会返回对象中的下一条数据，一直向后读取数据直到迭代了所有数据后结束。那么，在这个过程中就应该有一个“人”去记录每次访问到了第几条数据，以便每次迭代都可以返回下一条数据。我们把这个能帮助我们进行数据迭代的“人”称为迭代器(Iterator)。可迭代对象的本质就是可以向我们提供一个这样的中间“人”即迭代器帮助我们对其进行迭代遍历使用。可迭代对象通过__iter__方法向我们提供一个迭代器，我们在迭代一个可迭代对象的时候，实际上就是先获取该对象提供的一个迭代器，然后通过这个迭代器来依次获取对象中的每一个数据.那么也就是说，一个具备了__iter__方法的对象，就是一个可迭代对象。12345678910111213141516&gt;&gt;&gt; class MyList(object):... def __init__(self):... self.container = []... def add(self, item):... self.container.append(item)... def __iter__(self):... """返回一个迭代器"""... # 我们暂时忽略如何构造一个迭代器对象... pass...&gt;&gt;&gt; mylist = MyList()&gt;&gt;&gt; from collections import Iterable&gt;&gt;&gt; isinstance(mylist, Iterable)True&gt;&gt;&gt;# 这回测试发现添加了__iter__方法的mylist对象已经是一个可迭代对象了4. iter()函数与next()函数list、tuple等都是可迭代对象，我们可以通过iter()函数获取这些可迭代对象的迭代器。然后我们可以对获取到的迭代器不断使用next()函数来获取下一条数据。iter()函数实际上就是调用了可迭代对象的__iter__方法。1234567891011121314151617&gt;&gt;&gt; li = [11, 22, 33, 44, 55]&gt;&gt;&gt; li_iter = iter(li)&gt;&gt;&gt; next(li_iter)11&gt;&gt;&gt; next(li_iter)22&gt;&gt;&gt; next(li_iter)33&gt;&gt;&gt; next(li_iter)44&gt;&gt;&gt; next(li_iter)55&gt;&gt;&gt; next(li_iter)Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt;StopIteration&gt;&gt;&gt;注意，当我们已经迭代完最后一个数据之后，再次调用next()函数会抛出StopIteration的异常，来告诉我们所有数据都已迭代完成，不用再执行next()函数了。5. 如何判断一个对象是否是迭代器可以使用 isinstance() 判断一个对象是否是 Iterator 对象：12345678910In [56]: from collections import IteratorIn [57]: isinstance([], Iterator)Out[57]: FalseIn [58]: isinstance(iter([]), Iterator)Out[58]: TrueIn [59]: isinstance(iter("abc"), Iterator)Out[59]: True6. 迭代器Iterator通过上面的分析，我们已经知道，迭代器是用来帮助我们记录每次迭代访问到的位置，当我们对迭代器使用next()函数的时候，迭代器会向我们返回它所记录位置的下一个位置的数据。实际上，在使用next()函数的时候，调用的就是迭代器对象的__next__方法（Python3中是对象的__next__方法，Python2中是对象的next()方法）。所以，我们要想构造一个迭代器，就要实现它的next方法。但这还不够，python要求迭代器本身也是可迭代的，所以我们还要为迭代器实现__iter__方法，而__iter__方法要返回一个迭代器，迭代器自身正是一个迭代器，所以迭代器的__iter__方法返回自身即可。一个实现了iter方法和next方法的对象，就是迭代器。1234567891011121314151617181920212223242526272829303132333435363738394041class MyList(object): """自定义的一个可迭代对象""" def __init__(self): self.items = [] def add(self, val): self.items.append(val) def __iter__(self): myiterator = MyIterator(self) return myiteratorclass MyIterator(object): """自定义的供上面可迭代对象使用的一个迭代器""" def __init__(self, mylist): self.mylist = mylist # current用来记录当前访问到的位置 self.current = 0 def __next__(self): if self.current &lt; len(self.mylist.items): item = self.mylist.items[self.current] self.current += 1 return item else: raise StopIteration def __iter__(self): return selfif __name__ == '__main__': mylist = MyList() mylist.add(1) mylist.add(2) mylist.add(3) mylist.add(4) mylist.add(5) for num in mylist: print(num)7. for…in…循环的本质for item in Iterable 循环的本质就是先通过iter()函数获取可迭代对象Iterable的迭代器，然后对获取到的迭代器不断调用next()方法来获取下一个值并将其赋值给item，当遇到StopIteration的异常后循环结束。8. 迭代器的应用场景我们发现迭代器最核心的功能就是可以通过next()函数的调用来返回下一个数据值。如果每次返回的数据值不是在一个已有的数据集合中读取的，而是通过程序按照一定的规律计算生成的，那么也就意味着可以不用再依赖一个已有的数据集合，也就是说不用再将所有要迭代的数据都一次性缓存下来供后续依次读取，这样可以节省大量的存储（内存）空间。举个例子，比如，数学中有个著名的斐波拉契数列（Fibonacci），数列中第一个数为0，第二个数为1，其后的每一个数都可由前两个数相加得到：0, 1, 1, 2, 3, 5, 8, 13, 21, 34, …现在我们想要通过for…in…循环来遍历迭代斐波那契数列中的前n个数。那么这个斐波那契数列我们就可以用迭代器来实现，每次迭代都通过数学计算来生成下一个数。123456789101112131415161718192021222324252627282930313233class FibIterator(object): """斐波那契数列迭代器""" def __init__(self, n): """ :param n: int, 指明生成数列的前n个数 """ self.n = n # current用来保存当前生成到数列中的第几个数了 self.current = 0 # num1用来保存前前一个数，初始值为数列中的第一个数0 self.num1 = 0 # num2用来保存前一个数，初始值为数列中的第二个数1 self.num2 = 1 def __next__(self): """被next()函数调用来获取下一个数""" if self.current &lt; self.n: num = self.num1 self.num1, self.num2 = self.num2, self.num1+self.num2 self.current += 1 return num else: raise StopIteration def __iter__(self): """迭代器的__iter__返回自身即可""" return selfif __name__ == '__main__': fib = FibIterator(10) for num in fib: print(num, end=" ")9. 并不是只有for循环能接收可迭代对象除了for循环能接收可迭代对象，list、tuple等也能接收。1234li = list(FibIterator(15))print(li)tp = tuple(FibIterator(6))print(tp)生成器1. 生成器利用迭代器，我们可以在每次迭代获取数据（通过next()方法）时按照特定的规律进行生成。但是我们在实现一个迭代器时，关于当前迭代到的状态需要我们自己记录，进而才能根据当前状态生成下一个数据。为了达到记录当前状态，并配合next()函数进行迭代使用，我们可以采用更简便的语法，即生成器(generator)。生成器是一类特殊的迭代器。2. 创建生成器方法1要创建一个生成器，有很多种方法。第一种方法很简单，只要把一个列表生成式的 [ ] 改成 ( )1234567891011In [15]: L = [ x*2 for x in range(5)]In [16]: LOut[16]: [0, 2, 4, 6, 8]In [17]: G = ( x*2 for x in range(5))In [18]: GOut[18]: &lt;generator object &lt;genexpr&gt; at 0x7f626c132db0&gt;In [19]:创建 L 和 G 的区别仅在于最外层的 [ ] 和 ( ) ， L 是一个列表，而 G 是一个生成器。我们可以直接打印出列表L的每一个元素，而对于生成器G，我们可以按照迭代器的使用方法来使用，即可以通过next()函数、for循环、list()等方法使用。123456789101112131415161718192021222324252627282930313233343536In [19]: next(G)Out[19]: 0In [20]: next(G)Out[20]: 2In [21]: next(G)Out[21]: 4In [22]: next(G)Out[22]: 6In [23]: next(G)Out[23]: 8In [24]: next(G)---------------------------------------------------------------------------StopIteration Traceback (most recent call last)&lt;ipython-input-24-380e167d6934&gt; in &lt;module&gt;()----&gt; 1 next(G)StopIteration:In [25]:In [26]: G = ( x*2 for x in range(5))In [27]: for x in G: ....: print(x) ....: 02468In [28]:3. 创建生成器方法2generator非常强大。如果推算的算法比较复杂，用类似列表生成式的 for 循环无法实现的时候，还可以用函数来实现。我们仍然用上一节提到的斐波那契数列来举例，回想我们在上一节用迭代器的实现方式：123456789101112131415161718192021222324252627class FibIterator(object): """斐波那契数列迭代器""" def __init__(self, n): """ :param n: int, 指明生成数列的前n个数 """ self.n = n # current用来保存当前生成到数列中的第几个数了 self.current = 0 # num1用来保存前前一个数，初始值为数列中的第一个数0 self.num1 = 0 # num2用来保存前一个数，初始值为数列中的第二个数1 self.num2 = 1 def __next__(self): """被next()函数调用来获取下一个数""" if self.current &lt; self.n: num = self.num1 self.num1, self.num2 = self.num2, self.num1+self.num2 self.current += 1 return num else: raise StopIteration def __iter__(self): """迭代器的__iter__返回自身即可""" return self注意，在用迭代器实现的方式中，我们要借助几个变量(n、current、num1、num2)来保存迭代的状态。现在我们用生成器来实现一下。1234567891011121314151617181920212223242526272829303132333435In [30]: def fib(n): ....: current = 0 ....: num1, num2 = 0, 1 ....: while current &lt; n: ....: num = num1 ....: num1, num2 = num2, num1+num2 ....: current += 1 ....: yield num ....: return 'done' ....:In [31]: F = fib(5)In [32]: next(F)Out[32]: 1In [33]: next(F)Out[33]: 1In [34]: next(F)Out[34]: 2In [35]: next(F)Out[35]: 3In [36]: next(F)Out[36]: 5In [37]: next(F)---------------------------------------------------------------------------StopIteration Traceback (most recent call last)&lt;ipython-input-37-8c2b02b4361a&gt; in &lt;module&gt;()----&gt; 1 next(F)StopIteration: done在使用生成器实现的方式中，我们将原本在迭代器__next__方法中实现的基本逻辑放到一个函数中来实现，但是将每次迭代返回数值的return换成了yield，此时新定义的函数便不再是函数，而是一个生成器了。简单来说：只要在def函数中有yield关键字的 就称为 生成器此时按照调用函数的方式( 案例中为F = fib(5) )使用生成器就不再是执行函数体了，而是会返回一个生成器对象（ 案例中为F ），然后就可以按照使用迭代器的方式来使用生成器了。12345678910In [38]: for n in fib(5): ....: print(n) ....: 11235In [39]:但是用for循环调用generator时，发现拿不到generator的return语句的返回值。如果想要拿到返回值，必须捕获StopIteration错误，返回值包含在StopIteration的value中：123456789101112131415161718In [39]: g = fib(5)In [40]: while True: ....: try: ....: x = next(g) ....: print("value:%d"%x) ....: except StopIteration as e: ....: print("生成器返回值:%s"%e.value) ....: break ....: value:1value:1value:2value:3value:5生成器返回值:doneIn [41]:总结使用了yield关键字的函数不再是函数，而是生成器。（使用了yield的函数就是生成器）yield关键字有两点作用：保存当前运行状态（断点），然后暂停执行，即将生成器（函数）挂起将yield关键字后面表达式的值作为返回值返回，此时可以理解为起到了return的作用可以使用next()函数让生成器从断点处继续执行，即唤醒生成器（函数）Python3中的生成器可以使用return返回最终运行的返回值，而Python2中的生成器不允许使用return返回一个返回值（即可以使用return从生成器中退出，但return后不能有任何表达式）。4. 使用send唤醒我们除了可以使用next()函数来唤醒生成器继续执行外，还可以使用send()函数来唤醒执行。使用send()函数的一个好处是可以在唤醒的同时向断点处传入一个附加数据。例子：执行到yield时，gen函数作用暂时保存，返回i的值; temp接收下次c.send(“python”)，send发送过来的值，c.next()等价c.send(None)1234567In [10]: def gen(): ....: i = 0 ....: while i&lt;5: ....: temp = yield i ....: print(temp) ....: i+=1 ....:使用send123456789101112131415161718In [43]: f = gen()In [44]: next(f)Out[44]: 0In [45]: f.send('haha')hahaOut[45]: 1In [46]: next(f)NoneOut[46]: 2In [47]: f.send('haha')hahaOut[47]: 3In [48]:使用next函数1234567891011121314151617181920212223242526272829In [11]: f = gen()In [12]: next(f)Out[12]: 0In [13]: next(f)NoneOut[13]: 1In [14]: next(f)NoneOut[14]: 2In [15]: next(f)NoneOut[15]: 3In [16]: next(f)NoneOut[16]: 4In [17]: next(f)None---------------------------------------------------------------------------StopIteration Traceback (most recent call last)&lt;ipython-input-17-468f0afdf1b9&gt; in &lt;module&gt;()----&gt; 1 next(f)StopIteration:使用__next__()方法（不常使用）1234567891011121314151617181920212223242526272829In [18]: f = gen()In [19]: f.__next__()Out[19]: 0In [20]: f.__next__()NoneOut[20]: 1In [21]: f.__next__()NoneOut[21]: 2In [22]: f.__next__()NoneOut[22]: 3In [23]: f.__next__()NoneOut[23]: 4In [24]: f.__next__()None---------------------------------------------------------------------------StopIteration Traceback (most recent call last)&lt;ipython-input-24-39ec527346a9&gt; in &lt;module&gt;()----&gt; 1 f.__next__()StopIteration:协程协程，又称微线程，纤程。英文名Coroutine。1.协程是啥协程是python个中另外一种实现多任务的方式，只不过比线程更小占用更小执行单元（理解为需要的资源）。 为啥说它是一个执行单元，因为它自带CPU上下文。这样只要在合适的时机， 我们可以把一个协程 切换到另一个协程。 只要这个过程中保存或恢复 CPU上下文那么程序还是可以运行的。通俗的理解：在一个线程中的某个函数，可以在任何地方保存当前函数的一些临时变量等信息，然后切换到另外一个函数中执行，注意不是通过调用函数的方式做到的，并且切换的次数以及什么时候再切换到原来的函数都由开发者自己确定2.协程和线程差异在实现多任务时, 线程切换从系统层面远不止保存和恢复 CPU上下文这么简单。 操作系统为了程序运行的高效性每个线程都有自己缓存Cache等等数据，操作系统还会帮你做这些数据的恢复操作。 所以线程的切换非常耗性能。但是协程的切换只是单纯的操作CPU的上下文，所以一秒钟切换个上百万次系统都抗的住。3.简单实现协程1234567891011121314151617181920212223import timedef work1(): while True: print("----work1---") yield time.sleep(0.5)def work2(): while True: print("----work2---") yield time.sleep(0.5)def main(): w1 = work1() w2 = work2() while True: next(w1) next(w2)if __name__ == "__main__": main()运行结果：12345678910111213----work1-------work2-------work1-------work2-------work1-------work2-------work1-------work2-------work1-------work2-------work1-------work2---...省略...greenlet为了更好使用协程来完成多任务，python中的greenlet模块对其封装，从而使得切换任务变的更加简单安装方式使用如下命令安装greenlet模块:1234567891011121314151617181920212223sudo pip3 install greenlet#coding=utf-8from greenlet import greenletimport timedef test1(): while True: print &quot;---A--&quot; gr2.switch() time.sleep(0.5)def test2(): while True: print &quot;---B--&quot; gr1.switch() time.sleep(0.5)gr1 = greenlet(test1)gr2 = greenlet(test2)#切换到gr1中运行gr1.switch()运行效果123456789---A-----B-----A-----B-----A-----B-----A-----B--...省略...geventgreenlet已经实现了协程，但是这个还的人工切换，是不是觉得太麻烦了，不要捉急，python还有一个比greenlet更强大的并且能够自动切换任务的模块gevent其原理是当一个greenlet遇到IO(指的是input output 输入输出，比如网络、文件操作等)操作时，比如访问网络，就自动切换到其他的greenlet，等到IO操作完成，再在适当的时候切换回来继续执行。由于IO操作非常耗时，经常使程序处于等待状态，有了gevent为我们自动切换协程，就保证总有greenlet在运行，而不是等待IO安装1pip3 install gevent1. gevent的使用123456789101112import geventdef f(n): for i in range(n): print(gevent.getcurrent(), i)g1 = gevent.spawn(f, 5)g2 = gevent.spawn(f, 5)g3 = gevent.spawn(f, 5)g1.join()g2.join()g3.join()运行结果123456789101112131415&lt;Greenlet at 0x10e49f550: f(5)&gt; 0&lt;Greenlet at 0x10e49f550: f(5)&gt; 1&lt;Greenlet at 0x10e49f550: f(5)&gt; 2&lt;Greenlet at 0x10e49f550: f(5)&gt; 3&lt;Greenlet at 0x10e49f550: f(5)&gt; 4&lt;Greenlet at 0x10e49f910: f(5)&gt; 0&lt;Greenlet at 0x10e49f910: f(5)&gt; 1&lt;Greenlet at 0x10e49f910: f(5)&gt; 2&lt;Greenlet at 0x10e49f910: f(5)&gt; 3&lt;Greenlet at 0x10e49f910: f(5)&gt; 4&lt;Greenlet at 0x10e49f4b0: f(5)&gt; 0&lt;Greenlet at 0x10e49f4b0: f(5)&gt; 1&lt;Greenlet at 0x10e49f4b0: f(5)&gt; 2&lt;Greenlet at 0x10e49f4b0: f(5)&gt; 3&lt;Greenlet at 0x10e49f4b0: f(5)&gt; 4可以看到，3个greenlet是依次运行而不是交替运行2. gevent切换执行1234567891011121314import geventdef f(n): for i in range(n): print(gevent.getcurrent(), i) #用来模拟一个耗时操作，注意不是time模块中的sleep gevent.sleep(1)g1 = gevent.spawn(f, 5)g2 = gevent.spawn(f, 5)g3 = gevent.spawn(f, 5)g1.join()g2.join()g3.join()运行结果123456789101112131415&lt;Greenlet at 0x7fa70ffa1c30: f(5)&gt; 0&lt;Greenlet at 0x7fa70ffa1870: f(5)&gt; 0&lt;Greenlet at 0x7fa70ffa1eb0: f(5)&gt; 0&lt;Greenlet at 0x7fa70ffa1c30: f(5)&gt; 1&lt;Greenlet at 0x7fa70ffa1870: f(5)&gt; 1&lt;Greenlet at 0x7fa70ffa1eb0: f(5)&gt; 1&lt;Greenlet at 0x7fa70ffa1c30: f(5)&gt; 2&lt;Greenlet at 0x7fa70ffa1870: f(5)&gt; 2&lt;Greenlet at 0x7fa70ffa1eb0: f(5)&gt; 2&lt;Greenlet at 0x7fa70ffa1c30: f(5)&gt; 3&lt;Greenlet at 0x7fa70ffa1870: f(5)&gt; 3&lt;Greenlet at 0x7fa70ffa1eb0: f(5)&gt; 3&lt;Greenlet at 0x7fa70ffa1c30: f(5)&gt; 4&lt;Greenlet at 0x7fa70ffa1870: f(5)&gt; 4&lt;Greenlet at 0x7fa70ffa1eb0: f(5)&gt; 43. 给程序打补丁1234567891011121314from gevent import monkeyimport geventimport randomimport timedef coroutine_work(coroutine_name): for i in range(10): print(coroutine_name, i) time.sleep(random.random())gevent.joinall([ gevent.spawn(coroutine_work, "work1"), gevent.spawn(coroutine_work, "work2")])运行结果1234567891011121314151617181920work1 0work1 1work1 2work1 3work1 4work1 5work1 6work1 7work1 8work1 9work2 0work2 1work2 2work2 3work2 4work2 5work2 6work2 7work2 8work2 91234567891011121314151617from gevent import monkeyimport geventimport randomimport time# 有耗时操作时需要monkey.patch_all() # 将程序中用到的耗时操作的代码，换为gevent中自己实现的模块def coroutine_work(coroutine_name): for i in range(10): print(coroutine_name, i) time.sleep(random.random())gevent.joinall([ gevent.spawn(coroutine_work, &quot;work1&quot;), gevent.spawn(coroutine_work, &quot;work2&quot;)])运行结果1234567891011121314151617181920work1 0work2 0work1 1work1 2work1 3work2 1work1 4work2 2work1 5work2 3work1 6work1 7work1 8work2 4work2 5work1 9work2 6work2 7work2 8work2 9进程、线程、协程对比1.请仔细理解如下的通俗描述有一个老板想要开个工厂进行生产某件商品（例如剪子）他需要花一些财力物力制作一条生产线，这个生产线上有很多的器件以及材料这些所有的 为了能够生产剪子而准备的资源称之为：进程只有生产线是不能够进行生产的，所以老板的找个工人来进行生产，这个工人能够利用这些材料最终一步步的将剪子做出来，这个来做事情的工人称之为：线程这个老板为了提高生产率，想到3种办法：在这条生产线上多招些工人，一起来做剪子，这样效率是成倍増长，即单进程 多线程方式老板发现这条生产线上的工人不是越多越好，因为一条生产线的资源以及材料毕竟有限，所以老板又花了些财力物力购置了另外一条生产线，然后再招些工人这样效率又再一步提高了，即多进程 多线程方式老板发现，现在已经有了很多条生产线，并且每条生产线上已经有很多工人了（即程序是多进程的，每个进程中又有多个线程），为了再次提高效率，老板想了个损招，规定：如果某个员工在上班时临时没事或者再等待某些条件（比如等待另一个工人生产完谋道工序 之后他才能再次工作） ，那么这个员工就利用这个时间去做其它的事情，那么也就是说：如果一个线程等待某些条件，可以充分利用这个时间去做其它事情，其实这就是：协程方式2.简单总结进程是资源分配的单位线程是操作系统调度的单位进程切换需要的资源很最大，效率很低线程切换需要的资源一般，效率一般（当然了在不考虑GIL的情况下）协程切换任务资源很小，效率高多进程、多线程根据cpu核数不一样可能是并行的，但是协程是在一个线程中 所以是并发并发下载器1.并发下载原理123456789101112131415161718from gevent import monkeyimport geventimport urllib.request# 有耗时操作时需要monkey.patch_all()def my_downLoad(url): print('GET: %s' % url) resp = urllib.request.urlopen(url) data = resp.read() print('%d bytes received from %s.' % (len(data), url))gevent.joinall([ gevent.spawn(my_downLoad, 'http://www.baidu.com/'), gevent.spawn(my_downLoad, 'http://www.itcast.cn/'), gevent.spawn(my_downLoad, 'http://www.itheima.com/'),])运行结果123456GET: http://www.baidu.com/GET: http://www.itcast.cn/GET: http://www.itheima.com/111327 bytes received from http://www.baidu.com/.172054 bytes received from http://www.itheima.com/.215035 bytes received from http://www.itcast.cn/.从上能够看到是先发送的获取baidu的相关信息，然后依次是itcast、itheima，但是收到数据的先后顺序不一定与发送顺序相同，这也就体现出了异步，即不确定什么时候会收到数据，顺序不一定2.实现多个视频下载123456789101112131415161718192021from gevent import monkeyimport geventimport urllib.request#有IO才做时需要这一句monkey.patch_all()def my_downLoad(file_name, url): print('GET: %s' % url) resp = urllib.request.urlopen(url) data = resp.read() with open(file_name, "wb") as f: f.write(data) print('%d bytes received from %s.' % (len(data), url))gevent.joinall([ gevent.spawn(my_downLoad, "1.mp4", 'http://oo52bgdsl.bkt.clouddn.com/05day-08-%E3%80%90%E7%90%86%E8%A7%A3%E3%80%91%E5%87%BD%E6%95%B0%E4%BD%BF%E7%94%A8%E6%80%BB%E7%BB%93%EF%BC%88%E4%B8%80%EF%BC%89.mp4'), gevent.spawn(my_downLoad, "2.mp4", 'http://oo52bgdsl.bkt.clouddn.com/05day-03-%E3%80%90%E6%8E%8C%E6%8F%A1%E3%80%91%E6%97%A0%E5%8F%82%E6%95%B0%E6%97%A0%E8%BF%94%E5%9B%9E%E5%80%BC%E5%87%BD%E6%95%B0%E7%9A%84%E5%AE%9A%E4%B9%89%E3%80%81%E8%B0%83%E7%94%A8%28%E4%B8%8B%29.mp4'),])上面的url可以换为自己需要下载视频、音乐、图片等网址]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[在pandas.DataFrame.to_sql时指定数据库表的列类型]]></title>
    <url>%2F2019%2F12%2F31%2F%E5%9C%A8pandas.DataFrame.to_sql%E6%97%B6%E6%8C%87%E5%AE%9A%E6%95%B0%E6%8D%AE%E5%BA%93%E8%A1%A8%E7%9A%84%E5%88%97%E7%B1%BB%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[该文章转载自以下链接：https://www.jianshu.com/p/4c5e1ebe8470?utm_source=oschina-app问题在数据分析并存储到数据库时，Python的Pandas包提供了to_sql 方法使存储的过程更为便捷，但如果在使用to_sql方法前不在数据库建好相对应的表，to_sql则会默认为你创建一个新表，这时新表的列类型可能并不是你期望的。例如我们通过下段代码往数据库中插入一部分数据：123456import pandas as pdfrom datetime import datetimedf = pd.DataFrame([['a', 1, 1, 2.0, datetime.now(), True]], columns=['str', 'int', 'float', 'datetime', 'boolean'])print(df.dtypes)通过dtypes可知数据类型为object, int64, float64, datetime64[ns], bool 如果把数据通过to_sql方法插入到数据库中：12345from sqlalchemy import create_engineengine = create_engine("mysql+mysqldb://&#123;&#125;:&#123;&#125;@&#123;&#125;/&#123;&#125;".format('username', 'password', 'host:port', 'database'))con = engine.connect()df.to_sql(name='test', con=con, if_exists='append', index=False)用MySQL的desc可以发现数据库自动创建了表并默认指定了列的格式：12# 在MySQL中查看表的列类型desc test;FiledTypeNullKeyDefaultExtrastrtextYESNULLintbigint(20)YESNULLfloatdoubleYESNULLdatetimedatetimeYESNULLbooleantinyint(1)YESNULL其中str类型的数据在数据库表中被映射成text，int类型被映射成bigint(20)， float类型被映射成double类型。数据库中的列类型可能并非是我们所期望的格式，但我们又不想在数据插入前手动的创建数据库的表，而更希望根据DataFrame中数据的格式动态地改变数据库中表格式。分析通过查阅pandas.DataFrame.to_sql的api文档[^footnote]，可以通过指定dtype 参数值来改变数据库中创建表的列类型。dtype : dict of column name to SQL type, default NoneOptional specifying the datatype for columns. The SQL type should be a SQLAlchemy type, or a string for sqlite3 fallback connection.根据描述，可以在执行to_sql方法时，将映射好列名和指定类型的dict赋值给dtype参数即可上，其中对于MySQL表的列类型可以使用SQLAlchemy包中封装好的类型。12# 执行前先在MySQL中删除表drop table test;123456789from sqlalchemy.types import NVARCHAR, Float, Integerdtypedict = &#123; 'str': NVARCHAR(length=255), 'int': Integer(), 'float' Float()&#125;df.to_sql(name='test', con=con, if_exists='append', index=False, dtype=dtypedict)更新代码后，再查看数据库，可以看到数据库在建表时会根据dtypedict中的列名来指定相应的类型。12# 在MySQL中查看表的列类型desc test;FiledTypeNullKeyDefaultExtrastrvarchar(255)YESNULLintint(11)YESNULLfloatfloatYESNULLdatetimedatetimeYESNULLbooleantinyint(1)YESNULL答案通过分析，我们已经知道在执行to_sql的方法时，可以通过创建一个类似“{“column_name”：sqlalchemy_type}”的映射结构来控制数据库中表的列类型。但在实际使用时，我们更希望能通过pandas.DataFrame中的column的数据类型来映射数据库中的列类型，而不是每此都要列出pandas.DataFrame的column名字。写一个简单的def将pandas.DataFrame中列名和预指定的类型映射起来即可：12345678910def mapping_df_types(df): dtypedict = &#123;&#125; for i, j in zip(df.columns, df.dtypes): if "object" in str(j): dtypedict.update(&#123;i: NVARCHAR(length=255)&#125;) if "float" in str(j): dtypedict.update(&#123;i: Float(precision=2, asdecimal=True)&#125;) if "int" in str(j): dtypedict.update(&#123;i: Integer()&#125;) return dtypedict只要在执行to_sql前使用此方法获得一个映射dict再赋值给to_sql的dtype参数即可，执行的结果与上一节相同，不再累述。1234df = pd.DataFrame([['a', 1, 1, 2.0, datetime.now(), True]], columns=['str', 'int', 'float', 'datetime', 'boolean'])dtypedict = mapping_df_types(df)df.to_sql(name='test', con=con, if_exists='append', index=False, dtype=dtypedict)参考[^footnote]pandas官方文档]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>pandas</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[前端知识汇总]]></title>
    <url>%2F2019%2F12%2F31%2F%E5%89%8D%E7%AB%AF%E7%9F%A5%E8%AF%86%E6%B1%87%E6%80%BB%2F</url>
    <content type="text"><![CDATA[1. 前端开发概述包括html、css、PC端及移动端布局技巧、javascript、jquery、js特效制作、ajax前后台交互等。1.1 什么是前端开发？前端开发也叫做web前端开发，它指的是基于web的互联网产品的页面(也可叫界面)开发及功能开发。1.2 什么互联网产品？互联网产品就是指网站为满足用户需求而创建的用于运营的功能及服务，百度搜索、淘宝、QQ、微博、网易邮箱等都是互联网产品。1.3 互联网产品开发流程及前端开发岗位？1.4 前端开发需要哪些技术？前端工程师参照产品的效果图来开发页面(也可叫界面)，效果图是由UI设计师用Photoshop(少量设计师用firework)来设计的，为了方便与UI设计师对接工作，前端需要掌握一些Photoshop的技能，Photoshop还可以辅助页面开发。把效果图布局成页面，需要用到HTML语言和CSS语言，页面功能的开发需要用到javascript,为了快速开发和系统开发，还需要学习一些前端的javascript库和框架。2. HTML2.1 html概述和基本结构2.1.1 html概述HTML是 HyperText Mark-up Language 的首字母简写，意思是超文本标记语言，超文本指的是超链接，标记指的是标签，是一种用来制作网页的语言，这种语言由一个个的标签组成，用这种语言制作的文件保存的是一个文本文件，文件的扩展名为html或者htm，一个html文件就是一个网页，html文件用编辑器打开显示的是文本，可以用文本的方式编辑它，如果用浏览器打开，浏览器会按照标签描述内容将文件渲染成网页，显示的网页可以从一个网页链接跳转到另外一个网页。2.1.2 html基本结构一个html的基本结构如下：12345678910&lt;!DOCTYPE html&gt;&lt;html lang="en"&gt; &lt;head&gt; &lt;meta charset="UTF-8"&gt; &lt;title&gt;网页标题&lt;/title&gt; &lt;/head&gt; &lt;body&gt; 网页显示内容 &lt;/body&gt;&lt;/html&gt;第一行是文档声明，第二行“”标签和最后一行“”定义html文档的整体，“”标签中的‘lang=“en”’定义网页的语言为英文，定义成中文是’lang=”zh-CN”‘,不定义也没什么影响，它一般作为分析统计用。 “”标签和“”标签是它的第一层子元素，“”标签里面负责对网页进行一些设置以及定义标题，设置包括定义网页的编码格式，外链css样式文件和javascript文件等，设置的内容不会显示在网页上，标题的内容会显示在标题，“”内编写网页上显示的内容。L2Dwidget.init({log:!1,pluginJsPath:"lib/",pluginModelPath:"assets/",pluginRootPath:"live2dw/",tagMode:!1})2.1.3 HTML文档类型目前常用的两种文档类型是xhtml 1.0和html5xhtml 1.0xhtml 1.0 是html5之前的一个常用的版本，目前许多网站仍然使用此版本。此版本文档用sublime text创建方法： html:xt + tab文档示例：12345678910&lt;!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd"&gt;&lt;html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en"&gt;&lt;head&gt; &lt;meta http-equiv="Content-Type" content="text/html;charset=UTF-8"&gt; &lt;title&gt; xhtml 1.0 文档类型 &lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;/body&gt;&lt;/html&gt;html5pc端可以使用xhtml 1.0，也可以使用html5，html5是向下兼容的此版本文档用sublime text创建方法： html:5 + tab 或者 ! + tab文档示例：12345678910&lt;!DOCTYPE html&gt;&lt;html lang="en"&gt;&lt;head&gt; &lt;meta charset="UTF-8"&gt; &lt;title&gt; html5文档类型 &lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;/body&gt;&lt;/html&gt;两种文档的区别1、文档声明和编码声明2、html5新增了标签元素以及元素属性html注释html文档代码中可以插入注释，注释是对代码的说明和解释，注释的内容不会显示在页面上，html代码中插入注释的方法是：1&lt;!-- 这是一段注释 --&gt;2.2 html标题标签通过 &lt;h1&gt;、&lt;h2&gt;、&lt;h3&gt;、&lt;h4&gt;、&lt;h5&gt;、&lt;h6&gt;,标签可以在网页上定义6种级别的标题。6种级别的标题表示文档的6级目录层级关系，比如说： &lt;h1&gt;用作主标题，其后是 &lt;h2&gt;，再其次是 &lt;h3&gt;，以此类推。搜索引擎会使用标题将网页的结构和内容编制索引，所以网页上使用标题是很重要的。123&lt;h1&gt;这是一级标题&lt;/h1&gt;&lt;h2&gt;这是二级标题&lt;/h2&gt;&lt;h3&gt;这是三级标题&lt;/h3&gt;2.3 html段落标签、换行标签与字符实体2.3.1 html段落标签&lt;p&gt;标签定义一个文本段落，一个段落含有默认的上下间距，段落之间会用这种默认间距隔开，代码如下：1234567891011121314151617&lt;!DOCTYPE html&gt;&lt;html lang="en"&gt;&lt;head&gt; &lt;meta charset="UTF-8"&gt; &lt;title&gt;段落&lt;/title&gt;&lt;/head&gt;&lt;body&gt; &lt;p&gt;HTML是 HyperText Mark-up Language 的首字母简写，意思是超文本标记语言，超 文本指的是超链接，标记指的是标签，是一种用来制作网页的语言，这种语言由一个个的 标签组成，用这种语言制作的文件保存的是一个文本文件，文件的扩展名为html或者htm。 &lt;/p&gt; &lt;p&gt;一个html文件就是一个网页，html文件用编辑器打开显示的是文本，可以用文本的方 式编辑它，如果用浏览器打开，浏览器会按照标签描述内容将文件渲染成网页，显示的网 页可以从一个网页链接跳转到另外一个网页。&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;2.3.2 html换行标签代码中成段的文字，直接在代码中回车换行，在渲染成网页时候不认这种换行，如果真想换行，可以在代码的段落中插入&lt;br /&gt;来强制换行，代码如下：12345&lt;p&gt;一个html文件就是一个网页，html文件用编辑器打开显示的是文本，可以用&lt;br /&gt;文本的方式编辑它，如果用浏览器打开，浏览器会按照标签描述内容将文件&lt;br /&gt;渲染成网页，显示的网页可以从一个网页链接跳转到另外一个网页。&lt;/p&gt;2.3.3 html字符实体代码中成段的文字，如果文字间想空多个空格，在代码中空多个空格，在渲染成网页时只会显示一个空格，如果想显示多个空格，可以使用空格的字符实体,代码如下：12345&lt;!-- 在段落前想缩进两个文字的空格，使用空格的字符实体：&amp;nbsp; --&gt;&lt;p&gt;&amp;nbsp;&amp;nbsp;一个html文件就是一个网页，html文件用编辑器打开显示的是文本，可以用&lt;br /&gt;文本的方式编辑它，如果用浏览器打开，浏览器会按照标签描述内容将文件&lt;br /&gt;渲染成网页，显示的网页可以从一个网页链接跳转到另外一个网页。&lt;/p&gt;在网页上显示 “&lt;” 和 “&gt;” 会误认为是标签，想在网页上显示“&lt;”和“&gt;”可以使用它们的字符实体，比如：12345&lt;!-- “&lt;” 和 “&gt;” 的字符实体为 &amp;lt; 和 &amp;gt; --&gt;&lt;p&gt; 3 &amp;lt; 5 &lt;br&gt; 10 &amp;gt; 5&lt;/p&gt;2.4 html块标签、含样式的标签2.4.1 html块标签1、&lt;div&gt; 标签 块元素，表示一块内容，没有任何样式，不表示具体的语义，里面可以嵌套其他标签。2、&lt;span&gt; 标签 行内元素，表示一行中的一小段内容，没有具体的语义。2.4.2 含样式和语义的标签1、&lt;em&gt; 标签 行内元素，表示语气中的强调词2、&lt;i&gt; 标签 行内元素，表示专业词汇3、&lt;b&gt; 标签 行内元素，表示文档中的关键字或者产品名4、&lt;strong&gt; 标签 行内元素，表示非常重要的内容2.4.3 语义化的标签语义化的标签，就是在布局的时候多使用有语义的标签，搜索引擎在爬网的时候能认识这些标签，理解文档的结构，方便网站的收录。比如：h1标签是表示标题，p标签是表示段落，ul、li标签是表示列表，a标签表示链接，dl、dt、dd表示定义列表等，语义化的标签不多。2.5 html图像标签、绝对路径和相对路径2.5.1 html图像标签&lt;img&gt;标签可以在网页上插入一张图片，它是独立使用的标签，它的常用属性有：src属性 定义图片的引用地址alt属性 定义图片加载失败时显示的文字，搜索引擎会使用这个文字收录图片、盲人读屏软件会读取这个文字让盲人识别图片，所以此属性非常重要。1&lt;img src="images/pic.jpg" alt="产品图片" /&gt;2.5.2 绝对路径和相对路径像网页上插入图片这种外部文件，需要定义文件的引用地址，引用外部文件还包括引用外部样式表，javascript等等，引用地址分为绝对地址和相对地址。绝对地址：相对于磁盘的位置去定位文件的地址相对地址：相对于引用文件本身去定位被引用的文件地址绝对地址在整体文件迁移时会因为磁盘和顶层目录的改变而找不到文件，相对路径就没有这个问题。相对路径的定义技巧：“ ./ ” 表示当前文件所在目录下，比如：“./pic.jpg” 表示当前目录下的pic.jpg的图片，这个使用时可以省略。“ ../ ” 表示当前文件所在目录下的上一级目录，比如：“../images/pic.jpg” 表示当前目录下的上一级目录下的images文件夹中的pic.jpg的图片。2.6 html链接标签标签可以在网页上定义一个链接地址，它的常用属性有：href属性 定义跳转的地址title属性 定义鼠标悬停时弹出的提示文字框target属性 定义链接窗口打开的位置target=”_self” 缺省值，新页面替换原来的页面，在原来位置打开target=”_blank” 新页面会在新开的一个浏览器窗口打开123&lt;a href="#"&gt;&lt;/a&gt; &lt;!-- # 表示链接到页面顶部 --&gt;&lt;a href="https://www.baidu.com/" title="跳转的百度网站"&gt;百度一下&lt;/a&gt;&lt;a href="2.html" target="_blank"&gt;测试页面2&lt;/a&gt;2.7 html列表标签2.7.1 有序列表在网页上定义一个有编号的内容列表可以用&lt;ol&gt;、&lt;li&gt;配合使用来实现，代码如下：12345&lt;ol&gt; &lt;li&gt;列表文字一&lt;/li&gt; &lt;li&gt;列表文字二&lt;/li&gt; &lt;li&gt;列表文字三&lt;/li&gt;&lt;/ol&gt;在网页上生成的列表，每条项目上会按1、2、3编号，有序列表在实际开发中较少使用。2.7.2 无序列表在网页上定义一个无编号的内容列表可以用&lt;ul&gt;、&lt;li&gt;配合使用来实现，代码如下：12345&lt;ul&gt; &lt;li&gt;&lt;a href="#"&gt;新闻标题一&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="#"&gt;新闻标题二&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="#"&gt;新闻标题三&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;在网页上生成的列表，每条项目上会有一个小图标，这个小图标在不同浏览器上显示效果不同，所以一般会用样式去掉默认的小图标，如果需要图标，可以用样式自定义图标，从而达到在不同浏览器上显示的效果相同,实际开发中一般用这种列表。定义列表通常用于术语的定义。标签表示列表的整体。标签定义术语的题目。标签是术语的解释。一个中可以有多个题目和解释，代码如下：1234567891011&lt;h3&gt;前端三大块&lt;/h3&gt;&lt;dl&gt; &lt;dt&gt;html&lt;/dt&gt; &lt;dd&gt;负责页面的结构&lt;/dd&gt; &lt;dt&gt;css&lt;/dt&gt; &lt;dd&gt;负责页面的表现&lt;/dd&gt; &lt;dt&gt;javascript&lt;/dt&gt; &lt;dd&gt;负责页面的行为&lt;/dd&gt;&lt;/dl&gt;2.8 html表单表单用于搜集不同类型的用户输入，表单由不同类型的标签组成，相关标签及属性用法如下：1、&lt;form&gt;标签 定义整体的表单区域action属性 定义表单数据提交地址method属性 定义表单提交的方式，一般有“get”方式和“post”方式2、&lt;label&gt;标签 为表单元素定义文字标注3、&lt;input&gt;标签 定义通用的表单元素type属性type=”text” 定义单行文本输入框type=”password” 定义密码输入框type=”radio” 定义单选框type=”checkbox” 定义复选框type=”file” 定义上传文件type=”submit” 定义提交按钮type=”reset” 定义重置按钮type=”button” 定义一个普通按钮type=”image” 定义图片作为提交按钮，用src属性定义图片地址type=”hidden” 定义一个隐藏的表单域，用来存储值value属性 定义表单元素的值name属性 定义表单元素的名称，此名称是提交数据时的键名4、&lt;textarea&gt;标签 定义多行文本输入框5、&lt;select&gt;标签 定义下拉表单元素6、&lt;option&gt;标签 与&lt;select&gt;标签配合，定义下拉表单元素中的选项2.8.1 注册表单实例：1234567891011121314151617181920212223242526272829303132333435363738394041424344&lt;form action="http://www..." method="get"&gt;&lt;p&gt;&lt;label&gt;姓名：&lt;/label&gt;&lt;input type="text" name="username" /&gt;&lt;/p&gt;&lt;p&gt;&lt;label&gt;密码：&lt;/label&gt;&lt;input type="password" name="password" /&gt;&lt;/p&gt;&lt;p&gt;&lt;label&gt;性别：&lt;/label&gt;&lt;input type="radio" name="gender" value="0" /&gt; 男&lt;input type="radio" name="gender" value="1" /&gt; 女&lt;/p&gt;&lt;p&gt;&lt;label&gt;爱好：&lt;/label&gt;&lt;input type="checkbox" name="like" value="sing" /&gt; 唱歌&lt;input type="checkbox" name="like" value="run" /&gt; 跑步&lt;input type="checkbox" name="like" value="swiming" /&gt; 游泳&lt;/p&gt;&lt;p&gt;&lt;label&gt;照片：&lt;/label&gt;&lt;input type="file" name="person_pic"&gt;&lt;/p&gt;&lt;p&gt;&lt;label&gt;个人描述：&lt;/label&gt;&lt;textarea name="about"&gt;&lt;/textarea&gt;&lt;/p&gt;&lt;p&gt;&lt;label&gt;籍贯：&lt;/label&gt;&lt;select name="site"&gt; &lt;option value="0"&gt;北京&lt;/option&gt; &lt;option value="1"&gt;上海&lt;/option&gt; &lt;option value="2"&gt;广州&lt;/option&gt; &lt;option value="3"&gt;深圳&lt;/option&gt;&lt;/select&gt;&lt;/p&gt;&lt;p&gt;&lt;input type="submit" name="" value="提交"&gt;&lt;!-- input类型为submit定义提交按钮 还可以用图片控件代替submit按钮提交，一般会导致提交两次，不建议使用。如： &lt;input type="image" src="xxx.gif"&gt;--&gt;&lt;input type="reset" name="" value="重置"&gt;&lt;/p&gt;&lt;/form&gt;2.9 html表格1、&lt;table&gt;标签：声明一个表格，它的常用属性如下：border属性 定义表格的边框，设置值是数值cellpadding属性 定义单元格内容与边框的距离，设置值是数值cellspacing属性 定义单元格与单元格之间的距离，设置值是数值align属性 设置整体表格相对于浏览器窗口的水平对齐方式,设置值有：left | center | right ，默认left2、&lt;tr&gt;标签：定义表格中的一行3、&lt;td&gt;和&lt;th&gt;标签：定义一行中的一个单元格，td代表普通单元格，th表示表头单元格，它们的常用属性如下：align 设置单元格中内容的水平对齐方式,设置值有：left | center | right ，默认leftvalign 设置单元格中内容的垂直对齐方式 top | middle | bottom ，默认topcolspan 设置单元格水平合并，设置值是数值rowspan 设置单元格垂直合并，设置值是数值表格制作练习：12345678910111213141516171819202122232425262728293031323334353637&lt;h1&gt;表格&lt;/h1&gt;&lt;table border="1" width="600" height="300" align="center"&gt; &lt;tr&gt; &lt;td colspan="5"&gt;基本情况&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td width="25%"&gt;&lt;/td&gt; &lt;td width="15%"&gt;&lt;/td&gt; &lt;td width="25%"&gt;&lt;/td&gt; &lt;td width="15%"&gt;&lt;/td&gt; &lt;td rowspan="5" width="20%"&gt;&lt;img src="" alt="person"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;/table&gt;快速创建表格框架123456789101112131415161718192021222324252627282930比如：1. 输入table后按Tab键，就会出来一个&lt;table&gt;&lt;/table&gt;2. 输入table&gt;tr后按Tab键,就会出来一个&lt;table&gt; &lt;tr&gt;&lt;/tr&gt;&lt;/table&gt;3. 输入table&gt;tr&gt;td后按Tab键,就会出来一个&lt;table&gt; &lt;tr&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;/table&gt;4. 输入table&gt;tr&gt;td*2后按Tab键,就会出来一个&lt;table&gt; &lt;tr&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;/table&gt;5. 输入table&gt;(tr&gt;td*2)*2后按Tab键,就会出来一个&lt;table&gt; &lt;tr&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt;&lt;/table&gt;2.10 页面布局概述布局也可以叫做排版，它指的是把文字和图片等元素按照我们的意愿有机地排列在页面上，布局的方式分为两种：1、table布局：通过table元素将页面空间划分成若干个单元格，将文字或图片等元素放入单元格中，隐藏表格的边框，从而实现布局。这种布局方式也叫传统布局，目前主要使用在EDM(广告邮件中的页面)中，主流的布局方式不用这种。2、HTML+CSS布局(DIV+CSS)：主要通过CSS样式设置来布局文字或图片等元素，需要用到CSS盒子模型、盒子类型、CSS浮动、CSS定位、CSS背景图定位等知识来布局，它比传统布局要复杂，目前是主流的布局方式。2.11 table布局及实例table来做整体页面的布局，布局的技巧归纳为如下几点：1、按照设计图的尺寸设置表格的宽高以及单元格的宽高。2、将表格border、cellpadding、cellspacing全部设置为0，表格的边框和间距就不占有页面空间，它只起到划分空间的作用。3、针对局部复杂的布局，可以在单元格里面再嵌套表格，嵌套表格划分局部的空间。4、单元格中的元素或者嵌套的表格用align和valign设置对齐方式5、通过属性或者css样式设置单元格中元素的样式3. CSSCSS是 Cascading Style Sheets 的首字母缩写，意思是层叠样式表。有了CSS，html中大部分表现样式的标签就废弃不用了，html只负责文档的结构和内容，表现形式完全交给CSS，html文档变得更加简洁。3.1 css基本语法及页面引用3.1.1 css基本语法css的定义方法是：选择器 { 属性:值; 属性:值; 属性:值;}选择器是将样式和页面元素关联起来的名称，属性是希望设置的样式属性每个属性有一个或多个值。代码示例：12345678910/* css注释 ctrl+shift+"/"*/div&#123; width:100px; height:100px; color:red &#125;3.1.2 css页面引入方法：1、外联式：通过link标签，链接到外部样式表到页面中。1&lt;link rel="stylesheet" type="text/css" href="css/main.css"&gt;2、嵌入式：通过style标签，在网页上创建嵌入的样式表。1234&lt;style type="text/css"&gt; div&#123; width:100px; height:100px; color:red &#125; ......&lt;/style&gt;3、内联式：通过标签的style属性，在标签上直接写样式。1&lt;div style="width:100px; height:100px; color:red "&gt;......&lt;/div&gt;3.2 css文本设置常用的应用文本的css样式：color 设置文字的颜色，如： color:red;font-size 设置文字的大小，如：font-size:12px;font-family 设置文字的字体，如：font-family:’微软雅黑’;font-style 设置字体是否倾斜，如：font-style:’normal’; 设置不倾斜，font-style:’italic’;设置文字倾斜font-weight 设置文字是否加粗，如：font-weight:bold; 设置加粗 font-weight:normal 设置不加粗line-height 设置文字的行高，设置行高相当于在每行文字的上下同时加间距， 如：line-height:24px;font 同时设置文字的几个属性，写的顺序有兼容问题，建议按照如下顺序写： font：是否加粗 字号/行高 字体；如： font:normal 12px/36px ‘微软雅黑’;text-decoration 设置文字的下划线，如：text-decoration:none; 将文字下划线去掉text-indent 设置文字首行缩进，如：text-indent:24px; 设置文字首行缩进24pxtext-align 设置文字水平对齐方式，如text-align:center 设置文字水平居中3.3 css颜色表示法css颜色值主要有三种表示方法：1、颜色名表示，比如：red 红色，gold 金色2、rgb表示，比如：rgb(255,0,0)表示红色3、16进制数值表示，比如：#ff0000 表示红色，这种可以简写成 #f003.4 css选择器常用的选择器有如下几种：3.4.1 标签选择器标签选择器，此种选择器影响范围大，建议尽量应用在层级选择器中。举例：123456*&#123;margin:0;padding:0&#125;div&#123;color:red&#125; &lt;div&gt;....&lt;/div&gt; &lt;!-- 对应以上两条样式 --&gt;&lt;div class="box"&gt;....&lt;/div&gt; &lt;!-- 对应以上两条样式 --&gt;3.4.2 id选择器通过id名来选择元素，元素的id名称不能重复，所以一个样式设置项只能对应于页面上一个元素，不能复用，id名一般给程序使用，所以不推荐使用id作为选择器。举例：123#box&#123;color:red&#125; &lt;div id="box"&gt;....&lt;/div&gt; &lt;!-- 对应以上一条样式，其它元素不允许应用此样式 --&gt;3.4.3 类选择器通过类名来选择元素，一个类可应用于多个元素，一个元素上也可以使用多个类，应用灵活，可复用，是css中应用最多的一种选择器。举例：1234567.red&#123;color:red&#125;.big&#123;font-size:20px&#125;.mt10&#123;margin-top:10px&#125; &lt;div class="red"&gt;....&lt;/div&gt;&lt;h1 class="red big mt10"&gt;....&lt;/h1&gt;&lt;p class="red mt10"&gt;....&lt;/p&gt;3.4.4 层级选择器主要应用在选择父元素下的子元素，或者子元素下面的子元素，可与标签元素结合使用，减少命名，同时也可以通过层级，防止命名冲突。举例：12345678910.box span&#123;color:red&#125;.box .red&#123;color:pink&#125;.red&#123;color:red&#125;&lt;div class="box"&gt; &lt;span&gt;....&lt;/span&gt; &lt;a href="#" class="red"&gt;....&lt;/a&gt;&lt;/div&gt;&lt;h3 class="red"&gt;....&lt;/h3&gt;3.4.5 组选择器多个选择器，如果有同样的样式设置，可以使用组选择器。举例：12345678.box1,.box2,.box3&#123;width:100px;height:100px&#125;.box1&#123;background:red&#125;.box2&#123;background:pink&#125;.box3&#123;background:gold&#125;&lt;div class="box1"&gt;....&lt;/div&gt;&lt;div class="box2"&gt;....&lt;/div&gt;&lt;div class="box3"&gt;....&lt;/div&gt;3.4.6 伪类及伪元素选择器常用的伪类选择器有hover，表示鼠标悬浮在元素上时的状态，伪元素选择器有before和after,它们可以通过样式在元素中插入内容。12345678.box1:hover&#123;color:red&#125;.box2:before&#123;content:'行首文字';&#125;.box3:after&#123;content:'行尾文字';&#125;&lt;div class="box1"&gt;....&lt;/div&gt;&lt;div class="box2"&gt;....&lt;/div&gt;&lt;div class="box3"&gt;....&lt;/div&gt;3.5 CSS盒子模型3.5.1 盒子模型解释元素(标签)在页面中显示成一个方块，类似一个盒子，CSS盒子模型就是使用现实中盒子来做比喻，帮助我们设置元素对应的样式。盒子模型示意图如下：把元素叫做盒子，设置对应的样式分别为：盒子的宽度(width)、盒子的高度(height)、盒子的边框(border)、盒子内的内容和边框之间的间距(padding)、盒子与盒子之间的间距(margin)。3.5.2 设置宽高12width:200px; /* 设置盒子的宽度，此宽度是指盒子内容的宽度，不是盒子整体宽度(难点) */ height:200px; /* 设置盒子的高度，此高度是指盒子内容的高度，不是盒子整体高度(难点) */3.5.3 设置边框设置一边的边框，比如顶部边框，可以按如下设置：1234border-top-color:red; /* 设置顶部边框颜色为红色 */ border-top-width:10px; /* 设置顶部边框粗细为10px */ border-top-style:solid; /* 设置顶部边框的线性为实线，常用的有：solid(实线) dashed(虚线) dotted(点线); */上面三句可以简写成一句：1border-top:10px solid red; /* 顺序无所谓，怎么写都可以 */设置其它三个边的方法和上面一样，把上面的’top’换成’left’就是设置左边，换成’right’就是设置右边，换成’bottom’就是设置底边。四个边如果设置一样，可以将四个边的设置合并成一句：1border:10px solid red;3.5.4 设置内间距padding设置盒子四边的内间距，可设置如下：1234padding-top：20px; /* 设置顶部内间距20px */ padding-left:30px; /* 设置左边内间距30px */ padding-right:40px; /* 设置右边内间距40px */ padding-bottom:50px; /* 设置底部内间距50px */上面的设置可以简写如下：12padding：20px 40px 50px 30px; /* 四个值按照顺时针方向，分别设置的是 上 右 下 左 四个方向的内边距值。 */padding后面还可以跟3个值，2个值和1个值，它们分别设置的项目如下：123padding：20px 40px 50px; /* 设置顶部内边距为20px，左右内边距为40px，底部内边距为50px */ padding：20px 40px; /* 设置上下内边距为20px，左右内边距为40px*/ padding：20px; /* 设置四边内边距为20px */3.5.5 设置外间距margin外边距的设置方法和padding的设置方法相同，将上面设置项中的’padding’换成’margin’就是外边距设置方法。3.5.6 理解练习通过盒子模型的原理，制作下面的盒子：123456789.box &#123; width: 140px; height: 140px; background-color: gold; border: black 10px solid; padding: 20px;&#125;&lt;div class="box"&gt;盒子里面的文字内容，距离边框有一定距离。&lt;/div&gt;3.6 盒模型的实际尺寸按照下面代码制作页面：123456789101112131415161718192021&lt;!DOCTYPE html&gt;&lt;html lang="en"&gt;&lt;head&gt; &lt;meta charset="UTF-8"&gt; &lt;title&gt;盒子的真实尺寸&lt;/title&gt; &lt;style type="text/css"&gt; .box01&#123;width:50px;height:50px;background-color:gold;&#125; .box02&#123;width:50px;height:50px;background-color:gold;border:50px solid #000&#125; .box03&#123;width:50px;height:50px;background-color:gold;border:50px solid #000;padding: 50px;&#125; &lt;/style&gt;&lt;/head&gt;&lt;body&gt; &lt;div class="box01"&gt;1&lt;/div&gt; &lt;br /&gt; &lt;div class="box02"&gt;2&lt;/div&gt; &lt;br /&gt; &lt;div class="box03"&gt;3&lt;/div&gt;&lt;/body&gt;&lt;/html&gt;页面显示效果如下：通过上面的页面得出结论：盒子的width和height设置的是盒子内容的宽和高，不是盒子本身的宽和高，盒子的真实尺寸计算公式如下：盒子宽度 = width + padding左右 + border左右盒子高度 = height + padding上下 + border上下理解练习通过盒子模型的原理，制作下面的盒子：1234567891011121314.box &#123; width: 380px; height: 35px; font-size: 20px; font-family: "Microsoft YaHei"; color: #333; border-top: 1px solid #ff0000; border-bottom: 3px solid #666666; padding-top: 15px; padding-left: 20px; line-height: 20px;&#125;&lt;div class="box"&gt;新闻列表&lt;/div&gt;3.7 盒模型使用技巧及相关问题3.7.1 margin相关技巧1、设置元素水平居中： margin:x auto;2、margin负值让元素位移及边框合并理解练习1、制作一个600*100的盒子，边框1像素黑色，距离浏览器顶部100px，水平居中。123456789101112body &#123; margin: 0;&#125;.box &#123; width: 598px; height: 98px; border: 1px black solid; margin: 100px auto;&#125;&lt;div class="box"&gt;&lt;/div&gt;2、制作下面的菜单效果：1234567891011121314151617181920212223242526body &#123; margin: 0; /* body标签本身带有8像素的间隔，需要去掉这个盒子才能贴边 */&#125;.box &#123; width: 200px; height: 150px; margin: 50px auto 0; /* 设置auto表示水平居中 */ background-color: gold;&#125;.box div&#123; width: 200px; height: 30px; border: 1px green solid; background-color: gold; margin-top: -1px; /* 设置负值表示边框重叠 */&#125;&lt;div class="box"&gt; &lt;div class="box1"&gt;&lt;/div&gt; &lt;div class="box2"&gt;&lt;/div&gt; &lt;div class="box3"&gt;&lt;/div&gt; &lt;div class="box4"&gt;&lt;/div&gt; &lt;div class="box5"&gt;&lt;/div&gt;&lt;/div&gt;3.7.2 外边距合并外边距合并指的是，当两个垂直外边距相遇时，它们将形成一个外边距。合并后的外边距的高度等于两个发生合并的外边距的高度中的较大者。解决方法如下：1、使用这种特性2、设置一边的外边距，一般设置margin-top3、将元素浮动或者定位理解练习使用div标签制作如下布局:123456789101112131415161718192021222324252627282930313233.box &#123; width: 500px; border: 1px solid black; margin: 50px auto 0;&#125;.box div &#123; margin-left: 20px; margin-right: 20px; margin-top: 20px; /* 中间的间隔应该是40px,但是因为是合并，实际是20px*/ margin-bottom: 20px;&#125;/* 或者使用如下的这个汇总形式.box div &#123; margin: 20px;&#125;*/&lt;div class="box"&gt; &lt;div&gt; 外边距合并指的是，当两个垂直外边距相遇时，它们将形成一个外边距。合并后的外边距的高度等于两个发生合并的外边距的高度中的较大者。 &lt;/div&gt; &lt;div&gt; 外边距合并指的是，当两个垂直外边距相遇时，它们将形成一个外边距。合并后的外边距的高度等于两个发生合并的外边距的高度中的较大者。 &lt;/div&gt; &lt;div&gt; 外边距合并指的是，当两个垂直外边距相遇时，它们将形成一个外边距。合并后的外边距的高度等于两个发生合并的外边距的高度中的较大者。 &lt;/div&gt; &lt;div&gt; 外边距合并指的是，当两个垂直外边距相遇时，它们将形成一个外边距。合并后的外边距的高度等于两个发生合并的外边距的高度中的较大者。 &lt;/div&gt;&lt;/div&gt;3.7.3 margin-top 塌陷在两个盒子嵌套时候，内部的盒子设置的margin-top会加到外边的盒子上，导致内部的盒子margin-top设置失败，解决方法如下：1、外部盒子设置一个边框2、外部盒子设置 overflow:hidden3、使用伪元素类：1234.clearfix:before&#123; content: ''; display:table;&#125;理解练习分别使用margin间距和padding间距制作下面的例子： 第一个图123456789101112131415161718.box &#123; width: 200px; height: 200px; background-color: gold; border: 1px solid black;&#125;.box1 &#123; width: 120px; height: 50px; background-color: green; margin: 75px auto 40px;&#125;&lt;div class="box"&gt; &lt;div class="box1"&gt; &lt;/div&gt;&lt;/div&gt;第二个图123456789101112131415161718192021222324.box &#123; width: 200px; height: 200px; background-color: gold; /*border: 1px solid black; 第一种解决塌陷的方法：给外部盒子加个边框 */ /*overflow: hidden; 第二种解决塌陷的方法：处理css外部溢出 */&#125;.box1 &#123; width: 120px; height: 50px; background-color: green; margin: 75px auto 40px;&#125;.box:before&#123; /* 第三种解决塌陷的方法：使用伪元素类，推荐使用这个*/ content: ''; display:table;&#125;&lt;div class="box"&gt; &lt;div class="box1"&gt; &lt;/div&gt;&lt;/div&gt;3.8 css元素溢出当子元素的尺寸超过父元素的尺寸时，需要设置父元素显示溢出的子元素的方式，设置的方法是通过overflow属性来设置。overflow的设置项：1、visible 默认值。内容不会被修剪，会呈现在元素框之外。2、hidden 内容会被修剪，并且其余内容是不可见的，此属性还有清除浮动、清除margin-top塌陷的功能。3、scroll 内容会被修剪，但是浏览器会显示滚动条以便查看其余的内容。4、auto 如果内容被修剪，则浏览器会显示滚动条以便查看其余的内容。5、inherit 规定应该从父元素继承 overflow 属性的值。元素溢出示例：3.9 块元素、内联元素、内联块元素元素就是标签，布局中常用的有三种标签，块元素、内联元素、内联块元素，了解这三种元素的特性，才能熟练的进行页面布局。3.9.1 块元素块元素，也可以称为行元素，布局中常用的标签如：div、p、ul、li、h1~h6、dl、dt、dd等等都是块元素，它在布局中的行为：支持全部的样式如果没有设置宽度，默认的宽度为父级宽度100%盒子占据一行、即使设置了宽度3.9.2 内联元素内联元素，也可以称为行内元素，布局中常用的标签如：a、span、em、b、strong、i等等都是内联元素，它们在布局中的行为：支持部分样式（不支持宽、高、margin上下、padding上下）宽高由内容决定盒子并在一行代码换行，盒子之间会产生间距子元素是内联元素，父元素可以用text-align属性设置子元素水平对齐方式解决内联元素间隙的方法1、去掉内联元素之间的换行2、将内联元素的父级设置font-size为0，内联元素自身再设置font-size3.9.3 内联块元素内联块元素，也叫行内块元素，是新增的元素类型，现有元素没有归于此类别的，img和input元素的行为类似这种元素，但是也归类于内联元素，我们可以用display属性将块元素或者内联元素转化成这种元素。它们在布局中表现的行为：支持全部样式如果没有设置宽高，宽高由内容决定盒子并在一行代码换行，盒子会产生间距子元素是内联块元素，父元素可以用text-align属性设置子元素水平对齐方式。这三种元素，可以通过display属性来相互转化，不过实际开发中，块元素用得比较多，所以我们经常把内联元素转化为块元素，少量转化为内联块，而要使用内联元素时，直接使用内联元素，而不用块元素转化了。3.9.4 display属性display属性是用来设置元素的类型及隐藏的，常用的属性有：1、none 元素隐藏且不占位置2、block 元素以块元素显示3、inline 元素以内联元素显示4、inline-block 元素以内联块元素显示课堂练习请制作图中所示的菜单：123456789101112131415161718192021222324252627282930313233343536373839&lt;style type="text/css"&gt; .menu &#123; width: 694px; height: 50px; margin: 50px auto 0; font-size: 0; /* 解决内联元素间隙：父级设置font-size为0，内联元素自身再设置font-size */ &#125; .menu a &#123; width: 98px; height: 48px; background-color: #ffffff; /* 背景色 */ display: inline-block; /* 行内块 */ font-size: 16px; /* 字体大小 */ font-family: "Microsoft YaHei"; /* 字体样式 */ color: pink; /* 字体颜色 */ border: 1px solid gold; /* 边框线 */ margin-left: -1px; /* 去掉重叠的边框线 */ text-align: center; /* 文字水平居中 */ line-height: 48px; /* 文字垂直居中 */ text-decoration: none; /* 去掉超链接下划线 */ &#125; .menu a:hover &#123; /* 伪类 */ background-color: gold; color: white; &#125;&lt;/style&gt;&lt;div class="menu"&gt; &lt;a href="#"&gt;首页&lt;/a&gt; &lt;a href="#"&gt;公司简介&lt;/a&gt; &lt;a href="#"&gt;解决方案&lt;/a&gt; &lt;a href="#"&gt;公司新闻&lt;/a&gt; &lt;a href="#"&gt;行业动态&lt;/a&gt; &lt;a href="#"&gt;招贤纳士&lt;/a&gt; &lt;a href="#"&gt;联系我们&lt;/a&gt;&lt;/div&gt;display属性扩展none 元素隐藏且不占位置以下示例是元素默认隐藏不显示，只有当鼠标放在元素上时才会显示出来1234567891011121314151617181920212223.con &#123; width: 200px;&#125;.con h3&#123; font-size: 30px;&#125;.box2&#123; width: 200px; height: 200px; background-color: gold; font-size: 16px; display: none;&#125;.con:hover .box2&#123; display: block;&#125;&lt;div class="con"&gt; &lt;h3&gt;文字标题&lt;/h3&gt; &lt;div class="box2"&gt;文字标题的说明&lt;/div&gt;&lt;/div&gt;3.10 浮动3.10.1 浮动特性1、浮动元素有左浮动(float:left)和右浮动(float:right)两种2、浮动的元素会向左或向右浮动，碰到父元素边界、其他元素才停下来3、相邻浮动的块元素可以并在一行，超出父级宽度就换行4、浮动让行内元素或块元素自动转化为行内块元素(此时不会有行内块元素间隙问题)5、浮动元素后面没有浮动的元素会占据浮动元素的位置，没有浮动的元素内的文字会避开浮动的元素，形成文字饶图的效果6、父元素如果没有设置尺寸(一般是高度不设置)，父元素内整体浮动的元素无法撑开父元素，父元素需要清除浮动7、浮动元素之间没有垂直margin的合并理解练习1、两端对齐浮动12345678910111213141516171819202122232425262728293031323334&lt;style type="text/css"&gt; .con&#123; width: 400px; height: 80px; border: 1px solid gold; margin: 50px auto 0; &#125; .con div&#123; width: 60px; height: 60px; display: inline-block; margin: 10px; &#125; .box01&#123; background-color: #90ee90; float: left; &#125; .box02&#123; background-color: pink; float: right; &#125;&lt;/style&gt;&lt;!--div.con&gt;div.box01--&gt;&lt;div class="con"&gt; &lt;div class="box01"&gt;&lt;/div&gt; &lt;div class="box02"&gt;&lt;/div&gt;&lt;/div&gt;2、请使用浮动制作图中所示的菜单：1234567891011121314151617181920212223242526272829303132333435363738394041424344.menu &#123; width: 694px; height: 50px; list-style: none; /* 去掉无序列表前面的小圆点 */ margin: 50px auto 0; /* 覆盖原始的ul margin */ padding: 0; /* 覆盖原始的ul padding */&#125;.menu li&#123; width: 98px; height: 48px; border: 1px solid gold; background-color: white; /*display: inline-block;*/ float: left; margin-left: -1px;&#125;.menu li a&#123; display: block; width: 98px; height: 48px; text-align: center; line-height: 48px; text-decoration: none; font-size: 16px; font-family: "Microsoft YaHei"; color: pink;&#125;.menu li a:hover&#123; background-color: gold; color: white;&#125;&lt;ul class="menu"&gt; &lt;li&gt;&lt;a href="#"&gt;公司简介&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="#"&gt;公司简介&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="#"&gt;公司简介&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="#"&gt;公司简介&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="#"&gt;公司简介&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="#"&gt;公司简介&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="#"&gt;公司简介&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;3、文字饶图效果:123456789101112131415161718192021222324252627282930&lt;style type="text/css"&gt; .con&#123; width: 450px; height: 210px; border: 1px solid black; margin: 50px auto 0; &#125; .pic&#123; width: 80px; height: 80px; background-color: gold; float: left; margin: 10px; &#125; .text&#123; height: 130px; /*background-color: green;*/ color: #666; &#125;&lt;/style&gt;&lt;div class="con"&gt; &lt;div class="pic"&gt;&lt;/div&gt; &lt;div class="text"&gt;浮动元素后面没有浮动的元素会占据浮动元素的位置，没有浮动的元素内的文字会避开浮动的元素，形成文字饶图的效果浮动元素后面没有浮动的元素会占据浮动元素的位置，没有浮动的元素内的文字会避开浮动的元素，形成文字饶图的效果浮动元素后面没有浮动的元素会占据浮动元素的位置，没有浮动的元素内的文字会避开浮动的元素，形成文字饶图的效果&lt;/div&gt;&lt;/div&gt;3.10.2 清除浮动父级上增加属性overflow：hidden在最后一个子元素的后面加一个空的div，给它样式属性 clear:both（不推荐）使用成熟的清浮动样式类，clearfix123.clearfix:after,.clearfix:before&#123; content: "";display: table;&#125;.clearfix:after&#123; clear:both;&#125;.clearfix&#123;zoom:1;&#125;清除浮动的使用方法：123.con2&#123;... overflow:hidden&#125;或者&lt;div class="con2 clearfix"&gt;理解练习父级盒子不给高度，子集盒子浮动，父级盒子需要清除浮动1234567891011121314151617181920212223242526272829303132333435363738394041424344454647.list&#123; width: 210px; /*height: 400px;*/ border: 1px solid #000000; margin: 50px auto 0; list-style: none; padding: 0; /* 第一种清除浮动的方法 */ /*overflow: hidden;*/&#125;.list li&#123; width: 50px; height: 50px; background-color: gold; margin: 10px; float: left;&#125;/* 第二种清除浮动的方法 */.clearfix:after,.clearfix:before&#123; content: ""; display: table;&#125;.clearfix:after&#123; clear:both;&#125;/* 兼容IE，网页不缩放 */.clearfix&#123; zoom:1;&#125;&lt;!--ul.list&gt;li&#123;$&#125;*8--&gt;&lt;ul class="list clearfix"&gt; &lt;li&gt;1&lt;/li&gt; &lt;li&gt;2&lt;/li&gt; &lt;li&gt;3&lt;/li&gt; &lt;li&gt;4&lt;/li&gt; &lt;li&gt;5&lt;/li&gt; &lt;li&gt;6&lt;/li&gt; &lt;li&gt;7&lt;/li&gt; &lt;li&gt;8&lt;/li&gt; &lt;!--第二种清除浮动的方法--&gt; &lt;!--&lt;div style="clear: both"&gt;&lt;/div&gt;--&gt;&lt;/ul&gt;3.11 定位3.11.1 文档流文档流，是指盒子按照html标签编写的顺序依次从上到下，从左到右排列，块元素占一行，行内元素在一行之内从左到右排列，先写的先排列，后写的排在后面，每个盒子都占据自己的位置。3.11.2 关于定位我们可以使用css的position属性来设置元素的定位类型，postion的设置项如下：relative 生成相对定位元素，元素所占据的文档流的位置保留，元素本身相对自身原位置进行偏移。absolute 生成绝对定位元素，元素脱离文档流，不占据文档流的位置，可以理解为漂浮在文档流的上方，相对于上一个设置了定位的父级元素来进行定位，如果找不到，则相对于body元素进行定位。fixed 生成固定定位元素，元素脱离文档流，不占据文档流的位置，可以理解为漂浮在文档流的上方，相对于浏览器窗口进行定位。static 默认值，没有定位，元素出现在正常的文档流中，相当于取消定位属性或者不设置定位属性。inherit 从父元素继承 position 属性的值。123456789101112131415161718192021222324252627282930313233343536/* relative 相对定位元素 */&lt;style type="text/css"&gt; .con&#123; width: 400px; height: 400px; border: 1px solid #000000; margin: 50px auto 0;&#125;.box01,.box02&#123; width: 300px; height: 100px; margin: 10px;&#125;.box01&#123; background-color: green; position: relative; left: 50px; top: 50px;&#125;.box02&#123; background-color: gold;&#125;&lt;/style&gt;&lt;div class="con"&gt; &lt;div class="box01"&gt; &lt;/div&gt; &lt;div class="box02"&gt; &lt;/div&gt;&lt;/div&gt;123456789101112131415161718192021222324252627282930313233343536/* absolute 绝对定位元素:相对于body元素进行定位 */&lt;style type="text/css"&gt; .con&#123; width: 400px; height: 400px; border: 1px solid #000000; margin: 50px auto 0; &#125; .box01,.box02&#123; width: 300px; height: 100px; margin: 10px; &#125; .box01&#123; background-color: green; position: absolute; left: 50px; top: 50px; &#125; .box02&#123; background-color: gold; &#125;&lt;/style&gt;&lt;div class="con"&gt; &lt;div class="box01"&gt; &lt;/div&gt; &lt;div class="box02"&gt; &lt;/div&gt;&lt;/div&gt;12345678910111213141516171819202122232425262728293031323334353637/* absolute 绝对定位元素:相对于上一个设置了定位的父级元素来进行定位 */&lt;style type="text/css"&gt; .con&#123; width: 400px; height: 400px; border: 1px solid #000000; margin: 50px auto 0; position: relative;&#125; .box01,.box02&#123; width: 300px; height: 100px; margin: 10px; &#125; .box01&#123; background-color: green; position: absolute; left: 50px; top: 50px; &#125; .box02&#123; background-color: gold; &#125;&lt;/style&gt;&lt;div class="con"&gt; &lt;div class="box01"&gt; &lt;/div&gt; &lt;div class="box02"&gt; &lt;/div&gt;&lt;/div&gt;123456789101112131415161718192021222324252627282930313233343536/* fixed 固定定位元素:相对于浏览器窗口进行定位 */&lt;style type="text/css"&gt; .con&#123; width: 400px; height: 400px; border: 1px solid #000000; margin: 50px auto 0;&#125;.box01,.box02&#123; width: 300px; height: 100px; margin: 10px;&#125;.box01&#123; background-color: green; position: fixed; left: 50px; top: 50px;&#125;.box02&#123; background-color: gold;&#125;&lt;/style&gt;&lt;div class="con"&gt; &lt;div class="box01"&gt; &lt;/div&gt; &lt;div class="box02"&gt; &lt;/div&gt;&lt;/div&gt;3.11.3 定位元素的偏移定位的元素还需要用left、right、top或者bottom来设置相对于参照元素的偏移值。3.11.4 定位元素层级定位元素是浮动的正常的文档流之上的，可以用z-index属性来设置元素的层级伪代码如下:1234567.box01&#123; ...... position:absolute; /* 设置了绝对定位 */ left:200px; /* 相对于参照元素左边向右偏移200px */ top:100px; /* 相对于参照元素顶部向下偏移100px */ z-index:10 /* 将元素层级设置为10(没有单位) 弹框使用：盖过所有的元素 */&#125;1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950&lt;style type="text/css"&gt; .con&#123; width: 400px; height: 400px; border: 1px solid #000000; margin: 50px auto 0; position: relative; &#125; .con div&#123; width: 200px; height: 200px; position: absolute; &#125; .box01&#123; background-color: green; left: 20px; top: 20px; z-index: 10; &#125; .box02&#123; background-color: gold; left: 40px; top: 40px; z-index: 11; &#125; .box03&#123; background-color: rebeccapurple; left: 60px; top: 60px; z-index: 12; &#125; .box04&#123; background-color: pink; left: 80px; top: 80px; &#125;&lt;/style&gt;&lt;div class="con"&gt; &lt;div class="box01"&gt;&lt;/div&gt; &lt;div class="box02"&gt;&lt;/div&gt; &lt;div class="box03"&gt;&lt;/div&gt; &lt;div class="box04"&gt;&lt;/div&gt;&lt;/div&gt;3.11.5 定位元素特性绝对定位和固定定位的块元素和行内元素会自动转化为行内块元素3.11.6 理解练习1、制作如下布局：123456789101112131415161718192021222324252627282930313233&lt;style type="text/css"&gt; .con&#123; width: 100px; height: 100px; background-color: gold; margin: 50px auto 0; position: relative; /*去掉尖角成圆的，若是圆形需要设置成50px*/ border-radius: 14px; &#125; .box&#123; width: 28px; height: 28px; background-color: red; color: white; text-align: center; line-height: 28px; /* 位置 */ position: absolute; left: 86px; top: -14px; /* 变成圆的 */ border-radius: 14px; &#125;&lt;/style&gt;&lt;div class="con"&gt; &lt;div class="box"&gt;5&lt;/div&gt;&lt;/div&gt;2、固定在顶部的水平居中的菜单12345678910111213141516&lt;style type="text/css"&gt; .menu&#123; height: 80px; background-color: gold; position: fixed; width: 960px; top: 0px; /* 根据窗口大小设置百分比 */ left: 50%; /* 根据前面的百分比再进行偏移，负值表示往左侧偏移 */ margin-left: -480px; &#125;&lt;/style&gt;&lt;div class="menu"&gt;菜单文字&lt;/div&gt;3、相对于浏览器窗口水平垂直居中的弹框123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103&lt;style type="text/css"&gt; .menu&#123; height: 80px; background-color: gold; position: fixed; width: 960px; top: 0px; left: 50%; margin-left: -480px; &#125; p&#123; text-align: center; &#125; .popup&#123; width: 500px; height: 300px; border: 1px solid #000000; background-color: #ffffff; position: fixed; left: 50%; margin-left: -251px; top: 50%; margin-top: -151px; z-index: 9999; &#125; .popup h2&#123; background-color: gold; margin: 10px; height: 40px; &#125; .mask&#123; position: fixed; width: 100%; height: 100%; background-color: grey; left: 0; top: 0; /*透明度*/ opacity: 0.5; z-index: 9998; &#125; .popup&#123; display: block; &#125;&lt;/style&gt;&lt;div class="menu"&gt;菜单文字&lt;/div&gt;&lt;div class="pop_con"&gt; &lt;div class="popup"&gt; &lt;h2&gt;弹框的标题&lt;/h2&gt; &lt;/div&gt; &lt;div class="mask"&gt;&lt;/div&gt;&lt;/div&gt;&lt;p&gt;网页内容&lt;/p&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;p&gt;网页内容&lt;/p&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;p&gt;网页内容&lt;/p&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;3.12 background属性3.12.1 属性解释background属性是css中应用比较多，且比较重要的一个属性，它是负责给盒子设置背景图片和背景颜色的，background是一个复合属性，它可以分解成如下几个设置项：background-color 设置背景颜色background-image 设置背景图片地址background-repeat 设置背景图片如何重复平铺background-position 设置背景图片的位置background-attachment 设置背景图片是固定还是随着页面滚动条滚动实际应用中，我们可以用background属性将上面所有的设置项放在一起，而且也建议这么做，这样做性能更高，而且兼容性更好，比如：“background: #00FF00 url(bgimage.gif) no-repeat left center fixed”，这里面的“#00ff00”是设置background-color；“url(bgimage.gif)”是设置background-image；“no-repeat”是设置background-repeat；“left center”是设置background-position；“fixed”是设置background-attachment，各个设置项用空格隔开，有的设置项不写也是可以的，它会使用默认值。3.12.2 举例下面这些例子使用下面这张图片做为背景图：1、“background:url(bg.jpg)”，默认设置一个图片地址，图片会从盒子的左上角开始将盒子铺满。2、“background:cyan url(bg.jpg) repeat-x”，横向平铺盒子，盒子其他部分显示背景颜色“cyan”。3、“background:cyan url(bg.jpg) repeat-y”，纵向平铺盒子，盒子其他部分显示背景颜色“cyan”。4、“background:cyan url(bg.jpg) no-repeat”，背景不重复，背景和盒子左上角对齐，盒子其他部分显示背景颜色“cyan”。5、“background:cyan url(bg.jpg) no-repeat left center”，背景不重复，背景和盒子左中对齐，盒子其他部分显示背景颜色“cyan”。6、“background:cyan url(bg.jpg) no-repeat right center”，背景不重复，背景和盒子右中对齐，也就是背景图片的右边对齐盒子的右边，盒子其他部分显示背景颜色“cyan”。相关代码：123456789101112131415161718192021222324252627282930&lt;!DOCTYPE html&gt;&lt;html lang="en"&gt;&lt;head&gt; &lt;meta charset="UTF-8"&gt; &lt;title&gt;test background&lt;/title&gt; &lt;style type="text/css"&gt; .backshow&#123; width:320px; height:160px; border:3px solid #333; float:left; margin:10px; &#125; .bg1&#123;background:cyan url(bg.jpg);&#125; .bg2&#123;background:cyan url(bg.jpg) repeat-x;&#125; .bg3&#123;background:cyan url(bg.jpg) repeat-y;&#125; .bg4&#123;background:cyan url(bg.jpg) no-repeat;&#125; .bg5&#123;background:cyan url(bg.jpg) no-repeat left center;&#125; .bg6&#123;background:cyan url(bg.jpg) no-repeat right center;&#125; &lt;/style&gt;&lt;/head&gt;&lt;body&gt; &lt;div class="backshow bg1"&gt;&lt;/div&gt; &lt;div class="backshow bg2"&gt;&lt;/div&gt; &lt;div class="backshow bg3"&gt;&lt;/div&gt; &lt;div class="backshow bg4"&gt;&lt;/div&gt; &lt;div class="backshow bg5"&gt;&lt;/div&gt; &lt;div class="backshow bg6"&gt;&lt;/div&gt;&lt;/body&gt;&lt;/html&gt;例子说明：background-position的设置，可以在水平方向设置“left”、“center”、“right”，在垂直方向设置“top”、“center”、“bottom”，除了设置这些方位词之外，还可以设置具体的数值。比如说，我们想把下边的盒子用右边的图片作为背景，并且让背景显示图片中靠近底部的那朵花：用上面中间那张图片作为左边那个比它尺寸小的盒子的背景，上面右边的实现效果设置为：“background:url(location_bg.jpg) -110px -150px”，第一个数值表示背景图相对于自己的左上角向左偏移110px，负值向左，正值向右，第二个数值表示背景图相对于自己的左上角向上偏移150px，负值向上，正值向下。实现原理示意图：对应代码：123456789101112131415161718192021222324&lt;!DOCTYPE html&gt;&lt;html lang="en"&gt;&lt;head&gt; &lt;meta charset="UTF-8"&gt; &lt;title&gt;test background&lt;/title&gt; &lt;style type="text/css"&gt; .backshow&#123; width:320px; height:160px; border:3px solid #333; float:left; margin:10px; &#125; .bg&#123;width:94px; height:94px; border:3px solid #666; background:url(location_bg.jpg) -110px -150px; &#125; &lt;/style&gt;&lt;/head&gt;&lt;body&gt; &lt;div class="bg"&gt;&lt;/div&gt;&lt;/body&gt;&lt;/html&gt;理解练习：通过雪碧图制作如下布局：3.13 特征布局实例讲习经过对前面知识点的巩固和加深，我们可以使用前面学习到的知识来制作实际开发中碰到的一些典型的布局，以此来达到综合应用知识的目的。1、特征布局：翻页（所需知识点：盒模型、内联元素）2、特征布局：导航条01（所需知识点：盒模型、行内元素布局）3、特征布局：导航条02（所需知识点：盒模型、浮动、定位、字体对齐）4、特征布局：图片列表（所需知识点：盒模型、浮动）5、特征布局：新闻列表（所需知识点：盒模型、浮动）课后练习4. Photoshop4.1 常用图片格式图片是网页制作中很重要的素材，图片有不同的格式，每种格式都有自己的特性，了解这些特效，可以方便我们在制作网页时选取适合的图片格式，图片格式及特性如下：1、psdphotoshop的专用格式。优点：完整保存图像的信息，包括未压缩的图像数据、图层、透明等信息，方便图像的编辑。缺点：应用范围窄，图片容量相对比较大。2、jpg网页制作及日常使用最普遍的图像格式。优点：图像压缩效率高，图像容量相对最小。缺点：有损压缩，图像会丢失数据而失真，不支持透明背景，不能制作成动画。3、gif制作网页小动画的常用图像格式。优点：无损压缩，图像容量小、可以制作成动画、支持透明背景。缺点：图像色彩范围最多只有256色，不能保存色彩丰富的图像，不支持半透明，透明图像边缘有锯齿。4、png网页制作及日常使用比较普遍的图像格式。优点：无损压缩，图像容量小、支持透明背景和半透明色彩、透明图像的边缘光滑。缺点：不能制作成动画5、webp将要取代jpg的图像格式。优点：同jpg格式，容量相对比jpg还要小。缺点：同jpg格式，目前不支持所有的浏览器。位图和矢量图位图也叫点阵图，是由一个个的方形的像素点排列在一起拼接而成的，位图在放大时，图像会失真。上面讲的5种图像都属于位图。矢量图和位图组成图像的原理不同，它的图像轮廓是由函数曲线生成的，当放大图像时，实际的原理就是将曲线乘以一个倍数，图像可以轻易地放大，而且不会出现像素块，图像边缘也不会出现锯齿。svg目前首选的网页矢量图格式。优点：图像容量小、图像放大不失真、支持透明背景和半透明色彩、图像边缘光滑。缺点：色彩不够丰富。flash退出历史的重量级网页矢量图格式。优点：图像容量小、图像放大不失真、支持透明背景和半透明色彩、图像边缘光滑、还可以制作动画、可编写交互。缺点：不支持搜索引擎、运行慢、浏览器需要装插件才可支持。总结在网页制作中，如何选择合适的图片格式呢？1、使用大幅面图片时，如果要使用不透明背景的图片，就使用jpg图片；如果要使用透明或者半透明背景的图片，就使用png图片；2、使用小幅面图片或者图标图片时，如果图片含多种颜色，可以使用gif或png图片；如果图片是单色，而且要求有很好的显示效果，可以使用svg；如果是图片时动画的，可以使用gif。4.2 photoshop常用图片处理技巧photoshop是一款优秀的图像处理软件，作为前端开发工程师，掌握它的一些常用功能是必须的。photoshop的常用功能有：选择、裁剪图像、修图、取色、插入文字等等。除了这些常用功能，前端还需要掌握制作新图像、切图等技巧。本次讲解的photoshop版本为cs6。4.2.1 图片格式转换与压缩1、文件/存储为 选择图片类型以及压缩比；（不推荐）2、文件/存储为web所用格式 选择图片类型以及压缩比 （推荐）；4.2.2 图像放缩，平移1、 放缩工具 图像放大缩小，在图像上点击放大，按住alt键点击缩小，快捷键Ctrl+“+”放大 Ctrl+“-”缩小，双击此工具可以让图像按照原始大小显示。2、 平移工具 对图像进行移动，在使用其他工具时，按住空格键盘的空格键，可以切换到此工具，移动完后松开空格键回到原来的工具。双击此工具可以让图像放缩到显示区域完全显示。4.2.3 新建图像执行菜单命令 文件/新建 可以新建一张图片，设置大小，颜色模式选RGB，网页图片一般选择72像素/英寸，如果图像要打印，可设为300/英寸。背景按情况选透明或白色。4.2.4 移动选择与图层面板1、按住Ctrl，在图像上点击可以选中图层2、 选择此工具，勾选工具属性栏上的“自动选择图层”，可以在图像上点击选中图层3、移动元素同时按住Alt键可复制一个图层4、图层面板的操作，包括图层的显示隐藏、图层顺序、新建图层、图层删除4.2.5 针对图像中选中图层的操作1、移动2、自由变换 执行菜单命令 编辑/自由变换3、拖拽到另一张图像上完成图层拷贝4.2.6 历史记录面板记录20部操作，可以点击已经记录的操作步骤回到之前4.2.7 选择工具1、 矩形选择工具2、 椭圆选择工具 按住alt+shift键可以从中心拉出正圆3、 任意套索工具 用手任意画出选区，不精确，不常用4、 多边形套索 可以选择多边形物体，对于结构复杂的物体，可以点多个小段来选择。5、 磁性套索 可以自动在物体边缘生成选择线，但是由于太自动了，所以不够精确，也不常用。6、 魔术棒选择工具 按照点击的点的颜色范围来选择，可以设置范围的容差，容差越大，选择区域越大，对于有单色背景的图像中的元素，可以用它点选背景，然后反选，从而选中元素。7、 快速选择工具 直接在要选的元素上画，按照画的颜色范围进行选择。8、对图层创建选区：按住Ctrl，用鼠标点击图层面板中图层的图标，在图层外框生成选区。4.2.8 选区的编辑技巧1、新选区模式下移动选区2、选区的加、减、乘，工具属性栏上设置3、调整边缘 工具属性栏或者执行菜单命令 选择/调整边缘4、变换选区 执行菜单命令 选择/变换选区，可对选区进行缩放、移动等5、反选 执行菜单命令 选择/反向6、取消选择 执行菜单命令 选择/取消选择，快捷键ctrl+d4.2.9 选区特别注意选区（蚂蚁线）只对当前图层器起作用，选区操作失败一般是当前图层弄错了4.2.10 裁剪图像1、 裁切工具2、对选区执行菜单命令 图像/裁剪3、设置矩形框大小，创建固定宽高的矩形框，可进行固定尺寸裁剪4.2.11 针对确定选区的操作技巧1、复制 执行菜单命令 编辑/拷贝 快捷键ctrl+c2、粘贴 执行菜单命令 编辑/粘贴 快捷键ctrl+v3、填充 执行菜单命令 编辑/填充4、描边 执行菜单命令 编辑/描边5、删除 执行菜单命令 编辑/清除 快捷键 delete6、自由变换 执行菜单命令 编辑/自由变换 快捷键 ctrl+t4.2.12 擦除与修复工具1、 擦除工具2、 污点修复工具4.2.13 参考线技巧1、视图/标尺，显示标尺，在标尺上按住鼠标拖动可以拉出参考线2、视图/对齐到/参考线 让参考线移动时自动对齐到选框或者图像的边缘3、视图/新建参考线 可以精确创建参考线4.2.14 文本输入1、执行菜单命令 编辑/首选项/单位和标尺 设置文字的单位2、 文本输入3、文本编辑 属性工具栏上点击文本编辑按钮4.2.15 取色1、取色工具，点击前景色按钮，弹出取色对话框，当前工具切换成取色工具。2、点击前景色按钮，当前工具自动切换到取色工具4.2.16 图像大小与画布大小1、图像/图像大小 查看和设置图像的整体大小2、图像/画布大小 查看和设置图像的画板大小4.2.17 尺寸测量1、 切片工具 双击切片弹出切片对话框2、 切片选择工具2、 矩形框工具，打开信息面板4.3 photoshop批量切图技巧切图，就是从效果图中把网页制作需要的小图片裁剪出来。1、使用psd格式并且带有图层的图像切图2、在图像上用切片工具切出需要的小图3、双击切片，给切片命名4、将需要制作透明背景图像的切片的背景隐藏5、执行菜单命令 存储为web所用格式6、点选切片，设置切片的图片格式7、存储切片，选择“所有用户切片”，点存储(多个切片会自动存到所选文件夹中的images文件夹中)4.4 Photoshop制作雪碧图技巧雪碧图，就是将网页制作中使用的多个小图片合并成一个图片，使用css技术将这张合成的图片应用在网页不同的地方，雪碧图可以减少网页加载时的http请求数，优化网页性能。步骤：1、使用Photoshop新建一张背景透明的图片2、将小图片复制到此图片中，排列好每个图像的位置，图片幅面不够可以用画布大小调整大小3、按照所有小图片的范围裁剪图片，存为透明背景的png图片5. 前端页面开发流程1、创建页面项目目录2、使用Photoshop对效果图切图，切出网页制作中需要的小图片3、将装饰类图像合并，制作成雪碧图4、结合Photoshop和代码编辑器，参照效果图，进行html和css代码书写，制作页面6. HTML5和CSS36.1 CSS权重CSS权重指的是样式的优先级，有两条或多条样式作用于一个元素，权重高的那条样式对元素起作用,权重相同的，后写的样式会覆盖前面写的样式。6.1.1 权重的等级可以把样式的应用方式分为几个等级，按照等级来计算权重1、!important，加在样式属性值后，权重值为 100002、内联样式，如：style=””，权重值为10003、ID选择器，如：#content，权重值为1004、类，伪类和属性选择器，如： content、:hover 权重值为105、标签选择器和伪元素选择器，如：div、p、:before 权重值为16、通用选择器（*）、子选择器（&gt;）、相邻选择器（+）、同胞选择器（~）、权重值为06.1.2 权重的计算实例1、实例一：1234567891011&lt;style type="text/css"&gt; div&#123; color:red !important; &#125; &lt;/style&gt;......&lt;div style="color:blue"&gt;这是一个div元素&lt;/div&gt;&lt;!-- 两条样式同时作用一个div，上面的样式权重值为10000+1，下面的行间样式的权重值为1000，所以文字的最终颜色为red --&gt;2、实例二：12345678910111213141516171819&lt;style type="text/css"&gt; #content div.main_content h2&#123; color:red; &#125; #content .main_content h2&#123; color:blue; &#125;&lt;/style&gt;......&lt;div id="content"&gt; &lt;div class="main_content"&gt; &lt;h2&gt;这是一个h2标题&lt;/h2&gt; &lt;/div&gt;&lt;/div&gt;&lt;!-- 第一条样式的权重计算： 100+1+10+1，结果为112；第二条样式的权重计算： 100+10+1，结果为111；h2标题的最终颜色为red--&gt;6.2 CSS3新增选择器1、E:nth-child(n)：匹配元素类型为E且是父元素的第n个子元素123456789101112131415&lt;style type="text/css"&gt; .list div:nth-child(2)&#123; background-color:red; &#125;&lt;/style&gt;......&lt;div class="list"&gt; &lt;h2&gt;1&lt;/h2&gt; &lt;div&gt;2&lt;/div&gt; &lt;div&gt;3&lt;/div&gt; &lt;div&gt;4&lt;/div&gt; &lt;div&gt;5&lt;/div&gt;&lt;/div&gt;&lt;!-- 第2个子元素div匹配 --&gt;2、E:first-child：匹配元素类型为E且是父元素的第一个子元素3、E:last-child：匹配元素类型为E且是父元素的最后一个子元素4、E &gt; F E元素下面第一层子集5、E ~ F E元素后面的兄弟元素6、E + F 紧挨着的后面的兄弟元素属性选择器：1、E[attr] 含有attr属性的元素1234567&lt;style type="text/css"&gt; div[data-attr='ok']&#123; color:red; &#125;&lt;/style&gt;......&lt;div data-attr="ok"&gt;这是一个div元素&lt;/div&gt;2、E[attr=’ok’] 含有attr属性的元素且它的值为“ok”3、E[attr^=’ok’] 含有attr属性的元素且它的值的开头含有“ok”4、E[attr$=’ok’] 含有attr属性的元素且它的值的结尾含有“ok”5、E[attr*=’ok’] 含有attr属性的元素且它的值中含有“ok”6.3 CSS3圆角、rgba6.3.1 CSS3圆角设置某一个角的圆角，比如设置左上角的圆角：border-top-left-radius:30px 60px;同时分别设置四个角： border-radius:30px 60px 120px 150px;设置四个圆角相同：border-radius:50%;6.3.2 rgba（新的颜色值表示法）1、盒子透明度表示法：12345.box &#123; opacity:0.1; /* 兼容IE */ filter:alpha(opacity=10); &#125;2、rgba(0,0,0,0.1) 前三个数值表示颜色，第四个数值表示颜色的透明度6.4 CSS3 transition动画1、transition-property 设置过渡的属性，比如：width height background-color2、transition-duration 设置过渡的时间，比如：1s 500ms3、transition-timing-function 设置过渡的运动方式，常用有 linear(匀速)|ease(缓冲运动)4、transition-delay 设置动画的延迟5、transition: property duration timing-function delay 同时设置四个属性综合练习：制作鼠标移入图片时，图片说明滑入的效果6.5 CSS3 transform变换1、translate(x,y) 设置盒子位移2、scale(x,y) 设置盒子缩放3、rotate(deg) 设置盒子旋转4、skew(x-angle,y-angle) 设置盒子斜切5、perspective 设置透视距离6、transform-style flat | preserve-3d 设置盒子是否按3d空间显示7、translateX、translateY、translateZ 设置三维移动8、rotateX、rotateY、rotateZ 设置三维旋转9、scaleX、scaleY、scaleZ 设置三维缩放10、tranform-origin 设置变形的中心点11、backface-visibility 设置盒子背面是否可见举例：（翻面效果）12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152&lt;!DOCTYPE html&gt;&lt;html lang="en"&gt;&lt;head&gt; &lt;meta charset="UTF-8"&gt; &lt;title&gt;翻面&lt;/title&gt; &lt;style type="text/css"&gt; .box&#123; width:300px; height:272px; margin:50px auto 0; transform-style:preserve-3d; position:relative; &#125; .box .pic&#123; width:300px; height:272px; position:absolute; background-color:cyan; left:0; top:0; transform:perspective(800px) rotateY(0deg); backface-visibility:hidden; transition:all 500ms ease; &#125; .box .back_info&#123; width:300px; height:272px; text-align:center; line-height:272px; background-color:gold; position:absolute; left:0; top:0; transform:rotateY(180deg); backface-visibility:hidden; transition:all 500ms ease; &#125; .box:hover .pic&#123; transform:perspective(800px) rotateY(180deg); &#125; .box:hover .back_info&#123; transform:perspective(800px) rotateY(0deg); &#125; &lt;/style&gt;&lt;/head&gt;&lt;body&gt; &lt;div class="box"&gt; &lt;div class="pic"&gt;&lt;img src="images/location_bg.jpg"&gt;&lt;/div&gt; &lt;div class="back_info"&gt;背面文字说明&lt;/div&gt; &lt;/div&gt;&lt;/body&gt;&lt;/html&gt;6.6 CSS3 animation动画1、@keyframes 定义关键帧动画2、animation-name 动画名称3、animation-duration 动画时间4、animation-timing-function 动画曲线 linear(匀速)|ease(缓冲)|steps(步数)5、animation-delay 动画延迟6、animation-iteration-count 动画播放次数 n|infinite7、animation-direction 动画结束后是否反向还原 normal|alternate8、animation-play-state 动画状态 paused(停止)|running(运动)9、animation-fill-mode 动画前后的状态 none(缺省)|forwards(结束时停留在最后一帧)|backwards(开始时停留在定义的开始帧)|both(前后都应用)10、animation:name duration timing-function delay iteration-count direction;同时设置多个属性理解练习：1、风车动画2、loading动画3、人物走路动画123456789101112131415161718192021222324252627282930313233343536373839&lt;!DOCTYPE html&gt;&lt;html lang="en"&gt;&lt;head&gt; &lt;meta charset="UTF-8"&gt; &lt;title&gt;走路动画&lt;/title&gt; &lt;style type="text/css"&gt; .box&#123; width:120px; height:180px; border:1px solid #ccc; margin:50px auto 0; position:relative; overflow:hidden; &#125; .box img&#123; display:block; width:960px; height:182px; position: absolute; left:0; top:0; animation:walking 1.0s steps(8) infinite; &#125; @keyframes walking&#123; from&#123; left:0px; &#125; to&#123; left:-960px; &#125; &#125; &lt;/style&gt;&lt;/head&gt;&lt;body&gt; &lt;div class="box"&gt;&lt;img src="images/walking.png"&gt;&lt;/div&gt;&lt;/body&gt;&lt;/html&gt;动画中使用的图片如下：6.7 CSS3 浏览器前缀6.7.1 浏览器样式前缀为了让CSS3样式兼容，需要将某些样式加上浏览器前缀：-ms- 兼容IE浏览器-moz- 兼容firefox-o- 兼容opera-webkit- 兼容chrome 和 safari比如：12345678div&#123; -ms-transform: rotate(30deg); -webkit-transform: rotate(30deg); -o-transform: rotate(30deg); -moz-transform: rotate(30deg); transform: rotate(30deg);&#125;6.7.2 自动添加浏览器前缀目前的状况是，有些CSS3属性需要加前缀，有些不需要加，有些只需要加一部分，这些加前缀的工作可以交给插件来完成，比如安装： autoprefixer可以在Sublime text中通过package control 安装 autoprefixer6.7.3 Autoprefixer在Sublime text中的设置：1、preferences/key Bindings-User1&#123; "keys": ["ctrl+alt+x"], "command": "autoprefixer" &#125;2、Preferences&gt;package setting&gt;AutoPrefixer&gt;Setting-User12345&#123; "browsers": ["last 7 versions"], "cascade": true, "remove": true&#125;last 7 versions：最新的浏览器的7个版本cascade：缩进美化属性值remove：是否去掉不必要的前缀6.8 HTML5新增标签6.8.1 新增语义标签1、&lt;header&gt; 页面头部、页眉2、&lt;nav&gt; 页面导航3、&lt;article&gt; 一篇文章4、&lt;section&gt; 文章中的章节5、&lt;aside&gt; 侧边栏6、&lt;footer&gt; 页面底部、页脚6.8.2 音频视频1、&lt;audio&gt;2、&lt;video&gt;PC端兼容h5的新标签的方法，在页面中引入以下js文件:1&lt;script type="text/javascript" src="//cdn.bootcss.com/html5shiv/r29/html5.js"&gt;&lt;/script&gt;6.9 HTML5 新增表单控件新增类型：网址 邮箱 日期 时间 星期 数量 范围 电话 颜色 搜索12345678910&lt;label&gt;网址:&lt;/label&gt;&lt;input type=&quot;url&quot; name=&quot;&quot; required&gt;&lt;br&gt;&lt;br&gt; &lt;label&gt;邮箱:&lt;/label&gt;&lt;input type=&quot;email&quot; name=&quot;&quot; required&gt;&lt;br&gt;&lt;br&gt; &lt;label&gt;日期:&lt;/label&gt;&lt;input type=&quot;date&quot; name=&quot;&quot;&gt;&lt;br&gt;&lt;br&gt; &lt;label&gt;时间:&lt;/label&gt;&lt;input type=&quot;time&quot; name=&quot;&quot;&gt;&lt;br&gt;&lt;br&gt; &lt;label&gt;星期:&lt;/label&gt;&lt;input type=&quot;week&quot; name=&quot;&quot;&gt;&lt;br&gt;&lt;br&gt; &lt;label&gt;数量:&lt;/label&gt;&lt;input type=&quot;number&quot; name=&quot;&quot;&gt; &lt;br&gt;&lt;br&gt;&lt;label&gt;范围:&lt;/label&gt;&lt;input type=&quot;range&quot; name=&quot;&quot;&gt;&lt;br&gt;&lt;br&gt; &lt;label&gt;电话:&lt;/label&gt;&lt;input type=&quot;tel&quot; name=&quot;&quot;&gt;&lt;br&gt;&lt;br&gt; &lt;label&gt;颜色:&lt;/label&gt;&lt;input type=&quot;color&quot; name=&quot;&quot;&gt;&lt;br&gt;&lt;br&gt; &lt;label&gt;搜索:&lt;/label&gt;&lt;input type=&quot;search&quot; name=&quot;&quot;&gt;&lt;br&gt;&lt;br&gt;新增常用表单控件属性：1、placeholder 设置文本框默认提示文字2、autofocus 自动获得焦点3、autocomplete 联想关键词7. 移动端页面开发7.1 移动端与PC端页面布局区别7.1.1 视口视口是移动设备上用来显示网页的区域，一般会比移动设备可视区域大，宽度可能是980px或者1024px，目的是为了显示下整个为PC端设计的网页，这样带来的后果是移动端会出现横向滚动条，为了避免这种情况，移动端会将视口缩放到移动端窗口的大小。这样会让网页不容易观看，可以用 meta 标签，name=“viewport ” 来设置视口的大小，将视口的大小设置为和移动设备可视区一样的大小。设置方法如下( 快捷方式：meta:vp + tab )：123456&lt;head&gt;......&lt;meta name=&quot;viewport&quot; content=&quot;width=device-width, user-scalable=no, initial-scale=1.0, maximum-scale=1.0, minimum-scale=1.0&quot;&gt;......&lt;/head&gt;pc端与移动端渲染网页过程：7.1.2 视网膜屏幕（retina屏幕）清晰度解决方案视网膜屏幕指的是屏幕的物理像素密度更高的屏幕，物理像素可以理解为屏幕上的一个发光点，无数发光的点组成的屏幕，视网膜屏幕比一般屏幕的物理像素点更小，常见有2倍的视网膜屏幕和3倍的视网膜屏幕，2倍的视网膜屏幕，它的物理像素点大小是一般屏幕的1/4,3倍的视网膜屏幕，它的物理像素点大小是一般屏幕的1/9。图像在视网膜屏幕上显示的大小和在一般屏幕上显示的大小一样，但是由于视网膜屏幕的物理像素点比一般的屏幕小，图像在上面好像是被放大了，图像会变得模糊，为了解决这个问题，可以使用比原来大一倍的图像，然后用css样式强制把图像的尺寸设为原来图像尺寸的大小，就可以解决模糊的问题。清晰度解决过程示意图：背景图强制改变大小，可以使用background新属性background新属性background-size:length：用长度值指定背景图像大小。不允许负值。percentage：用百分比指定背景图像大小。不允许负值。auto：背景图像的真实大小。cover：将背景图像等比缩放到完全覆盖容器，背景图像有可能超出容器。contain：将背景图像等比缩放到宽度或高度与容器的宽度或高度相等，背景图像始终被包含在容器内。7.2 适配布局类型7.2.1 PC及移动端页面适配方法设备屏幕有多种不同的分辨率，页面适配方案有如下几种：1、全适配：响应式布局+流体布局2、移动端适配：流体布局+少量响应式基于rem的布局7.2.2 流体布局流体布局，就是使用百分比来设置元素的宽度，元素的高度按实际高度写固定值，流体布局中，元素的边线无法用百分比，可以使用样式中的计算函数 calc() 来设置宽度，或者使用 box-sizing 属性将盒子设置为从边线计算盒子尺寸。calc()可以通过计算的方式给元素加尺寸，比如： width：calc(25% - 4px);box-sizing1、content-box 默认的盒子尺寸计算方式2、border-box 置盒子的尺寸计算方式为从边框开始，盒子的尺寸，边框和内填充算在盒子尺寸内7.2.3 响应式布局响应式布局就是使用媒体查询的方式，通过查询浏览器宽度，不同的宽度应用不同的样式块，每个样式块对应的是该宽度下的布局方式，从而实现响应式布局。响应式布局的页面可以适配多种终端屏幕（pc、平板、手机）。相应布局的伪代码如下：12345678@media (max-width:960px)&#123; .left_con&#123;width:58%;&#125; .right_con&#123;width:38%;&#125;&#125;@media (max-width:768px)&#123; .left_con&#123;width:100%;&#125; .right_con&#123;width:100%;&#125;&#125;7.2.4 基于rem的布局首先了解em单位，em单位是参照元素自身的文字大小来设置尺寸，rem指的是参照根节点的文字大小，根节点指的是html标签，设置html标签的文字大小，其他的元素相关尺寸设置用rem，这样，所有元素都有了统一的参照标准，改变html文字的大小，就会改变所有元素用rem设置的尺寸大小。cssrem安装cssrem插件可以动态地将px尺寸换算成rem尺寸下载本项目，比如：git clone https://github.com/flashlizi/cssrem 进入packages目录：Sublime Text -&gt; Preferences -&gt; Browse Packages… 复制下载的cssrem目录到刚才的packges目录里。 重启Sublime Text。配置参数 参数配置文件：Sublime Text -&gt; Preferences -&gt; Package Settings -&gt; cssrem px_to_rem - px转rem的单位比例，默认为40。 max_rem_fraction_length - px转rem的小数部分的最大长度。默认为6。 available_file_types - 启用此插件的文件类型。默认为：[“.css”, “.less”, “.sass”]。8. 常用css列表color 设置文字的颜色，如： color:red;font-size 设置文字的大小，如：font-size:12px;font-family 设置文字的字体，如：font-family:’微软雅黑’;font-style 设置字体是否倾斜，如：font-style:’normal’; 设置不倾斜，font-style:’italic’;设置文字倾斜font-weight 设置文字是否加粗，如：font-weight:bold; 设置加粗 font-weight:normal 设置不加粗line-height 设置文字的行高，设置行高相当于在每行文字的上下同时加间距， 如：line-height:24px;font 同时设置文字的几个属性，写的顺序有兼容问题，建议按照如下顺序写： font：是否加粗 字号/行高 字体；如： font:normal 12px/36px ‘微软雅黑’;text-decoration 设置文字的下划线，如：text-decoration:none; 将文字下划线去掉text-indent 设置文字首行缩进，如：text-indent:24px; 设置文字首行缩进24pxtext-align 设置文字水平对齐方式，如text-align:center 设置文字水平居中text-overflow 设置一行文字宽度超过容器宽度时的显示方式，如：text-overflow:clip 将多出的文字裁剪掉 text-overflow:ellipsis 将多出的文字显示成省略号white-space 一般用来设置文本不换行，如：white-space:nowrap 设置文本不换行 一般与text-overflow和overflow属性配合使用来让一行文字超出宽度时显示省略号list-style 一般用来设置去掉ul或者ol列表中的小圆点或数字 如：list-style:nonewidth 设置盒子内容的宽度，如： width：100px;height 设置盒子内容的高度，如： height：100px;border-top 设置盒子顶部边框的三个属性 如：border-top:5px solid red;设置盒子顶部边框为3像素宽的红色的实线，详细设置说明：盒子模型border-left 设置盒子左边边框的三个属性 如：border-left:3px dotted red;设置盒子左边边框为3像素宽的红色的点线，详细设置说明：盒子模型border-right 设置盒子右边边框的三个属性 如：border-right:2px dashed red;设置盒子右边框为2像素宽的红色的虚线，详细设置说明：盒子模型border-bottom 设置盒子底部边框的三个属性 如：border-bottom:1px solid red;设置盒子底部边框为1像素宽的红色的实线，详细设置说明：盒子模型border 同时设置盒子的四个边框，如果四个边的样式统一就使用它 如：border:1px solid #000 设置盒子四个边都是1像素宽的黑色实线，详细设置说明：盒子模型padding 设置盒子四个边的内边距 如：padding:10px 20px 30px 40px 分别设置盒子上边(10px)、右边(20px)、下边(30px)、左边(40px)的内边距(顺时针)，详细设置说明：盒子模型margin 设置盒子四个边的外边距 如：margin:10px 20px 30px 40px 分别设置盒子上边(10px)、右边(20px)、下边(30px)、左边(40px)的外边距(顺时针)，详细设置说明：盒子模型overflow 设置当子元素的尺寸超过父元素的尺寸时，盒子及子元素的显示方式 如：overflow:hidden 超出的子元素被裁切，详细设置说明：元素溢出display 设置盒子的显示类型及隐藏，如：display:block 将盒子设置为以块元素显示 display:none 将元素隐藏，详细设置说明：元素类型float 设置元浮动 如：float:left 设置左浮动 float:right 设置右浮动，详细设置说明：元素浮动clear 在盒子两侧清除浮动 如：clear:both 在盒子两侧都不允许浮动，详细设置说明：元素浮动position 设置元素定位 如：position:relative 设置元素相对定位，详细设置说明：元素定位background 设置元素的背景色和背景图片，如：background:url(bg.jpg) cyan;设置盒子的背景图片为bg.jpg，背景色为cyan，详细设置说明：元素背景background-size 设置盒子背景图的尺寸，如：background-size:30px 40px;设置背景图的尺寸宽为30px，高为40px，这个属性不能合到background属性中，详细设置说明：retina屏适配opacity 设置元素整体透明度，一般为了兼容需要加上filter属性设置 如：opacity:0.1;filter:alpha(opacity=10)cursor 设置鼠标悬停在元素上时指针的形状 如：cursor:pointer 设置为手型outline 设置文本输入框周围凸显的蓝色的线，一般是设为没有 如：outline:noneborder-radius 设置盒子的圆角 如：border-radius:10px 设置盒子的四个角为10px半径的圆角，详细设置说明：css圆角box-shadow 设置盒子的阴影，如：box-shadow:10px 10px 5px 2px pink;设置盒子有粉色的阴影，详细设置说明：css阴影transition 设置盒子的过渡动画，如：transition:all 1s ease;设置元素过渡动画为1秒完成，所有变动的属性都做动画，详细设置说明：过渡动画animation 设置盒子的关键帧动画，详细设置说明：关键帧动画transform 设置盒子的位移、旋转、缩放、斜切等变形，如：transform:rotate(45deg);设置盒子旋转45度，详细设置说明：元素变形box-sizing 设置盒子的尺寸计算方式，如：box-sizing:border-box 将盒子的尺寸计算方法设置为按边框计算，此时width和height的值就是盒子的实际尺寸border-collapse 设置表格边框是否合并，如：border-collapse:collapse，将表格边框合并，这样就可以制作1px边框的表格。9. JavaScript9.1 JavaScript介绍JavaScript是运行在浏览器端的脚步语言，JavaScript主要解决的是前端与用户交互的问题，包括使用交互与数据交互。 JavaScript是浏览器解释执行的，前端脚本语言还有JScript（微软，IE独有），ActionScript( Adobe公司，需要插件)等。前端三大块1、HTML：页面结构2、CSS：页面表现：元素大小、颜色、位置、隐藏或显示、部分动画效果3、JavaScript：页面行为：部分动画效果、页面与用户的交互、页面功能9.2 JavaScript嵌入页面的方式1、行间事件（主要用于事件）1&lt;input type="button" name="" onclick="alert('ok！');"&gt;2、页面script标签嵌入123&lt;script type="text/javascript"&gt; alert('ok！');&lt;/script&gt;3、外部引入1&lt;script type="text/javascript" src="js/index.js"&gt;&lt;/script&gt;9.3 变量JavaScript 是一种弱类型语言，javascript的变量类型由它的值来决定。 定义变量需要用关键字 ‘var’123456var iNum = 123;var sTr = 'asd';//同时定义多个变量可以用","隔开，公用一个‘var’关键字var iNum = 45,sTr='qwe',sCount='68';9.3.1 变量类型5种基本数据类型：1、number 数字类型2、string 字符串类型3、boolean 布尔类型 true 或 false4、undefined undefined类型，变量声明未初始化，它的值就是undefined5、null null类型，表示空对象，如果定义的变量将来准备保存对象，可以将变量初始化为null,在页面上获取不到对象，返回的值就是null1种复合类型：object9.3.2 javascript语句与注释1、一条javascript语句应该以“;”结尾12345678&lt;script type="text/javascript"&gt; var iNum = 123; var sTr = 'abc123'; function fnAlert()&#123; alert(sTr); &#125;; fnAlert();&lt;/script&gt;2、javascript注释1234567891011&lt;script type="text/javascript"&gt; // 单行注释 var iNum = 123; /* 多行注释 1、... 2、... */ var sTr = 'abc123';&lt;/script&gt;9.3.3 变量、函数、属性、函数参数命名规范1、区分大小写2、第一个字符必须是字母、下划线（_）或者美元符号（$）3、其他字符可以是字母、下划线、美元符或数字9.3.4 匈牙利命名风格对象o Object 比如：oDiv数组a Array 比如：aItems字符串s String 比如：sUserName整数i Integer 比如：iItemCount布尔值b Boolean 比如：bIsComplete浮点数f Float 比如：fPrice函数fn Function 比如：fnHandler正则表达式re RegExp 比如：reEmailCheck9.4 获取元素方法一可以使用内置对象document上的getElementById方法来获取页面上设置了id属性的元素，获取到的是一个html对象，然后将它赋值给一个变量，比如：12345&lt;script type="text/javascript"&gt; var oDiv = document.getElementById('div1');&lt;/script&gt;....&lt;div id="div1"&gt;这是一个div元素&lt;/div&gt;上面的语句，如果把javascript写在元素的上面，就会出错，因为页面上从上往下加载执行的，javascript去页面上获取元素div1的时候，元素div1还没有加载，解决方法有两种：第一种方法：将javascript放到页面最下边12345678....&lt;div id="div1"&gt;这是一个div元素&lt;/div&gt;....&lt;script type="text/javascript"&gt; var oDiv = document.getElementById('div1');&lt;/script&gt;&lt;/body&gt;第二种方法：将javascript语句放到window.onload触发的函数里面,获取元素的语句会在页面加载完后才执行，就不会出错了。123456789&lt;script type="text/javascript"&gt; window.onload = function()&#123; var oDiv = document.getElementById('div1'); &#125;&lt;/script&gt;....&lt;div id="div1"&gt;这是一个div元素&lt;/div&gt;9.5 操作元素属性获取的页面元素，就可以对页面元素的属性进行操作，属性的操作包括属性的读和写。9.5.1 操作属性的方法1、“.” 操作2、“[ ]”操作9.5.2 属性写法1、html的属性和js里面属性写法一样2、“class” 属性写成 “className”3、“style” 属性里面的属性，有横杠的改成驼峰式，比如：“font-size”，改成”style.fontSize”通过“.”操作属性：具体来说是操作属性的值1234567891011121314151617181920212223242526272829303132333435363738394041&lt;script type="text/javascript"&gt; // 当整个页面加载完毕之后再执行花括号里面的语句 window.onload = function () &#123; // 通过ID名获取元素赋值给oDiv变量 var oDiv = document.getElementById("div1"); var oA = document.getElementById("link"); var oDiv2 = document.getElementById("div2"); // 改变元素的属性 oDiv.style.color = "red"; oDiv.style.fontSize = "20px"; oA.href = "http://www.baidu.com"; oA.title = "这是一个百度网的链接"; oDiv2.className = "box2"; // 读取元素的属性 var sId = oDiv.id; alert(sId) &#125;&lt;/script&gt;&lt;style type="text/css"&gt; .box&#123; font-size: 20px; color: gold; &#125; .box2&#123; font-size: 30px; color: pink; &#125;&lt;/style&gt;&lt;div id="div1"&gt;这是一个div元素&lt;/div&gt;&lt;a href="#" id="link"&gt;这是一个链接&lt;/a&gt;&lt;div class="box" id="div2"&gt;这是第二个div元素&lt;/div&gt;通过“[ ]”操作属性：具体来说是操作属性，把属性作为变量来处理，而不是作为一个值来处理123456789101112131415161718192021&lt;script type="text/javascript"&gt; window.onload = function()&#123; var oInput1 = document.getElementById('input1'); var oInput2 = document.getElementById('input2'); var oA = document.getElementById('link1'); // 读取属性 var sVal1 = oInput1.value; var sVal2 = oInput2.value; // 写(设置)属性 // oA.style.val1 = val2; 没反应 oA.style[sVal1] = sVal2; &#125;&lt;/script&gt;......&lt;input type="text" name="setattr" id="input1" value="fontSize"&gt;&lt;input type="text" name="setnum" id="input2" value="30px"&gt;&lt;a href="http://www.baidu.com" id="link1"&gt;这是百度网站地址&lt;/a&gt;9.5.3 innerHTMLinnerHTML可以读取或者写入标签包裹的内容1234567891011121314&lt;script type="text/javascript"&gt; window.onload = function()&#123; var oDiv = document.getElementById('div1'); //读取 var sTxt = oDiv.innerHTML; alert(sTxt); //写入 oDiv.innerHTML = '&lt;a href="http://http://www.baidu.com"&gt;这是百度网站地址&lt;a/&gt;'; &#125;&lt;/script&gt;......&lt;div id="div1"&gt;这是一个div元素&lt;/div&gt;9.6 函数函数就是重复执行的代码片。9.6.1 函数定义与执行123456789101112131415161718192021&lt;script type="text/javascript"&gt; // 函数的定义 function fnMyalert() &#123; alert('Hello world!') &#125; function fnChange() &#123; var oDiv = document.getElementById("div1"); oDiv.style.color = "red"; oDiv.style.fontSize = "30px"; &#125; //函数执行 // fnChange();&lt;/script&gt;&lt;!--标签里调用函数执行--&gt;&lt;div id="div1" onclick="fnMyalert()"&gt;这是一个div元素&lt;/div&gt;&lt;input type="button" name="" value="改变div" onclick="fnChange()"&gt;9.6.2 变量与函数预解析JavaScript解析过程分为两个阶段，先是编译阶段，然后执行阶段，在编译阶段会将function定义的函数提前，并且将var定义的变量声明提前，将它赋值为undefined。12345678&lt;script type="text/javascript"&gt; fnAlert(); // 弹出 hello！ alert(iNum); // 弹出 undefined function fnAlert()&#123; alert('hello!'); &#125; var iNum = 123;&lt;/script&gt;9.6.3 提取行间事件在html行间调用的事件可以提取到javascript中调用，从而做到结构与行为分离。说白了就是在html的标签中不含有任何关于js的代码，如下例，通过html标签中的id跟js关联起来，从而使用js。12345678910111213141516171819202122&lt;!--行间事件调用函数 --&gt;&lt;script type="text/javascript"&gt; function fnAlert()&#123; alert('ok!'); &#125;&lt;/script&gt;......&lt;input type="button" name="" value="弹出" onclick="fnAlert()"&gt;&lt;!-- 提取行间事件 --&gt;&lt;script type="text/javascript"&gt;window.onload = function()&#123; var oBtn = document.getElementById('btn1'); oBtn.onclick = fnAlert; function fnAlert()&#123; alert('ok!'); &#125;&#125; &lt;/script&gt;......&lt;input type="button" name="" value="弹出" id="btn1"&gt;9.6.4 匿名函数定义的函数可以不给名称，这个叫做匿名函数，可以将匿名函数直接赋值给元素绑定的事件来完成匿名函数的调用。123456789101112131415161718&lt;script type="text/javascript"&gt;window.onload = function()&#123; var oBtn = document.getElementById('btn1'); /* oBtn.onclick = myalert; function myalert()&#123; alert('ok!'); &#125; */ // 直接将匿名函数赋值给绑定的事件 oBtn.onclick = function ()&#123; alert('ok!'); &#125;&#125;&lt;/script&gt;9.6.5 综合练习网页换肤1234567891011121314151617181920212223242526272829303132333435&lt;!DOCTYPE html&gt;&lt;html lang="en"&gt;&lt;head&gt; &lt;meta charset="UTF-8"&gt; &lt;title&gt;Title&lt;/title&gt; &lt;link rel="stylesheet" href="css/004_01.css" id="link01"&gt; &lt;script type="text/javascript"&gt; window.onload = function () &#123; var oBtn01 = document.getElementById("btn01"); var oBtn02 = document.getElementById("btn01"); var oLink = document.getElementById("link01"); oBtn01.onclick = function () &#123; oLink.href = "css/004_01.css"; &#125;; oBtn02.onclick = function () &#123; oLink.href = "css/004_02.css"; &#125; &#125; &lt;/script&gt;&lt;/head&gt;&lt;body&gt;&lt;input type="button" name="" value="皮肤1" id="btn01"&gt;&lt;input type="button" name="" value="皮肤2" id="btn02"&gt;&lt;/body&gt;&lt;/html&gt;1234567891011/* css/004_01.css */body&#123; background-color: gold;&#125;input&#123; width: 200px; height: 50px; background-color: aqua; border: 0;&#125;12345678910/* css/004_02.css */body&#123; background-color: pink;&#125;input&#123; width: 200px; height: 50px; background-color: aqua; border: 0;&#125;9.6.6 函数传参123456&lt;script type="text/javascript"&gt; function fnAlert(a)&#123; alert(a); &#125; fnAlert(12345);&lt;/script&gt;9.6.7 函数’return’关键字函数中’return’关键字的作用：1、返回函数执行的结果2、结束函数的运行3、阻止默认行为1234567891011121314151617181920212223242526272829303132// 函数返回值+结束函数运行&lt;script type="text/javascript"&gt; function fnAdd(a,b) &#123; var c = a + b; alert('hello-1'); // hello-1 // 返回c的值，结束函数的运行 return c; // 这一句不弹出 alert('hello-2'); &#125; var result = fnAdd(3,4); alert(result); // 弹出7&lt;/script&gt;// 函数传参&lt;script type="text/javascript"&gt; window.onload = function () &#123; function fnChangestyle(mystyle,val) &#123; var oDiv = document.getElementById("div1"); oDiv.style[mystyle] = val; &#125; fnChangestyle("color","pink"); fnChangestyle("fontSize","30px"); &#125;&lt;/script&gt;&lt;div id="div1"&gt;这是一个div元素&lt;/div&gt;9.7 条件语句通过条件来控制程序的走向，就需要用到条件语句。9.7.1 运算符1、算术运算符： +(加)、 -(减)、 (乘)、 /(除)、 %(求余数)2、赋值运算符：=、 +=、 -=、 =、 /=、 %=3、条件运算符：==、===、&gt;、&gt;=、&lt;、&lt;=、!=、&amp;&amp;(而且)、||(或者)、!(否)123456789101112131415161718192021&lt;script type="text/javascript"&gt; window.onload = function () &#123; var oInput01 = document.getElementById("input01"); var oInput02 = document.getElementById("input02"); var oBtn = document.getElementById("btn"); oBtn.onclick = function () &#123; // parseInt()函数转换成数字 var iVal01 = parseInt(oInput01.value); var iVal02 = parseInt(oInput02.value); var iBtn = iVal01 + iVal02; alert(iBtn); &#125; &#125;;&lt;/script&gt;&lt;input type="text" name="" id="input01"&gt; +&lt;input type="text" name="" id="input02"&gt;&lt;input type="button" name="" value="相加" id="btn"&gt;9.7.2 if else1234567891011&lt;script type="text/javascript"&gt; var iNum01 = 3; var iNum02 = 5; var sTr; if (iNum01 &gt; iNum02) &#123; sTr = '大于'; &#125; else &#123; sTr = '小于'; &#125; alert(sTr);&lt;/script&gt;9.7.3 理解练习制作单个按钮点击切换元素的显示和隐藏效果12345678910111213141516171819202122232425&lt;script type="text/javascript"&gt; window.onload = function () &#123; var oBtn = document.getElementById("btn01"); var oDiv = document.getElementById("box01"); oBtn.onclick = function () &#123; if (oDiv.style.display == "none") &#123; oDiv.style.display = 'block'; &#125; else &#123; oDiv.style.display = "none"; &#125; &#125; &#125;&lt;/script&gt;&lt;style type="text/css"&gt; .box &#123; width: 200px; height: 400px; background-color: gold; &#125;&lt;/style&gt;&lt;input type="button" value="切换" id="btn01"&gt;&lt;div class="box" id="box01"&gt;&lt;/div&gt;9.7.4 多重if else语句12345678910111213var iNow = 1;if(iNow==1)&#123; ... ;&#125;else if(iNow==2)&#123; ... ;&#125;else&#123; ... ;&#125;9.7.5 switch语句多重if else语句可以换成性能更高的switch语句123456789101112var iNow = 1;switch (iNow)&#123; case 1: ...; break; case 2: ...; break; default: ...;&#125;9.7.6 理解练习制作随着星期换背景的页面123456789101112131415161718192021222324252627282930313233343536373839&lt;!DOCTYPE html&gt;&lt;html lang="en"&gt;&lt;head&gt; &lt;meta charset="UTF-8"&gt; &lt;title&gt;Title&lt;/title&gt; &lt;script type="text/javascript"&gt; window.onload = function () &#123; var iWeek = 5; var oBody = document.getElementById("body01"); switch (iWeek) &#123; case 1: oBody.style.backgroundColor = "gold"; break; case 2: oBody.style.backgroundColor = "pink"; break; case 3: oBody.style.backgroundColor = "red"; break; case 4: oBody.style.backgroundColor = "green"; break; case 5: oBody.style.backgroundColor = "yellow"; break; default : oBody.style.backgroundColor = "blue"; break; &#125; &#125; &lt;/script&gt;&lt;/head&gt;&lt;body id="body01"&gt;&lt;/body&gt;&lt;/html&gt;9.8 数组及操作方法数组就是一组数据的集合，javascript中，数组里面的数据可以是不同类型的。9.8.1 定义数组的方法12345//对象的实例创建var aList = new Array(1,2,3);//直接量创建var aList2 = [1,2,3,'asd'];9.8.2 操作数组中数据的方法1、获取数组的长度：aList.length;12var aList = [1,2,3,4];alert(aList.length); // 弹出42、用下标操作数组的某个数据：aList[0];12var aList = [1,2,3,4];alert(aList[0]); // 弹出13、join() 将数组成员通过一个分隔符合并成字符串12var aList = [1,2,3,4];alert(aList.join('-')); // 弹出 1-2-3-44、push() 和 pop() 从数组最后增加成员或删除成员12345var aList = [1,2,3,4];aList.push(5);alert(aList); //弹出1,2,3,4,5aList.pop();alert(aList); // 弹出1,2,3,45、unshift()和 shift() 从数组前面增加成员或删除成员12345var aList = [1,2,3,4];aList.unshift(5);alert(aList); //弹出5,1,2,3,4aList.shift();alert(aList); // 弹出1,2,3,46、reverse() 将数组反转123var aList = [1,2,3,4];aList.reverse();alert(aList); // 弹出4,3,2,17、indexOf() 返回数组中元素第一次出现的索引值12var aList = [1,2,3,4,1,3,4];alert(aList.indexOf(1));8、splice() 在数组中增加或删除成员123var aList = [1,2,3,4];aList.splice(2,1,7,8,9); //从第2个元素开始，删除1个元素，然后在此位置增加'7,8,9'三个元素alert(aList); //弹出 1,2,7,8,9,49.8.3 多维数组多维数组指的是数组的成员也是数组的数组。123var aList = [[1,2,3],['a','b','c']];alert(aList[0][1]); //弹出2;批量操作数组中的数据，需要用到循环语句9.9 循环语句程序中进行有规律的重复性操作，需要用到循环语句。9.9.1 for循环1234for(var i=0;i&lt;len;i++)&#123; ......&#125;9.9.2 课堂练习1、将数组中的数据分别用弹框弹出12345678910111213&lt;script type="text/javascript"&gt; for (var i = 0; i &lt; 5; i++) &#123; alert(i); &#125; var aList = ['a','b','c','d','e']; var iLen = aList.length; for (var i = 0; i &lt; iLen; i++) &#123; alert(aList[i]); &#125;&lt;/script&gt;2、将数组中的数据放入到页面中的列表中1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556&lt;script type="text/javascript"&gt; window.onload = function () &#123; var oLl = document.getElementsByTagName("li"); var aList = ['apple', 'banana', 'orange', 'xigua', 'boluo']; var aLen = aList.length; for (var i = 0; i &lt; aLen; i++) &#123; var name = aList[i]; // 获取标签内的值 oLl[i].innerHTML = name; &#125; &#125;&lt;/script&gt;&lt;ul&gt; &lt;li&gt;&lt;/li&gt; &lt;li&gt;&lt;/li&gt; &lt;li&gt;&lt;/li&gt; &lt;li&gt;&lt;/li&gt; &lt;li&gt;&lt;/li&gt;&lt;/ul&gt;// 另一种方法&lt;script type="text/javascript"&gt; window.onload = function () &#123; var oUl = document.getElementById('list'); var aList = ['apple', 'banana', 'orange', 'xigua', 'boluo']; var aLen = aList.length; var sTr = ''; for (var i = 0; i &lt; aLen; i++) &#123; sTr += '&lt;li&gt;' + aList[i] + '&lt;/li&gt;'; oUl.innerHTML = sTr; &#125; &#125;&lt;/script&gt;&lt;style type="text/css"&gt; .list &#123; list-style: none; margin: 50px auto 0; padding: 0; width: 300px; height: 305px; &#125; .list li &#123; height: 60px; border-bottom: 1px dotted #000; line-height: 60px; font-size: 16px; &#125;&lt;/style&gt;&lt;ul class="list" id="list"&gt;&lt;/ul&gt;9.9.3 while循环123456var i=0;while(i&lt;8)&#123; ...... i++;&#125;9.9.4 数组去重12345678910111213var aList = [1,2,3,4,4,3,2,1,2,3,4,5,6,5,5,3,3,4,2,1];var aList2 = [];for(var i=0;i&lt;aList.length;i++)&#123; if(aList.indexOf(aList[i])==i) &#123; aList2.push(aList[i]); &#125;&#125;alert(aList2);9.10 获取元素方法二可以使用内置对象document上的getElementsByTagName方法来获取页面上的某一种标签，获取的是一个选择集，不是数组，但是可以用下标的方式操作选择集里面的标签元素。1234567891011121314151617&lt;script type="text/javascript"&gt; window.onload = function()&#123; var aLi = document.getElementsByTagName('li'); // aLi.style.backgroundColor = 'gold'; // 出错！不能同时设置多个li alert(aLi.length); aLi[0].style.backgroundColor = 'gold'; &#125;&lt;/script&gt;....&lt;ul&gt; &lt;li&gt;1&lt;/li&gt; &lt;li&gt;2&lt;/li&gt; &lt;li&gt;3&lt;/li&gt; &lt;li&gt;4&lt;/li&gt; &lt;li&gt;5&lt;/li&gt; &lt;li&gt;6&lt;/li&gt;&lt;/ul&gt;课堂练习使用循环操作列表中的每个元素1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859&lt;script type="text/javascript"&gt; window.onload = function () &#123; // 通过标签名称获取li元素，生成一个选择集，赋值给aLi var aLi = document.getElementsByTagName('li'); // 读取选择集内元素的个数 //alert(aLi.length); // 弹出13 var iLen = aLi.length; //给一个li设置背景色 //aLi[0].style.backgroundColor = 'gold'; // 不能给选择集设置样式属性 //aLi.style.backgroundColor = 'gold'; /* 同时给所有的li加背景色 for(var i=0;i&lt;iLen;i++) &#123; aLi[i].style.backgroundColor = 'gold'; &#125; */ var oUl = document.getElementById('list1'); var aLi2 = oUl.getElementsByTagName('li'); var iLen2 = aLi2.length; for (var i = 0; i &lt; iLen2; i++) &#123; if (i % 2 == 0) &#123; aLi2[i].style.backgroundColor = 'gold'; &#125; &#125; &#125;&lt;/script&gt;&lt;ul id="list1"&gt; &lt;li&gt;1&lt;/li&gt; &lt;li&gt;2&lt;/li&gt; &lt;li&gt;3&lt;/li&gt; &lt;li&gt;4&lt;/li&gt; &lt;li&gt;5&lt;/li&gt; &lt;li&gt;6&lt;/li&gt; &lt;li&gt;7&lt;/li&gt; &lt;li&gt;8&lt;/li&gt;&lt;/ul&gt;&lt;ul id="list2"&gt; &lt;li&gt;1&lt;/li&gt; &lt;li&gt;2&lt;/li&gt; &lt;li&gt;3&lt;/li&gt; &lt;li&gt;4&lt;/li&gt; &lt;li&gt;5&lt;/li&gt;&lt;/ul&gt;9.11 Javascript组成1、ECMAscript javascript的语法（变量、函数、循环语句等语法）2、DOM 文档对象模型 操作html和css的方法3、BOM 浏览器对象模型 操作浏览器的一些方法9.12 字符串处理方法1、字符串合并操作：“ + ”1234567var iNum01 = 12;var iNum02 = 24;var sNum03 = '12';var sTr = 'abc';alert(iNum01+iNum02); //弹出36alert(iNum01+sNum03); //弹出1212 数字和字符串相加等同于字符串相加alert(sNum03+sTr); // 弹出12abc2、parseInt() 将数字字符串转化为整数123456var sNum01 = '12';var sNum02 = '24';var sNum03 = '12.32';alert(sNum01+sNum02); //弹出1224alert(parseInt(sNum01)+parseInt(sNum02)) //弹出36alert(sNum03) //弹出数字12 将字符串小数转化为数字整数3、parseFloat() 将数字字符串转化为小数12var sNum03 = '12.32'alert(parseFloat(sNum03)); //弹出 12.32 将字符串小数转化为数字小数4、split() 把一个字符串分隔成字符串组成的数组123456var sTr = '2017-4-22';var aRr = sTr.split("-");var aRr2= sTr.split("");alert(aRr); //弹出['2017','4','2']alert(aRr2); //弹出['2','0','1','7','-','4','-','2','2']5、charAt() 获取字符串中的某一个字符123var sId = "#div1";var sTr = sId.charAt(0);alert(sTr); //弹出 #6、indexOf() 查找字符串是否含有某字符123var sTr = "abcdefgh";var iNum = sTr.indexOf("c");alert(iNum); //弹出27、substring() 截取字符串 用法： substring(start,end)（不包括end）123456var sTr = "abcdefghijkl";var sTr2 = sTr.substring(3,5);var sTr3 = sTr.substring(1);alert(sTr2); //弹出 dealert(sTr3); //弹出 bcdefghijkl8、toUpperCase() 字符串转大写123var sTr = "abcdef";var sTr2 = sTr.toUpperCase();alert(sTr2); //弹出ABCDEF9、toLowerCase() 字符串转小写123var sTr = "ABCDEF";var sTr2 = sTr.toLowerCase();alert(sTr2); //弹出abcdef字符串反转1234var str = 'asdfj12jlsdkf098';var str2 = str.split('').reverse().join('');alert(str2);9.13 类型转换1、直接转换 parseInt() 与 parseFloat()12345678alert('12'+7); //弹出127alert( parseInt('12') + 7 ); //弹出19 alert( parseInt(5.6)); // 弹出5alert('5.6'+2.3); // 弹出5.62.3alert(parseFloat('5.6')+2.3); // 弹出7.8999999999999995alert(0.1+0.2); //弹出 0.3000000000000004alert((0.1*100+0.2*100)/100); //弹出0.3alert((parseFloat('5.6')*100+2.3*100)/100); //弹出7.92、隐式转换 “==” 和 “-”1234567if('3'==3)&#123; alert('相等');&#125;// 弹出'相等'alert('10'-3); // 弹出73、NaN 和 isNaN12alert( parseInt('123abc') ); // 弹出123alert( parseInt('abc123') ); // 弹出NaN9.14 调试程序的方法1、alert弹出的时候程序会卡住，也就是会阻止程序的运行2、console.log3、document.title12345678// 改变的是标签&lt;title&gt;的值window.onload = function () &#123; var iNum01 = 20; setInterval(function () &#123; iNum01++; document.title = iNum01; &#125;,100);&#125;9.15 定时器9.15.1 定时器在javascript中的作用1、制作动画2、异步操作3、函数缓冲与节流定时器类型及语法123456789101112131415161718/* 定时器： setTimeout 只执行一次的定时器 clearTimeout 关闭只执行一次的定时器 setInterval 反复执行的定时器 clearInterval 关闭反复执行的定时器*/var time1 = setTimeout(myalert,2000);var time2 = setInterval(myalert,2000);/*clearTimeout(time1);clearInterval(time2);*/function myalert()&#123; alert('ok!');&#125;9.15.2 课堂练习1、定时器制作移动动画12345678910111213141516171819202122232425262728293031323334353637383940&lt;script type="text/javascript"&gt; window.onload = function () &#123; var oDiv = document.getElementById('div1'); var iLeft = 0; var iSpeed = 3; /* var timer = setInterval(moving,30); function moving()&#123; iLeft += 3; oDiv.style.left = iLeft + 'px'; &#125; */ var timer = setInterval(function () &#123; iLeft += iSpeed; oDiv.style.left = iLeft + 'px'; if (iLeft &gt; 700) &#123; // clearInterval(timer); // 定时器停止执行 iSpeed = -3; // 动画返回 &#125; if (iLeft &lt; 0) &#123; // 动画返回之后再次折返过去 iSpeed = 3; &#125; &#125;, 30); &#125;&lt;/script&gt;&lt;style type="text/css"&gt; .box &#123; width: 200px; height: 200px; background-color: gold; position: absolute; left: 0; top: 100px; &#125;&lt;/style&gt;&lt;div id="div1" class="box"&gt;&lt;/div&gt;2、定时器制作无缝滚动123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139&lt;!DOCTYPE html&gt;&lt;html lang="en"&gt;&lt;head&gt; &lt;meta charset="UTF-8"&gt; &lt;title&gt;无缝滚动&lt;/title&gt; &lt;style type="text/css"&gt; *&#123; margin:0; padding:0; &#125; .list_con&#123; width:1000px; height:200px; border:1px solid #000; margin:10px auto 0; background-color:#f0f0f0; position:relative; overflow:hidden; &#125; .list_con ul&#123; list-style:none; width:2000px; height:200px; position:absolute; left:0; top:0; &#125; .list_con li&#123; width:180px; height:180px; float:left; margin:10px; &#125; .btns_con&#123; width:1000px; height:30px; margin:50px auto 0; position:relative; &#125; .left,.right&#123; width:30px; height:30px; background-color:gold; position:absolute; left:-40px; top:124px; font-size:30px; line-height:30px; color:#000; font-family: 'Arial'; text-align:center; cursor:pointer; border-radius:15px; opacity:0.5; &#125; .right&#123; left:1010px; top:124px; &#125; &lt;/style&gt; &lt;script type="text/javascript"&gt; window.onload = function()&#123; var oDiv = document.getElementById('slide'); var oBtn01 = document.getElementById('btn01'); var oBtn02 = document.getElementById('btn02'); //通过标签获取元素，获取的是选择集，加上下标才能获取到元素 var oUl = oDiv.getElementsByTagName('ul')[0]; var iLeft = 0; var iSpeed = -2; var iNowspeed = 0; //将ul里面的内容复制一份，整个ul里面就包含了10个li oUl.innerHTML = oUl.innerHTML + oUl.innerHTML; function moving()&#123; iLeft += iSpeed; // 当ul向左滚动到第5个li时，瞬间将整个ul拉回到初始位置 if(iLeft&lt;-1000) &#123; iLeft=0; &#125; //当ul在起始位置往右滚动时候，瞬间将整个ul拉回到往左的第5个li的位置 if(iLeft&gt;0) &#123; iLeft = -1000; &#125; oUl.style.left = iLeft + 'px'; &#125; var timer = setInterval(moving,30); oBtn01.onclick = function()&#123; iSpeed = -2; &#125; oBtn02.onclick = function()&#123; iSpeed = 2; &#125; // 当鼠标移入的时候 oDiv.onmouseover = function()&#123; iNowspeed = iSpeed; iSpeed = 0; &#125; // 当鼠标移出的时候 oDiv.onmouseout = function()&#123; iSpeed = iNowspeed; &#125; &#125; &lt;/script&gt;&lt;/head&gt;&lt;body&gt; &lt;div class="btns_con"&gt; &lt;div class="left" id="btn01"&gt;&amp;lt;&lt;/div&gt; &lt;div class="right" id="btn02"&gt;&amp;gt;&lt;/div&gt; &lt;/div&gt; &lt;div class="list_con" id="slide"&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=""&gt;&lt;img src="images/goods001.jpg" alt="商品图片"&gt;&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=""&gt;&lt;img src="images/goods002.jpg" alt="商品图片"&gt;&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=""&gt;&lt;img src="images/goods003.jpg" alt="商品图片"&gt;&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=""&gt;&lt;img src="images/goods004.jpg" alt="商品图片"&gt;&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=""&gt;&lt;img src="images/goods005.jpg" alt="商品图片"&gt;&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;/body&gt;&lt;/html&gt;3、定时器制作时钟12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364&lt;script type="text/javascript"&gt; window.onload = function()&#123; var oDiv = document.getElementById('div1'); function timego()&#123; var now = new Date(); var year = now.getFullYear(); var month = now.getMonth()+1; var date = now.getDate(); var week = now.getDay(); var hour = now.getHours(); var minute = now.getMinutes(); var second = now.getSeconds(); var str = '当前时间是：'+ year + '年'+month+'月'+date+'日 '+toweek(week)+' '+todou(hour)+':'+todou(minute)+':'+todou(second); oDiv.innerHTML = str; &#125; timego(); setInterval(timego,1000); &#125; function toweek(n)&#123; if(n==0) &#123; return '星期日'; &#125; else if(n==1) &#123; return '星期一'; &#125; else if(n==2) &#123; return '星期二'; &#125; else if(n==3) &#123; return '星期三'; &#125; else if(n==4) &#123; return '星期四'; &#125; else if(n==5) &#123; return '星期五'; &#125; else &#123; return '星期六'; &#125; &#125; function todou(n)&#123; if(n&lt;10) &#123; return '0'+n; &#125; else &#123; return n; &#125; &#125;&lt;/script&gt;......&lt;div id="div1"&gt;&lt;/div&gt;4、定时器制作倒计时123456789101112131415161718192021&lt;script type="text/javascript"&gt; window.onload = function()&#123; var oDiv = document.getElementById('div1'); function timeleft()&#123; var now = new Date(); var future = new Date(2016,8,12,24,0,0); var lefts = parseInt((future-now)/1000); var day = parseInt(lefts/86400); var hour = parseInt(lefts%86400/3600); var min = parseInt(lefts%86400%3600/60); var sec = lefts%60; str = '距离2016年9月12日晚24点还剩下'+day+'天'+hour+'时'+min+'分'+sec+'秒'; oDiv.innerHTML = str; &#125; timeleft(); setInterval(timeleft,1000); &#125;&lt;/script&gt;......&lt;div id="div1"&gt;&lt;/div&gt;9.16 变量作用域变量作用域指的是变量的作用范围，javascript中的变量分为全局变量和局部变量。1、全局变量：在函数之外定义的变量，为整个页面公用，函数内部外部都可以访问。2、局部变量：在函数内部定义的变量，只能在定义该变量的函数内部访问，外部无法访问。1234567891011121314&lt;script type="text/javascript"&gt; //全局变量 var a = 12; function myalert() &#123; //局部变量 var b = 23; alert(a); alert(b); &#125; myalert(); //弹出12和23 alert(a); //弹出12 alert(b); //出错&lt;/script&gt;9.17 封闭函数封闭函数是javascript中匿名函数的另外一种写法，创建一个一开始就执行而不用命名的函数。一般定义的函数和执行函数：12345function myalert()&#123; alert('hello!');&#125;;myalert();封闭函数：123(function myalert()&#123; alert('hello!');&#125;)();还可以在函数定义前加上“~”和“!”等符号来定义匿名函数123!function myalert()&#123; alert('hello!');&#125;()9.17.1 封闭函数的好处封闭函数可以创造一个独立的空间，在封闭函数内定义的变量和函数不会影响外部同名的函数和变量，可以避免命名冲突，在页面上引入多个js文件时，用这种方式添加js文件比较安全，比如：1234567891011121314var iNum01 = 12;function myalert()&#123; alert('hello!');&#125;(function()&#123; var iNum01 = 24; function myalert()&#123; alert('hello!world'); &#125; alert(iNum01); myalert()&#125;)()alert(iNum01);myalert();9.18 常用内置对象1、document123document.getElementById //通过id获取元素document.getElementsByTagName //通过标签名获取元素document.referrer //获取上一个跳转页面的地址(需要服务器环境)2、location123window.location.href //获取或者重定url地址window.location.search //获取地址参数部分window.location.hash //获取页面锚点或者叫哈希值1234567891011121314&lt;script type="text/javascript"&gt; window.onload = function () &#123; // 存储上一个页面的地址 var sUrl = document.referrer; var oBtn = document.getElementById("btn01"); oBtn.onclick = function () &#123; window.location.href = "http://www.baidu.com"; &#125;; alert(window.location.search); &#125;&lt;/script&gt;&lt;input type="button" id="btn01" value="跳转" name=""&gt;课堂练习通过地址栏的参数改变页面状态，如下的示例是通过地址栏中的参数改变页面背景色1234567891011121314151617181920212223242526&lt;!DOCTYPE html&gt;&lt;html lang="en"&gt;&lt;head&gt; &lt;meta charset="UTF-8"&gt; &lt;title&gt;Test&lt;/title&gt; &lt;script type="text/javascript"&gt; window.onload = function () &#123; var oBody01 = document.getElementById("body01"); var sData = window.location.search; if (sData != '') &#123; var iNum = sData.split("=")[1]; // console.log(iNum); if (iNum == 1) &#123; oBody01.style.backgroundColor = "red"; &#125; else &#123; oBody01.style.backgroundColor = "pink"; &#125;; &#125;; &#125; &lt;/script&gt;&lt;/head&gt;&lt;body id="body01"&gt;&lt;input type="button" id="btn01" value="跳转" name=""&gt;&lt;/body&gt;&lt;/html&gt;3、Math123Math.random 获取0-1的随机数Math.floor 向下取整Math.ceil 向上取整123456789101112131415&lt;script type="text/javascript"&gt; // var iPi = Math.PI; // alert(iPi); var sList = []; for (var i = 0; i &lt; 20; i++) &#123; // Math.random 返回0到1之间的随机数,不包括1 var iNum = Math.random(); sList.push(iNum); &#125; console.log(sList); alert(Math.floor(5.6)); // 向下取整，结果是5 alert(Math.ceil(5.2)); // 向上取整，结果是6&lt;/script&gt;课堂练习制作一定范围内的随机整数12345// 获取10到20之间的随机整数var iN01 = 10;var iN02 = 20;var result = Math.floor((iN02 - iN01+1)*Math.random()) + iN01;alert(result)10.JQuery10.1 jquery介绍jQuery是目前使用最广泛的javascript函数库。据统计，全世界排名前100万的网站，有46%使用jQuery，远远超过其他库。微软公司甚至把jQuery作为他们的官方库。jQuery的版本分为1.x系列和2.x、3.x系列，1.x系列兼容低版本的浏览器，2.x、3.x系列放弃支持低版本浏览器，目前使用最多的是1.x系列的。jquery是一个函数库，一个js文件，页面用script标签引入这个js文件就可以使用。1&lt;script type=&quot;text/javascript&quot; src=&quot;js/jquery-1.12.2.js&quot;&gt;&lt;/script&gt;jquery的口号和愿望 Write Less, Do More（写得少，做得多）1、http://jquery.com/ 官方网站2、https://code.jquery.com/ 版本下载10.2 jquery加载将获取元素的语句写到页面头部，会因为元素还没有加载而出错，jquery提供了ready方法解决这个问题，它的速度比原生的 window.onload 更快。123456789&lt;script type="text/javascript"&gt;$(document).ready(function()&#123; ......&#125;);&lt;/script&gt;可以简写为：123456789&lt;script type="text/javascript"&gt;$(function()&#123; ......&#125;);&lt;/script&gt;1234567891011121314151617181920212223242526272829303132333435&lt;!DOCTYPE html&gt;&lt;html lang="en"&gt;&lt;head&gt; &lt;meta charset="UTF-8"&gt; &lt;title&gt;Title&lt;/title&gt; &lt;!--不能直接在jquery的引入标签里写js代码，需要另写一个script标签，在这个里面那些js代码--&gt; &lt;script type="text/javascript" src="js/jquery-3.4.1.min.js"&gt;&lt;/script&gt; &lt;script type="text/javascript"&gt; // 原生js写法,onload是整个页面加载完后，再渲染完之后才运行 window.onload = function () &#123; var oDiv = document.getElementById("div1"); alert('这是原生js弹出的div' + oDiv); &#125;; // 完整写法，ready是标签加载完就执行，速度比原生js的快 $(document).ready(function () &#123; var $div = $("#div1"); alert('这是jquery弹出的div-1' + $div); &#125;); // 简单写法 $(function () &#123; var $div = $("#div1"); alert('这是jquery弹出的div-2' + $div); &#125;); &lt;/script&gt;&lt;/head&gt;&lt;body&gt;&lt;div id="div1"&gt; 这是一个div元素&lt;/div&gt;&lt;/body&gt;&lt;/html&gt;10.3 jquery选择器10.3.1 jquery用法思想一选择某个网页元素，然后对它进行某种操作10.3.2 jquery选择器jquery选择器可以快速地选择元素，选择规则和css样式相同，使用length属性判断是否选择成功。12345$('#myId') //选择id为myId的网页元素$('.myClass') // 选择class为myClass的元素$('li') //选择所有的li元素$('#ul1 li span') //选择id为ul1元素下的所有li下的span元素$('input[name=first]') // 选择name属性等于first的input元素12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758&lt;!DOCTYPE html&gt;&lt;html lang="en"&gt;&lt;head&gt; &lt;meta charset="UTF-8"&gt; &lt;title&gt;Title&lt;/title&gt; &lt;!--不能直接在jquery的引入标签里写js代码，需要另写一个script标签，在这个里面那些js代码--&gt; &lt;script type="text/javascript" src="js/jquery-3.4.1.min.js"&gt;&lt;/script&gt; &lt;script type="text/javascript"&gt; $(function () &#123; var $div = $("#div1"); $div.css(&#123;"color":"red"&#125;); var $div2 = $(".box"); $div2.css(&#123;"color":"green"&#125;); var $li = $(".list li"); // 带'-'的样式属性可以写成驼峰式，也可以写成原始的，容错 // $li.css(&#123;"backgroundColor":"pink"&#125;); $li.css(&#123;"background-color":"pink"&#125;); &#125;); &lt;/script&gt; &lt;!--&lt;style type="text/css"&gt;--&gt; &lt;!--#div1&#123;--&gt; &lt;!--color: gold;--&gt; &lt;!--&#125;--&gt; &lt;!--.box&#123;--&gt; &lt;!--color: green;--&gt; &lt;!--&#125;--&gt; &lt;!--.list li&#123;--&gt; &lt;!--background-color: pink;--&gt; &lt;!--&#125;--&gt; &lt;!--&lt;/style&gt;--&gt;&lt;/head&gt;&lt;body&gt;&lt;div id="div1"&gt;这是一个div元素&lt;/div&gt;&lt;div class="box"&gt;这是第二个div元素&lt;/div&gt;&lt;div class="box"&gt;这是第三个div元素&lt;/div&gt;&lt;!--ul.list&gt;li&#123;$&#125;*8--&gt;&lt;ul class="list"&gt; &lt;li&gt;1&lt;/li&gt; &lt;li&gt;2&lt;/li&gt; &lt;li&gt;3&lt;/li&gt; &lt;li&gt;4&lt;/li&gt; &lt;li&gt;5&lt;/li&gt; &lt;li&gt;6&lt;/li&gt; &lt;li&gt;7&lt;/li&gt; &lt;li&gt;8&lt;/li&gt;&lt;/ul&gt;&lt;/body&gt;&lt;/html&gt;10.3.3对选择集进行过滤1234$('div').has('p'); // 选择包含p元素的div元素$('div').not('.myClass'); //选择class不等于myClass的div元素$('div').filter('.myClass'); //选择class等于myClass的div元素$('div').eq(5); //选择第6个div元素12345678910111213141516171819202122232425262728293031&lt;!DOCTYPE html&gt;&lt;html lang="en"&gt;&lt;head&gt; &lt;meta charset="UTF-8"&gt; &lt;title&gt;Title&lt;/title&gt; &lt;!--不能直接在jquery的引入标签里写js代码，需要另写一个script标签，在这个里面那些js代码--&gt; &lt;script type="text/javascript" src="js/jquery-3.4.1.min.js"&gt;&lt;/script&gt; &lt;script type="text/javascript"&gt; $(function () &#123; // var $div = $("div"); // $div.css(&#123;"background-color":"red"&#125;); $("div").css(&#123;"background-color":"gold"&#125;); $("div").has('p').css(&#123;"background-color":"red"&#125;); $("div").eq(4).css(&#123;"text-indent":"30px"&#125;); &#125;); &lt;/script&gt;&lt;/head&gt;&lt;body&gt;&lt;div&gt;1&lt;/div&gt;&lt;div&gt;&lt;p&gt;2&lt;/p&gt;&lt;/div&gt;&lt;div&gt;3&lt;/div&gt;&lt;div&gt;4&lt;/div&gt;&lt;div&gt;5&lt;/div&gt;&lt;div&gt;6&lt;/div&gt;&lt;div&gt;7&lt;/div&gt;&lt;div&gt;8&lt;/div&gt;&lt;/body&gt;&lt;/html&gt;10.3.4 选择集转移12345678$('div').prev(); //选择div元素前面紧挨的同辈元素$('div').prevAll(); //选择div元素之前所有的同辈元素$('div').next(); //选择div元素后面紧挨的同辈元素$('div').nextAll(); //选择div元素后面所有的同辈元素$('div').parent(); //选择div的父元素$('div').children(); //选择div的所有子元素$('div').siblings(); //选择div的同级元素$('div').find('.myClass'); //选择div内的class等于myClass的元素12345678910111213141516171819202122232425262728&lt;!DOCTYPE html&gt;&lt;html lang="en"&gt;&lt;head&gt; &lt;meta charset="UTF-8"&gt; &lt;title&gt;Title&lt;/title&gt; &lt;!--不能直接在jquery的引入标签里写js代码，需要另写一个script标签，在这个里面那些js代码--&gt; &lt;script type="text/javascript" src="js/jquery-3.4.1.min.js"&gt;&lt;/script&gt; &lt;script type="text/javascript"&gt; $(function () &#123; $("div").eq(4).prev().css(&#123;"color":"green"&#125;); $("div").find(".tip").css(&#123;"color":"red"&#125;); &#125;); &lt;/script&gt;&lt;/head&gt;&lt;body&gt;&lt;div&gt;1&lt;/div&gt;&lt;div&gt;&lt;p&gt;2&lt;/p&gt;&lt;/div&gt;&lt;div&gt;7&lt;/div&gt;&lt;div&gt; &lt;span&gt;8&lt;/span&gt; &lt;span class="tip"&gt;9&lt;/span&gt;&lt;/div&gt;&lt;/body&gt;&lt;/html&gt;10.3.5 判断是否选择到了元素jquery有容错机制，即使没有找到元素，也不会出错，可以用length属性来判断是否找到了元素,length等于0，就是没选择到元素，length大于0，就是选择到了元素。123456789101112131415161718192021222324&lt;!DOCTYPE html&gt;&lt;html lang="en"&gt;&lt;head&gt; &lt;meta charset="UTF-8"&gt; &lt;title&gt;Title&lt;/title&gt; &lt;!--不能直接在jquery的引入标签里写js代码，需要另写一个script标签，在这个里面那些js代码--&gt; &lt;script type="text/javascript" src="js/jquery-3.4.1.min.js"&gt;&lt;/script&gt; &lt;script type="text/javascript"&gt; $(function () &#123; var $div1 = $('#div1'); // 没有选中元素，也不会报错，程序正常运行 var $div2 = $('#div2'); alert($div1.length); // 弹出1 alert($div2.length); // 弹出0 &#125;); &lt;/script&gt;&lt;/head&gt;&lt;body&gt;&lt;div id="div1"&gt;div元素&lt;/div&gt;&lt;/body&gt;&lt;/html&gt;10.4 jquery样式操作10.4.1 jquery用法思想二同一个函数完成取值和赋值10.4.2 操作行间样式12345678// 获取div的样式$("div").css("width");$("div").css("color");//设置div的样式$("div").css("width","30px");$("div").css("height","30px");$("div").css(&#123;fontSize:"30px",color:"red"&#125;);12345678910111213141516171819202122232425262728293031323334&lt;!DOCTYPE html&gt;&lt;html lang="en"&gt;&lt;head&gt; &lt;meta charset="UTF-8"&gt; &lt;title&gt;Title&lt;/title&gt; &lt;!--不能直接在jquery的引入标签里写js代码，需要另写一个script标签，在这个里面那些js代码--&gt; &lt;script type="text/javascript" src="js/jquery-3.4.1.min.js"&gt;&lt;/script&gt; &lt;script type="text/javascript"&gt; $(function () &#123; var $div1 = $('#div1'); $div1.css("width", "30px"); $div1.css("height", "30px"); $div1.css(&#123;fontSize: "30px", color: "red"&#125;); alert($div1.css('width')); alert($div1.css('color')); // 原生js无法读取行间没有定义的css属性值,但是可以读取行间已定义的css属性值 var oDiv = document.getElementById("box"); alert(oDiv.style.height); // 空 alert(oDiv.style.fontSize); // 20px &#125;); &lt;/script&gt;&lt;/head&gt;&lt;body&gt;&lt;div id="div1"&gt;div元素&lt;/div&gt;&lt;div id="box" style="font-size: 20px;"&gt;div元素&lt;/div&gt;&lt;/body&gt;&lt;/html&gt;特别注意选择器获取的多个元素，获取信息获取的是第一个，比如：$(“div”).css(“width”)，获取的是第一个div的width。123456789101112131415161718192021222324&lt;!DOCTYPE html&gt;&lt;html lang="en"&gt;&lt;head&gt; &lt;meta charset="UTF-8"&gt; &lt;title&gt;Title&lt;/title&gt; &lt;!--不能直接在jquery的引入标签里写js代码，需要另写一个script标签，在这个里面那些js代码--&gt; &lt;script type="text/javascript" src="js/jquery-3.4.1.min.js"&gt;&lt;/script&gt; &lt;script type="text/javascript"&gt; $(function () &#123; var $div = $('div'); alert($div.css("color")); // rgb(0,128,0) =&gt; green &#125;); &lt;/script&gt;&lt;/head&gt;&lt;body&gt;&lt;div style="color: green"&gt;1.div元素&lt;/div&gt;&lt;div style="color: red"&gt;2.div元素&lt;/div&gt;&lt;/body&gt;&lt;/html&gt;10.4.3 操作样式类名1234$("#div1").addClass("divClass2") //为id为div1的对象追加样式divClass2,不是替换成divClass2$("#div1").removeClass("divClass") //移除id为div1的对象的class名为divClass的样式$("#div1").removeClass("divClass divClass2") //移除多个样式$("#div1").toggleClass("anotherClass") //重复切换anotherClass样式，比如点击增加，再点击移除等操作12345678910111213141516171819202122232425262728293031323334353637&lt;!DOCTYPE html&gt;&lt;html lang="en"&gt;&lt;head&gt; &lt;meta charset="UTF-8"&gt; &lt;title&gt;Title&lt;/title&gt; &lt;!--不能直接在jquery的引入标签里写js代码，需要另写一个script标签，在这个里面那些js代码--&gt; &lt;script type="text/javascript" src="js/jquery-3.4.1.min.js"&gt;&lt;/script&gt; &lt;script type="text/javascript"&gt; $(function () &#123; var $div = $('.box'); $div.addClass("big"); $div.removeClass("box"); $div.toggleClass("box"); &#125;); &lt;/script&gt; &lt;style type="text/css"&gt; .box&#123; width: 100px; height: 100px; background-color: gold; &#125; .big&#123; font-size: 30px; &#125; &lt;/style&gt;&lt;/head&gt;&lt;body&gt;&lt;div class="box"&gt;div元素&lt;/div&gt;&lt;/body&gt;&lt;/html&gt;10.5 绑定click事件给元素绑定click事件，可以用如下方法：123456$('#btn1').click(function()&#123; // 内部的this指的是原生对象 // 使用jquery对象用 $(this)&#125;)1234567891011121314151617181920212223242526272829303132333435363738394041424344&lt;!DOCTYPE html&gt;&lt;html lang="en"&gt;&lt;head&gt; &lt;meta charset="UTF-8"&gt; &lt;title&gt;Title&lt;/title&gt; &lt;!--不能直接在jquery的引入标签里写js代码，需要另写一个script标签，在这个里面那些js代码--&gt; &lt;script type="text/javascript" src="js/jquery-3.4.1.min.js"&gt;&lt;/script&gt; &lt;script type="text/javascript"&gt; $(function () &#123; // 绑定click事件 $("#btn").click(function () &#123; // if ($(".box").hasClass("col01")) &#123; // $(".box").removeClass("col01"); // &#125; else &#123; // $(".box").addClass("col01"); // &#125; // 简化写法 $(".box").toggleClass("col01"); &#125;); &#125;); &lt;/script&gt; &lt;style type="text/css"&gt; .box &#123; width: 100px; height: 100px; background-color: gold; &#125; .col01 &#123; background-color: green; &#125; &lt;/style&gt;&lt;/head&gt;&lt;body&gt;&lt;input type="button" name="" value="切换样式" id="btn"&gt;&lt;div class="box"&gt;div元素&lt;/div&gt;&lt;/body&gt;&lt;/html&gt;10.5.1 获取元素的索引值有时候需要获得匹配元素相对于其同胞元素的索引位置，此时可以用index()方法获取12345678910var $li = $('.list li').eq(1);alert($li.index()); // 弹出1......&lt;ul class="list"&gt; &lt;li&gt;1&lt;/li&gt; &lt;li&gt;2&lt;/li&gt; &lt;li&gt;4&lt;/li&gt; &lt;li&gt;5&lt;/li&gt; &lt;li&gt;6&lt;/li&gt;&lt;/ul&gt;课堂练习选项卡1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374&lt;!DOCTYPE html&gt;&lt;html lang="en"&gt;&lt;head&gt; &lt;meta charset="UTF-8"&gt; &lt;title&gt;Title&lt;/title&gt; &lt;!--不能直接在jquery的引入标签里写js代码，需要另写一个script标签，在这个里面那些js代码--&gt; &lt;script type="text/javascript" src="js/jquery-3.4.1.min.js"&gt;&lt;/script&gt; &lt;script type="text/javascript"&gt; $(function () &#123; var $btn = $(".btns input"); var $div = $(".cons div"); $btn.click(function () &#123; // this指的是原生this,它表示当前点击的对象 // 当前点击的按钮加上current样式后，除了当前，其他的按钮去掉current样式 $(this).addClass("current").siblings().removeClass("current"); // alert($(this).index()); // 查看当前点击对象的索引值 // 当前点击的按钮对应索引值的div加上active样式，其他的去掉active样式 $div.eq($(this).index()).addClass("active").siblings().removeClass("active"); &#125;) &#125;); &lt;/script&gt; &lt;style type="text/css"&gt; .btns input &#123; width: 100px; height: 40px; background-color: #dddddd; &#125; .btns .current &#123; background-color: gold; &#125; .cons div &#123; width: 500px; height: 300px; background-color: gold; display: none; text-align: center; line-height: 300px; font-size: 30px; &#125; .cons .active &#123; display: block; &#125; &lt;/style&gt;&lt;/head&gt;&lt;body&gt;&lt;div class="btns"&gt; &lt;input type="button" name="" value="01" class="current"&gt; &lt;input type="button" name="" value="02"&gt; &lt;input type="button" name="" value="03"&gt;&lt;/div&gt;&lt;div class="cons"&gt; &lt;div class="active"&gt;选项卡一的内容&lt;/div&gt; &lt;div&gt;选项卡二的内容&lt;/div&gt; &lt;div&gt;选项卡三的内容&lt;/div&gt;&lt;/div&gt;&lt;/body&gt;&lt;/html&gt;10.6 jquery特殊效果123456789101112131415161718fadeIn() 淡入 $btn.click(function()&#123; $('#div1').fadeIn(1000,'swing',function()&#123; alert('done!'); &#125;); &#125;);fadeOut() 淡出fadeToggle() 切换淡入淡出hide() 隐藏元素show() 显示元素toggle() 切换元素的可见状态slideDown() 向下展开slideUp() 向上卷起slideToggle() 依次展开或卷起某个元素10.7 jquery链式调用jquery对象的方法会在执行完后返回这个jquery对象，所有jquery对象的方法可以连起来写：1234567$('#div1') // id为div1的元素.children('ul') //该元素下面的ul子元素.slideDown('fast') //高度从零变到实际高度来显示ul元素.parent() //跳到ul的父元素，也就是id为div1的元素.siblings() //跳到div1元素平级的所有兄弟元素.children('ul') //这些兄弟元素中的ul子元素.slideUp('fast'); //高度实际高度变换到零来隐藏ul元素课堂练习 - 层级菜单123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139&lt;!DOCTYPE html&gt;&lt;html lang="en"&gt;&lt;head&gt; &lt;meta charset="UTF-8"&gt; &lt;title&gt;Title&lt;/title&gt; &lt;!--不能直接在jquery的引入标签里写js代码，需要另写一个script标签，在这个里面那些js代码--&gt; &lt;script type="text/javascript" src="js/jquery-3.4.1.min.js"&gt;&lt;/script&gt; &lt;script type="text/javascript"&gt; $(function () &#123; $('.level1').click(function () &#123; //当前点击的元素紧挨的同辈元素向下展开，再跳到此元素的父级(li),再跳到此父级的其他的同辈元素(li),选择其他同辈元素(li)的子元素ul，然后将它向上收起。 // 通过stop() 可以修正反复点击导致的持续动画的问题 $(this).next().stop().slideToggle().parent().siblings().children('ul').slideUp(); &#125;) &#125;) &lt;/script&gt; &lt;style type="text/css"&gt; body &#123; font-family: 'Microsoft Yahei'; &#125; body, ul &#123; margin: 0px; padding: 0px; &#125; ul &#123; list-style: none; &#125; .menu &#123; width: 200px; margin: 20px auto 0; &#125; .menu .level1, .menu li ul a &#123; display: block; width: 200px; height: 30px; line-height: 30px; text-decoration: none; background-color: #3366cc; color: #fff; font-size: 16px; text-indent: 10px; &#125; .menu .level1 &#123; border-bottom: 1px solid #afc6f6; &#125; .menu li ul a &#123; font-size: 14px; text-indent: 20px; background-color: #7aa1ef; &#125; .menu li ul li &#123; border-bottom: 1px solid #afc6f6; &#125; .menu li ul &#123; display: none; &#125; .menu li ul.current &#123; display: block; &#125; .menu li ul li a:hover &#123; background-color: #f6b544; &#125; &lt;/style&gt;&lt;/head&gt;&lt;body&gt;&lt;ul class="menu"&gt; &lt;li&gt; &lt;a href="#" class="level1"&gt;水果&lt;/a&gt; &lt;ul class="current"&gt; &lt;li&gt;&lt;a href="#"&gt;苹果&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="#"&gt;梨子&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="#"&gt;葡萄&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="#"&gt;火龙果&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;li&gt; &lt;a href="#" class="level1"&gt;海鲜&lt;/a&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="#"&gt;蛏子&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="#"&gt;扇贝&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="#"&gt;龙虾&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="#"&gt;象拔蚌&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;li&gt; &lt;a href="#" class="level1"&gt;肉类&lt;/a&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="#"&gt;内蒙古羊肉&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="#"&gt;进口牛肉&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="#"&gt;野猪肉&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;li&gt; &lt;a href="#" class="level1"&gt;蔬菜&lt;/a&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="#"&gt;娃娃菜&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="#"&gt;西红柿&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="#"&gt;西芹&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="#"&gt;胡萝卜&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;li&gt; &lt;a href="#" class="level1"&gt;速冻&lt;/a&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="#"&gt;冰淇淋&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="#"&gt;湾仔码头&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="#"&gt;海参&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="#"&gt;牛肉丸&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/li&gt;&lt;/ul&gt;&lt;/body&gt;&lt;/html&gt;10.8 jquery动画通过animate方法可以设置元素某属性值上的动画，可以设置一个或多个属性值，动画执行完成后会执行一个函数。123456$('#div1').animate(&#123; width:300, height:300&#125;,1000,'swing',function()&#123; alert('done!');&#125;);参数可以写成数字表达式：123456$('#div1').animate(&#123; width:'+=100', height:300&#125;,1000,'swing',function()&#123; alert('done!');&#125;);10.9 尺寸相关、滚动事件1、获取和设置元素的尺寸1234width()、height() 获取元素width和height innerWidth()、innerHeight() 包括padding的width和height outerWidth()、outerHeight() 包括padding和border的width和height outerWidth(true)、outerHeight(true) 包括padding和border以及margin的width和height2、获取元素相对页面的绝对位置1offset()3、获取浏览器可视区宽度高度12$(window).width();$(window).height();4、获取页面文档的宽度高度12$(document).width();$(document).height();5、获取页面滚动距离12$(document).scrollTop(); $(document).scrollLeft();6、页面滚动事件123$(window).scroll(function()&#123; ...... &#125;)10.10 jquery属性操作1、html() 取出或设置html内容1234567// 取出html内容var $htm = $('#div1').html();// 设置html内容$('#div1').html('&lt;span&gt;添加文字&lt;/span&gt;');2、prop() 取出或设置某个属性的值1234567// 取出图片的地址var $src = $('#img1').prop('src');// 设置图片的地址和alt属性$('#img1').prop(&#123;src: "test.jpg", alt: "Test Image" &#125;);10.11 jquery循环对jquery选择的对象集合分别进行操作，需要用到jquery循环操作，此时可以用对象上的each方法：1234567891011121314$(function()&#123; $('.list li').each(function(i)&#123; $(this).html(i); &#125;)&#125;)......&lt;ul class="list"&gt; &lt;li&gt;&lt;/li&gt; &lt;li&gt;&lt;/li&gt; &lt;li&gt;&lt;/li&gt; &lt;li&gt;&lt;/li&gt; &lt;li&gt;&lt;/li&gt; &lt;li&gt;&lt;/li&gt;&lt;/ul&gt;10.12 jquery事件10.12.1 事件函数列表：123456789101112blur() 元素失去焦点focus() 元素获得焦点click() 鼠标单击mouseover() 鼠标进入（进入子元素也触发）mouseout() 鼠标离开（离开子元素也触发）mouseenter() 鼠标进入（进入子元素不触发）mouseleave() 鼠标离开（离开子元素不触发）hover() 同时为mouseenter和mouseleave事件指定处理函数ready() DOM加载完成resize() 浏览器窗口的大小发生改变scroll() 滚动条的位置发生变化submit() 用户递交表单10.12.2 绑定事件的其他方式12345$(function()&#123; $('#div1').bind('mouseover click', function(event) &#123; alert($(this).html()); &#125;);&#125;);10.12.3 取消绑定事件123456789$(function()&#123; $('#div1').bind('mouseover click', function(event) &#123; alert($(this).html()); // $(this).unbind(); $(this).unbind('mouseover'); &#125;);&#125;);10.13 事件冒泡10.13.1 什么是事件冒泡在一个对象上触发某类事件（比如单击onclick事件），如果此对象定义了此事件的处理程序，那么此事件就会调用这个处理程序，如果没有定义此事件处理程序或者事件返回true，那么这个事件会向这个对象的父级对象传播，从里到外，直至它被处理（父级对象所有同类事件都将被激活），或者它到达了对象层次的最顶层，即document对象（有些浏览器是window）。10.13.2 事件冒泡的作用事件冒泡允许多个操作被集中处理（把事件处理器添加到一个父级元素上，避免把事件处理器添加到多个子级元素上），它还可以让你在对象层的不同级别捕获事件。10.13.3 阻止事件冒泡事件冒泡机制有时候是不需要的，需要阻止掉，通过 event.stopPropagation() 来阻止123456789101112131415161718192021222324252627$(function()&#123; var $box1 = $('.father'); var $box2 = $('.son'); var $box3 = $('.grandson'); $box1.click(function() &#123; alert('father'); &#125;); $box2.click(function() &#123; alert('son'); &#125;); $box3.click(function(event) &#123; alert('grandson'); event.stopPropagation(); &#125;); $(document).click(function(event) &#123; alert('grandfather'); &#125;);&#125;)......&lt;div class="father"&gt; &lt;div class="son"&gt; &lt;div class="grandson"&gt;&lt;/div&gt; &lt;/div&gt;&lt;/div&gt;10.13.4 阻止默认行为阻止表单提交123$('#form1').submit(function(event)&#123; event.preventDefault();&#125;)10.14.5 合并阻止操作实际开发中，一般把阻止冒泡和阻止默认行为合并起来写，合并写法可以用12345// event.stopPropagation();// event.preventDefault();// 合并写法：return false;10.14 事件委托事件委托就是利用冒泡的原理，把事件加到父级上，通过判断事件来源的子集，执行相应的操作，事件委托首先可以极大减少事件绑定次数，提高性能；其次可以让新加入的子元素也可以拥有相同的操作。10.14.1 一般绑定事件的写法1234567891011121314$(function()&#123; $ali = $('#list li'); $ali.click(function() &#123; $(this).css(&#123;background:'red'&#125;); &#125;);&#125;)...&lt;ul id="list"&gt; &lt;li&gt;1&lt;/li&gt; &lt;li&gt;2&lt;/li&gt; &lt;li&gt;3&lt;/li&gt; &lt;li&gt;4&lt;/li&gt; &lt;li&gt;5&lt;/li&gt;&lt;/ul&gt;10.14.2 事件委托的写法1234567891011121314$(function()&#123; $list = $('#list'); $list.delegate('li', 'click', function() &#123; $(this).css(&#123;background:'red'&#125;); &#125;);&#125;)...&lt;ul id="list"&gt; &lt;li&gt;1&lt;/li&gt; &lt;li&gt;2&lt;/li&gt; &lt;li&gt;3&lt;/li&gt; &lt;li&gt;4&lt;/li&gt; &lt;li&gt;5&lt;/li&gt;&lt;/ul&gt;10.15 jquery元素节点操作10.15.1 创建节点12var $div = $('&lt;div&gt;');var $div2 = $('&lt;div&gt;这是一个div元素&lt;/div&gt;');10.15.2 插入节点1、append()和appendTo()：在现存元素的内部，从后面插入元素1234var $span = $('&lt;span&gt;这是一个span元素&lt;/span&gt;');$('#div1').append($span);......&lt;div id="div1"&gt;&lt;/div&gt;2、prepend()和prependTo()：在现存元素的内部，从前面插入元素3、after()和insertAfter()：在现存元素的外部，从后面插入元素4、before()和insertBefore()：在现存元素的外部，从前面插入元素10.15.3 删除节点1$('#div1').remove();10.16 滚轮事件与函数节流10.16.1 jquery.mousewheel插件使用jquery中没有鼠标滚轮事件，原生js中的鼠标滚轮事件不兼容，可以使用jquery的滚轮事件插件jquery.mousewheel.js。10.16.2 函数节流javascript中有些事件的触发频率非常高，比如onresize事件(jq中是resize)，onmousemove事件(jq中是mousemove)以及上面说的鼠标滚轮事件，在短事件内多处触发执行绑定的函数，可以巧妙地使用定时器来减少触发的次数，实现函数节流。10.17 jsonjson是 JavaScript Object Notation 的首字母缩写，单词的意思是javascript对象表示法，这里说的json指的是类似于javascript对象的一种数据格式，目前这种数据格式比较流行，逐渐替换掉了传统的xml数据格式。javascript自定义对象：1234567var oMan = &#123; name:'tom', age:16, talk:function(s)&#123; alert('我会说'+s); &#125;&#125;json格式的数据：1234&#123; "name":"tom", "age":18&#125;与json对象不同的是，json数据格式的属性名称和字符串值需要用双引号引起来，用单引号或者不用引号会导致读取数据错误。json的另外一个数据格式是数组，和javascript中的数组字面量相同。1["tom",18,"programmer"]10.18 ajax与jsonpajax技术的目的是让javascript发送http请求，与后台通信，获取数据和信息。ajax技术的原理是实例化xmlhttp对象，使用此对象与后台通信。ajax通信的过程不会影响后续javascript的执行，从而实现异步。10.18.1 同步和异步现实生活中，同步指的是同时做几件事情，异步指的是做完一件事后再做另外一件事，程序中的同步和异步是把现实生活中的概念对调，也就是程序中的异步指的是现实生活中的同步，程序中的同步指的是现实生活中的异步。10.18.2 局部刷新和无刷新ajax可以实现局部刷新，也叫做无刷新，无刷新指的是整个页面不刷新，只是局部刷新，ajax可以自己发送http请求，不用通过浏览器的地址栏，所以页面整体不会刷新，ajax获取到后台数据，更新页面显示数据的部分，就做到了页面局部刷新。10.18.3 同源策略ajax请求的页面或资源只能是同一个域下面的资源，不能是其他域的资源，这是在设计ajax时基于安全的考虑。特征报错提示：123XMLHttpRequest cannot load https://www.baidu.com/. No &apos;Access-Control-Allow-Origin&apos; header is present on the requested resource. Origin &apos;null&apos; is therefore not allowed access.10.18.4 $.ajax使用方法常用参数：1、url 请求地址2、type 请求方式，默认是’GET’，常用的还有’POST’3、dataType 设置返回的数据格式，常用的是’json’格式，也可以设置为’html’4、data 设置发送给服务器的数据5、success 设置请求成功后的回调函数6、error 设置请求失败后的回调函数7、async 设置是否异步，默认值是’true’，表示异步以前的写法：123456789101112$.ajax(&#123; url: 'js/data.json', type: 'GET', dataType: 'json', data:&#123;'aa':1&#125; success:function(data)&#123; alert(data.name); &#125;, error:function()&#123; alert('服务器超时，请重试！'); &#125;&#125;);新的写法(推荐)：1234567891011121314$.ajax(&#123; url: &apos;js/data.json&apos;, type: &apos;GET&apos;, dataType: &apos;json&apos;, data:&#123;&apos;aa&apos;:1&#125;&#125;).done(function(data) &#123; alert(data.name);&#125;).fail(function() &#123; alert(&apos;服务器超时，请重试！&apos;);&#125;);// data.json里面的数据： &#123;&quot;name&quot;:&quot;tom&quot;,&quot;age&quot;:18&#125;10.18.5 jsonpajax只能请求同一个域下的数据或资源，有时候需要跨域请求数据，就需要用到jsonp技术，jsonp可以跨域请求数据，它的原理主要是利用了标签可以跨域链接资源的特性。jsonp和ajax原理完全不一样，不过jquery将它们封装成同一个函数。 1234567891011121314$.ajax(&#123; url:'js/data.js', type:'get', dataType:'jsonp', jsonpCallback:'fnBack'&#125;).done(function(data)&#123; alert(data.name);&#125;).fail(function() &#123; alert('服务器超时，请重试！');&#125;);// data.js里面的数据： fnBack(&#123;"name":"tom","age":18&#125;); 12345678910111213141516171819202122232425$(function()&#123; $('#txt01').keyup(function()&#123; var sVal = $(this).val(); $.ajax(&#123; url:'https://sug.so.360.cn/suggest?', type:'get', dataType:'jsonp', data: &#123;word: sVal&#125; &#125;) .done(function(data)&#123; var aData = data.s; $('.list').empty(); for(var i=0;i&lt;aData.length;i++) &#123; var $li = $('&lt;li&gt;'+ aData[i] +'&lt;/li&gt;'); $li.appendTo($('.list')); &#125; &#125;) &#125;)&#125;)//......&lt;input type="text" name="" id="txt01"&gt;&lt;ul class="list"&gt;&lt;/ul&gt; 10.19 本地存储本地存储分为cookie，以及新增的localStorage和sessionStorage 1、cookie 存储在本地，容量最大4k，在同源的http请求时携带传递，损耗带宽，可设置访问路径，只有此路径及此路径的子路径才能访问此cookie，在设置的过期时间之前有效。 1234jquery 设置cookie$.cookie('mycookie','123',&#123;expires:7,path:'/'&#125;);jquery 获取cookie$.cookie('mycookie'); 2、localStorage 存储在本地，容量为5M或者更大，不会在请求时候携带传递，在所有同源窗口中共享，数据一直有效，除非人为删除，可作为长期数据。 12345678910//设置：localStorage.setItem("dat", "456");localStorage.dat = '456';//获取：localStorage.getItem("dat");localStorage.dat//删除localStorage.removeItem("dat"); 3、sessionStorage 存储在本地，容量为5M或者更大，不会在请求时候携带传递，在同源的当前窗口关闭前有效。 localStorage 和 sessionStorage 合称为Web Storage , Web Storage支持事件通知机制，可以将数据更新的通知监听者，Web Storage的api接口使用更方便。 iPhone的无痕浏览不支持Web Storage，只能用cookie。 10.20 jqueryUIjQuery UI是以 jQuery 为基础的代码库。包含底层用户交互、动画、特效和可更换主题的可视控件。我们可以直接用它来构建具有很好交互性的web应用程序。 jqueryUI 网址http://jqueryui.com/ 11.移动端库和框架11.1 移动端js事件移动端的操作方式和PC端是不同的，移动端主要用手指操作，所以有特殊的touch事件，touch事件包括如下几个事件： 1、touchstart: //手指放到屏幕上时触发2、touchmove: //手指在屏幕上滑动式触发3、touchend: //手指离开屏幕时触发4、touchcancel: //系统取消touch事件的时候触发，比较少用 移动端一般有三种操作，点击、滑动、拖动，这三种操作一般是组合使用上面的几个事件来完成的，所有上面的4个事件一般很少单独使用，一般是封装使用来实现这三种操作，可以使用封装成熟的js库。 11.2 zeptojsZepto是一个轻量级的针对现代高级浏览器的JavaScript库， 它与jquery有着类似的api。 如果你会用jquery，那么你也会用zepto。Zepto的一些可选功能是专门针对移动端浏览器的；它的最初目标是在移动端提供一个精简的类似jquery的js库。 zepto官网：http://zeptojs.com/zepto中文api：http://www.css88.com/doc/zeptojs_api/zepto包含很多模块，默认下载版本包含的模块有Core, Ajax, Event, Form, IE模块，如果还需要其他的模块，可以自定义构建。zepto自定义构建地址：http://github.e-sites.nl/zeptobuilder/ 11.3 swiperswiper.js是一款成熟稳定的应用于PC端和移动端的滑动效果插件，一般用来触屏焦点图、触屏整屏滚动等效果。 swiper分为2.x版本和3.x版本，2.x版本支持低版本浏览器(IE7)，3.x放弃支持低版本浏览器，适合应用在移动端。 2.x版本中文网址：http://2.swiper.com.cn/3.x版本中文网地址：http://www.swiper.com.cn/ 11.3.1 swiper使用方法：123456789101112131415161718192021222324252627282930313233343536&lt;script type="text/javascript" src="js/swiper.min.js"&gt;&lt;/script&gt;&lt;!-- 如果页面引用了jquery或者zepto，就引用 swiper.jquery.min.js,它的容量比swiper.min.js &lt;script src="path/to/swiper.jquery.min.js"&gt;&lt;/script&gt;--&gt;......&lt;link rel="stylesheet" type="text/css" href="css/swiper.min.css"&gt;......&lt;div class="swiper-container"&gt; &lt;div class="swiper-wrapper"&gt; &lt;div class="swiper-slide"&gt;slider1&lt;/div&gt; &lt;div class="swiper-slide"&gt;slider2&lt;/div&gt; &lt;div class="swiper-slide"&gt;slider3&lt;/div&gt; &lt;/div&gt; &lt;div class="swiper-pagination"&gt;&lt;/div&gt; &lt;div class="swiper-button-prev"&gt;&lt;/div&gt; &lt;div class="swiper-button-next"&gt;&lt;/div&gt;&lt;/div&gt;&lt;script&gt; var swiper = new Swiper('.swiper-container', &#123; pagination: '.swiper-pagination', prevButton: '.swiper-button-prev', nextButton: '.swiper-button-next', initialSlide :1, paginationClickable: true, loop: true, autoplay:3000, autoplayDisableOnInteraction:false&#125;);&lt;/script&gt; 11.13.2 swiper使用参数：1、initialSlide：初始索引值，从0开始2、direction：滑动方向 horizontal | vertical3、speed：滑动速度，单位ms4、autoplay：设置自动播放及播放时间5、autoplayDisableOnInteraction：用户操作swipe后是否还自动播放，默认是true，不再自动播放6、pagination：分页圆点7、paginationClickable：分页圆点是否点击8、prevButton：上一页箭头9、nextButton：下一页箭头10、loop：是否首尾衔接 11.4 bootstrap简单、直观、强悍的前端开发框架，让web开发更迅速、简单。 来自Twitter，是目前很受欢迎的前端框架之一。 Bootrstrap是基于HTML、CSS、JavaScript的，让书写代码更容易。 移动优先，响应式布局开发。 bootstrap中文网址：http://www.bootcss.com/ 11.4.1 bootstrap 容器 container-fluid 流体 container 1170 970 750 100% 12&lt;div class=&quot;container-fluid&quot;&gt;流体容器&lt;/div&gt;&lt;div class=&quot;container&quot;&gt;响应式固定容器&lt;/div&gt; 11.4.2 bootstrap响应式查询区间1、大于等于7682、大于等于9923、大于等于1200 11.4.3 bootstrap 栅格系统bootstrap将页面横向分为12等分，按照12等分定义了适应不同宽度等分的样式类，这些样式类组成了一套响应式、移动设备优先的流式栅格系统： 1、col-lg- 大于1200排成一行，小于1200分别占一行2、col-md- 大于992排成一行，小于992分别占一行3、col-sm- 大于768排成一行，小于768分别占一行4、col-xs- 始终排列成一行 12345678910111213141516171819202122232425262728293031323334353637383940414243&lt;style type="text/css"&gt; div[class*='col-']&#123; background-color:cyan; border:1px solid #ddd; height:50px; &#125;&lt;/style&gt;......&lt;div class="container"&gt; &lt;div class="row"&gt; &lt;div class="col-lg-3"&gt;col-lg-3&lt;/div&gt; &lt;div class="col-lg-3"&gt;col-lg-3&lt;/div&gt; &lt;div class="col-lg-5"&gt;col-lg-5&lt;/div&gt; &lt;div class="col-lg-1"&gt;col-lg-1&lt;/div&gt; &lt;/div&gt; &lt;br&gt; &lt;br&gt; &lt;div class="row"&gt; &lt;div class="col-md-3"&gt;col-md-3&lt;/div&gt; &lt;div class="col-md-3"&gt;col-md-3&lt;/div&gt; &lt;div class="col-md-3"&gt;col-md-3&lt;/div&gt; &lt;div class="col-md-3"&gt;col-md-3&lt;/div&gt; &lt;/div&gt; &lt;br&gt; &lt;br&gt; &lt;div class="row"&gt; &lt;div class="col-sm-3"&gt;col-sm-3&lt;/div&gt; &lt;div class="col-sm-3"&gt;col-sm-3&lt;/div&gt; &lt;div class="col-sm-3"&gt;col-sm-3&lt;/div&gt; &lt;div class="col-sm-3"&gt;col-sm-3&lt;/div&gt; &lt;/div&gt; &lt;br&gt; &lt;br&gt; &lt;div class="row"&gt; &lt;div class="col-xs-3"&gt;col-xs-3&lt;/div&gt; &lt;div class="col-xs-3"&gt;col-xs-3&lt;/div&gt; &lt;div class="col-xs-3"&gt;col-xs-3&lt;/div&gt; &lt;div class="col-xs-3"&gt;col-xs-3&lt;/div&gt; &lt;/div&gt;&lt;/div&gt; 11.4.4 列偏移1、col-lg-offset-2、col-md-offset-3、col-sm-offset-4、col-xs-offset- 11.4.5 bootstrap 按钮1、btn 声明按钮2、btn-default 默认按钮样式3、btn-primay4、btn-success5、btn-info6、btn-warning7、btn-danger8、btn-link9、btn-lg10、btn-md11、btn-xs12、btn-block 宽度是父级宽100%的按钮13、active14、disabled15、btn-group 定义按钮组 1234567891011121314151617181920212223242526272829&lt;!-- 一般按钮组 --&gt;&lt;div class="btn-group"&gt; &lt;input type="button" name="" value="按钮一" class="btn btn-primary"&gt; &lt;input type="button" name="" value="按钮二" class="btn btn-warning"&gt; &lt;input type="button" name="" value="按钮三" class="btn btn-danger"&gt;&lt;/div&gt;&lt;!-- 通栏按钮组 如果用input标签做按钮，需要将它用 btn-group的容器包起来--&gt;&lt;div class="btn-group btn-group-justified"&gt; &lt;div class="btn-group"&gt; &lt;input type="button" name="" value="按钮一" class="btn btn-primary"&gt; &lt;/div&gt; &lt;div class="btn-group"&gt; &lt;input type="button" name="" value="按钮二" class="btn btn-warning"&gt; &lt;/div&gt; &lt;div class="btn-group"&gt; &lt;input type="button" name="" value="按钮三" class="btn btn-danger"&gt; &lt;/div&gt;&lt;/div&gt;&lt;!-- 通栏按钮组，如果用a标签做按钮，就不用上面的结构，直接写--&gt;&lt;div class="btn-group btn-group-justified"&gt; &lt;a href="#" class="btn btn-primary"&gt;按钮一&lt;/a&gt; &lt;a href="#" class="btn btn-default"&gt;按钮二&lt;/a&gt; &lt;a href="#" class="btn btn-default"&gt;按钮三&lt;/a&gt;&lt;/div&gt; 11.4.6 bootstrap 表单1、form 声明一个表单域2、form-inline 内联表单域3、form-horizontal 水平排列表单域4、form-group 表单组、包括表单文字和表单控件5、form-control 文本输入框、下拉列表控件样式6、checkbox checkbox-inline 多选框样式7、radio radio-inline 单选框样式8、input-group 表单控件组9、input-group-addon 表单控件组物件样式10、input-group-btn 表单控件组物件为按钮的样式11、form-group-lg 大尺寸表单12、form-group-sm 小尺寸表单 123456789101112131415161718192021222324252627282930313233343536&lt;!-- 表单 --&gt;&lt;form role="form"&gt; &lt;div class="form-group form-group-lg"&gt; &lt;label for="exampleInputEmail1"&gt;Email address&lt;/label&gt; &lt;input type="email" class="form-control" id="exampleInputEmail1" placeholder="Enter email"&gt; &lt;/div&gt; &lt;div class="form-group"&gt; &lt;label for="exampleInputPassword1"&gt;Password&lt;/label&gt; &lt;input type="password" class="form-control" id="exampleInputPassword1" placeholder="Password"&gt; &lt;/div&gt; &lt;div class="form-group"&gt; &lt;label for="exampleInputFile"&gt;File input&lt;/label&gt; &lt;input type="file" id="exampleInputFile"&gt; &lt;p class="help-block"&gt;Example block-level help text here.&lt;/p&gt; &lt;/div&gt; &lt;div class="checkbox"&gt; &lt;label&gt; &lt;input type="checkbox"&gt; Check me out &lt;/label&gt; &lt;/div&gt; &lt;button type="submit" class="btn btn-default"&gt;Submit&lt;/button&gt;&lt;/form&gt;&lt;!-- 表单控件组 --&gt;&lt;div class="input-group"&gt; &lt;input type="text" class="form-control"&gt; &lt;span class="input-group-addon"&gt;@&lt;/span&gt;&lt;/div&gt;&lt;!-- 表单控件组 --&gt;&lt;div class="input-group"&gt; &lt;input type="text" class="form-control"&gt; &lt;span class="input-group-btn"&gt; &lt;button class="btn btn-default" type="button"&gt;Go!&lt;/button&gt; &lt;/span&gt;&lt;/div&gt; 11.4.7 bootstrap 图片img-responsive 声明响应式图片 11.4.8 bootstrap 字体图标通过字体代替图标，font文件夹需要和css文件夹在同一目录 11.4.9 bootstrap 导航条1、navbar 声明导航条2、navbar-default 声明默认的导航条样式3、navbar-inverse 声明反白的导航条样式4、navbar-static-top 去掉导航条的圆角5、navbar-fixed-top 固定到顶部的导航条6、navbar-fixed-bottom 固定到底部的导航条7、navbar-header 申明logo的容器8、navbar-brand 针对logo等固定内容的样式11、nav navbar-nav 定义导航条中的菜单12、navbar-form 定义导航条中的表单13、navbar-btn 定义导航条中的按钮14、navbar-text 定义导航条中的文本15、navbar-left 菜单靠左16、navbar-right 菜单靠右 123456789101112131415161718192021222324252627282930&lt;!-- 可伸缩菜单 data-target="#.." 需要加# --&gt;&lt;div class="navbar navbar-inverse navbar-static-top "&gt; &lt;div class="container"&gt; &lt;div class="navbar-header"&gt; &lt;button class="navbar-toggle" data-toggle="collapse" data-target="#mymenu"&gt; &lt;span class="icon-bar"&gt;&lt;/span&gt; &lt;span class="icon-bar"&gt;&lt;/span&gt; &lt;span class="icon-bar"&gt;&lt;/span&gt; &lt;/button&gt; &lt;a href="#" class="navbar-brand"&gt;LOGO&lt;/a&gt; &lt;/div&gt; &lt;div class="collapse navbar-collapse" id="mymenu"&gt; &lt;ul class="nav navbar-nav"&gt; &lt;li class="active"&gt;&lt;a href="#"&gt;首页&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="#"&gt;公司新闻&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="#"&gt;行业动态&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;form class="navbar-form navbar-right"&gt; &lt;div class="form-group"&gt; &lt;div class="input-group"&gt; &lt;input type="text" class="form-control"&gt; &lt;span class="input-group-btn"&gt; &lt;button class="btn btn-default" type="button"&gt;Go!&lt;/button&gt; &lt;/span&gt; &lt;/div&gt; &lt;/div&gt; &lt;/form&gt; &lt;/div&gt; &lt;/div&gt;&lt;/div&gt; 11.4.10 路径导航12345&lt;ol class="breadcrumb"&gt; &lt;li&gt;&lt;a href="#"&gt;Home&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="#"&gt;Library&lt;/a&gt;&lt;/li&gt; &lt;li class="active"&gt;Data&lt;/li&gt;&lt;/ol&gt; 11.4.11 巨幕12345&lt;div class="jumbotron"&gt; &lt;div class="container"&gt; ... &lt;/div&gt;&lt;/div&gt; 11.4.12 bootstrap 模态框1、modal 声明一个模态框2、modal-dialog 定义模态框尺寸3、modal-lg 定义大尺寸模态框4、modal-sm 定义小尺寸模态框5、modal-header6、modal-body7、modal-footer 123456789101112131415161718&lt;button class="btn btn-primary" data-toggle="modal" data-target="#mymodal"&gt;大弹出框按钮&lt;/button&gt;&lt;div class="modal fade" id="mymodal"&gt; &lt;div class="modal-dialog modal-lg"&gt; &lt;div class="modal-content"&gt; &lt;div class="modal-header"&gt; 大尺寸弹出框 &lt;/div&gt; &lt;div class="modal-body"&gt; 模态框主体 &lt;/div&gt; &lt;div class="modal-footer"&gt; &lt;button type="button" class="btn btn-default" data-dismiss="modal"&gt;Close&lt;/button&gt; &lt;button type="button" class="btn btn-primary"&gt;Save changes&lt;/button&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt;&lt;/div&gt; 11.4.13 bootstrap 下拉菜单1、dropdown-toggle2、dropdown-menu 12345678910111213&lt;div class="row"&gt; &lt;div class="dropdown"&gt; &lt;div class="btn btn-primary dropdown-toggle" data-toggle="dropdown"&gt; 下拉菜单 &lt;span class="caret"&gt;&lt;/span&gt; &lt;/div&gt; &lt;ul class="dropdown-menu"&gt; &lt;li&gt;&lt;a href="#"&gt;菜单一&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="#"&gt;菜单二&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="#"&gt;菜单三&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;/div&gt; 11.4.14 bootstrap 隐藏类1、hidden-xs2、hidden-sm3、hidden-md4、hidden-lg 12. 正则表达式12.1 什么是正则表达式能让计算机读懂的字符串匹配规则。 12.2 正则表达式的写法var re=new RegExp(‘规则’, ‘可选参数’);var re=/规则/参数; 12.3 规则中的字符1）普通字符匹配：如：/a/ 匹配字符 ‘a’，/a,b/ 匹配字符 ‘a,b’ 2）转义字符匹配：\d 匹配一个数字，即0-9\D 匹配一个非数字，即除了0-9\w 匹配一个单词字符（字母、数字、下划线）\W 匹配任何非单词字符。等价于[^A-Za-z0-9_]\s 匹配一个空白符\S 匹配一个非空白符\b 匹配单词边界\B 匹配非单词边界. 匹配一个任意字符 123456var sTr01 = '123456asdf';var re01 = /\d+/;//匹配纯数字字符串var re02 = /^\d+$/;alert(re01.test(sTr01)); //弹出truealert(re02.test(sTr01)); //弹出false 12.4 量词：对左边的匹配字符定义个数? 出现零次或一次（最多出现一次）+ 出现一次或多次（至少出现一次）* 出现零次或多次（任意次）{n} 出现n次{n,m} 出现n到m次{n,} 至少出现n次 12.5 任意一个或者范围[abc123] : 匹配‘abc123’中的任意一个字符[a-z0-9] : 匹配a到z或者0到9中的任意一个字符 12.6 限制开头结尾^ 以紧挨的元素开头$ 以紧挨的元素结尾 12.7 修饰参数g： global，全文搜索，默认搜索到第一个结果接停止i： ingore case，忽略大小写，默认大小写敏感 12.8 常用函数1、test用法：正则.test(字符串) 匹配成功，就返回真，否则就返回假 2、replace用法：字符串.replace(正则，新的字符串) 匹配成功的字符去替换新的字符 12.9 正则默认规则匹配成功就结束，不会继续匹配，区分大小写 1234567891011var sTr01 = 'abcdefedcbaCef';var re01 = /c/;var re02 = /c/g;var re03 = /c/gi;var sTr02 = sTr01.replace(re01,'*');var sTr03 = sTr01.replace(re02,'*');var sTr04 = sTr01.replace(re03,'*');alert(sTr02); // 弹出 ab*defedcbaCefalert(sTr03); // 弹出 ab*defed*baCefalert(sTr04); // 弹出 ab*defed*ba*ef 12.10 常用正则规则1234567891011//用户名验证：(数字字母或下划线6到20位)var reUser = /^\w&#123;6,20&#125;$/;//邮箱验证： var reMail = /^[a-z0-9][\w\.\-]*@[a-z0-9\-]+(\.[a-z]&#123;2,5&#125;)&#123;1,2&#125;$/i;//密码验证：var rePass = /^[\w!@#$%^&amp;*]&#123;6,20&#125;$/;//手机号码验证：var rePhone = /^1[3458]\d&#123;9&#125;$/; 13. 前端性能优化从用户访问资源到资源完整的展现在用户面前的过程中，通过技术手段和优化策略，缩短每个步骤的处理时间从而提升整个资源的访问和呈现速度。网站的性能直接会影响到用户的数量，所有前端性能优化很重要。 前端性能优化分为如下几个方面： 13.1代码部署1、代码的压缩与合并2、图片、js、css等静态资源使用和主站不同域名地址存储，从而使得在传输资源时不会带上不必要的cookie信息。3、使用内容分发网络 CDN4、为文件设置Last-Modified、Expires和Etag5、使用GZIP压缩传送6、权衡DNS查找次数(使用不同域名会增加DNS查找)7、避免不必要的重定向(加”/“) 13.2 编码13.2.1 html1、使用结构清晰，简单，语义化标签2、避免空的src和href3、不要在HTML中缩放图片 13.2.2 css1、精简css选择器2、把CSS放到顶部3、避免@import方式引入样式4、css中使用base64图片数据取代图片文件，减少请求数，在线转base64网站：http://tool.css-js.com/base64.html5、使用css动画来取代javascript动画6、使用字体图标7、使用css sprite(雪碧图)8、使用svg图形9、避免使用CSS表达式 123body&#123; background-color: expression( (new Date()).getSeconds()%2 ? "#B8D4FF" : "#F08A00" ); &#125; 10、避免使用css滤镜 13.2.3 javascript1、减少引用库的个数2、使用requirejs或seajs异步加载js3、JS放到页面底部引入4、避免全局查找5、使用原生方法6、用switch语句代替复杂的if else语句7、减少语句数，比如说多个变量声明可以写成一句8、使用字面量表达式来初始化数组或者对象9、使用innerHTML取代复杂的元素注入10、使用事件代理(事件委托)11、避免多次访问dom选择集12、高频触发事件设置使用函数节流13、使用Web Storage缓存数据]]></content>
      <categories>
        <category>HTML&amp;CSS&amp;JavaScript|JQuery</category>
      </categories>
      <tags>
        <tag>JavaScript</tag>
        <tag>HTML</tag>
        <tag>CSS</tag>
        <tag>JQuery</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用nc传输超大文件]]></title>
    <url>%2F2019%2F12%2F31%2F%E4%BD%BF%E7%94%A8nc%E4%BC%A0%E8%BE%93%E8%B6%85%E5%A4%A7%E6%96%87%E4%BB%B6%2F</url>
    <content type="text"><![CDATA[说明linux下的文件传输，大家首先会想到rsync、scp之类的工具，但这类工具有一个特点——慢， 因为这类工具都是加密传输，发送端加密，接收端解密，当我们传输一些非敏感文件的时候，完全可以不加密，直接在网络上传输。 安装1. linux系统安装 # yum install -y nc | nmap-ncat ps.ubuntu自带的nc是netcat-openbsd版,不带-c/-e参数。 2. windows系统安装 (1)下载 下载netcat。下载地址：https://eternallybored.org/misc/netcat/, (2)解压文件夹 (3)将文件夹所在路径添加到用户环境变量里 (4)打开命令界面：Windows+R cmd。输入nc 命令即可 参数想要连接到某处: nc [-options] hostname port[s] [ports] … 绑定端口等待连接: nc -l port [-options] [hostname] [port] -g&lt;网关&gt;：设置路由器跃程通信网关，最多设置8个; -G&lt;指向器数目&gt;：设置来源路由指向器，其数值为4的倍数; -h：在线帮助; -i&lt;延迟秒数&gt;：设置时间间隔，以便传送信息及扫描通信端口; -l：使用监听模式，监控传入的资料; -n：直接使用ip地址，而不通过域名服务器; -o&lt;输出文件&gt;：指定文件名称，把往来传输的数据以16进制字码倾倒成该文件保存; -p&lt;通信端口&gt;：设置本地主机使用的通信端口; -r：指定源端口和目的端口都进行随机的选择; -s&lt;来源位址&gt;：设置本地主机送出数据包的IP地址; -u：使用UDP传输协议; -v：显示指令执行过程; -w&lt;超时秒数&gt;：设置等待连线的时间; -z：使用0输入/输出模式，只在扫描通信端口时使用。 用法连接远程主机Client连接到Server的TCP 80端口: $nc -nvv 192.168.x.x 8000 Server监听本机的TCP8000端口: $nc -l 8000 超时控制: 多数情况我们不希望连接一直保持，那么我们可以使用 -w 参数来指定连接的空闲超时时间，该参数紧接一个数值，代表秒数，如果连接超过指定时间则连接会被终止。 Server: $nc -l 2389 Client: $nc -w 10 localhost 2389 该连接将在 10 秒后中断。注意: 不要在服务器端同时使用 -w 和 -l 参数，因为 -w 参数将在服务器端无效果。 端口扫描端口扫描经常被系统管理员和黑客用来发现在一些机器上开放的端口，帮助他们识别系统中的漏洞。 $nc -z -v -n 192.168.1.1 21-25 可以运行在TCP或者UDP模式，默认是TCP，-u参数调整为udp. z 参数告诉netcat使用0 IO,连接成功后立即关闭连接， 不进行数据交换. v 参数指详细输出. n 参数告诉netcat 不要使用DNS反向查询IP地址的域名. 以上命令会打印21到25 所有开放的端口。 $nc -v 127.0.0.1 22 localhost [127.0.0.1] 22 (ssh) open SSH-2.0-OpenSSH_5.9p1 Debian-5ubuntu1.4 &quot;SSH-2.0-OpenSSH_5.9p1 Debian-5ubuntu1.4&quot;为Banner信息。 Banner是一个文本，Banner是一个你连接的服务发送给你的文本信息。当你试图鉴别漏洞或者服务的类型和版本的时候，Banner信息是非常有用的。 但是，并不是所有的服务都会发送banner.一旦你发现开放的端口，你可以容易的使用netcat 连接服务抓取他们的banner。 Chat Server内网聊天,netcat提供了这样一种方法，只需要创建一个Chat服务器，一个预先确定好的端口，这样子就可以在内网聊天沟通了. Server: $nc -l 20000 netcat 命令在20000端口启动了一个tcp 服务器，所有的标准输出和输入会输出到该端口。输出和输入都在此shell中展示。 Client:$nc 192.168.1.1 20000 不管你在机器Client上键入什么都会出现在机器Server上。 文件传输linux下的文件传输，大家首先会想到rsync、scp之类的工具，但这类工具有一个特点——慢，因为这类工具都是加密传输，发送端加密，接收端解密，当我们传输一些非敏感文件的时候，完全可以不加密，直接在网络上传输。 Server: $ time nc -l 20000 &lt; file.txt 命令最前面的time是用来检测该命令运行耗时的。 Client: $nc -n 192.168.1.1 20000 &gt; file.txt 我们创建了一个服务器在A上并且重定向netcat的输入为文件file.txt，那么当任何成功连接到该端口，netcat会发送file的文件内容。 在客户端我们重定向输出到file.txt，当B连接到A，A发送文件内容，B保存文件内容到file.txt.没有必要创建文件源作为Server，我们也可以相反的方法使用。 像下面的我们发送文件从B到A，但是服务器创建在A上，这次我们仅需要重定向netcat的输出并且重定向B的输入文件。 B作为Server Server: $nc -l 20000 &gt; file.txt Client: $nc 192.168.1.2 20000 &lt; file.txt 用nc传输有两个特点： ➤速度快 ➤传输简单，不需要登录对方服务器，不需要验证信息。 nc进度显示 若你文件实在太大，想看到传输进度，用PV yum install epel-release -y yum install pv -y cat file.txt |pv -b | nc 192.168.1.1 20000 中转文件A、B、C三台主机，A美国，C日本，C只能访问到B，不能直接访问A，B和AC互通。C要怎么才能拿到A上的文件呢？ C上执行：nc -l 9999 &gt; google_file.txt B上执行：nc -l 9999 | nc (C的外网IP) 9999 A上执行：nc (B的外网IP) 9999 &lt; google_file.txt 目录传输想要发送多个文件，或者整个目录，一样很简单，只需要使用压缩工具tar，压缩后发送压缩包。 如果你想要通过网络传输一个目录从A到B。 Server: $tar -cvf – dir_name | nc -l 20000 Client: $nc -n 192.168.1.1 20000 | tar -xvf - 在A服务器上，我们创建一个tar归档包并且通过-在控制台重定向它，然后使用管道，重定向给netcat，netcat可以通过网络发送它。 在客户端我们下载该压缩包通过netcat 管道然后打开文件。 如果想要节省带宽传输压缩包，我们可以使用bzip2或者其他工具压缩。 Server: $tar -cvf – dir_name| bzip2 -z | nc -l 20000 通过bzip2压缩 Client: $nc -n 192.168.1.1 20000 | bzip2 -d |tar -xvf - 还可以把目录制作成iso文件进行传输 $ yum install mkisofs mkisofs -r -o 路径/ISO 文件名 目录文件路径 例子：mkisofs -r -o /opt/mycd.iso /home 加密通过网络发送的数据如果担心你在网络上发送数据的安全，可以在发送你的数据之前用如mcrypt的工具加密。 使用mcrypt工具加密数据。 Server: $nc localhost 20000 | mcrypt –flush –bare -F -q -d -m ecb &gt; file.txt 使用mcrypt工具解密数据。 Client: $mcrypt –flush –bare -F -q -m ecb &lt; file.txt | nc -l 20000 以上两个命令会提示需要密码，确保两端使用相同的密码。 这里我们是使用mcrypt用来加密，使用其它任意加密工具都可以。 流视频虽然不是生成流视频的最好方法，但如果服务器上没有特定的工具，使用netcat，我们仍然有希望做成这件事。 这里我们只是从一个视频文件中读入并重定向输出到netcat客户端 Server: $cat video.avi | nc -l 20000 这里我们从socket中读入数据并重定向到mplayer。 Client: $nc 192.168.1.1 20000 | mplayer -vo x11 -cache 3000 - 克隆一个设备如果你已经安装配置一台Linux机器并且需要重复同样的操作对其他的机器，而你不想在重复配置一遍。 不在需要重复配置安装的过程，只启动另一台机器的一些引导可以随身碟和克隆你的机器。 克隆Linux PC很简单，假如你的系统在磁盘/dev/sda上 Server: $dd if=/dev/sda | nc -l 20000 Client: $nc -n 192.168.1.1 20000 | dd of=/dev/sda dd是一个从磁盘读取原始数据的工具，我通过netcat服务器重定向它的输出流到其他机器并且写入到磁盘中，它会随着分区表拷贝所有的信息。 但是如果我们已经做过分区并且只需要克隆root分区，我们可以根据我们系统root分区的位置，更改sda 为sda1，sda2.等等。 打开一个shell假设你的netcat支持 -c -e 参数(原生 netcat) Server: $nc -l 20000 -e /bin/bash -i Client: $nc 192.168.1.1 20000 这里我们已经创建了一个netcat服务器并且表示当它连接成功时执行/bin/bash 假如netcat 不支持-c 或者 -e 参数（openbsd netcat）,我们仍然能够创建远程shell Server: $mkfifo /tmp/tmp_fifo $cat /tmp/tmp_fifo | /bin/sh -i 2&gt;&amp;1 | nc -l 20000 &gt; /tmp/tmp_fifo 这里我们创建了一个fifo文件，然后使用管道命令把这个fifo文件内容定向到shell 2&gt;&amp;1中。 2&gt;&amp;1是用来重定向标准错误输出和标准输出，然后管道到netcat 运行的端口20000上。至此，我们已经把netcat的输出重定向到fifo文件中。 说明： 从网络收到的输入写到fifo文件中 cat 命令读取fifo文件并且其内容发送给sh命令 sh命令进程受到输入并把它写回到netcat。 netcat 通过网络发送输出到client 至于为什么会成功是因为管道使命令平行执行，fifo文件用来替代正常文件，因为fifo使读取等待而如果是一个普通文件，cat命令会尽快结束并开始读取空文件。 在客户端仅仅简单连接到服务器 Client: $nc -n 192.168.1.1 20000 你会得到一个shell提示符在客户端 反向shell反向shell是指在客户端打开的shell。反向shell这样命名是因为不同于其他配置，这里服务器使用的是由客户提供的服务。 Server: $nc -l 20000 在客户端，简单地告诉netcat在连接完成后，执行shell。 Client: $nc 192.168.1.1 20000 -e /bin/bash 现在，什么是反向shell的特别之处呢 反向shell经常被用来绕过防火墙的限制，如阻止入站连接。 例如，我有一个专用IP地址为192.168.1.1，我使用代理服务器连接到外部网络。如果我想从网络外部访问 这台机器如1.2.3.4的shell，那么我会用反向外壳用于这一目的。 #### 指定源端口假设你的防火墙过滤除25端口外其它所有端口，你需要使用-p选项指定源端口。Server：$nc -l 20000Client：$nc 192.168.1.1 20000 25使用1024以内的端口需要root权限。 该命令将在客户端开启25端口用于通讯，否则将使用随机端口。 指定源地址假设你的机器有多个地址，希望明确指定使用哪个地址用于外部数据通讯。我们可以在netcat中使用-s选项指定ip地址。 Server: $nc -u -l 20000 &lt; file.txt Client: $nc -u 192.168.1.1 20000 -s 172.31.100.5 &gt; file.txt 该命令将绑定地址172.31.100.5。 静态web页面服务器新建一个网页,命名为somepage.html; 新建一个shell script: while true; do nc -l 80 -q 1 &lt; somepage.html; done 用root权限执行，然后在浏览器中输入127.0.0.1打开看看是否正确运行。 nc 指令通常都是给管理者进行除错或测试等作用的，所以如果只是单纯需要临时的网页服务器，使用 Python 的 SimpleHTTPServer 组会比较方便。 模拟HTTP Headers$nc www.huanxiangwu.com 80 GET / HTTP/1.1 Host: ispconfig.org Referrer: mypage.com User-Agent: my-browser HTTP/1.1 200 OK Date: Tue, 16 Dec 2008 07:23:24 GMT Server: Apache/2.2.6 (Unix) DAV/2 mod_mono/1.2.1 mod_python/3.2.8 Python/2.4.3 mod_perl/2.0.2 Perl/v5.8.8 Set-Cookie: PHPSESSID=bbadorbvie1gn037iih6lrdg50; path=/ Expires: 0 Cache-Control: no-store, no-cache, must-revalidate, post-check=0, pre-check=0 Pragma: no-cache Cache-Control: private, post-check=0, pre-check=0, max-age=0 Set-Cookie: oWn_sid=xRutAY; expires=Tue, 23-Dec-2008 07:23:24 GMT; path=/ Vary: Accept-Encoding Transfer-Encoding: chunked Content-Type: text/html [......] 在nc命令后，输入红色部分的内容(命令下方的4行内容)，然后按两次回车，即可从对方获得HTTP Headers内容。 Netcat支持IPv6netcat 的 -4 和 -6 参数用来指定 IP 地址类型，分别是 IPv4 和 IPv6： Server: $ nc -4 -l 2389 Client: $ nc -4 localhost 2389 然后我们可以使用 netstat 命令来查看网络的情况： $ netstat | grep 2389 tcp 0 0 localhost:2389 localhost:50851 ESTABLISHED tcp 0 0 localhost:50851 localhost:2389 ESTABLISHED 接下来我们看看IPv6 的情况： Server: $ nc -6 -l 2389 Client: $ nc -6 localhost 2389 再次运行 netstat 命令： $ netstat | grep 2389 tcp6 0 0 localhost:2389 localhost:33234 ESTABLISHED tcp6 0 0 localhost:33234 localhost:2389 ESTABLISHED 前缀是 tcp6 表示使用的是 IPv6 的地址。 在 Netcat 中禁止从标准输入中读取数据该功能使用 -d 参数，请看下面例子： Server: $ nc -l 2389 Client: $ nc -d localhost 2389 Hi 你输入的 Hi 文本并不会送到服务器端 强制 Netcat 服务器端保持启动状态如果连接到服务器的客户端断开连接，那么服务器端也会跟着退出。 Server: $ nc -l 2389 Client: $ nc localhost 2389 ^C Server: $ nc -l 2389 上述例子中，但客户端断开时服务器端也立即退出。 我们可以通过 -k 参数来控制让服务器不会因为客户端的断开连接而退出。 Server: $ nc -k -l 2389 Client: $ nc localhost 2389 ^C Server: $ nc -k -l 2389 配置 Netcat 客户端不会因为 EOF 而退出Netcat 客户端可以通过 -q 参数来控制接收到 EOF 后隔多长时间才退出，该参数的单位是秒： Client: $nc -q 5 localhost 2389 现在如果客户端接收到 EOF ，它将等待 5 秒后退出。 手动使用 SMTP 协议寄信在测试邮件服务器是否正常时，可以使用这样的方式手动发送 Email： $nc localhost 25 &lt;&lt; EOF HELO host.example.com MAIL FROM: &lt;user@host.example.com&gt; RCPT TO: &lt;user2@host.example.com&gt; DATA Body of email. . QUIT EOF 透过代理服务器（Proxy）连线这指令会使用 10.2.3.4:8080 这个代理服务器，连线至 host.example.com 的42端口。 $nc -x10.2.3.4:8080 -Xconnect host.example.com 42 使用 Unix Domain Socket这行指令会建立一个 Unix Domain Socket，并接收资料： $nc -lU /var/tmp/dsocket]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>nc</tag>
        <tag>Windows</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[与房屋中介公司要押金的斗智斗勇]]></title>
    <url>%2F2019%2F12%2F31%2F%E4%B8%8E%E6%88%BF%E5%B1%8B%E4%B8%AD%E4%BB%8B%E5%85%AC%E5%8F%B8%E8%A6%81%E6%8A%BC%E9%87%91%E7%9A%84%E6%96%97%E6%99%BA%E6%96%97%E5%8B%87%2F</url>
    <content type="text"><![CDATA[我是在2017年3月15号开始入住到福元路玉凤路战友苑小区南区的，签了一年的合同，当时还是纸质版的合同，这一年结交了几位朋友，其余没啥说的，直接略过。进入到2018年，租房快到期了，我又续费一年，这一年就没有纸质合同了，而是电子合同，支付宝蘑菇租房的那种，在线查看合同租约，在线缴费等。合同上写明的有每次缴费的日期和金额。费用呢，房租还是跟上一年的一样，押一付三，物业费一次性交一年，还有水电燃气押金200元。教训第一条：物业费：不同的小区物业费不一样，我这个租的物业费TTMD的贵了，一年730元。后来我换地方租，也就是我现在租的这个，一年的物业费合计396元，相差334元。租房子的时候物业费这个一定要货比三家。因2017年的租户租期到了陆续都搬走了，新搬进来其他租户，我就自发的让新租户加入微信群，快到缴费的时候就组织大家合计缴费，但是事情发生了，部分租户对费用分摊有很大抵触，各种刁难，有时联系不上，有时联系上了故意拖延转账，耽误了缴费的时间，导致被强行断电，我租住了第一次缴费，实在厌恶这样的，索性懒得管，爱咋地咋地，以后就不管缴费这事儿了。后来由中介那边助理负责这件事儿，每次到缴费的时候就在群里大声疾呼，但是还是会出现强行断电的情况。关于停电，有以下两种情况：因欠物业费而停电租户把物业费交给中介，中介再把物业费交给小区的物业管理中心，当屋内房间没有租完时，中介有时会不交物业费给物业管理中心，等有人租房子交了物业费中介公司再把物业费给物业管理中心，这中间的时间间隔一长，就会发生物业管理中心因中介未交物业费而强行给房间断电的情况。因欠电费而停电2017年的缴费是租户在微信群里自发组织在支付宝里缴费的，大家相处的也很融洽，临走时还海底捞聚餐来着。从未发生过因欠电费、水费或燃气费而被物业管理中心强行停电的情况，2018年则恰恰相反。教训第二条：水电和燃气费：现在这些费用都可以通过户号在支付宝里缴纳，水电和燃气的缴纳户号都不一样，具体可以问物业管理中心。这个缴费一定要跟其他租户提前商量好由租户负责，天知道中介助理哪天脑袋抽筋忘了这茬事儿，最主要的是助理摆不平那些刁难的租户。我现在租的这个房子，水电和燃气各每天一块钱，合计三者每月90元，然后就随便用。注意这个电是指公用的用电，比如走廊、厨房，卫生间等用电。每个房间内有单独的一个电表，还有一个总电表。这样一来，费用这个就很好计算，也不会出现费用分摊不合理的情况。延伸一下：之前租的房间内有一个老旧的中央空调，到夏天的时候，关于是否用这个空调的问题也产生过激烈的争斗。这空调不仅制冷效果不好，还很费电。因部分租户上夜班，这就使空调的使用颇费周折。结果导致空调白天黑夜连轴转，因制冷效果不好，温度还调的很低，导致用电量大大大大增。然后就因电费分摊这个互相扯皮。教训第三条：关于房间内的公用设施，比如空凋，洗衣机等，尤其是空调这个电老虎，或者不用，或者不租这类的房子。我现在租的这个，每个房间有一个电表，还有一个总电表，这样一来就把公用电和每个房间的用电给分隔开了。谁想用空调谁自己安装，电费自己缴纳，跟其他费用都不掺和。说的有点多了，话题扯的有点远，接下来继续说正事儿。房租不是快到期了么，就以上情况，都不想再续租了，提前好几天就跟中介公司联系，让他们过来验收一下房屋，给我尽快办理退房手续。连续三天放我鸽子，明明白天说好了，说是晚上几点几点过来验收办理退房手续，可到点了人没来，也没给我打电话，我直接打电话过去，对方不是说忙，就是说在开会，我当时就有点生气，来不了怎么不打电话或发微信说一声。经历过这件事儿，彻底对这个中介公司很失望，当时心里想着赶紧办完这个事儿，不想跟这样的公司员工打交道。终于到房租到期的那一天他们来了，验收一下房屋情况，写了一个费用清单，写了三项，房间卫生，厨房卫生和床板断裂。然后又让我直接在下面写上我的银行账户，签上字。我问他们，说是房间卫生要扣多少，厨房卫生要扣多少，他们不说，只说放心吧，这费用扣的很少。我又问啥时候能把押金给退回来，他们说十五天之内。我把房间钥匙交给他们就走了。心想总算交接完了，接下来等着对方退还押金就是了。教训第四条：关于退房验收这个，凡是需要扣款的地方一定要详细的问清楚，比如房屋打扫费用要扣多少，断裂一个床板要扣多少，这个他们肯定知道，不给你的话这里面肯定有他们的阴谋。接下来的事情坏就坏在自己想的太想当然了。十五天的截止日期快到了，我跟中介助理联系，说是快到期了，押金啥时候退还，他说还有个水费没有结算，让再等几天。没办法，钱在别人手里，别人就是大爷，我只能耐心等了。每天催促他们去抄水表，每次我问都有不同的回复，有一次是有个车压着水表井盖了，还得等等。有一次是抄错水表了，还得再去一趟。看样子目前就卡在水费这一块儿了。令我很不解的是都是我主动联系他们那边，问啥情况他们才说，有啥新情况从不主动跟我打电话说。公司就恁穷么，连个电话都舍不得打。。。这个难产的水费终于计算出来了，问中介助理啥时候退押金，他说卡在他们总监那边了，他这边处理不了，得，我直接找他总监联系。此时已经进入4月份了，我是3月15号退的房，说好的15天之内退押金这都已经超好几天了。跟他们总监联系的时候我当时正在南阳出差，让他把费用清单发过来，按他这清单的费用情况，我那押金不仅一分都要不过来，相反还要倒贴他们部分费用。是可忍孰不可忍，立马就开始跟他怼起来了。房间打扫费用扣50，我认了；厨房打扫费用扣50，我也认了；马桶维修费是啥，当时验收的时候明明没这个的，他们竟私自加上去也不跟我说一声，当时就表明这个费用我不认；断裂了一块床板扣200，我一问才知道，他们说断裂一块床板直接给新换一个床，TMD，还得这样处理的，床板我自己都能换，用得着换新的么；重头戏还在后面，还要扣我两天的滞纳金，说是我有一次晚交几天房租，我当时就很纳闷，我这么一个诚实守信的人，做事情都是有时间计划的，绝不可能出现晚交房租的情况，他还理直气壮的说看合同，看合同，按合同办事。。。出现这种情况看来跟我当时退房时的那种想当然的结果有很大差距我当时在南阳出差，不能因为这个分心，当时就决定这件事儿先放一放，等我出差的事儿办好回郑州之后再慢慢梳理。回郑州后我仔细查看合同，终于发现这其中的关窍所在了。对方想的是：我是15号租的房子，按理说应该是到第三个月的15号交下三个月的房租，我有一次是在17号交的房租，其余三次都是在15号交的房租。晚交两天理应要扣滞纳金。可合同很打脸，合同中明明写的有我每次需要交房租的具体时间，都是17号，从来没有15号这一说，我那三次15号交的还是提前两天交的呢。撇开这个不说，假如就算我应该在15号交房租，可是违约责任中写的也很清楚，若超过交租日期，贰日内仍不缴纳的，才算我违约。TDYD，给我口口声声说看合同，按合同办事儿，你都没看合同还好意思这样说。等我把合同截图给他发过去，尤其是给他注明交租日期和违约责任条款时，就不信你不认。终于这个滞纳金不扣了。不过查看合同我才发现，合同中有一个不足之处，违约责任中只说明对租户违约责任的处罚，对他们违约责任的处罚则没有。这一点尤其要注意，就算把他们告上法庭，合同中并没有对他们的违约处罚，究竟要如何处罚他们估计仍免不了一份唇枪舌剑。这个考虑也是我后来妥协的原因，因为这件事儿我耽搁的时间太多了，他们的时间不珍惜，我得珍惜我的时间才行。教训第五条：租房子之前已经要看请合同内容，看仔细合同内容，看明白合同内容。就算后期维权也是以这个合同为准，当时的口头协议就怕到时候他们翻脸不认。维修马桶费用不扣了，滞纳金费用不扣了，还剩一个断裂的床板儿，床板断裂一块儿，我是应该赔偿的，可是用得着直接赔一个新床么，就算要赔一个新床，最起码也得跟我打电话说一声吧，可是没给我打电话。然后问他新床是谁换的，有收据没，对方也不提供，只说你损坏了你就得赔偿，后面的用不着你管，你只管赔偿就行。我反问拖欠我恁长时间押金不退换你们咋赔偿，哑巴了吧。。懒得再跟他们扯皮了，我浪费不起这就时间， 这个费用我认了，也认栽了。商量好应该退还的剩余费用后，说好是两天之内退还到我账户，那我就没啥可说的了，等待到账就行了。可是我想的还是太单纯了，两天之内的第二天我快下班的时候还没到账，再次一个电话打过去，对方说这几天财务休息，没法转账。没发转账你倒是给我打电话说呀，我不主动打电话问就啥都不说。正好那一天的明天我休息，正好到他们公司去要账去。接待我的是另一个，问了情况之后说是他今天下午6点之前一定转账，若不转账明天再过来直接给你现金。我给他说要留一下他的手机号，他还让我联系之前那一个人，呵呵。。。我还特意上网搜索了一下这个中介公司，一查不得了，2019年还没过几个月呢，都已经被告上多次了。百度贴吧郑州吧截图教训第六条：当自己的合法权益收到侵害时，不要退缩，要勇于站出来跟他们抗争，据礼以搏，就算结果不怎么如意，最起码心里出了这口气，也不能让他们过的太安生。。。]]></content>
      <categories>
        <category>生活</category>
      </categories>
      <tags>
        <tag>生活</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[scrapy学习]]></title>
    <url>%2F2019%2F12%2F31%2Fscrapy%E5%AD%A6%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[scrapy是什么Scrapy 是用 Python 实现的一个为了爬取网站数据、提取结构性数据而编写的应用框架。常应用在包括数据挖掘，信息处理或存储历史数据等一系列的程序中。通常我们可以很简单的通过 Scrapy 框架实现一个爬虫，抓取指定网站的内容或图片。 scrapy架构图绿线是数据流向 Scrapy Engine(引擎): 负责Spider、ItemPipeline、Downloader、Scheduler中间的通讯，信号、数据传递等。 Scheduler(调度器): 它负责接受引擎发送过来的Request请求，并按照一定的方式进行整理排列，入队，当引擎需要时，交还给引擎。 Downloader（下载器）：负责下载Scrapy Engine(引擎)发送的所有Requests请求，并将其获取到的Responses交还给Scrapy Engine(引擎)，由引擎交给Spider来处理， Spider（爬虫）：它负责处理所有Responses,从中分析提取数据，获取Item字段需要的数据，并将需要跟进的URL提交给引擎，再次进入Scheduler(调度器). Item Pipeline(管道)：它负责处理Spider中获取到的Item，并进行进行后期处理（详细分析、过滤、存储等）的地方。 Downloader Middlewares（下载中间件）：你可以当作是一个可以自定义扩展下载功能的组件。 Spider Middlewares（Spider中间件）：你可以理解为是一个可以自定扩展和操作引擎和Spider中间通信的功能组件（比如进入Spider的Responses;和从Spider出去的Requests） 注意！只有当调度器中不存在任何request了，整个程序才会停止，（也就是说，对于下载失败的URL，Scrapy也会重新下载。） 制作 Scrapy 爬虫 一共需要4步： 新建项目 (scrapy startproject xxx)：新建一个新的爬虫项目 明确目标 （编写items.py）：明确你想要抓取的目标 制作爬虫 （spiders/xxspider.py）：制作爬虫开始爬取网页 存储内容 （pipelines.py）：设计管道存储爬取内容 scrapy安装1. 如果你用的是Anaconda或者Minconda，可以使用下面的命令：conda install -c conda-forge scrapy 2. 如果你已经安装了python包管理工具PyPI，可以使用下面命令进行安装：pip install Scrapy。值得注意的是，如果你使用的是pip安装，你需要解决相应的包依赖。 scrapy依赖的一些包： lxml：一种高效的XML和HTML解析器， PARSEL：一个HTML / XML数据提取库，基于上面的lxml， w3lib：一种处理URL和网页编码多功能辅助 twisted,：一个异步网络框架 cryptography and pyOpenSSL，处理各种网络级安全需求 以上包需要的最低版本： Twisted 14.0 lxml 3.4 pyOpenSSL 0.14 常见依赖问题: 1.错误提示：ModuleNotFoundError: No module named &apos;win32api&apos; 解决方法： (1)到这个网站下载跟使用的Python版本相匹配的软件：https://github.com/mhammond/pywin32/releases (2)进入使用的Python解释器里的Scripts目录，里面有一个easy_install.exe文件 (3)打开命令行，使用如下命令进行安装：easy_install.exe pywin32-224.win-amd64-py3.6.exe 2.错误提示：building &apos;twisted.test.raiser&apos; extension 解决方法： (1)到这个网站下载跟使用的Python版本相匹配的软件：https://www.lfd.uci.edu/~gohlke/pythonlibs/#twisted (2)进入使用的Python解释器里的Scripts目录，里面有一个pip.exe文件 (3)打开命令行，使用如下命令进行安装：pip.exe Twisted-18.9.0-cp36-cp36m-win_amd64.whl win7安装scrapy推荐使用Anaconda进行安装 CentOS 7安装scrapyCentOS 7系统自带的python版本是2.7，若是python3.5+版本，则不用再安装pip了。 (1)安装pip # yum -y install epel-release # yum install python-pip # pip install --upgrade pip (2)安装依赖包 # yum install gcc libffi-devel python-devel openssl-devel -y (3)安装scrapy # pip install scrapy scrapy入门新建项目在开始爬取之前，首先要创建一个scrapy项目，在命令行输入一下命令即可创建: # scrapy startproject mySpider scrapy startproject是固定写法，注意scrapy和startproject和mySpider中间是有空格的！ mySpider 为项目名称，可以看到将会创建一个 mySpider 文件夹，目录结构大致如下： mySpider/ scrapy.cfg mySpider/ __init__.py items.py pipelines.py settings.py spiders/ __init__.py ...... 这些文件分别是: scrapy.cfg: 项目的配置文件。 mySpider/: 项目的Python模块，将会从这里引用代码。 mySpider/items.py: 项目的目标文件。 mySpider/pipelines.py: 项目的管道文件。 mySpider/settings.py: 项目的设置文件。 mySpider/spiders/: 存储爬虫代码目录。 明确目标打开 mySpider 目录下的 items.py， Item 定义结构化数据字段，用来保存爬取到的数据，有点像 Python 中的 dict，但是提供了一些额外的保护减少错误。 可以通过创建一个 scrapy.Item 类， 并且定义类型为 scrapy.Field 的类属性来定义一个 Item（可以理解成类似于 ORM 的映射关系）。 创建一个 ItcastItem 类，和构建 item 模型（model）： import scrapy class ItcastItem(scrapy.Item): name = scrapy.Field() title = scrapy.Field() info = scrapy.Field() 制作爬虫命令：scrapy genspider mingyan2 mingyan2.com mingyan2为蜘蛛名，mingyan2.com为要爬取的网站地址 运行蜘蛛命令：scrapy crawl mingyan2 要重点提醒一下，我们一定要进入：mingyan 这个目录，也就是我们创建的蜘蛛项目目录，以上命令才有效！还有 crawl 后面跟的mingyan2是你类里面定义的蜘蛛名，也就是：name，并不是项目名、也不是类名。 scrapy start_url（初始链接）的两种不同写法第一种： start_urls = [ # 另外一种写法，无需定义start_requests方法 &apos;http://lab.scrapyd.cn/page/1/&apos;, &apos;http://lab.scrapyd.cn/page/2/&apos;, ]， 必须定义一个方法为：def parse(self, response)，方法名一定是：parse 第二种： 自己定义一个start_requests()方法 示例代码： &quot;&quot;&quot; scrapy初始Url的两种写法， 一种是常量start_urls，并且需要定义一个方法parse（） 另一种是直接定义一个方法：star_requests() &quot;&quot;&quot; import scrapy class simpleUrl(scrapy.Spider): name = &quot;simpleUrl&quot; start_urls = [ #另外一种写法，无需定义start_requests方法 &apos;http://lab.scrapyd.cn/page/1/&apos;, &apos;http://lab.scrapyd.cn/page/2/&apos;, ] # 另外一种初始链接写法 # def start_requests(self): # urls = [ #爬取的链接由此方法通过下面链接爬取页面 # &apos;http://lab.scrapyd.cn/page/1/&apos;, # &apos;http://lab.scrapyd.cn/page/2/&apos;, # ] # for url in urls: # yield scrapy.Request(url=url, callback=self.parse) # 如果是简写初始url，此方法名必须为：parse def parse(self, response): page = response.url.split(&quot;/&quot;)[-2] filename = &apos;mingyan-%s.html&apos; % page with open(filename, &apos;wb&apos;) as f: f.write(response.body) self.log(&apos;保存文件: %s&apos; % filename) scrapy调试工具：scrapy shell使用方法进入scrapy shell调试命令：scrapy shell http://lab.scrapyd.cn scrapy shell 是固定格式，后面跟的是你要调试的页面。这段代码就是一个下载的过程，一执行这么一段代码scrapy就立马把我们相应链接的相应页面给拿到了 scrapy css选择器使用进入scrapy shell调试命令：scrapy shell http://lab.scrapyd.cn 在命令行输入如下命令： &gt;&gt;&gt; response.css(&apos;title&apos;) [&lt;Selector xpath=&apos;descendant-or-self::title&apos; data=&apos;&lt;title&gt;SCRAPY爬虫实验室 - SCRAPY中文网提供&lt;/title&gt;&apos;&gt;] 使用这个命令提取的一个Selector的列表，并不是我们想要的数据；那我们再使用scrapy给我们准备的一些函数来进一步提取，那我们改变一下上面的写法， &gt;&gt;&gt; response.css(&apos;title&apos;).extract() [&apos;&lt;title&gt;SCRAPY爬虫实验室 - SCRAPY中文网提供&lt;/title&gt;&apos;] 我们只是在后面加入了：extract() 这么一个函数你就提取到了我们标签的一个列表，更近一步了，那如果我们不要列表，只要title这个标签，要怎么处理呢，看我们的输入： &gt;&gt;&gt; response.css(&apos;title&apos;).extract()[0] &apos;&lt;title&gt;爬虫实验室 - SCRAPY中文网提供&lt;/title&gt;&apos; 这里的话，我们只需要在后面添加：[0]，那代表提取这个列表中的第一个元素，那就得到了我们的title字符串；这里的话scrapy也给我提供了另外一个函数，可以这样来写，一样的效果： &gt;&gt;&gt; response.css(&apos;title&apos;).extract_first() &apos;&lt;title&gt;爬虫实验室 - SCRAPY中文网提供&lt;/title&gt;&apos; extract_first()就代表提取第一个元素，和我们的：[0]，一样的效果，只是更简洁些， 至此我们已经成功提取到了我们的title，但是你会发现，肿么多了一个title标签，这并不是你需要的，那要肿么办呢， 我们可以继续改变一下以上的输入： &gt;&gt;&gt; response.css(&apos;title::text&apos;).extract_first() &apos;爬虫实验室 - SCRAPY中文网提供&apos; 在title后面加上了 ::text ,这代表提取标签里面的数据，至此，我们已经成功提取到了我们需要的数据： &apos;爬虫实验室 - SCRAPY中文网提供&apos; 总结一下，其实就这么一段代码： response.css(&apos;title::text&apos;).extract_first() scrapy提取一组数据class选择器使用的是&quot;.&quot;,比如.text ，如果是id选择器的话：使用&quot;#&quot;,比如 #text 示例代码： import scrapy class itemSpider(scrapy.Spider): name = &apos;itemSpider&apos; start_urls = [&apos;http://lab.scrapyd.cn&apos;] def parse(self, response): mingyan = response.css(&apos;div.quote&apos;)[0] text = mingyan.css(&apos;.text::text&apos;).extract_first() # 提取名言 autor = mingyan.css(&apos;.author::text&apos;).extract_first() # 提取作者 tags = mingyan.css(&apos;.tags .tag::text&apos;).extract() # 提取标签 tags = &apos;,&apos;.join(tags) # 数组转换为字符串 fileName = &apos;%s-语录.txt&apos; % autor # 爬取的内容存入文件，文件名为：作者-语录.txt f = open(fileName, &quot;a+&quot;) # 追加写入文件 f.write(text) # 写入名言内容 f.write(&apos;\n&apos;) # 换行 f.write(&apos;标签：&apos;+tags) # 写入标签 f.close() # 关闭文件操作 scrapy 爬取多条数据这次比上次唯一多了个递归调用，我们来看一下关键变化，原先我们取出一条数据，用的是如下表达式：mingyan = response.css(&apos;div.quote&apos;)[0] 我们在后面添加了游标 [0] 表示只取出第一条，那我们要取出全部，那我们就不用加了，直接：mingyan = response.css(&apos;div.quote&apos;) 那现在的变量就是一个数据集，里面有多条数据了，那接下来我们要做的就是循环取出数据集里面的每一条数据，那我们看一下怎么做： mingyan = response.css(&apos;div.quote&apos;) # 提取首页所有名言，保存至变量mingyan for v in mingyan: # 循环获取每一条名言里面的：名言内容、作者、标签 text = v.css(&apos;.text::text&apos;).extract_first() # 提取名言 autor = v.css(&apos;.author::text&apos;).extract_first() # 提取作者 tags = v.css(&apos;.tags .tag::text&apos;).extract() # 提取标签 tags = &apos;,&apos;.join(tags) # 数组转换为字符串 # 接下来，进行保存 可以看到，关键是：for v in mingyan: 表示把 mingyan 这个数据集里面的数据，循环赋值给：v ，第一次循环的话 v 就代表第一条数据， 那text = v.css(&apos;.text::text&apos;).extract_first() 就代表第一条数据的名言内容，以此类推，把所有数据都取了出来，最终进行保存，我们看一下完整的代码： import scrapy class itemSpider(scrapy.Spider): name = &apos;listSpider&apos; start_urls = [&apos;http://lab.scrapyd.cn&apos;] def parse(self, response): mingyan = response.css(&apos;div.quote&apos;) # 提取首页所有名言，保存至变量mingyan for v in mingyan: # 循环获取每一条名言里面的：名言内容、作者、标签 text = v.css(&apos;.text::text&apos;).extract_first() # 提取名言 autor = v.css(&apos;.author::text&apos;).extract_first() # 提取作者 tags = v.css(&apos;.tags .tag::text&apos;).extract() # 提取标签 tags = &apos;,&apos;.join(tags) # 数组转换为字符串 &quot;&quot;&quot; 接下来进行写文件操作，每个名人的名言储存在一个txt文档里面 &quot;&quot;&quot; fileName = &apos;%s-语录.txt&apos; % autor # 定义文件名,如：木心-语录.txt with open(fileName, &quot;a+&quot;) as f: # 不同人的名言保存在不同的txt文档，“a+”以追加的形式 f.write(text) f.write(&apos;\n&apos;) # ‘\n’ 表示换行 f.write(&apos;标签：&apos; + tags) f.write(&apos;\n-------\n&apos;) f.close() scrapy 爬取下一页要爬取下一页，那我们首先要分析链接格式，找到下一页的链接，那爬取就简单了。下一页的链接如下： &lt;li class=&quot;next&quot;&gt; &lt;a href=&quot;http://lab.scrapyd.cn/page/2/&quot;&gt;下一页 »&lt;/a&gt; &lt;/li&gt; 每爬一页就用css选择器来查询，是否存在下一页链接，存在：则爬取下一页链接：http://lab.scrapyd.cn/page/*/， 然后把下一页链接提交给当前爬取的函数，继续爬取，继续查找下一页，知道找不到下一页，说明所有页面已经爬完，那结束爬虫。 爬取内容的代码和上一文档（listSpider）一模一样，唯一区别的是这么一个地方，我们在：listSpider 蜘蛛下面添加了这么几段代码： next_page = response.css(&apos;li.next a::attr(href)&apos;).extract_first() if next_page is not None: next_page = response.urljoin(next_page) yield scrapy.Request(next_page, callback=self.parse) 首先：我们使用：response.css(&apos;li.next a::attr(href)&apos;).extract_first()查看有木有存在下一页链接，如果存在的话，我们使用：urljoin(next_page)把相对路径，如：page/1转换为绝对路径，其实也就是加上网站域名，如：http://lab.scrapyd.cn/page/1； 接下来就是爬取下一页或是内容页的秘诀所在，scrapy给我们提供了这么一个方法：scrapy.Request() 这个方法还有许多参数，后面我们慢慢说，这里我们只使用了两个参数，一个是：我们继续爬取的链接（next_page）， 这里是下一页链接，当然也可以是内容页；另一个是：我们要把链接提交给哪一个函数爬取，这里是parse函数，也就是本函数； 当然，我们也可以在下面另写一个函数，比如：内容页，专门处理内容页的数据。 经过这么一个函数，下一页链接又提交给了parse，那就可以不断的爬取了，直到不存在下一页； scrapy arguments：指定蜘蛛参数爬取scrapy提供了可传参的爬虫，首先按scrapy 参数格式定义好参数，如下： def start_requests(self): url = &apos;http://lab.scrapyd.cn/&apos; tag = getattr(self, &apos;tag&apos;, None) # 获取tag值，也就是爬取时传过来的参数 if tag is not None: # 判断是否存在tag，若存在，重新构造url url = url + &apos;tag/&apos; + tag # 构造url若tag=爱情，url= &quot;http://lab.scrapyd.cn/tag/爱情&quot; yield scrapy.Request(url, self.parse) # 发送请求爬取参数内容 可以看到 tag = getattr(self, &apos;tag&apos;, None) 就是获取传过来的参数，然后根据不同的参数，构造不同的url，然后进行不同的爬取，经过这么一个处理，我们的蜘蛛就灰常的灵活了，我们来看一下完整代码： # -*- coding: utf-8 -*- import scrapy class ArgsspiderSpider(scrapy.Spider): name = &quot;argsSpider&quot; def start_requests(self): url = &apos;http://lab.scrapyd.cn/&apos; tag = getattr(self, &apos;tag&apos;, None) # 获取tag值，也就是爬取时传过来的参数 if tag is not None: # 判断是否存在tag，若存在，重新构造url url = url + &apos;tag/&apos; + tag # 构造url若tag=爱情，url= &quot;http://lab.scrapyd.cn/tag/爱情&quot; yield scrapy.Request(url, self.parse) # 发送请求爬取参数内容 &quot;&quot;&quot; 以下内容为上一讲知识，若不清楚具体细节，请查看上一讲！ &quot;&quot;&quot; def parse(self, response): mingyan = response.css(&apos;div.quote&apos;) for v in mingyan: text = v.css(&apos;.text::text&apos;).extract_first() tags = v.css(&apos;.tags .tag::text&apos;).extract() tags = &apos;,&apos;.join(tags) fileName = &apos;%s-语录.txt&apos; % tags with open(fileName, &quot;a+&quot;) as f: f.write(text) f.write(&apos;\n&apos;) f.write(&apos;标签：&apos; + tags) f.write(&apos;\n-------\n&apos;) f.close() next_page = response.css(&apos;li.next a::attr(href)&apos;).extract_first() if next_page is not None: next_page = response.urljoin(next_page) yield scrapy.Request(next_page, callback=self.parse) 要如何传参,可以这样：scrapy crawl argsSpider -a tag=爱情 详解scrapyscrapy如何打开页面那蜘蛛要发送请求，那总得要有请求链接，如果木有，蜘蛛肯定得不到返回，那页面也就打不开了，因此引出了scrapy spiders的第一个必须的常量：start_urls URL有两种写法，一种作为类的常量、一种作为start_requests(self)方法的常量，无论哪一种写法，URL都是必须的！ 有了URL那就可以发送请求了，如果URL是定义在start_request(self)这个方法里面，那我们就要使用： yield scrapy.Request 方法发送请求：如下： import scrapy class simpleUrl(scrapy.Spider): name = &quot;simpleUrl&quot; # 另外一种初始链接写法 def start_requests(self): urls = [ #爬取的链接由此方法通过下面链接爬取页面 &apos;http://lab.scrapyd.cn/page/1/&apos;, &apos;http://lab.scrapyd.cn/page/2/&apos;, ] for url in urls: #发送请求 yield scrapy.Request(url=url, callback=self.parse) 这样写的一个麻烦之处就是我们需要处理我们的返回，也就是我们还需要写一个callback方法来处理response； 因此大多数我们都是把URL作为类的常量，然后再加上另外一个方法： parse(response) 使用这个方法来发送请求，可以看到里面有个参数已经是：response（返回），也就是说这个方法自动化的完成了：request（请求页面）-response（返回页面）的过程，我们就不必要再写函数接受返回 import scrapy class simpleUrl(scrapy.Spider): name = &quot;simpleUrl&quot; start_urls = [ #另外一种写法，无需定义start_requests方法 &apos;http://lab.scrapyd.cn/page/1/&apos;, &apos;http://lab.scrapyd.cn/page/2/&apos;, ] def parse(self, response): page = response.url.split(&quot;/&quot;)[-2] filename = &apos;mingyan-%s.html&apos; % page with open(filename, &apos;wb&apos;) as f: f.write(response.body) self.log(&apos;保存文件: %s&apos; % filename) scrapy css选择器和scrapy相关的函数就这么三个而已：response.css(&quot;css表达式&quot;)、extract()、extract_first()。 有变化的就是：css表达式的写法,按照HTML标签的结构可以分为：标签属性值提取、标签内容提取 1. 标签属性值的提取 提取属性是用：“标签名::attr(属性名)”，首先找到要提取的标签最近的class或id，缩小范围！ 比如我们要提取url表达式就是：a::attr(href)，要提取图片地址的表达式就是：img::attr(src) 限定一下提取的范围，最好的方法就是找到要提取目标最近的class或是id，可以看到这段代码中有个class=&quot;page-navigator&quot;，那我们就可以这样来写：response.css(&quot;.page-navigator a::attr(href)&quot;).extract() 说明：.page-navigator，其中点代表class选择器，如果代码中是：id=“page-navigator”，那我们这里就要写成：“#page-navigator” 2. 标签内容的提取 提取标签内容是用：“::text” 含有嵌套标签文字的提取：response.css(&quot;.post-content *::text&quot;).extract() 可以看到，“::tex“t前面有个“*”号，表示当前class或id下所有标签 3. CSS 高级用法 CSS选择器用于选择你想要的元素的样式的模式。&quot;CSS&quot;列表示在CSS版本的属性定义（CSS1，CSS2，或对CSS3） scrapy xpath选择器从几个方面说：一、属性提取；二、内容提取；三、标签内包含标签又包含标签的最外层标签里的所有内容提取； 1. scrapy xpath 属性提取 XPath 使用路径表达式在 XML 文档中选取节点。节点是通过沿着路径或者 step 来选取的。 下面列出了最有用的路径表达式： 调试的话我们还是在命令行使用下面命令：scrapy shell lab.scrapyd.cn 函数：response.xpath(&quot;表达式&quot;)，提取属性的话既然使用：@，那我们要提取href就是：@href，试一下：response.xpath(&quot;//@href&quot;) 限定我们的属性，使用的是：标签[@属性名=&apos;属性值&apos;]； 表达式就是：//@属性名，缩小标签范围、限定属性的方式 2. scrapy xpath 标签内容提取 表达式为：//text() 3. 包含HTML标签的所有文字内容提取 这种用法主要是提取一些内容页，标签里夹杂着文字，但我们只要文字！比如下面的这段代码： &lt;div class=&quot;post-content&quot; itemprop=&quot;articleBody&quot;&gt; &lt;p&gt;如果你因失去了太阳而流泪，那么你也将失去群星了。 &lt;br&gt;If you shed tears when you miss the sun, you also miss the stars. &lt;/p&gt; &lt;p&gt;&lt;a href=&quot;http://www.scrapyd.cn&quot;&gt;scrapy中文网（&lt;/a&gt;&lt;a href=&quot;http://www.scrapyd.cn&quot;&gt;http://www.scrapyd.cn&lt;/a&gt;）整理&lt;/p&gt; &lt;/div&gt; 如果我们用表达式：//div[@class=&apos;post-content&apos;]//text()，你会发现虽然能提取但是一个列表，不是整段文字。 那就用到一个xpath函数：string()，可以把表达式这样写：response.xpath(&quot;string(//div[@class=&apos;post-content&apos;])&quot;).extract()，可看到我们没有使用：text()，而是用：string(要提取内容的标签)，这样的话就能把数据都提取出来了，而且都合成为一条，并非一个列表。 这一种用法在我们提取商品详情、小说内容的时候经常用到 4. xpath实例 scrapy命令行工具1. scrapy全局命令 scrapy startproject project_name scrapy genspider example example.com (cd project_name) scrapy crawl XX（运行XX蜘蛛） scrapy shell www.example.com (1)startproject 创建项目的，如，创建一个名为：scrapyChina的项目：scrapy strartproject scrapychina (2)genspider 根据蜘蛛模板创建蜘蛛的命令 (3)settings scray设置参数,比如我们想得到蜘蛛的下载延迟，我们可以使用：scrapy settings --get DOWNLOAD_DELAY;比如我们想得到蜘蛛的名字：scrapy settings --get BOT_NAME (4)runspider 运行蜘蛛除了使用：scrapy crawl XX之外，我们还能用：runspider， 前者是基于项目运行，后者是基于文件运行，也就是说你按照scrapy的蜘蛛格式编写了一个py文件，那你不想创建项目，那你就可以使用runspider，比如你编写了一个：scrapyd_cn.py的蜘蛛，你要直接运行就是：scrapy runspider scrapy_cn.py (5)shell 主要是调试用 (6)fetch 模拟蜘蛛下载页面，也就是说用这个命令下载的页面就是蜘蛛运行时下载的页面，好处是能准确诊断出，得到的html结构到底是不是我们所看到的，然后能及时调整我们编写爬虫的策略。演示window下如下如何把下载的页面保存：scrapy fetch http://www.scrapyd.cn &gt;d:/3.html (7)view 和fetch类似都是查看蜘蛛看到的是否和你看到的一致，便于排错，用法：scrapy view http://www.scrapyd.cn (8)version 查看scrapy版本，用法：scrapy version 2. scrapy项目命令 需要在项目文件夹下面打开CMD命令，然后再执行下面的这些命令 (1)crawl 运行蜘蛛 (2)check 检查蜘蛛 (3)list 显示有多少个蜘蛛,这里的蜘蛛就是指spider文件夹下面xx.py文件中定义的name，你有10个py文件但是只有一个定义了蜘蛛的name，那只算一个蜘蛛]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Scrapy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python_面向对象]]></title>
    <url>%2F2019%2F12%2F31%2Fpython_%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1%2F</url>
    <content type="text"><![CDATA[面向对象(OOP)基本概念面向对象编程 —— Object Oriented Programming 简写 OOP01. 面向对象基本概念我们之前学习的编程方式就是 面向过程 的面相过程 和 面相对象，是两种不同的 编程方式对比 面向过程 的特点，可以更好地了解什么是 面向对象1.1 过程和函数（科普）过程 是早期的一个编程概念过程 类似于函数，只能执行，但是没有返回值函数 不仅能执行，还可以返回结果1.2 面相过程 和 面相对象 基本概念1) 面相过程 —— 怎么做？1. 把完成某一个需求的 所有步骤 从头到尾 逐步实现 2. 根据开发需求，将某些 功能独立 的代码 封装 成一个又一个 函数 3. 最后完成的代码，就是顺序地调用 不同的函数 特点1. 注重 步骤与过程，不注重职责分工 2. 如果需求复杂，代码会变得很复杂 3. 开发复杂项目，没有固定的套路，开发难度很大！ 2) 面向对象 —— 谁来做？相比较函数，面向对象 是 更大 的 封装，根据 职责 在 一个对象中 封装 多个方法在完成某一个需求前，首先确定 职责 —— 要做的事情（方法）根据 职责 确定不同的 对象，在 对象 内部封装不同的 方法（多个）最后完成的代码，就是顺序地让 不同的对象 调用 不同的方法特点1. 注重 对象和职责，不同的对象承担不同的职责 2. 更加适合应对复杂的需求变化，是专门应对复杂项目开发，提供的固定套路 3. 需要在面向过程基础上，再学习一些面向对象的语法 类和对象01. 类和对象的概念类 和 对象 是 面向对象编程的 两个 核心概念1.1 类类 是对一群具有 相同 特征 或者 行为 的事物的一个统称，是抽象的，不能直接使用特征 被称为 属性行为 被称为 方法类 就相当于制造飞机时的图纸，是一个 模板，是 负责创建对象的1.2 对象对象 是 由类创建出来的一个具体存在，可以直接使用由 哪一个类 创建出来的 对象，就拥有在 哪一个类 中定义的：属性方法对象 就相当于用 图纸 制造 的飞机在程序开发中，应该 先有类，再有对象02. 类和对象的关系类是模板，对象 是根据 类 这个模板创建出来的，应该 先有类，再有对象类 只有一个，而 对象 可以有很多个不同的对象 之间 属性 可能会各不相同类 中定义了什么 属性和方法，对象 中就有什么属性和方法，不可能多，也不可能少03. 类的设计在使用面相对象开发前，应该首先分析需求，确定一下，程序中需要包含哪些类！在程序开发中，要设计一个类，通常需要满足一下三个要素：1. 类名 这类事物的名字，满足大驼峰命名法 2. 属性 这类事物具有什么样的特征 3. 方法 这类事物具有什么样的行为 大驼峰命名法每一个单词的首字母大写单词与单词之间没有下划线3.1 类名的确定名词提炼法 分析 整个业务流程，出现的 名词，通常就是找到的类3.2 属性和方法的确定对 对象的特征描述，通常可以定义成 属性对象具有的行为（动词），通常可以定义成 方法提示：需求中没有涉及的属性或者方法在设计类时，不需要考虑面相对象基础语法01. dir 内置函数（知道）在 Python 中 对象几乎是无所不在的，我们之前学习的 变量、数据、函数 都是对象在 Python 中可以使用以下两个方法验证：1. 在 标识符 / 数据 后输入一个 .，然后按下 TAB 键，iPython 会提示该对象能够调用的 方法列表 2. 使用内置函数 dir 传入 标识符 / 数据，可以查看对象内的 所有属性及方法 3. 提示 `__方法名__` 格式的方法是 Python 提供的 内置方法 / 属性，稍后会给大家介绍一些常用的 内置方法 / 属性 | 序号 | 方法名 | 类型 | 作用 || —- | ———- | —- | ——————————————– || 01 | __new__ | 方法 | 创建对象时，会被 自动 调用 || 02 | __init__ | 方法 | 对象被初始化时，会被 自动 调用 || 03 | __del__ | 方法 | 对象被从内存中销毁前，会被 自动 调用 || 04 | __str__ | 方法 | 返回对象的描述信息，print 函数输出使用 |02.定义简单的类（只包含方法）面向对象 是 更大 的 封装，在 一个类中 封装 多个方法，这样 通过这个类创建出来的对象，就可以直接调用这些方法了！2.1 定义只包含方法的类在 Python 中要定义一个只包含方法的类，语法格式如下：1234567class 类名: def 方法1(self, 参数列表): pass def 方法2(self, 参数列表): pass方法 的定义格式和之前学习过的函数 几乎一样区别在于第一个参数必须是 self，大家暂时先记住，稍后介绍 self注意：类名 的 命名规则 要符合 大驼峰命名法2.2 创建对象当一个类定义完成之后，要使用这个类来创建对象，语法格式如下：1对象变量 = 类名()2.3 第一个面向对象程序需求小猫 爱 吃 鱼，小猫 要 喝 水分析定义一个猫类 Cat定义两个方法 eat 和 drink按照需求 —— 不需要定义属性123456789101112class Cat(): """这是一个猫类""" def eat(self): print("小猫爱吃鱼") def drink(self): print("小猫在喝水")tom = Cat()tom.drink()tom.eat()引用概念的强调在面向对象开发中，引用的概念是同样适用的！在 Python 中使用类 创建对象之后，tom 变量中 仍然记录的是 对象在内存中的地址也就是 tom 变量 引用 了 新建的猫对象使用 print 输出 对象变量，默认情况下，是能够输出这个变量 引用的对象 是 由哪一个类创建的对象，以及 在内存中的地址（十六进制表示）提示：在计算机中，通常使用 十六进制 表示 内存地址十进制 和 十六进制 都是用来表达数字的，只是表示的方式不一样十进制 和 十六进制 的数字之间可以来回转换%d 可以以 10 进制 输出数字%x 可以以 16 进制 输出数字03.方法中的 self 参数3.1 案例改造 —— 给对象增加属性在 Python 中，要 给对象设置属性，非常的容易，但是不推荐使用因为：对象属性的封装应该封装在类的内部只需要在 类的外部的代码 中直接通过 . 设置一个属性即可注意：这种方式虽然简单，但是不推荐使用！123tom.name = "Tom"...lazy_cat.name = "大懒猫"3.2 使用 self 在方法内部输出每一只猫的名字由 哪一个对象 调用的方法，方法内的 self 就是 哪一个对象的引用在类封装的方法内部，self 就表示 当前调用方法的对象自己调用方法时，程序员不需要传递 self 参数在方法内部可以通过 self. 访问对象的属性也可以通过 self. 调用其他的对象方法改造代码如下：123456789101112class Cat: def eat(self): print("%s 爱吃鱼" % self.name)tom = Cat()tom.name = "Tom"tom.eat()lazy_cat = Cat()lazy_cat.name = "大懒猫"lazy_cat.eat()在 类的外部，通过 变量名. 访问对象的 属性和方法(tom.name)在 类封装的方法中，通过 self. 访问对象的 属性和方法(self.name)04. 初始化方法4.1 之前代码存在的问题 —— 在类的外部给对象增加属性将案例代码进行调整，先调用方法 再设置属性，观察一下执行效果12345tom = Cat()tom.drink()tom.eat()tom.name = "Tom"print(tom)程序执行报错如下：12AttributeError: 'Cat' object has no attribute 'name'属性错误：'Cat' 对象没有 'name' 属性提示在日常开发中，不推荐在 类的外部 给对象增加属性如果在运行时，没有找到属性，程序会报错对象应该包含有哪些属性，应该 封装在类的内部4.2 初始化方法当使用 类名() 创建对象时，会 自动 执行以下操作：为对象在内存中 分配空间 —— 创建对象为对象的属性 设置初始值 —— 初始化方法(init)这个 初始化方法 就是 init 方法，init 是对象的内置方法init 方法是 专门 用来定义一个类 具有哪些属性的方法！在 Cat 中增加 init 方法，验证该方法在创建对象时会被自动调用12345class Cat: """这是一个猫类""" def __init__(self): print("初始化方法")4.3 在初始化方法内部定义属性在 init 方法内部使用 self.属性名 = 属性的初始值 就可以 定义属性定义属性之后，再使用 Cat 类创建的对象，都会拥有该属性12345678910111213class Cat: def __init__(self): print("这是一个初始化方法") # 定义用 Cat 类创建的猫对象都有一个 name 的属性 self.name = "Tom" def eat(self): print("%s 爱吃鱼" % self.name)# 使用类名()创建对象的时候，会自动调用初始化方法 __init__tom = Cat()tom.eat()4.4 改造初始化方法 —— 初始化的同时设置初始值在开发中，如果希望在 创建对象的同时，就设置对象的属性，可以对 init 方法进行 改造把希望设置的属性值，定义成 init 方法的参数在方法内部使用 self.属性 = 形参 接收外部传递的参数在创建对象时，使用 类名(属性1, 属性2…) 调用12345678910class Cat: def __init__(self, name): print("初始化方法 %s" % name) self.name = name ... tom = Cat("Tom")...lazy_cat = Cat("大懒猫")...05. 内置方法和属性序号方法名类型作用01__del__方法对象被从内存中销毁前，会被 自动 调用02__str__方法返回对象的描述信息，print 函数输出使用5.1 del 方法（知道）在 Python 中当使用 类名() 创建对象时，为对象 分配完空间后，自动 调用 init 方法当一个 对象被从内存中销毁 前，会 自动 调用 del 方法应用场景init 改造初始化方法，可以让创建对象更加灵活del 如果希望在对象被销毁前，再做一些事情，可以考虑一下 del 方法生命周期一个对象从调用 类名() 创建，生命周期开始一个对象的 del 方法一旦被调用，生命周期结束在对象的生命周期内，可以访问对象属性，或者让对象调用方法12345678910111213141516class Cat: def __init__(self, new_name): self.name = new_name print("%s 来了" % self.name) def __del__(self): print("%s 去了" % self.name)# tom 是一个全局变量tom = Cat("Tom")print(tom.name)# del 关键字可以删除一个对象del tomprint("-" * 50)5.2 str 方法在 Python 中，使用 print 输出 对象变量，默认情况下，会输出这个变量 引用的对象 是 由哪一个类创建的对象，以及 在内存中的地址（十六进制表示）如果在开发中，希望使用 print 输出 对象变量 时，能够打印 自定义的内容，就可以利用 str 这个内置方法了注意：str 方法必须返回一个字符串12345678910111213class Cat: def __init__(self, new_name): self.name = new_name print("%s 来了" % self.name) def __del__(self): print("%s 去了" % self.name) def __str__(self): return "我是小猫：%s" % self.nametom = Cat("Tom")print(tom)返回结果：123Tom 来了我是小猫：TomTom 去了解释：123__del__用于当对象的引用计数为0时自动调用。__del__一般出现在两个地方：1、手工使用del减少对象引用计数至0，被垃圾回收处理时调用。2、程序结束时调用。__del__一般用于需要声明在对象被删除前需要处理的资源回收操作12345678910# 手工调用del 可以将对象引用计数减一，如果减到0，将会触发垃圾回收class Student: def __del__(self): print('调用对象的del方法，此方法将会回收此对象内存地址')stu = Student() # 调用对象的__del__方法回收此对象内存地址del stuprint('下面还有程序其他代码')12345class Student: def __del__(self): print('调用对象的del方法，此方法将会回收此对象内存地址')stu = Student() # 程序直接结束，也会调用对象的__del__方法回收地址面向对象封装案例01. 封装封装 是面向对象编程的一大特点面向对象编程的 第一步 —— 将 属性 和 方法 封装 到一个抽象的 类 中外界 使用 类 创建 对象，然后 让对象调用方法对象方法的细节 都被 封装 在 类的内部02. 小明爱跑步需求小明 体重 75.0 公斤小明每次 跑步 会减肥 0.5 公斤小明每次 吃东西 体重增加 1 公斤Personnameweight__init__(self,name,weight):__str__(self):run(self):eat(self):提示：在 对象的方法内部，是可以 直接访问对象的属性 的！代码实现：1234567891011121314151617181920212223242526272829303132class Person: """人类""" def __init__(self, name, weight): self.name = name self.weight = weight def __str__(self): return "我的名字叫 %s 体重 %.2f 公斤" % (self.name, self.weight) def run(self): """跑步""" print("%s 爱跑步，跑步锻炼身体" % self.name) self.weight -= 0.5 def eat(self): """吃东西""" print("%s 是吃货，吃完这顿再减肥" % self.name) self.weight += 1xiaoming = Person("小明", 75)xiaoming.run()xiaoming.eat()xiaoming.eat()print(xiaoming)2.1 小明爱跑步扩展 —— 小美也爱跑步需求小明 和 小美 都爱跑步小明 体重 75.0 公斤小美 体重 45.0 公斤每次 跑步 都会减少 0.5 公斤每次 吃东西 都会增加 1 公斤Personnameweight__init__(self,name,weight):__str__(self):run(self):eat(self):提示1. 在 对象的方法内部，是可以 直接访问对象的属性 的 2. 同一个类 创建的 多个对象 之间，属性 互不干扰！ 03. 摆放家具需求房子(House) 有 户型、总面积 和 家具名称列表新房子没有任何的家具家具(HouseItem) 有 名字 和 占地面积，其中席梦思(bed) 占地 4 平米衣柜(chest) 占地 2 平米餐桌(table) 占地 1.5 平米将以上三件 家具 添加 到 房子 中打印房子时，要求输出：户型、总面积、剩余面积、家具名称列表HouseItemnamearea__init__(self,name,area):__str__(self):Househouse_typeareafree_areaitem_list__init__(self,house_type,area):__str__(self):add_item(self):剩余面积在创建房子对象时，定义一个 剩余面积的属性，初始值和总面积相等当调用 add_item 方法，向房间 添加家具 时，让 剩余面积 -= 家具面积思考：应该先开发哪一个类？答案 —— 家具类家具简单房子要使用到家具，被使用的类，通常应该先开发3.1 创建家具123456789101112131415161718192021class HouseItem: def __init__(self, name, area): """ :param name: 家具名称 :param area: 占地面积 """ self.name = name self.area = area def __str__(self): return "[%s] 占地面积 %.2f" % (self.name, self.area)# 1. 创建家具bed = HouseItem("席梦思", 4)chest = HouseItem("衣柜", 2)table = HouseItem("餐桌", 1.5)print(bed)print(chest)print(table)小结创建了一个 家具类，使用到 init 和 str 两个内置方法使用 家具类 创建了 三个家具对象，并且 输出家具信息3.2 创建房间123456789101112131415161718192021222324252627282930313233class House: def __init__(self, house_type, area): """ :param house_type: 户型 :param area: 总面积 """ self.house_type = house_type self.area = area # 剩余面积默认和总面积一致 self.free_area = area # 默认没有任何的家具 self.item_list = [] def __str__(self): # Python 能够自动的将一对括号内部的代码连接在一起 return ("户型：%s\n总面积：%.2f[剩余：%.2f]\n家具：%s" % (self.house_type, self.area, self.free_area, self.item_list)) def add_item(self, item): print("要添加 %s" % item)...# 2. 创建房子对象my_home = House("两室一厅", 60)my_home.add_item(bed)my_home.add_item(chest)my_home.add_item(table)print(my_home)小结创建了一个 房子类，使用到 init 和 str 两个内置方法准备了一个 add_item 方法 准备添加家具使用 房子类 创建了 一个房子对象让 房子对象 调用了三次 add_item 方法，将 三件家具 以实参传递到 add_item 内部3.3 添加家具需求1&gt; 判断 家具的面积 是否 超过剩余面积，如果超过，提示不能添加这件家具2&gt; 将 家具的名称 追加到 家具名称列表 中3&gt; 用 房子的剩余面积 - 家具面积12345678910111213def add_item(self, item): print("要添加 %s" % item) # 1. 判断家具面积是否大于剩余面积 if item.area &gt; self.free_area: print("%s 的面积太大，不能添加到房子中" % item.name) return # 2. 将家具的名称追加到名称列表中 self.item_list.append(item.name) # 3. 计算剩余面积 self.free_area -= item.area3.4 小结主程序只负责创建 房子 对象和 家具 对象让 房子 对象调用 add_item 方法 将家具添加到房子中面积计算、剩余面积、家具列表 等处理都被 封装 到 房子类的内部面向对象封装案例 II封装封装 是面向对象编程的一大特点面向对象编程的 第一步 —— 将 属性 和 方法 封装 到一个抽象的 类 中外界 使用 类 创建 对象，然后 让对象调用方法细节 都被 封装 在 类的内部一个对象的 属性 可以是 另外一个类创建的对象01. 士兵突击需求许三多 有一把 AK47可以 开火够 发射 子弹填 装填子弹 —— 增加子弹数量Soldiernamegun__init__(self):fire(self):Gunmodelbullet_count__init__(self,model):add_bullet(self,count):shoot(self):1.1 开发枪类shoot 方法需求1&gt; 判断是否有子弹，没有子弹无法射击2&gt; 使用 print 提示射击，并且输出子弹数量1234567891011121314151617181920212223class Gun: def __init__(self, model): # 枪的型号 self.model = model # 子弹数量 self.bullet_count = 0 def add_bullet(self, count): self.bullet_count += count def shoot(self): # 判断是否还有子弹 if self.bullet_count &lt;= 0: print("没有子弹了...") return # 发射一颗子弹 self.bullet_count -= 1 print("%s 发射子弹[%d]..." % (self.model, self.bullet_count))# 创建枪对象ak47 = Gun("ak47")ak47.add_bullet(50)ak47.shoot()1.2 开发士兵类假设：每一个新兵 都 没有枪定义没有初始值的属性在定义属性时，如果 不知道设置什么初始值，可以设置为 NoneNone 关键字 表示 什么都没有表示一个 空对象，没有方法和属性，是一个特殊的常量可以将 None 赋值给任何一个变量fire 方法需求1&gt; 判断是否有枪，没有枪没法冲锋2&gt; 喊一声口号3&gt; 装填子弹4&gt; 射击123456789101112131415161718class Soldier: def __init__(self, name): # 姓名 self.name = name # 枪，士兵初始没有枪 None 关键字表示什么都没有 self.gun = None def fire(self): # 1. 判断士兵是否有枪 if self.gun is None: print("[%s] 还没有枪..." % self.name) return # 2. 高喊口号 print("冲啊...[%s]" % self.name) # 3. 让枪装填子弹 self.gun.add_bullet(50) # 4. 让枪发射子弹 self.gun.shoot()小结创建了一个 士兵类，使用到 init 内置方法在定义属性时，如果 不知道设置什么初始值，可以设置为 None在 封装的 方法内部，还可以让 自己的 使用其他类创建的对象属性 调用已经 封装好的方法02. 身份运算符身份运算符用于 比较 两个对象的 内存地址 是否一致 —— 是否是对同一个对象的引用在 Python 中针对 None 比较时，建议使用 is 判断运算符描述实例isis 是判断两个标识符是不是引用同一个对象x is y，类似 id(x) == id(y)is notis not 是判断两个标识符是不是引用不同对象x is not y，类似 id(a) != id(b)is 与 == 区别：is 用于判断 两个变量 引用对象是否为同一个== 用于判断 引用变量的值 是否相等123456&gt;&gt;&gt; a = [1, 2, 3]&gt;&gt;&gt; b = [1, 2, 3]&gt;&gt;&gt; b is a False&gt;&gt;&gt; b == aTrue私有属性和私有方法01. 应用场景及定义方式应用场景在实际开发中，对象 的 某些属性或方法 可能只希望 在对象的内部被使用，而 不希望在外部被访问到私有属性 就是 对象 不希望公开的 属性私有方法 就是 对象 不希望公开的 方法定义方式在 定义属性或方法时，在 属性名或者方法名前 增加 两个下划线，定义的就是 私有 属性或方法12345678910111213141516class Women: def __init__(self, name): self.name = name # 不要问女生的年龄 self.__age = 18 def __secret(self): print("我的年龄是 %d" % self.__age)xiaofang = Women("小芳")# 私有属性，外部不能直接访问# print(xiaofang.__age)# 私有方法，外部不能直接调用# xiaofang.__secret()02. 伪私有属性和私有方法（知识点）提示：在日常开发中，不要使用这种方式，访问对象的 私有属性 或 私有方法Python 中，并没有 真正意义 的 私有在给 属性、方法 命名时，实际是对 名称 做了一些特殊处理，使得外界无法访问到处理方式：在 名称 前面加上 _类名 =&gt; _类名__名称12345# 私有属性，外部不能直接访问到print(xiaofang._Women__age)# 私有方法，外部不能直接调用xiaofang._Women__secret()继承面向对象三大特性封装 根据 职责 将 属性 和 方法 封装 到一个抽象的 类 中继承 实现代码的重用，相同的代码不需要重复的编写多态 不同的对象调用相同的方法，产生不同的执行结果，增加代码的灵活度01. 单继承1.1 继承的概念、语法和特点继承的概念：子类 拥有 父类 的所有 方法 和 属性1) 继承的语法12class 类名(父类名): pass子类 继承自 父类，可以直接 享受 父类中已经封装好的方法，不需要再次开发子类 中应该根据 职责，封装 子类特有的 属性和方法2) 专业术语Dog 类是 Animal 类的子类，Animal 类是 Dog 类的父类，Dog 类从 Animal 类继承Dog 类是 Animal 类的派生类，Animal 类是 Dog 类的基类，Dog 类从 Animal 类派生3) 继承的传递性C 类从 B 类继承，B 类又从 A 类继承那么 C 类就具有 B 类和 A 类的所有属性和方法子类 拥有 父类 以及 父类的父类 中封装的所有 属性 和 方法1.2 方法的重写子类 拥有 父类 的所有 方法 和 属性子类 继承自 父类，可以直接 享受 父类中已经封装好的方法，不需要再次开发应用场景当 父类 的方法实现不能满足子类需求时，可以对方法进行 重写(override)重写 父类方法有两种情况：覆盖 父类的方法对父类方法进行 扩展1) 覆盖父类的方法如果在开发中，父类的方法实现 和 子类的方法实现，完全不同就可以使用 覆盖 的方式，在子类中 重新编写 父类的方法实现具体的实现方式，就相当于在 子类中 定义了一个 和父类同名的方法并且实现重写之后，在运行时，只会调用 子类中重写的方法，而不再会调用 父类封装的方法2) 对父类方法进行 扩展如果在开发中，子类的方法实现 中 包含 父类的方法实现父类原本封装的方法实现 是 子类方法的一部分就可以使用 扩展 的方式在子类中 重写 父类的方法在需要的位置使用 super().父类方法 来调用父类方法的执行代码其他的位置针对子类的需求，编写 子类特有的代码实现关于 super在 Python 中 super 是一个 特殊的类super() 就是使用 super 类创建出来的对象最常 使用的场景就是在 重写父类方法时，调用 在父类中封装的方法实现调用父类方法的另外一种方式（知道）在 Python 2.x 时，如果需要调用父类的方法，还可以使用以下方式：1父类名.方法(self)这种方式，目前在 Python 3.x 还支持这种方式这种方法 不推荐使用，因为一旦 父类发生变化，方法调用位置的 类名 同样需要修改提示在开发时，父类名 和 super() 两种方式不要混用如果使用 当前子类名 调用方法，会形成递归调用，出现死循环1.3 父类的 私有属性 和 私有方法子类对象 不能 在自己的方法内部，直接 访问 父类的 私有属性 或 私有方法子类对象 可以通过 父类 的 公有方法 间接 访问到 私有属性 或 私有方法私有属性、方法 是对象的隐私，不对外公开，外界 以及 子类 都不能直接访问私有属性、方法 通常用于做一些内部的事情02. 多继承概念子类 可以拥有 多个父类，并且具有 所有父类 的 属性 和 方法例如：孩子 会继承自己 父亲 和 母亲 的 特性语法12class 子类名(父类名1, 父类名2...) pass2.1 多继承的使用注意事项如果 不同的父类 中存在 同名的方法，子类对象 在调用方法时，会调用 哪一个父类中的方法呢？开发时，应该尽量避免这种容易产生混淆的情况！ —— 如果 父类之间 存在 同名的属性或者方法，应该 尽量避免 使用多继承Python 中的 MRO —— 方法搜索顺序（知道）Python 中针对 类 提供了一个 内置属性 mro 可以查看 方法 搜索顺序MRO 是 method resolution order，主要用于 在多继承时判断 方法、属性 的调用 路径1print(类名.__mro__)在搜索方法时，是按照 mro 的输出结果 从左至右 的顺序查找的如果在当前类中 找到方法，就直接执行，不再搜索如果 没有找到，就查找下一个类 中是否有对应的方法，如果找到，就直接执行，不再搜索如果找到最后一个类，还没有找到方法，程序报错2.2 新式类与旧式（经典）类object 是 Python 为所有对象提供的 基类，提供有一些内置的属性和方法，可以使用 dir 函数查看新式类：以 object 为基类的类，推荐使用经典类：不以 object 为基类的类，不推荐使用在 Python 3.x 中定义类时，如果没有指定父类，会 默认使用 object 作为该类的 基类 —— Python 3.x 中定义的类都是 新式类在 Python 2.x 中定义类时，如果没有指定父类，则不会以 object 作为 基类新式类 和 经典类 在多继承时 —— 会影响到方法的搜索顺序为了保证编写的代码能够同时在 Python 2.x 和 Python 3.x 运行！今后在定义类时，如果没有父类，建议统一继承自 object12class 类名(object): pass多态面向对象三大特性封装 根据 职责 将 属性 和 方法 封装 到一个抽象的 类 中定义类的准则继承 实现代码的重用，相同的代码不需要重复的编写设计类的技巧子类针对自己特有的需求，编写特定的代码多态 不同的 子类对象 调用相同的 父类方法，产生不同的执行结果多态 可以 增加代码的灵活度以 继承 和 重写父类方法 为前提是调用方法的技巧，不会影响到类的内部设计多态案例演练需求在 Dog 类中封装方法 game普通狗只是简单的玩耍定义 XiaoTianDog 继承自 Dog，并且重写 game 方法哮天犬需要在天上玩耍定义 Person 类，并且封装一个 和狗玩 的方法在方法内部，直接让 狗对象 调用 game 方法案例小结Person 类中只需要让 狗对象 调用 game 方法，而不关心具体是 什么狗game 方法是在 Dog 父类中定义的在程序执行时，传入不同的 狗对象 实参，就会产生不同的执行效果多态 更容易编写出出通用的代码，做出通用的编程，以适应需求的不断变化！1234567891011121314151617181920212223242526272829class Dog(object): def __init__(self, name): self.name = name def game(self): print("%s 蹦蹦跳跳的玩耍..." % self.name)class XiaoTianDog(Dog): def game(self): print("%s 飞到天上去玩耍..." % self.name)class Person(object): def __init__(self, name): self.name = name def game_with_dog(self, dog): print("%s 和 %s 快乐的玩耍..." % (self.name, dog.name)) # 让狗玩耍 dog.game()# 1. 创建一个狗对象# wangcai = Dog("旺财")wangcai = XiaoTianDog("飞天旺财")# 2. 创建一个小明对象xiaoming = Person("小明")# 3. 让小明调用和狗玩的方法xiaoming.game_with_dog(wangcai)类属性和类方法01. 类的结构1.1 术语 —— 实例使用面相对象开发，第 1 步 是设计 类使用 类名() 创建对象，创建对象 的动作有两步：1) 在内存中为对象 分配空间2) 调用初始化方法 init 为 对象初始化对象创建后，内存 中就有了一个对象的 实实在在 的存在 —— 实例因此，通常也会把：创建出来的 对象 叫做 类 的 实例创建对象的 动作 叫做 实例化对象的属性 叫做 实例属性对象调用的方法 叫做 实例方法在程序执行时：对象各自拥有自己的 实例属性调用对象方法，可以通过 self.访问自己的属性调用自己的方法结论每一个对象 都有自己 独立的内存空间，保存各自不同的属性多个对象的方法，在内存中只有一份，在调用方法时，需要把对象的引用 传递到方法内部1.2 类是一个特殊的对象Python 中 一切皆对象：class AAA: 定义的类属于 类对象obj1 = AAA() 属于 实例对象在程序运行时，类 同样 会被加载到内存在 Python 中，类 是一个特殊的对象 —— 类对象在程序运行时，类对象 在内存中 只有一份，使用 一个类 可以创建出 很多个对象实例除了封装 实例 的 属性 和 方法外，类对象 还可以拥有自己的 属性 和 方法类属性类方法通过 类名. 的方式可以 访问类的属性 或者 调用类的方法02. 类属性和实例属性2.1 概念和使用类属性 就是给 类对象 中定义的 属性通常用来记录 与这个类相关 的特征类属性 不会用于记录 具体对象的特征示例需求定义一个 工具类每件工具都有自己的 name需求 —— 知道使用这个类，创建了多少个工具对象？12345678910111213141516class Tool(object): # 使用赋值语句，定义类属性，记录创建工具对象的总数 count = 0 def __init__(self, name): self.name = name # 针对类属性做一个计数+1 Tool.count += 1# 创建工具对象tool1 = Tool("斧头")tool2 = Tool("榔头")tool3 = Tool("铁锹")# 知道使用 Tool 类到底创建了多少个对象?print("现在创建了 %d 个工具" % Tool.count)2.2 属性的获取机制（科普）在 Python 中 属性的获取 存在一个 向上查找机制因此，要访问类属性有两种方式：类名.类属性对象.类属性 （不推荐）注意如果使用 对象.类属性 = 值 赋值语句，只会 给对象添加一个属性，而不会影响到 类属性的值03. 类方法和静态方法3.1 类方法类属性 就是针对 类对象 定义的属性使用 赋值语句 在 class 关键字下方可以定义 类属性类属性 用于记录 与这个类相关 的特征类方法 就是针对 类对象 定义的方法在 类方法 内部可以直接访问 类属性 或者调用其他的 类方法语法如下123@classmethoddef 类方法名(cls): pass类方法需要用 修饰器 @classmethod 来标识，告诉解释器这是一个类方法类方法的 第一个参数 应该是 cls由 哪一个类 调用的方法，方法内的 cls 就是 哪一个类的引用这个参数和 实例方法 的第一个参数是 self 类似提示 使用其他名称也可以，不过习惯使用 cls通过 类名. 调用 类方法，调用方法时，不需要传递 cls 参数在方法内部可以通过 cls. 访问类的属性也可以通过 cls. 调用其他的类方法示例需求定义一个 工具类每件工具都有自己的 name需求 —— 在 类 封装一个 show_tool_count 的类方法，输出使用当前这个类，创建的对象个数1234@classmethoddef show_tool_count(cls): """显示工具对象的总数""" print("工具对象的总数 %d" % cls.count)在类方法内部，可以直接使用 cls 访问 类属性 或者 调用类方法3.2 静态方法在开发时，如果需要在 类 中封装一个方法，这个方法：既 不需要 访问 实例属性 或者调用 实例方法也 不需要 访问 类属性 或者调用 类方法这个时候，可以把这个方法封装成一个 静态方法语法如下：123@staticmethoddef 静态方法名(): pass静态方法 需要用 修饰器 @staticmethod 来标识，告诉解释器这是一个静态方法通过 类名. 调用 静态方法1234567891011class Dog(object): # 狗对象计数 dog_count = 0 @staticmethod def run(): # 不需要访问实例属性也不需要访问类属性的方法 print("狗在跑...") def __init__(self, name): self.name = name3.3 方法综合案例需求设计一个 Game 类属性：定义一个 类属性 top_score 记录游戏的 历史最高分定义一个 实例属性 player_name 记录 当前游戏的玩家姓名方法：静态方法 show_help 显示游戏帮助信息类方法 show_top_score 显示历史最高分实例方法 start_game 开始当前玩家的游戏主程序步骤1) 查看帮助信息2) 查看历史最高分3) 创建游戏对象，开始游戏案例小结实例方法 —— 方法内部需要访问 实例属性实例方法 内部可以使用 类名. 访问类属性类方法 —— 方法内部 只 需要访问 类属性静态方法 —— 方法内部，不需要访问 实例属性 和 类属性提问如果方法内部 即需要访问 实例属性，又需要访问 类属性，应该定义成什么方法？答案应该定义 实例方法因为，类只有一个，在 实例方法 内部可以使用 类名. 访问类属性123456789101112131415161718192021222324252627282930313233class Game(object): # 游戏最高分，类属性 top_score = 0 @staticmethod def show_help(): print("帮助信息：让僵尸走进房间") @classmethod def show_top_score(cls): print("游戏最高分是 %d" % cls.top_score) def __init__(self, player_name): self.player_name = player_name def start_game(self): print("[%s] 开始游戏..." % self.player_name) # 使用类名.修改历史最高分 Game.top_score = 999# 1. 查看游戏帮助Game.show_help()# 2. 查看游戏最高分Game.show_top_score()# 3. 创建游戏对象，开始游戏game = Game("小明")game.start_game()# 4. 游戏结束，查看游戏最高分Game.show_top_score()单例01. 单例设计模式设计模式设计模式 是 前人工作的总结和提炼，通常，被人们广泛流传的设计模式都是针对 某一特定问题 的成熟的解决方案使用 设计模式 是为了可重用代码、让代码更容易被他人理解、保证代码可靠性单例设计模式目的 —— 让 类 创建的对象，在系统中 只有 唯一的一个实例每一次执行 类名() 返回的对象，内存地址是相同的单例设计模式的应用场景音乐播放 对象回收站 对象打印机 对象……02. new 方法使用 类名() 创建对象时，Python 的解释器 首先 会 调用 new 方法为对象 分配空间__new__ 是一个 由 object 基类提供的 内置的静态方法，主要作用有两个：1) 在内存中为对象 分配空间2)返回 对象的引用Python 的解释器获得对象的 引用 后，将引用作为 第一个参数，传递给 init 方法重写 new 方法 的代码非常固定！重写 new 方法 一定要 return super().new(cls)否则 Python 的解释器 得不到 分配了空间的 对象引用，就不会调用对象的初始化方法注意：new 是一个静态方法，在调用时需要 主动传递 cls 参数示例代码:12345678910class MusicPlayer(object): def __new__(cls, *args, **kwargs): # 如果不返回任何结果， return super().__new__(cls) def __init__(self): print("初始化音乐播放对象")player = MusicPlayer()print(player)03. Python 中的单例单例 —— 让 类 创建的对象，在系统中 只有 唯一的一个实例定义一个 类属性，初始值是 None，用于记录 单例对象的引用重写 new 方法如果 类属性 is None，调用父类方法分配空间，并在类属性中记录结果返回 类属性 中记录的 对象引用1234567891011class MusicPlayer(object): # 定义类属性记录单例对象引用 instance = None def __new__(cls, *args, **kwargs): # 1. 判断类属性是否已经被赋值 if cls.instance is None: cls.instance = super().__new__(cls) # 2. 返回类属性的单例引用 return cls.instance3.1只执行一次初始化工作在每次使用 类名() 创建对象时，Python 的解释器都会自动调用两个方法：new 分配空间init 对象初始化在上一小节对 new 方法改造之后，每次都会得到 第一次被创建对象的引用但是：初始化方法还会被再次调用需求让 初始化动作 只被 执行一次解决办法定义一个类属性 init_flag 标记是否 执行过初始化动作，初始值为 False在 init 方法中，判断 init_flag，如果为 False 就执行初始化动作然后将 init_flag 设置为 True这样，再次 自动 调用 init 方法时，初始化动作就不会被再次执行 了12345678910111213141516171819202122232425262728class MusicPlayer(object): # 记录第一个被创建对象的引用 instance = None # 记录是否执行过初始化动作 init_flag = False def __new__(cls, *args, **kwargs): # 1. 判断类属性是否是空对象 if cls.instance is None: # 2. 调用父类的方法，为第一个对象分配空间 cls.instance = super().__new__(cls) # 3. 返回类属性保存的对象引用 return cls.instance def __init__(self): if not MusicPlayer.init_flag: print("初始化音乐播放器") MusicPlayer.init_flag = True# 创建多个对象player1 = MusicPlayer()print(player1)player2 = MusicPlayer()print(player2)]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python_闭包和装饰器]]></title>
    <url>%2F2019%2F12%2F31%2Fpython_%E9%97%AD%E5%8C%85%E5%92%8C%E8%A3%85%E9%A5%B0%E5%99%A8%2F</url>
    <content type="text"><![CDATA[闭包1. 函数引用1234567891011121314def test1(): print("--- in test1 func----")# 调用函数test1()# 引用函数ret = test1print(id(ret))print(id(test1))#通过引用调用函数ret()运行结果:1234--- in test1 func----140212571149040140212571149040--- in test1 func----2. 什么是闭包12345678910111213141516171819# 定义一个函数def test(number): # 在函数内部再定义一个函数，并且这个函数用到了外边函数的变量，那么将这个函数以及用到的一些变量称之为闭包 def test_in(number_in): print("in test_in 函数, number_in is %d" % number_in) return number+number_in # 其实这里返回的就是闭包的结果 return test_in# 给test函数赋值，这个20就是给参数numberret = test(20)# 注意这里的100其实给参数number_inprint(ret(100))#注 意这里的200其实给参数number_inprint(ret(200))运行结果：12345in test_in 函数, number_in is 100120in test_in 函数, number_in is 2002203. 看一个闭包的实际例子：123456789def line_conf(a, b): def line(x): return a*x + b return lineline1 = line_conf(1, 1)line2 = line_conf(4, 5)print(line1(5))print(line2(5))这个例子中，函数line与变量a,b构成闭包。在创建闭包的时候，我们通过line_conf的参数a,b说明了这两个变量的取值，这样，我们就确定了函数的最终形式(y = x + 1和y = 4x + 5)。我们只需要变换参数a,b，就可以获得不同的直线表达函数。由此，我们可以看到，闭包也具有提高代码可复用性的作用。如果没有闭包，我们需要每次创建直线函数的时候同时说明a,b,x。这样，我们就需要更多的参数传递，也减少了代码的可移植性。注意点:由于闭包引用了外部函数的局部变量，则外部函数的局部变量没有及时释放，消耗内存4. 修改外部函数中的变量python3的方法1234567891011121314151617181920def counter(start=0): def incr(): nonlocal start start += 1 return start return incrc1 = counter(5)print(c1())print(c1())c2 = counter(50)print(c2())print(c2())print(c1())print(c1())print(c2())print(c2())python2的方法12345678910111213def counter(start=0): count=[start] def incr(): count[0] += 1 return count[0] return incrc1 = closeure.counter(5)print(c1()) # 6print(c1()) # 7c2 = closeure.counter(100)print(c2()) # 101print(c2()) # 102装饰器装饰器是程序开发中经常会用到的一个功能，用好了装饰器，开发效率如虎添翼，所以这也是Python面试中必问的问题，但对于好多初次接触这个知识的人来讲，这个功能有点绕，自学时直接绕过去了，然后面试问到了就挂了，因为装饰器是程序开发的基础知识，这个都不会，别跟人家说你会Python, 看了下面的文章，保证你学会装饰器。1、先明白这段代码1234567891011121314#### 第一波 ####def foo(): print('foo')foo # 表示是函数foo() # 表示执行foo函数#### 第二波 ####def foo(): print('foo')foo = lambda x: x + 1foo() # 执行lambda表达式，而不再是原来的foo函数，因为foo这个名字被重新指向了另外一个匿名函数函数名仅仅是个变量，只不过指向了定义的函数而已，所以才能通过 函数名()调用，如果 函数名=xxx被修改了，那么当在执行 函数名()时，调用的就不知之前的那个函数了2、需求来了初创公司有N个业务部门，基础平台部门负责提供底层的功能，如：数据库操作、redis调用、监控API等功能。业务部门使用基础功能时，只需调用基础平台提供的功能即可。如下：123456789101112131415161718192021222324252627############### 基础平台提供的功能如下 ###############def f1(): print('f1')def f2(): print('f2')def f3(): print('f3')def f4(): print('f4')############### 业务部门A 调用基础平台提供的功能 ###############f1()f2()f3()f4()############### 业务部门B 调用基础平台提供的功能 ###############f1()f2()f3()f4()目前公司有条不紊的进行着，但是，以前基础平台的开发人员在写代码时候没有关注验证相关的问题，即：基础平台的提供的功能可以被任何人使用。现在需要对基础平台的所有功能进行重构，为平台提供的所有功能添加验证机制，即：执行功能前，先进行验证。老大把工作交给 Low B，他是这么做的：跟每个业务部门交涉，每个业务部门自己写代码，调用基础平台的功能之前先验证。诶，这样一来基础平台就不需要做任何修改了。太棒了，有充足的时间泡妹子…当天Low B 被开除了…老大把工作交给 Low BB，他是这么做的：12345678910111213141516171819202122232425262728293031323334353637383940############### 基础平台提供的功能如下 ############### def f1(): # 验证1 # 验证2 # 验证3 print('f1')def f2(): # 验证1 # 验证2 # 验证3 print('f2')def f3(): # 验证1 # 验证2 # 验证3 print('f3')def f4(): # 验证1 # 验证2 # 验证3 print('f4')############### 业务部门不变 ############### ### 业务部门A 调用基础平台提供的功能### f1()f2()f3()f4()### 业务部门B 调用基础平台提供的功能 ### f1()f2()f3()f4()过了一周 Low BB 被开除了…老大把工作交给 Low BBB，他是这么做的：只对基础平台的代码进行重构，其他业务部门无需做任何修改1234567891011121314151617181920212223242526272829303132############### 基础平台提供的功能如下 ############### def check_login(): # 验证1 # 验证2 # 验证3 passdef f1(): check_login() print('f1')def f2(): check_login() print('f2')def f3(): check_login() print('f3')def f4(): check_login() print('f4')老大看了下Low BBB 的实现，嘴角漏出了一丝的欣慰的笑，语重心长的跟Low BBB聊了个天：老大说：写代码要遵循开放封闭原则，虽然在这个原则是用的面向对象开发，但是也适用于函数式编程，简单来说，它规定已经实现的功能代码不允许被修改，但可以被扩展，即：封闭：已实现的功能代码块开放：对扩展开发如果将开放封闭原则应用在上述需求中，那么就不允许在函数 f1 、f2、f3、f4的内部进行修改代码，老板就给了Low BBB一个实现方案：1234567891011121314151617181920def w1(func): def inner(): # 验证1 # 验证2 # 验证3 func() return inner@w1def f1(): print('f1')@w1def f2(): print('f2')@w1def f3(): print('f3')@w1def f4(): print('f4')对于上述代码，也是仅仅对基础平台的代码进行修改，就可以实现在其他人调用函数 f1 f2 f3 f4 之前都进行【验证】操作，并且其他业务部门无需做任何操作。Low BBB心惊胆战的问了下，这段代码的内部执行原理是什么呢？老大正要生气，突然Low BBB的手机掉到地上，恰巧屏保就是Low BBB的女友照片，老大一看一紧一抖，喜笑颜开，决定和Low BBB交个好朋友。详细的开始讲解了：单独以f1为例：1234567891011def w1(func): def inner(): # 验证1 # 验证2 # 验证3 func() return inner@w1def f1(): print('f1')python解释器就会从上到下解释代码，步骤如下：def w1(func): ==&gt;将w1函数加载到内存@w1没错， 从表面上看解释器仅仅会解释这两句代码，因为函数在 没有被调用之前其内部代码不会被执行。从表面上看解释器着实会执行这两句，但是 @w1 这一句代码里却有大文章， @函数名 是python的一种语法糖。上例@w1内部会执行一下操作：执行w1函数执行w1函数 ，并将 @w1 下面的函数作为w1函数的参数，即：@w1 等价于 w1(f1) 所以，内部就会去执行：1234567&gt; def inner(): &gt; #验证 1&gt; #验证 2&gt; #验证 3&gt; f1() # func是参数，此时 func 等于 f1 &gt; return inner# 返回的 inner，inner代表的是函数，非执行函数 ,其实就是将原来的 f1 函数塞进另外一个函数中&gt;w1的返回值将执行完的w1函数返回值 赋值 给@w1下面的函数的函数名f1 即将w1的返回值再重新赋值给 f1，即：1234567&gt; 新f1 = def inner(): &gt; #验证 1&gt; #验证 2&gt; #验证 3&gt; 原来f1()&gt; return inner&gt;所以，以后业务部门想要执行 f1 函数时，就会执行 新f1 函数，在新f1 函数内部先执行验证，再执行原来的f1函数，然后将原来f1 函数的返回值返回给了业务调用者。如此一来， 即执行了验证的功能，又执行了原来f1函数的内容，并将原f1函数返回值 返回给业务调用着Low BBB 你明白了吗？要是没明白的话，我晚上去你家帮你解决吧！！！3. 再议装饰器12345678910111213141516171819202122232425262728# 定义函数：完成包裹数据def makeBold(fn): def wrapped(): return "&lt;b&gt;" + fn() + "&lt;/b&gt;" return wrapped# 定义函数：完成包裹数据def makeItalic(fn): def wrapped(): return "&lt;i&gt;" + fn() + "&lt;/i&gt;" return wrapped@makeBolddef test1(): return "hello world-1"@makeItalicdef test2(): return "hello world-2"@makeBold@makeItalicdef test3(): return "hello world-3"print(test1())print(test2())print(test3())运行结果:123&lt;b&gt;hello world-1&lt;/b&gt;&lt;i&gt;hello world-2&lt;/i&gt;&lt;b&gt;&lt;i&gt;hello world-3&lt;/i&gt;&lt;/b&gt;4. 装饰器(decorator)功能引入日志函数执行时间统计执行函数前预备处理执行函数后清理功能权限校验等场景缓存5. 装饰器示例例1:无参数的函数123456789101112131415from time import ctime, sleepdef timefun(func): def wrapped_func(): print("%s called at %s" % (func.__name__, ctime())) func() return wrapped_func@timefundef foo(): print("I am foo")foo()sleep(2)foo()上面代码理解装饰器执行行为可理解成123456foo = timefun(foo)# foo先作为参数赋值给func后,foo接收指向timefun返回的wrapped_funcfoo()# 调用foo(),即等价调用wrapped_func()# 内部函数wrapped_func被引用，所以外部函数的func变量(自由变量)并没有释放# func里保存的是原foo函数对象例2:被装饰的函数有参数12345678910111213141516from time import ctime, sleepdef timefun(func): def wrapped_func(a, b): print("%s called at %s" % (func.__name__, ctime())) print(a, b) func(a, b) return wrapped_func@timefundef foo(a, b): print(a+b)foo(3,5)sleep(2)foo(2,4)例3:被装饰的函数有不定长参数123456789101112131415from time import ctime, sleepdef timefun(func): def wrapped_func(*args, **kwargs): print("%s called at %s"%(func.__name__, ctime())) func(*args, **kwargs) return wrapped_func@timefundef foo(a, b, c): print(a+b+c)foo(3,5,7)sleep(2)foo(2,4,9)例4:装饰器中的return12345678910111213141516171819202122from time import ctime, sleepdef timefun(func): def wrapped_func(): print("%s called at %s" % (func.__name__, ctime())) func() return wrapped_func@timefundef foo(): print("I am foo")@timefundef getInfo(): return '----hahah---'foo()sleep(2)foo()print(getInfo())执行结果:123456foo called at Fri Nov 4 21:55:35 2016I am foofoo called at Fri Nov 4 21:55:37 2016I am foogetInfo called at Fri Nov 4 21:55:37 2016None如果修改装饰器为return func()，则运行结果：123456foo called at Fri Nov 4 21:55:57 2016I am foofoo called at Fri Nov 4 21:55:59 2016I am foogetInfo called at Fri Nov 4 21:55:59 2016----hahah---总结：一般情况下为了让装饰器更通用，可以有return例5:装饰器带参数,在原有装饰器的基础上，设置外部变量1234567891011121314151617181920212223242526272829303132#decorator2.pyfrom time import ctime, sleepdef timefun_arg(pre="hello"): def timefun(func): def wrapped_func(): print("%s called at %s %s" % (func.__name__, ctime(), pre)) return func() return wrapped_func return timefun# 下面的装饰过程# 1. 调用timefun_arg("itcast")# 2. 将步骤1得到的返回值，即time_fun返回， 然后time_fun(foo)# 3. 将time_fun(foo)的结果返回，即wrapped_func# 4. 让foo = wrapped_fun，即foo现在指向wrapped_func@timefun_arg("itcast")def foo(): print("I am foo")@timefun_arg("python")def too(): print("I am too")foo()sleep(2)foo()too()sleep(2)too()可以理解为1foo()==timefun_arg(&quot;itcast&quot;)(foo)()例6：类装饰器（扩展，非重点）装饰器函数其实是这样一个接口约束，它必须接受一个callable对象作为参数，然后返回一个callable对象。在Python中一般callable对象都是函数，但也有例外。只要某个对象重写了 __call__() 方法，那么这个对象就是callable的。123456class Test(): def __call__(self): print('call me!')t = Test()t() # call me类装饰器demo123456789101112131415161718192021222324class Test(object): def __init__(self, func): print("---初始化---") print("func name is %s"%func.__name__) self.__func = func def __call__(self): print("---装饰器中的功能---") self.__func()#说明：#1. 当用Test来装作装饰器对test函数进行装饰的时候，首先会创建Test的实例对象# 并且会把test这个函数名当做参数传递到__init__方法中# 即在__init__方法中的属性__func指向了test指向的函数##2. test指向了用Test创建出来的实例对象##3. 当在使用test()进行调用时，就相当于让这个对象()，因此会调用这个对象的__call__方法##4. 为了能够在__call__方法中调用原来test指向的函数体，所以在__init__方法中就需要一个实例属性来保存这个函数体的引用# 所以才有了self.__func = func这句代码，从而在调用__call__方法中能够调用到test之前的函数体@Testdef test(): print("----test---")test()showpy()#如果把这句话注释，重新运行程序，依然会看到"--初始化--"运行结果如下：1234---初始化---func name is test---装饰器中的功能-------test---]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
</search>
