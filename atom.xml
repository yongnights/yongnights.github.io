<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>个人博客</title>
  
  <subtitle>记录工作中的点点滴滴</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://yongnights.github.io/"/>
  <updated>2020-04-10T10:06:45.966Z</updated>
  <id>https://yongnights.github.io/</id>
  
  <author>
    <name>永夜初晗凝碧天</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Linux日志切割方法[Logrotate、python、shell实现方式]</title>
    <link href="https://yongnights.github.io/2020/04/10/Linux%E6%97%A5%E5%BF%97%E5%88%87%E5%89%B2%E6%96%B9%E6%B3%95%5BLogrotate%E3%80%81python%E3%80%81shell%E5%AE%9E%E7%8E%B0%E6%96%B9%E5%BC%8F%5D/"/>
    <id>https://yongnights.github.io/2020/04/10/Linux日志切割方法[Logrotate、python、shell实现方式]/</id>
    <published>2020-04-10T10:05:58.456Z</published>
    <updated>2020-04-10T10:06:45.966Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Fri Apr 10 2020 18:07:52 GMT+0800 (GMT+08:00) --><p><strong>Linux日志切割方法[Logrotate、python、shell实现方式]</strong></p><p>​ 对于Linux系统安全来说，日志文件是极其重要的工具。不知为何，我发现很多运维同学的服务器上都运行着一些诸如每天切分Nginx日志之类的cron脚本，大家似乎遗忘了Logrotate，争相发明自己的轮子，这真是让人沮丧啊！就好比明明身边躺着现成的性感美女，大家却忙着自娱自乐，罪过！logrotate程序是一个日志文件管理工具。用于分割日志文件，删除旧的日志文件，并创建新的日志文件，起到“转储”作用。可以节省磁盘空间。下面就对logrotate日志轮转操作做一梳理记录。</p><p><strong>1、什么是轮转？</strong></p><p><strong>日志轮循（轮转）：日志轮转，切割，备份，归档</strong></p><h2 id="2、为什么需要轮转？"><a href="#2、为什么需要轮转？" class="headerlink" title="2、为什么需要轮转？"></a><strong>2、为什么需要轮转？</strong></h2><p>☆ 避免日志过大占满/var/log的文件系统</p><p>☆ 方便日志查看 ☆ 将丢弃系统中最旧的日志文件，以节省空间 ☆ 日志轮转的程序是logrotate</p><p>☆ logrotate本身不是系统守护进程，它是通过计划任务crond每天执行</p><a id="more"></a><h2 id="3、安装与配置logrotate"><a href="#3、安装与配置logrotate" class="headerlink" title="3、安装与配置logrotate"></a><strong>3、安装与配置logrotate</strong></h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install logrotate -y</span><br></pre></td></tr></table></figure><p>3.1、配置文件介绍</p><p>Linux系统默认安装logrotate工具，它默认的配置文件在：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span>/etc/logrotate.conf</span><br><span class="line"><span class="meta">#</span>/etc/logrotate.d/</span><br></pre></td></tr></table></figure><p>logrotate.conf 是主要的配置文件，logrotate.d 是一个目录，该目录里的所有文件都会被主动的读入/etc/logrotate.conf中执行。另外，如果 /etc/logrotate.d/ 里面的文件中没有设定一些细节，则会以/etc/logrotate.conf这个文件的设定来作为默认值。</p><p>logrotate是基于cron来运行的，其脚本是/etc/cron.daily/logrotate，日志轮转是系统自动完成的。实际运行时，Logrotate会调用配置文件/etc/logrotate.conf。可以在/etc/logrotate.d目录里放置自定义好的配置文件，用来覆盖Logrotate的缺省值。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[root@huanqiu_web1 ~]# cat /etc/cron.daily/logrotate</span><br><span class="line"><span class="meta">#</span>!/bin/sh</span><br><span class="line"></span><br><span class="line">/usr/sbin/logrotate /etc/logrotate.conf &gt;/dev/null 2&gt;&amp;1</span><br><span class="line">EXITVALUE=$?</span><br><span class="line">if [ $EXITVALUE != 0 ]; then </span><br><span class="line">/usr/bin/logger -t logrotate "ALERT exited abnormally with [$EXITVALUE]"</span><br><span class="line">fi</span><br><span class="line">exit 0</span><br></pre></td></tr></table></figure><p><strong>注意：如果等不及cron自动执行日志轮转，想手动强制切割日志，需要加 -f 参数；不过正式执行前最好通过Debug选项来验证一下（-d参数），这对调试也很重要</strong>！</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span> /usr/sbin/logrotate -f /etc/logrotate.d/nginx</span><br><span class="line"><span class="meta">#</span> /usr/sbin/logrotate -d -f /etc/logrotate.d/nginx</span><br></pre></td></tr></table></figure><p><strong>logrotate命令格式</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">logrotate [OPTION...] &lt;configfile&gt;</span><br><span class="line">-d, --debug ：debug模式，测试配置文件是否有错误。</span><br><span class="line">-f, --force ：强制转储文件。</span><br><span class="line">-m, --mail=command ：压缩日志后，发送日志到指定邮箱。</span><br><span class="line">-s, --state=statefile ：使用指定的状态文件。</span><br><span class="line">-v, --verbose ：显示转储过程。</span><br></pre></td></tr></table></figure><p><strong>根据日志切割设置进行操作，并显示详细信息</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@huanqiu_web1 ~]# /usr/sbin/logrotate -v /etc/logrotate.conf</span><br><span class="line">[root@huanqiu_web1 ~]# /usr/sbin/logrotate -v /etc/logrotate.d/php</span><br></pre></td></tr></table></figure><p><strong>根据日志切割设置进行执行，并显示详细信息,但是不进行具体操作，debug模式</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@huanqiu_web1 ~]# /usr/sbin/logrotate -d /etc/logrotate.conf</span><br><span class="line">[root@huanqiu_web1 ~]# /usr/sbin/logrotate -d /etc/logrotate.d/nginx</span><br></pre></td></tr></table></figure><p><strong>查看各log文件的具体执行情况</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@fangfull_web1 ~]# cat /var/lib/logrotate.status</span><br></pre></td></tr></table></figure><h3 id="3-2、logrotate配置文件"><a href="#3-2、logrotate配置文件" class="headerlink" title="3.2、logrotate配置文件"></a>3.2、logrotate配置文件</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span> vim /etc/logrotate.conf</span><br><span class="line">1 # see "man logrotate" for details</span><br><span class="line">2 # rotate log files weekly</span><br><span class="line">3 weekly</span><br><span class="line">4 # 以7天为一个周期(每周轮转)syslog子配置文件：</span><br><span class="line">5 # keep 4 weeks worth of backlogs</span><br><span class="line">6 rotate 4</span><br><span class="line"><span class="meta">#</span> 一次将存储4个归档日志。对于第五个归档，时间最久的归档将被删除</span><br><span class="line">7 8</span><br><span class="line"><span class="meta">#</span> create new (empty) log files after rotating old ones</span><br><span class="line">9 create</span><br><span class="line"><span class="meta">#</span> 当老的转储文件被归档后,创建一个新的空的转储文件重新记录,权限和原来的转储文件权限一样</span><br><span class="line">10</span><br><span class="line">11 # use date as a suffix of the rotated file</span><br><span class="line">12 dateext</span><br><span class="line"><span class="meta">#</span> 用日期来做轮转之后的文件的后缀名</span><br><span class="line">13</span><br><span class="line">14 # uncomment this if you want your log files compressed</span><br><span class="line">15 #compress</span><br><span class="line"><span class="meta">#</span> 指定不压缩转储文件,如需压缩去掉注释就可以了，主要是通过gzip压缩</span><br><span class="line">16</span><br><span class="line">17 # RPM packages drop log rotation information into this directory</span><br><span class="line">18 include /etc/logrotate.d</span><br><span class="line"><span class="meta">#</span> 加载外部目录</span><br><span class="line">19</span><br><span class="line">20 # no packages own wtmp and btmp -- we'll rotate them here</span><br><span class="line">21 /var/log/wtmp &#123;</span><br><span class="line">22 monthly 表示此文件是每月轮转，而不会用到上面的每周轮转</span><br><span class="line">23 create 0664 root utmp 轮转之后创建新文件，权限是0664，属于root用户和utmp组</span><br><span class="line">24 minsize 1M 文件大于1M，而且周期到了，才会轮转</span><br><span class="line"><span class="meta">#</span> size 1M 文件大小大于1M立马轮转，不管有没有到周期</span><br><span class="line">25 rotate 1 保留1份日志文件，每1个月备份一次日志文件</span><br><span class="line">26 &#125;</span><br><span class="line">27</span><br><span class="line">28 /var/log/btmp &#123;</span><br><span class="line">29 missingok 如果日志文件不存在，不报错</span><br><span class="line">30 monthly</span><br><span class="line">31 create 0600 root utmp</span><br><span class="line">32 rotate 1</span><br><span class="line">33 &#125;</span><br><span class="line">34</span><br><span class="line">35 # system-specific logs may be also be configured here.</span><br></pre></td></tr></table></figure><h3 id="3-3、syslog-子配置文件"><a href="#3-3、syslog-子配置文件" class="headerlink" title="3.3、syslog 子配置文件"></a>3.3、syslog 子配置文件</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">[root@yunwei ~]# cat /etc/logrotate.d/syslog</span><br><span class="line">//这个子配置文件，没有指定的参数都会以默认方式轮转</span><br><span class="line">/var/log/cron</span><br><span class="line">/var/log/maillog</span><br><span class="line">/var/log/messages</span><br><span class="line">/var/log/secure</span><br><span class="line">/var/log/spooler</span><br><span class="line">&#123;</span><br><span class="line">sharedscripts</span><br><span class="line">不管有多少个文件待轮转，prerotate 和 postrotate 代码只执行一次</span><br><span class="line">postrotate日志轮转常见参数：</span><br><span class="line">4、实践：SSH服务日志轮转</span><br><span class="line">要求：</span><br><span class="line">☆ 每天进行轮转，保留5天的日志文件</span><br><span class="line">☆ 日志文件大小大于5M进行轮转，不管是否到轮转周期</span><br><span class="line">思路：</span><br><span class="line">☆ 将ssh服务的日志单独记录 /var/log/ssh.log</span><br><span class="line">☆ 修改logrotate程序的主配置文件或者在/etc/logrotate.d/目录创建一个文件</span><br><span class="line">轮转完后执行postrotate 和 endscript 之间的shell代码</span><br><span class="line">/bin/kill -HUP `cat /var/run/syslogd.pid 2&gt; /dev/null` 2&gt; /dev/null || true</span><br><span class="line">上面这一句话表示轮转后对rsyslog的pid进行刷新（但pid其实不变)</span><br><span class="line">endscript</span><br><span class="line">&#125; </span><br><span class="line">思考：</span><br><span class="line">为什么轮转后需要对rsyslog的pid进行刷新呢？</span><br></pre></td></tr></table></figure><h3 id="3-4、日志轮转常见参数"><a href="#3-4、日志轮转常见参数" class="headerlink" title="3.4、日志轮转常见参数"></a>3.4、日志轮转常见参数</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">常用的指令解释，这些指令都可以在man logrotate 中找得到。</span><br><span class="line">daily 指定转储周期为每天</span><br><span class="line">monthly 指定转储周期为每月</span><br><span class="line">weekly &lt;-- 每周轮转一次(monthly)</span><br><span class="line">rotate 4 &lt;-- 同一个文件最多轮转4次，4次之后就删除该文件</span><br><span class="line">create 0664 root utmp &lt;-- 轮转之后创建新文件，权限是0664，属于root用户和utmp组</span><br><span class="line">dateext &lt;-- 用日期来做轮转之后的文件的后缀名</span><br><span class="line">compress &lt;-- 用gzip对轮转后的日志进行压缩</span><br><span class="line">minsize 30K &lt;-- 文件大于30K，而且周期到了，才会轮转</span><br><span class="line">size 30k &lt;-- 文件必须大于30K才会轮转，而且文件只要大于30K就会轮转不管周期是否已到</span><br><span class="line">missingok &lt;-- 如果日志文件不存在，不报错</span><br><span class="line">notifempty &lt;-- 如果日志文件是空的，不轮转</span><br><span class="line">delaycompress &lt;-- 下一次轮转的时候才压缩</span><br><span class="line">sharedscripts &lt;-- 不管有多少文件待轮转，prerotate和postrotate 代码只执行一次</span><br><span class="line">prerotate &lt;-- 如果符合轮转的条件</span><br><span class="line">则在轮转之前执行prerotate和endscript 之间的shell代码</span><br><span class="line">postrotate &lt;-- 轮转完后执行postrotate 和 endscript 之间的shell代码</span><br></pre></td></tr></table></figure><h3 id="3-5、logrotate默认生效以及相关配置进行解释"><a href="#3-5、logrotate默认生效以及相关配置进行解释" class="headerlink" title="3.5、logrotate默认生效以及相关配置进行解释"></a>3.5、logrotate默认生效以及相关配置进行解释</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br></pre></td><td class="code"><pre><span class="line">Logrotate是基于CRON来运行的，其脚本是/etc/cron.daily/logrotate，</span><br><span class="line">实际运行时，Logrotate会调用配置文件/etc/logrotate.conf。</span><br><span class="line">[root@test ~]# cat /etc/cron.daily/logrotate</span><br><span class="line"><span class="meta">#</span>!/bin/sh</span><br><span class="line"></span><br><span class="line">/usr/sbin/logrotate /etc/logrotate.conf</span><br><span class="line">EXITVALUE=$?</span><br><span class="line">if [ $EXITVALUE != 0 ]; then</span><br><span class="line">    /usr/bin/logger -t logrotate "ALERT exited abnormally with [$EXITVALUE]"</span><br><span class="line">fi</span><br><span class="line">exit 0</span><br><span class="line">   </span><br><span class="line">  </span><br><span class="line">Logrotate是基于CRON运行的，所以这个时间是由CRON控制的，</span><br><span class="line">具体可以查询CRON的配置文件/etc/anacrontab（老版本的文件是/etc/crontab）</span><br><span class="line">[root@test ~]# cat /etc/anacrontab</span><br><span class="line"><span class="meta">#</span> /etc/anacrontab: configuration file for anacron</span><br><span class="line">   </span><br><span class="line"><span class="meta">#</span> See anacron(8) and anacrontab(5) for details.</span><br><span class="line">   </span><br><span class="line">SHELL=/bin/sh</span><br><span class="line">PATH=/sbin:/bin:/usr/sbin:/usr/bin</span><br><span class="line">MAILTO=root</span><br><span class="line"><span class="meta">#</span> the maximal random delay added to the base delay of the jobs</span><br><span class="line">RANDOM_DELAY=45                                                                  </span><br><span class="line">//这个是随机的延迟时间，表示最大45分钟</span><br><span class="line"><span class="meta">#</span> the jobs will be started during the following hours only</span><br><span class="line">START_HOURS_RANGE=3-22                                                          </span><br><span class="line"> //这个是开始时间</span><br><span class="line">   </span><br><span class="line"><span class="meta">#</span>period in days   delay in minutes   job-identifier   command</span><br><span class="line">1 5 cron.daily    nice run-parts /etc/cron.daily</span><br><span class="line">7 25  cron.weekly   nice run-parts /etc/cron.weekly</span><br><span class="line">@monthly 45 cron.monthly    nice run-parts /etc/cron.monthly</span><br><span class="line">   </span><br><span class="line">第一个是Recurrence period</span><br><span class="line">第二个是延迟时间</span><br><span class="line">所以cron.daily会在3:22+(5,45)这个时间段执行，/etc/cron.daily是个文件夹</span><br><span class="line">   </span><br><span class="line">通过默认/etc/anacrontab文件配置，会发现logrotate自动切割日志文件的默认时间是凌晨3点多。</span><br><span class="line">   </span><br><span class="line">=====================================================================================</span><br><span class="line">现在需要将切割时间调整到每天的晚上12点，即每天切割的日志是前一天的0-24点之间的内容。</span><br><span class="line">操作如下：</span><br><span class="line">[root@kevin ~]# mv /etc/anacrontab /etc/anacrontab.bak          //取消日志自动轮转的设置</span><br><span class="line"> </span><br><span class="line">[root@G6-bs02 logrotate.d]# cat nstc_nohup.out</span><br><span class="line">/data/nstc/nohup.out &#123;</span><br><span class="line">rotate 30</span><br><span class="line">dateext</span><br><span class="line">daily</span><br><span class="line">copytruncate</span><br><span class="line">compress</span><br><span class="line">notifempty</span><br><span class="line">missingok</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line">[root@G6-bs02 logrotate.d]# cat syslog</span><br><span class="line">/var/log/cron</span><br><span class="line">/var/log/maillog</span><br><span class="line">/var/log/messages</span><br><span class="line">/var/log/secure</span><br><span class="line">/var/log/history</span><br><span class="line">&#123;</span><br><span class="line">    sharedscripts</span><br><span class="line">    compress</span><br><span class="line">    rotate 30</span><br><span class="line">    daily</span><br><span class="line">    dateext</span><br><span class="line">    postrotate</span><br><span class="line">    /bin/kill -HUP `cat /var/run/syslogd.pid 2&gt; /dev/null` 2&gt; /dev/null || true</span><br><span class="line">    endscript</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line">结合crontab进行自定义的定时轮转操作</span><br><span class="line">[root@kevin ~]# crontab -l</span><br><span class="line"><span class="meta">#</span>log logrotate</span><br><span class="line">59 23 * * * /usr/sbin/logrotate -f /etc/logrotate.d/syslog &gt;/dev/null 2&gt;&amp;1</span><br><span class="line">59 23 * * * /usr/sbin/logrotate -f /etc/logrotate.d/nstc_nohup.out &gt;/dev/null 2&gt;&amp;1</span><br><span class="line"> </span><br><span class="line">[root@G6-bs02 ~]# ll /data/nstc/nohup.out*</span><br><span class="line">-rw------- 1 app app 33218 1月  25 09:43 /data/nstc/nohup.out</span><br><span class="line">-rw------- 1 app app 67678 1月  25 23:59 /data/nstc/nohup.out-20180125.gz</span><br></pre></td></tr></table></figure><h2 id="4、相关案例"><a href="#4、相关案例" class="headerlink" title="4、相关案例"></a><strong>4、相关案例</strong></h2><h3 id="4-1、logrotate实现Nginx日志切割"><a href="#4-1、logrotate实现Nginx日志切割" class="headerlink" title="4.1、logrotate实现Nginx日志切割"></a>4.1、logrotate实现Nginx日志切割</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">[root@master-server ~]# vim /etc/logrotate.d/nginx</span><br><span class="line">/usr/local/nginx/logs/*.log &#123;</span><br><span class="line">daily</span><br><span class="line">rotate 7</span><br><span class="line">missingok</span><br><span class="line">notifempty</span><br><span class="line">dateext</span><br><span class="line">sharedscripts</span><br><span class="line">postrotate</span><br><span class="line">    if [ -f /usr/local/nginx/logs/nginx.pid ]; then</span><br><span class="line">        kill -USR1 `cat /usr/local/nginx/logs/nginx.pid`</span><br><span class="line">    fi</span><br><span class="line">endscript</span><br><span class="line">&#125;</span><br><span class="line">kill -USR1 指的是Nginx平滑重启</span><br></pre></td></tr></table></figure><h3 id="4-2、shell脚本实现Nginx日志切割"><a href="#4-2、shell脚本实现Nginx日志切割" class="headerlink" title="4.2、shell脚本实现Nginx日志切割"></a>4.2、shell脚本实现Nginx日志切割</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">[root@bastion-IDC ~]# vim /usr/local/sbin/logrotate-nginx.sh</span><br><span class="line"><span class="meta">#</span>!/bin/bash</span><br><span class="line"><span class="meta">#</span>创建转储日志压缩存放目录</span><br><span class="line">mkdir -p /data/nginx_logs/days</span><br><span class="line"><span class="meta">#</span>手工对nginx日志进行切割转换</span><br><span class="line">/usr/sbin/logrotate -vf /etc/logrotate.d/nginx</span><br><span class="line"><span class="meta">#</span>当前时间</span><br><span class="line">time=$(date -d "yesterday" +"%Y-%m-%d")</span><br><span class="line"><span class="meta">#</span>进入转储日志存放目录</span><br><span class="line">cd /data/nginx_logs/days</span><br><span class="line"><span class="meta">#</span>对目录中的转储日志文件的文件名进行统一转换</span><br><span class="line">for i in $(ls ./ | grep "^\(.*\)\.[[:digit:]]$")</span><br><span class="line">do</span><br><span class="line">mv $&#123;i&#125; ./$(echo $&#123;i&#125;|sed -n 's/^\(.*\)\.\([[:digit:]]\)$/\1/p')-$(echo $time)</span><br><span class="line">done</span><br><span class="line"><span class="meta">#</span>对转储的日志文件进行压缩存放，并删除原有转储的日志文件，只保存压缩后的日志文件。以节约存储空间</span><br><span class="line">for i in $(ls ./ | grep "^\(.*\)\-\([[:digit:]-]\+\)$")</span><br><span class="line">do</span><br><span class="line">tar jcvf $&#123;i&#125;.bz2 ./$&#123;i&#125;</span><br><span class="line">rm -rf ./$&#123;i&#125;</span><br><span class="line">done</span><br><span class="line"><span class="meta">#</span>只保留最近7天的压缩转储日志文件</span><br><span class="line">find /data/nginx_logs/days/* -name "*.bz2" -mtime 7 -type f -exec rm -rf &#123;&#125; \;</span><br></pre></td></tr></table></figure><p><strong>crontab定时执行</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@bastion-IDC ~# crontab -e</span><br><span class="line"><span class="meta">#</span>logrotate</span><br><span class="line">0 0 * * * /bin/bash -x /usr/local/sbin/logrotate-nginx.sh &gt; /dev/null 2&gt;&amp;1</span><br></pre></td></tr></table></figure><p><strong>手动执行脚本</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@bastion-IDC ~]# /bin/bash -x /usr/local/sbin/logrotate-nginx.sh</span><br><span class="line">[root@bastion-IDC ~]# cd /data/nginx_logs/days</span><br><span class="line">[root@bastion-IDC days]# lshuantest.access_log-2017-01-18.bz2</span><br></pre></td></tr></table></figure><h3 id="4-3、PHP日志切割"><a href="#4-3、PHP日志切割" class="headerlink" title="4.3、PHP日志切割"></a>4.3、PHP日志切割</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">[root@huanqiu_web1 ~]# cat /etc/logrotate.d/php</span><br><span class="line">/Data/logs/php/*log &#123;</span><br><span class="line">    daily</span><br><span class="line">    rotate 365</span><br><span class="line">    missingok</span><br><span class="line">    notifempty</span><br><span class="line">    compress</span><br><span class="line">    dateext</span><br><span class="line">    sharedscripts</span><br><span class="line">    postrotate</span><br><span class="line">        if [ -f /Data/app/php5.6.26/var/run/php-fpm.pid ]; then</span><br><span class="line">            kill -USR1 `cat /Data/app/php5.6.26/var/run/php-fpm.pid`</span><br><span class="line">        fi</span><br><span class="line">    endscript</span><br><span class="line">    postrotate</span><br><span class="line">        /bin/chmod 644 /Data/logs/php/*gz</span><br><span class="line">    endscript</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line">[root@huanqiu_web1 ~]# ll /Data/app/php5.6.26/var/run/php-fpm.pid</span><br><span class="line">-rw-r--r-- 1 root root 4 Dec 28 17:03 /Data/app/php5.6.26/var/run/php-fpm.pid</span><br><span class="line"> </span><br><span class="line">[root@huanqiu_web1 ~]# cd /Data/logs/php</span><br><span class="line">[root@huanqiu_web1 php]# ll</span><br><span class="line">total 25676</span><br><span class="line">-rw-r--r-- 1 root   root         0 Jun  1  2016 error.log</span><br><span class="line">-rw-r--r-- 1 nobody nobody     182 Aug 30  2015 error.log-20150830.gz</span><br><span class="line">-rw-r--r-- 1 nobody nobody     371 Sep  1  2015 error.log-20150901.gz</span><br><span class="line">-rw-r--r-- 1 nobody nobody     315 Sep  7  2015 error.log-20150907.gz</span><br><span class="line">.........</span><br></pre></td></tr></table></figure><h3 id="4-4、linux系统日志切割"><a href="#4-4、linux系统日志切割" class="headerlink" title="4.4、linux系统日志切割"></a>4.4、linux系统日志切割</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">[root@huanqiu_web1 ~]# cat /etc/logrotate.d/syslog</span><br><span class="line">/var/log/cron</span><br><span class="line">/var/log/maillog</span><br><span class="line">/var/log/messages</span><br><span class="line">/var/log/secure</span><br><span class="line">/var/log/spooler</span><br><span class="line">&#123;</span><br><span class="line">    sharedscripts</span><br><span class="line">    postrotate</span><br><span class="line">    /bin/kill -HUP `cat /var/run/syslogd.pid 2&gt; /dev/null` 2&gt; /dev/null || true</span><br><span class="line">    endscript</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line">[root@huanqiu_web1 ~]# ll /var/log/messages*</span><br><span class="line">-rw------- 1 root root 34248975 Jan 19 18:42 /var/log/messages</span><br><span class="line">-rw------- 1 root root 51772994 Dec 25 03:11 /var/log/messages-20161225</span><br><span class="line">-rw------- 1 root root 51800210 Jan  1 03:05 /var/log/messages-20170101</span><br><span class="line">-rw------- 1 root root 51981366 Jan  8 03:36 /var/log/messages-20170108</span><br><span class="line">-rw------- 1 root root 51843025 Jan 15 03:40 /var/log/messages-20170115</span><br><span class="line">[root@huanqiu_web1 ~]# ll /var/log/cron*</span><br><span class="line">-rw------- 1 root root 2155681 Jan 19 18:43 /var/log/cron</span><br><span class="line">-rw------- 1 root root 2932618 Dec 25 03:11 /var/log/cron-20161225</span><br><span class="line">-rw------- 1 root root 2939305 Jan  1 03:06 /var/log/cron-20170101</span><br><span class="line">-rw------- 1 root root 2951820 Jan  8 03:37 /var/log/cron-20170108</span><br><span class="line">-rw------- 1 root root 3203992 Jan 15 03:41 /var/log/cron-20170115</span><br><span class="line">[root@huanqiu_web1 ~]# ll /var/log/secure*</span><br><span class="line">-rw------- 1 root root  275343 Jan 19 18:36 /var/log/secure</span><br><span class="line">-rw------- 1 root root 2111936 Dec 25 03:06 /var/log/secure-20161225</span><br><span class="line">-rw------- 1 root root 2772744 Jan  1 02:57 /var/log/secure-20170101</span><br><span class="line">-rw------- 1 root root 1115543 Jan  8 03:26 /var/log/secure-20170108</span><br><span class="line">-rw------- 1 root root  731599 Jan 15 03:40 /var/log/secure-20170115</span><br><span class="line">[root@huanqiu_web1 ~]# ll /var/log/spooler*</span><br><span class="line">-rw------- 1 root root 0 Jan 15 03:41 /var/log/spooler</span><br><span class="line">-rw------- 1 root root 0 Dec 18 03:21 /var/log/spooler-20161225</span><br><span class="line">-rw------- 1 root root 0 Dec 25 03:11 /var/log/spooler-20170101</span><br><span class="line">-rw------- 1 root root 0 Jan  1 03:06 /var/log/spooler-20170108</span><br><span class="line">-rw------- 1 root root 0 Jan  8 03:37 /var/log/spooler-20170115</span><br></pre></td></tr></table></figure><h3 id="4-5、Tomcat日志切割"><a href="#4-5、Tomcat日志切割" class="headerlink" title="4.5、Tomcat日志切割"></a>4.5、Tomcat日志切割</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">[root@huanqiu_web1 ~]# cat /etc/logrotate.d/syslog</span><br><span class="line">/var/log/cron</span><br><span class="line">/var/log/maillog</span><br><span class="line">/var/log/messages</span><br><span class="line">/var/log/secure</span><br><span class="line">/var/log/spooler</span><br><span class="line">&#123;</span><br><span class="line">    sharedscripts</span><br><span class="line">    postrotate</span><br><span class="line">    /bin/kill -HUP `cat /var/run/syslogd.pid 2&gt; /dev/null` 2&gt; /dev/null || true</span><br><span class="line">    endscript</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line">[root@huanqiu_web1 ~]# ll /var/log/messages*</span><br><span class="line">-rw------- 1 root root 34248975 Jan 19 18:42 /var/log/messages</span><br><span class="line">-rw------- 1 root root 51772994 Dec 25 03:11 /var/log/messages-20161225</span><br><span class="line">-rw------- 1 root root 51800210 Jan  1 03:05 /var/log/messages-20170101</span><br><span class="line">-rw------- 1 root root 51981366 Jan  8 03:36 /var/log/messages-20170108</span><br><span class="line">-rw------- 1 root root 51843025 Jan 15 03:40 /var/log/messages-20170115</span><br><span class="line">[root@huanqiu_web1 ~]# ll /var/log/cron*</span><br><span class="line">-rw------- 1 root root 2155681 Jan 19 18:43 /var/log/cron</span><br><span class="line">-rw------- 1 root root 2932618 Dec 25 03:11 /var/log/cron-20161225</span><br><span class="line">-rw------- 1 root root 2939305 Jan  1 03:06 /var/log/cron-20170101</span><br><span class="line">-rw------- 1 root root 2951820 Jan  8 03:37 /var/log/cron-20170108</span><br><span class="line">-rw------- 1 root root 3203992 Jan 15 03:41 /var/log/cron-20170115</span><br><span class="line">[root@huanqiu_web1 ~]# ll /var/log/secure*</span><br><span class="line">-rw------- 1 root root  275343 Jan 19 18:36 /var/log/secure</span><br><span class="line">-rw------- 1 root root 2111936 Dec 25 03:06 /var/log/secure-20161225</span><br><span class="line">-rw------- 1 root root 2772744 Jan  1 02:57 /var/log/secure-20170101</span><br><span class="line">-rw------- 1 root root 1115543 Jan  8 03:26 /var/log/secure-20170108</span><br><span class="line">-rw------- 1 root root  731599 Jan 15 03:40 /var/log/secure-20170115</span><br><span class="line">[root@huanqiu_web1 ~]# ll /var/log/spooler*</span><br><span class="line">-rw------- 1 root root 0 Jan 15 03:41 /var/log/spooler</span><br><span class="line">-rw------- 1 root root 0 Dec 18 03:21 /var/log/spooler-20161225</span><br><span class="line">-rw------- 1 root root 0 Dec 25 03:11 /var/log/spooler-20170101</span><br><span class="line">-rw------- 1 root root 0 Jan  1 03:06 /var/log/spooler-20170108</span><br><span class="line">-rw------- 1 root root 0 Jan  8 03:37 /var/log/spooler-20170115</span><br></pre></td></tr></table></figure><h3 id="4-6、使用python脚本进行jumpserver日志切割"><a href="#4-6、使用python脚本进行jumpserver日志切割" class="headerlink" title="4.6、使用python脚本进行jumpserver日志切割"></a>4.6、使用python脚本进行jumpserver日志切割</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">[root@test-vm01 mnt]# cat log_rotate.py</span><br><span class="line"><span class="meta">#</span>!/usr/bin/env python</span><br><span class="line">   </span><br><span class="line">import datetime,os,sys,shutil</span><br><span class="line">   </span><br><span class="line">log_path = '/opt/jumpserver/logs/'</span><br><span class="line">log_file = 'jumpserver.log'</span><br><span class="line">   </span><br><span class="line">yesterday = (datetime.datetime.now() - datetime.timedelta(days = 1))</span><br><span class="line">   </span><br><span class="line">try:</span><br><span class="line">    os.makedirs(log_path + yesterday.strftime('%Y') + os.sep + yesterday.strftime('%m'))</span><br><span class="line">except OSError,e:</span><br><span class="line">    print</span><br><span class="line">    print e</span><br><span class="line">    sys.exit()</span><br><span class="line">   </span><br><span class="line">   </span><br><span class="line">shutil.move(log_path + log_file,log_path \</span><br><span class="line">            + yesterday.strftime('%Y') + os.sep \</span><br><span class="line">            + yesterday.strftime('%m') + os.sep \</span><br><span class="line">            + log_file + '_' + yesterday.strftime('%Y%m%d') + '.log')</span><br><span class="line">   </span><br><span class="line">   </span><br><span class="line">os.popen("sudo /opt/jumpserver/service.sh restart")</span><br><span class="line"> </span><br><span class="line">手动执行这个脚本：</span><br><span class="line">[root@test-vm01 mnt]# chmod 755 log_rotate.py</span><br><span class="line">[root@test-vm01 mnt]# python log_rotate.py</span><br><span class="line"> </span><br><span class="line">查看日志切割后的效果：</span><br><span class="line">[root@test-vm01 mnt]# ls /opt/jumpserver/logs/</span><br><span class="line">2017  jumpserver.log </span><br><span class="line">[root@test-vm01 mnt]# ls /opt/jumpserver/logs/2017/</span><br><span class="line">09</span><br><span class="line">[root@test-vm01 mnt]# ls /opt/jumpserver/logs/2017/09/</span><br><span class="line">jumpserver.log_20170916.log</span><br><span class="line"> </span><br><span class="line">然后做每日的定时切割任务：</span><br><span class="line">[root@test-vm01 mnt]# crontab -e</span><br><span class="line">30 1 * * * /usr/bin/python /mnt/log_rotate.py &gt; /dev/null 2&gt;&amp;1</span><br></pre></td></tr></table></figure><h3 id="4-7、使用python脚本进行Nginx日志切割"><a href="#4-7、使用python脚本进行Nginx日志切割" class="headerlink" title="4.7、使用python脚本进行Nginx日志切割"></a>4.7、使用python脚本进行Nginx日志切割</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">[root@test-vm01 mnt]# vim log_rotate.py</span><br><span class="line"><span class="meta">#</span>!/usr/bin/env python</span><br><span class="line">   </span><br><span class="line">import datetime,os,sys,shutil</span><br><span class="line">   </span><br><span class="line">log_path = '/app/nginx/logs/'</span><br><span class="line">log_file = 'www_access.log'</span><br><span class="line">   </span><br><span class="line">yesterday = (datetime.datetime.now() - datetime.timedelta(days = 1))</span><br><span class="line">   </span><br><span class="line">try:</span><br><span class="line">    os.makedirs(log_path + yesterday.strftime('%Y') + os.sep + yesterday.strftime('%m'))</span><br><span class="line">except OSError,e:</span><br><span class="line">    print</span><br><span class="line">    print e</span><br><span class="line">    sys.exit()</span><br><span class="line">   </span><br><span class="line">   </span><br><span class="line">shutil.move(log_path + log_file,log_path \</span><br><span class="line">            + yesterday.strftime('%Y') + os.sep \</span><br><span class="line">            + yesterday.strftime('%m') + os.sep \</span><br><span class="line">            + log_file + '_' + yesterday.strftime('%Y%m%d') + '.log')</span><br><span class="line">   </span><br><span class="line">   </span><br><span class="line">os.popen("sudo kill -USR1 `cat /app/nginx/logs/nginx.pid`")</span><br></pre></td></tr></table></figure><h2 id="5、logrotate无法自动轮询日志的解决办法"><a href="#5、logrotate无法自动轮询日志的解决办法" class="headerlink" title="5、logrotate无法自动轮询日志的解决办法"></a><strong>5、logrotate无法自动轮询日志的解决办法</strong></h2><p>起始原因：使用logrotate轮询nginx日志，配置好之后，发现nginx日志连续两天没被切割，这是为什么呢？？然后开始检查日志切割的配置文件是否有问题，检查后确定配置文件一切正常。于是怀疑是logrotate预定的cron没执行，查看了cron的日志，发现有一条”Dec 7 04:02:01 www crond[18959]: (root) CMD (run-parts /etc/cron.daily)”的日志，证明cron在04:02分时已经执行/etc/cron.daily目录下的程序。接着查看/etc /cron.daily/logrotate（这是logrotate自动轮转的脚本）的内容：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[root@huanqiu_test ~]# cat /etc/cron.daily/logrotate</span><br><span class="line"><span class="meta">#</span>!/bin/sh</span><br><span class="line"></span><br><span class="line">/usr/sbin/logrotate /etc/logrotate.conf &gt;/dev/null 2&gt;&amp;1</span><br><span class="line">EXITVALUE=$?</span><br><span class="line">if [ $EXITVALUE != 0 ]; then</span><br><span class="line">    /usr/bin/logger -t logrotate "ALERT exited abnormally with [$EXITVALUE]"</span><br><span class="line">fi</span><br><span class="line">exit 0</span><br></pre></td></tr></table></figure><p>有发现异常，配置好的日志轮转操作都是由这个脚本完成的，一切运行正常，脚本应该就没问题。 直接执行命令：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@huanqiu_test ~]# /usr/sbin/logrotate /etc/logrotate.conf</span><br></pre></td></tr></table></figure><p>这些系统日志是正常轮询了，但nginx日志却还是没轮询,接着强行启动记录文件维护操作，纵使logrotate指令认为没有需要，应该有可能是logroate认为nginx日志太小，不进行轮询。 故需要强制轮询，即在/etc/cron.daily/logrotate脚本中将 -t 参数替换成 -f 参数</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[root@huanqiu_test ~]# cat /etc/cron.daily/logrotate</span><br><span class="line"><span class="meta">#</span>!/bin/sh</span><br><span class="line"></span><br><span class="line">/usr/sbin/logrotate /etc/logrotate.conf &gt;/dev/null 2&gt;&amp;1</span><br><span class="line">EXITVALUE=$?</span><br><span class="line">if [ $EXITVALUE != 0 ]; then</span><br><span class="line">    /usr/bin/logger -f logrotate "ALERT exited abnormally with [$EXITVALUE]"</span><br><span class="line">fi</span><br><span class="line">exit 0</span><br></pre></td></tr></table></figure><p><strong>重启下cron服务</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@huanqiu_test ~]# /etc/init.d/crond restart</span><br><span class="line">Stopping crond: [ OK ]Starting crond: [ OK ]</span><br></pre></td></tr></table></figure><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Fri Apr 10 2020 18:07:52 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;&lt;strong&gt;Linux日志切割方法[Logrotate、python、shell实现方式]&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;​ 对于Linux系统安全来说，日志文件是极其重要的工具。不知为何，我发现很多运维同学的服务器上都运行着一些诸如每天切分Nginx日志之类的cron脚本，大家似乎遗忘了Logrotate，争相发明自己的轮子，这真是让人沮丧啊！就好比明明身边躺着现成的性感美女，大家却忙着自娱自乐，罪过！logrotate程序是一个日志文件管理工具。用于分割日志文件，删除旧的日志文件，并创建新的日志文件，起到“转储”作用。可以节省磁盘空间。下面就对logrotate日志轮转操作做一梳理记录。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;1、什么是轮转？&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;日志轮循（轮转）：日志轮转，切割，备份，归档&lt;/strong&gt;&lt;/p&gt;&lt;h2 id=&quot;2、为什么需要轮转？&quot;&gt;&lt;a href=&quot;#2、为什么需要轮转？&quot; class=&quot;headerlink&quot; title=&quot;2、为什么需要轮转？&quot;&gt;&lt;/a&gt;&lt;strong&gt;2、为什么需要轮转？&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;☆ 避免日志过大占满/var/log的文件系统&lt;/p&gt;&lt;p&gt;☆ 方便日志查看 ☆ 将丢弃系统中最旧的日志文件，以节省空间 ☆ 日志轮转的程序是logrotate&lt;/p&gt;&lt;p&gt;☆ logrotate本身不是系统守护进程，它是通过计划任务crond每天执行&lt;/p&gt;
    
    </summary>
    
      <category term="shell" scheme="https://yongnights.github.io/categories/shell/"/>
    
      <category term="Linux" scheme="https://yongnights.github.io/categories/shell/Linux/"/>
    
    
      <category term="Linux" scheme="https://yongnights.github.io/tags/Linux/"/>
    
      <category term="shell" scheme="https://yongnights.github.io/tags/shell/"/>
    
  </entry>
  
  <entry>
    <title>使用 Auditbeat 模块监控 shell 命令</title>
    <link href="https://yongnights.github.io/2020/04/03/%E4%BD%BF%E7%94%A8%20Auditbeat%20%E6%A8%A1%E5%9D%97%E7%9B%91%E6%8E%A7%20shell%20%E5%91%BD%E4%BB%A4/"/>
    <id>https://yongnights.github.io/2020/04/03/使用 Auditbeat 模块监控 shell 命令/</id>
    <published>2020-04-03T02:26:23.282Z</published>
    <updated>2020-04-03T02:38:18.649Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Tue Apr 07 2020 18:03:33 GMT+0800 (GMT+08:00) --><blockquote><p>使用 Auditbeat 模块监控 shell 命令<br>Auditbeat Audited 模块可以用来监控所有用户在系统上执行的 shell 命令。在终端用户偶尔才会登录的服务器上，通常需要进行监控。<br>该示例是在 CentOS Linux 7.6 上使用 Auditbeat 7.4.2 RPM 软件包和 Elasticsearch Service（ESS）[<a href="https://www.elastic.co/products/elasticsearch/service]上的" target="_blank" rel="noopener">https://www.elastic.co/products/elasticsearch/service]上的</a> Elastic Stack ] 7.4.2 部署的。</p></blockquote><p><em>可以参考其中的思路，配置流程等，使用本机自建的ES，不使用Elasticsearch Service（ESS）集群</em></p><h1 id="禁用-Auditd"><a href="#禁用-Auditd" class="headerlink" title="禁用 Auditd"></a>禁用 Auditd</h1><p>系统守护进程 auditd 会影响 Auditbeat Audited 模块的正常使用，所以必须将其禁用。<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># 停止 auditd：</span><br><span class="line">service auditd stop</span><br><span class="line"></span><br><span class="line"># 禁用服务：</span><br><span class="line">systemctl disable auditd.service</span><br></pre></td></tr></table></figure><p></p><p>如果您在使用 Auditbeat Auditd 模块的同时也必须要运行 Audited 进程，那么在内核版本为 3.16 或者更高的情况下可以考虑设置 socket_type: multicast 参数。默认值为 unicast。有关此参数的更多信息，请参见文档[<a href="https://www.elastic.co/guide/en/beats/auditbeat/master/auditbeat-module-auditd.html#_configuration_options_14]的配置选项部分。" target="_blank" rel="noopener">https://www.elastic.co/guide/en/beats/auditbeat/master/auditbeat-module-auditd.html#_configuration_options_14]的配置选项部分。</a></p><a id="more"></a><h1 id="配置-Auditbeat"><a href="#配置-Auditbeat" class="headerlink" title="配置 Auditbeat"></a>配置 Auditbeat</h1><p>Auditbeat 守护进程将事件数据发送到一个 Elasticsearch Service（ESS）集群中。有关更多详细信息，请参见文档Auditbeat[<a href="https://www.elastic.co/guide/en/beats/auditbeat/master/configuring-howto-auditbeat.html]" target="_blank" rel="noopener">https://www.elastic.co/guide/en/beats/auditbeat/master/configuring-howto-auditbeat.html]</a><br>中的配置部分。<br>要想获取工作示例，必须配置 Auditbeat 的 cloud.id 和 cloud.auth 参数，详情参见此文档[<a href="https://www.elastic.co/guide/en/beats/auditbeat/master/configure-cloud-id.html]。" target="_blank" rel="noopener">https://www.elastic.co/guide/en/beats/auditbeat/master/configure-cloud-id.html]。</a><br>编辑 /etc/auditbeat/auditbeat.yml：<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cloud.id: &lt;your_cloud_id&gt;</span><br><span class="line">cloud.auth: ingest_user:password</span><br></pre></td></tr></table></figure><p></p><p>如果您要将数据发送到 Elasticsearch 集群（例如本地实例），请参见此文档：[<a href="https://www.elastic.co/guide/en/beats/auditbeat/master/configure-cloud-id.html]。" target="_blank" rel="noopener">https://www.elastic.co/guide/en/beats/auditbeat/master/configure-cloud-id.html]。</a></p><h1 id="Auditbeat-模块规则"><a href="#Auditbeat-模块规则" class="headerlink" title="Auditbeat 模块规则"></a>Auditbeat 模块规则</h1><p>Audited 模块订阅内核以接收系统事件。定义规则以捕获这些事件，并且使用Linux Auditctl 进程所使用的格式，详情参见此文档：[<a href="https://linux.die.net/man/8/auditctl]。" target="_blank" rel="noopener">https://linux.die.net/man/8/auditctl]。</a><br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># cat /etc/auditbeat/audit.rules.d/rules.conf</span><br><span class="line">-a exit,always -F arch=b64 -F euid=0 -S execve -k root_acct</span><br><span class="line">-a exit,always -F arch=b32 -F euid=0 -S execve -k root_acct</span><br><span class="line">-a exit,always -F arch=b64 -F euid&gt;=1000 -S execve -k user_acct</span><br><span class="line">-a exit,always -F arch=b32 -F euid&gt;=1000 -S execve -k user_acct</span><br></pre></td></tr></table></figure><p></p><ul><li>euid 是用户的有效ID。0 代表会获取 root 用户和 uid &gt;=1000 或者权限更高的其他用户的所有活动。</li><li>-k<key>用于为事件分配任意“键”，它将显示在 tags 字段中。它还可以在 Kibana 中用来对事件进行过滤和分类。</key></li></ul><h1 id="Auditbeat-设置命令"><a href="#Auditbeat-设置命令" class="headerlink" title="Auditbeat 设置命令"></a>Auditbeat 设置命令</h1><p>运行Auditbeat 加载索引模板，读取 node pipelines，索引文件周期策略和Kibana 仪表板。<br><code>auditbeat -e setup</code><br>如果您不使用ESS，欢迎参考此文档[<a href="https://www.elastic.co/guide/en/beats/auditbeat/current/setup-kibana-endpoint.html]" target="_blank" rel="noopener">https://www.elastic.co/guide/en/beats/auditbeat/current/setup-kibana-endpoint.html]</a> 来设置您的 Kibana 端点。</p><h1 id="开始使用"><a href="#开始使用" class="headerlink" title="开始使用"></a>开始使用</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">systemctl start auditbeat</span><br><span class="line"></span><br><span class="line"># 列出启用的规则：</span><br><span class="line">auditbeat show auditd-rules</span><br><span class="line">-a never,exit -S all -F pid=23617</span><br><span class="line">-a always,exit -F arch=b64 -S execve -F euid=root -F key=root_acct</span><br><span class="line">-a always,exit -F arch=b32 -S execve -F euid=root -F key=root_acct</span><br><span class="line">-a always,exit -F arch=b64 -S execve -F euid&gt;=vagrant -F key=user_acct</span><br><span class="line">-a always,exit -F arch=b32 -S execve -F euid&gt;=vagrant -F key=user_acct</span><br></pre></td></tr></table></figure><h1 id="监控数据"><a href="#监控数据" class="headerlink" title="监控数据"></a>监控数据</h1><p>当用户执行一些类似于 whoami，ls 以及 lsblk 的 shell 命令时，kibana 中就会发现这些事件。</p><ul><li>Kibana 会显示出 user.name，process.executable，process.args 和 tags 这些选定的字段。</li><li>过滤的字段是 user.name: root 和 auditd.data.syscall: execve。</li><li>每秒刷新一次数据。</li></ul><h1 id="TTY-审计"><a href="#TTY-审计" class="headerlink" title="TTY 审计"></a>TTY 审计</h1><p>当系统中发生 TTY 事件时，Auditbeat Audited 模块也可以接收它们。配置system-auth PAM 配置文件以启用 TTY。只有 root 用户的 TTY 事件将被实时记录。其他用户的事件通常会被缓冲直到 exit。TTY 审计会捕获系统内置命令像pwd，test 等。<br>追加以下内容到 /etc/pam.d/system-auth 便可以对所有用户启用审核，关于 pam_tty_audit 的详细信息，参见此文档：[<a href="https://linux.die.net/man/8/pam_tty_audit]。" target="_blank" rel="noopener">https://linux.die.net/man/8/pam_tty_audit]。</a><br><code>session required pam_tty_audit.so enable=*</code></p><h1 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ sudo su -</span><br><span class="line">Last login: Fri Nov 22 23:43:00 UTC 2019 on pts/0</span><br><span class="line">$ helllloooo there!</span><br><span class="line">-bash: helllloooo: command not found</span><br><span class="line">$ exit</span><br></pre></td></tr></table></figure><h1 id="Kibana-发现"><a href="#Kibana-发现" class="headerlink" title="Kibana 发现"></a>Kibana 发现</h1><h1 id="思考"><a href="#思考" class="headerlink" title="思考"></a>思考</h1><p>Auditbeat 还可以做什么：</p><ul><li>当一个文件在磁盘上更改（创建，更新或删除）时可以发送事件，得益于 file_integrity 模块，详情参考此文档：[<a href="https://www.elastic.co/guide/en/beats/auditbeat/current/auditbeat-module-file_integrity.html]。" target="_blank" rel="noopener">https://www.elastic.co/guide/en/beats/auditbeat/current/auditbeat-module-file_integrity.html]。</a></li><li>通过 system 模块发送有关系统的指标，详情参考此文档：[<a href="https://www.elastic.co/guide/en/beats/auditbeat/current/auditbeat-module-system.html]。" target="_blank" rel="noopener">https://www.elastic.co/guide/en/beats/auditbeat/current/auditbeat-module-system.html]。</a><br>该链接还提供了 Auditbeat 的相关文档，详情参考此文档：[<a href="https://www.elastic.co/guide/en/beats/auditbeat/current/index.html]。" target="_blank" rel="noopener">https://www.elastic.co/guide/en/beats/auditbeat/current/index.html]。</a></li></ul><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Tue Apr 07 2020 18:03:33 GMT+0800 (GMT+08:00) --&gt;&lt;blockquote&gt;&lt;p&gt;使用 Auditbeat 模块监控 shell 命令&lt;br&gt;Auditbeat Audited 模块可以用来监控所有用户在系统上执行的 shell 命令。在终端用户偶尔才会登录的服务器上，通常需要进行监控。&lt;br&gt;该示例是在 CentOS Linux 7.6 上使用 Auditbeat 7.4.2 RPM 软件包和 Elasticsearch Service（ESS）[&lt;a href=&quot;https://www.elastic.co/products/elasticsearch/service]上的&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://www.elastic.co/products/elasticsearch/service]上的&lt;/a&gt; Elastic Stack ] 7.4.2 部署的。&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;&lt;em&gt;可以参考其中的思路，配置流程等，使用本机自建的ES，不使用Elasticsearch Service（ESS）集群&lt;/em&gt;&lt;/p&gt;&lt;h1 id=&quot;禁用-Auditd&quot;&gt;&lt;a href=&quot;#禁用-Auditd&quot; class=&quot;headerlink&quot; title=&quot;禁用 Auditd&quot;&gt;&lt;/a&gt;禁用 Auditd&lt;/h1&gt;&lt;p&gt;系统守护进程 auditd 会影响 Auditbeat Audited 模块的正常使用，所以必须将其禁用。&lt;br&gt;&lt;/p&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;# 停止 auditd：&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;service auditd stop&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;# 禁用服务：&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;systemctl disable auditd.service&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;如果您在使用 Auditbeat Auditd 模块的同时也必须要运行 Audited 进程，那么在内核版本为 3.16 或者更高的情况下可以考虑设置 socket_type: multicast 参数。默认值为 unicast。有关此参数的更多信息，请参见文档[&lt;a href=&quot;https://www.elastic.co/guide/en/beats/auditbeat/master/auditbeat-module-auditd.html#_configuration_options_14]的配置选项部分。&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://www.elastic.co/guide/en/beats/auditbeat/master/auditbeat-module-auditd.html#_configuration_options_14]的配置选项部分。&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="elk" scheme="https://yongnights.github.io/categories/elk/"/>
    
      <category term="Auditbeat" scheme="https://yongnights.github.io/categories/elk/Auditbeat/"/>
    
      <category term="shell" scheme="https://yongnights.github.io/categories/elk/Auditbeat/shell/"/>
    
    
      <category term="elk" scheme="https://yongnights.github.io/tags/elk/"/>
    
      <category term="Auditbeat" scheme="https://yongnights.github.io/tags/Auditbeat/"/>
    
      <category term="shell" scheme="https://yongnights.github.io/tags/shell/"/>
    
  </entry>
  
  <entry>
    <title>Elasticsearch：如何对PDF文件进行搜索</title>
    <link href="https://yongnights.github.io/2020/04/03/Elasticsearch%EF%BC%9A%E5%A6%82%E4%BD%95%E5%AF%B9PDF%E6%96%87%E4%BB%B6%E8%BF%9B%E8%A1%8C%E6%90%9C%E7%B4%A2/"/>
    <id>https://yongnights.github.io/2020/04/03/Elasticsearch：如何对PDF文件进行搜索/</id>
    <published>2020-04-03T01:57:15.620Z</published>
    <updated>2020-04-03T02:25:31.514Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Tue Apr 07 2020 18:03:33 GMT+0800 (GMT+08:00) --><blockquote><p>Elasticsearch 通常用于字符串，数字，日期等数据类型的检索，但是在 HCM、ERP 和电子商务等应用程序中经常存在对办公文档进行搜索的需求。今天的这篇文章中我们来讲一下如何实现 PDF、DOC、XLS 等办公文件的搜索，本解决方案适用于 Elasticsearch 5.0 以后的版本。</p></blockquote><h1 id="实现原理"><a href="#实现原理" class="headerlink" title="实现原理"></a>实现原理</h1><p>首先把我们的 .pdf 文件进行 Base64 处理，然后上传到 Elasticsearch 中的 ingest node 中进行处理。我们可以通过 Ingest attachment plugin 来使得 Elasticsearch 提取通用格式的文件附件比如 PPT、XLS及PDF。最终，数据进入到 Elasticsearch 的 data node 中以便让我们进行搜索。</p><h1 id="导入PDF文件到Elasticsearch中"><a href="#导入PDF文件到Elasticsearch中" class="headerlink" title="导入PDF文件到Elasticsearch中"></a>导入PDF文件到Elasticsearch中</h1><h2 id="准备PDF文件"><a href="#准备PDF文件" class="headerlink" title="准备PDF文件"></a>准备PDF文件</h2><p>我们可以使用 Word 或其它编辑软件来生产一个 PDF 文件，暂且我们叫这个文件的名字为 sample.pdf，而它的内容非常简单，在 sample.pdf 文件中，我们只有一句话：“I like this useful tool”。</p><h2 id="安装-Ingest-attachment-plugin"><a href="#安装-Ingest-attachment-plugin" class="headerlink" title="安装 Ingest attachment plugin"></a>安装 Ingest attachment plugin</h2><p>Ingest attachment plugin 允许 Elasticsearch 通过使用 Apache 文本提取库 Tika 提取通用格式（例如：PPT，XLS 和 PDF）的文件附件。Apache Tika 工具包可从一千多种不同的文件类型中检测并提取元数据和文本。所有这些文件类型都可以通过一个界面进行解析，从而使 Tika 对搜索引擎索引，内容分析，翻译等有用。<br>需要注意的是，源字段必须是 Base64 编码的二进制，如果不想增加在 Base64 之间来回转换的开销，则可以使用 CBOR 格式而不是 JSON，并将字段指定为字节数组而不是字符串表示形式，这样处理器将跳过 Base64 解码。<br>可以使用插件管理器安装此插件，该插件必须安装在集群中的每个节点上，并且每个节点必须在安装后重新启动。<br><code>sudo bin/elasticsearch-plugin install ingest-attachment</code><br>等我们安装好这个插件后，我们可以通过如下的命令来查看该插件是否已经被成功安装好了:<br><code>./bin/elasticsearch-plugin list</code></p><a id="more"></a><h2 id="创建-attachment-pipeline"><a href="#创建-attachment-pipeline" class="headerlink" title="创建 attachment pipeline"></a>创建 attachment pipeline</h2><p>在我们的 ingest node 上创建一个叫做 pdfattachment 的 pipleline：<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">PUT _ingest/pipeline/pdfattachment</span><br><span class="line">&#123;</span><br><span class="line">  &quot;description&quot;: &quot;Extract attachment information encoded in Base64 with UTF-8 charset&quot;,</span><br><span class="line">  &quot;processors&quot;: [</span><br><span class="line">    &#123;</span><br><span class="line">      &quot;attachment&quot;: &#123;</span><br><span class="line">        &quot;field&quot;: &quot;file&quot;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><h2 id="转换并上传PDF文件的内容到Elasticsearch中"><a href="#转换并上传PDF文件的内容到Elasticsearch中" class="headerlink" title="转换并上传PDF文件的内容到Elasticsearch中"></a>转换并上传PDF文件的内容到Elasticsearch中</h2><p>对于 Ingest attachment plugin 来说，它的数据必须是 Base64 的。我们可以在网站Base64 encoder 来进行转换，我们可以直接通过下面的脚本来进行操作：<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">!/bin/bash</span><br><span class="line"></span><br><span class="line">encodedPdf=`cat sample.pdf | base64`</span><br><span class="line"></span><br><span class="line">json=&quot;&#123;\&quot;file\&quot;:\&quot;$&#123;encodedPdf&#125;\&quot;&#125;&quot;</span><br><span class="line"></span><br><span class="line">echo &quot;$json&quot; &gt; json.file</span><br><span class="line"></span><br><span class="line">curl -XPOST &apos;http://localhost:9200/pdf-test1/_doc?pipeline=pdfattachment&amp;pretty&apos; -H &apos;Content-Type: application/json&apos; -d @json.file</span><br></pre></td></tr></table></figure><p></p><p>在上面的脚本中，我们针对 sample.pdf 进行 Base64 的转换，并生成一个叫做 json.file 的文件。在最后，我们把这个 json.file 文件的内容通过 curl 指令上传到 Elasticsearch 中，我们可以在 Elasticsearch 中查看一个叫做 pdf-test1 的索引。</p><h1 id="查看索引并搜索"><a href="#查看索引并搜索" class="headerlink" title="查看索引并搜索"></a>查看索引并搜索</h1><p>可以通过如下的命令来查询 pdf-test1 索引：<br><code>GET pdf-test1/_search</code><br>可以看出来，我们的索引中有一个叫做 content 的字段，它包含了我们的 pdf 文件的内容，这个字段可以同我们进行搜索。在上面我们也看到了一个很大的一个字段 file，它含有我们转换过的 Base64 格式的内容。如果我们不想要这个字段，我们可以通过添加另外一个 remove processor 来除去这个字段：<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">PUT _ingest/pipeline/pdfattachment</span><br><span class="line">&#123;</span><br><span class="line">  &quot;description&quot;: &quot;Extract attachment information encoded in Base64 with UTF-8 charset&quot;,</span><br><span class="line">  &quot;processors&quot;: [</span><br><span class="line">    &#123;</span><br><span class="line">      &quot;attachment&quot;: &#123;</span><br><span class="line">        &quot;field&quot;: &quot;file&quot;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">      &quot;remove&quot;: &#123;</span><br><span class="line">        &quot;field&quot;: &quot;file&quot;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Tue Apr 07 2020 18:03:33 GMT+0800 (GMT+08:00) --&gt;&lt;blockquote&gt;&lt;p&gt;Elasticsearch 通常用于字符串，数字，日期等数据类型的检索，但是在 HCM、ERP 和电子商务等应用程序中经常存在对办公文档进行搜索的需求。今天的这篇文章中我们来讲一下如何实现 PDF、DOC、XLS 等办公文件的搜索，本解决方案适用于 Elasticsearch 5.0 以后的版本。&lt;/p&gt;&lt;/blockquote&gt;&lt;h1 id=&quot;实现原理&quot;&gt;&lt;a href=&quot;#实现原理&quot; class=&quot;headerlink&quot; title=&quot;实现原理&quot;&gt;&lt;/a&gt;实现原理&lt;/h1&gt;&lt;p&gt;首先把我们的 .pdf 文件进行 Base64 处理，然后上传到 Elasticsearch 中的 ingest node 中进行处理。我们可以通过 Ingest attachment plugin 来使得 Elasticsearch 提取通用格式的文件附件比如 PPT、XLS及PDF。最终，数据进入到 Elasticsearch 的 data node 中以便让我们进行搜索。&lt;/p&gt;&lt;h1 id=&quot;导入PDF文件到Elasticsearch中&quot;&gt;&lt;a href=&quot;#导入PDF文件到Elasticsearch中&quot; class=&quot;headerlink&quot; title=&quot;导入PDF文件到Elasticsearch中&quot;&gt;&lt;/a&gt;导入PDF文件到Elasticsearch中&lt;/h1&gt;&lt;h2 id=&quot;准备PDF文件&quot;&gt;&lt;a href=&quot;#准备PDF文件&quot; class=&quot;headerlink&quot; title=&quot;准备PDF文件&quot;&gt;&lt;/a&gt;准备PDF文件&lt;/h2&gt;&lt;p&gt;我们可以使用 Word 或其它编辑软件来生产一个 PDF 文件，暂且我们叫这个文件的名字为 sample.pdf，而它的内容非常简单，在 sample.pdf 文件中，我们只有一句话：“I like this useful tool”。&lt;/p&gt;&lt;h2 id=&quot;安装-Ingest-attachment-plugin&quot;&gt;&lt;a href=&quot;#安装-Ingest-attachment-plugin&quot; class=&quot;headerlink&quot; title=&quot;安装 Ingest attachment plugin&quot;&gt;&lt;/a&gt;安装 Ingest attachment plugin&lt;/h2&gt;&lt;p&gt;Ingest attachment plugin 允许 Elasticsearch 通过使用 Apache 文本提取库 Tika 提取通用格式（例如：PPT，XLS 和 PDF）的文件附件。Apache Tika 工具包可从一千多种不同的文件类型中检测并提取元数据和文本。所有这些文件类型都可以通过一个界面进行解析，从而使 Tika 对搜索引擎索引，内容分析，翻译等有用。&lt;br&gt;需要注意的是，源字段必须是 Base64 编码的二进制，如果不想增加在 Base64 之间来回转换的开销，则可以使用 CBOR 格式而不是 JSON，并将字段指定为字节数组而不是字符串表示形式，这样处理器将跳过 Base64 解码。&lt;br&gt;可以使用插件管理器安装此插件，该插件必须安装在集群中的每个节点上，并且每个节点必须在安装后重新启动。&lt;br&gt;&lt;code&gt;sudo bin/elasticsearch-plugin install ingest-attachment&lt;/code&gt;&lt;br&gt;等我们安装好这个插件后，我们可以通过如下的命令来查看该插件是否已经被成功安装好了:&lt;br&gt;&lt;code&gt;./bin/elasticsearch-plugin list&lt;/code&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="elk" scheme="https://yongnights.github.io/categories/elk/"/>
    
      <category term="Elasticsearch" scheme="https://yongnights.github.io/categories/elk/Elasticsearch/"/>
    
    
      <category term="elk" scheme="https://yongnights.github.io/tags/elk/"/>
    
      <category term="Elasticsearch" scheme="https://yongnights.github.io/tags/Elasticsearch/"/>
    
  </entry>
  
  <entry>
    <title>Elasticsearch索引和查询性能调优的21条建议</title>
    <link href="https://yongnights.github.io/2020/04/03/Elasticsearch%E7%B4%A2%E5%BC%95%E5%92%8C%E6%9F%A5%E8%AF%A2%E6%80%A7%E8%83%BD%E8%B0%83%E4%BC%98%E7%9A%8421%E6%9D%A1%E5%BB%BA%E8%AE%AE/"/>
    <id>https://yongnights.github.io/2020/04/03/Elasticsearch索引和查询性能调优的21条建议/</id>
    <published>2020-04-03T01:30:32.227Z</published>
    <updated>2020-04-03T01:49:18.635Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Tue Apr 07 2020 18:03:33 GMT+0800 (GMT+08:00) --><h1 id="Elasticsearch部署建议"><a href="#Elasticsearch部署建议" class="headerlink" title="Elasticsearch部署建议"></a>Elasticsearch部署建议</h1><h2 id="1-选择合理的硬件配置：尽可能使用-SSD"><a href="#1-选择合理的硬件配置：尽可能使用-SSD" class="headerlink" title="1. 选择合理的硬件配置：尽可能使用 SSD"></a>1. 选择合理的硬件配置：尽可能使用 SSD</h2><p>Elasticsearch 最大的瓶颈往往是磁盘读写性能，尤其是随机读取性能。使用SSD（PCI-E接口SSD卡/SATA接口SSD盘）通常比机械硬盘（SATA盘/SAS盘）查询速度快5~10倍，写入性能提升不明显。<br>对于文档检索类查询性能要求较高的场景，建议考虑 SSD 作为存储，同时按照 1:10 的比例配置内存和硬盘。对于日志分析类查询并发要求较低的场景，可以考虑采用机械硬盘作为存储，同时按照 1:50 的比例配置内存和硬盘。单节点存储数据建议在2TB以内，不要超过5TB，避免查询速度慢、系统不稳定。</p><h2 id="2-给JVM配置机器一半的内存，但是不建议超过32G"><a href="#2-给JVM配置机器一半的内存，但是不建议超过32G" class="headerlink" title="2. 给JVM配置机器一半的内存，但是不建议超过32G"></a>2. 给JVM配置机器一半的内存，但是不建议超过32G</h2><p>修改 conf/jvm.options 配置，-Xms 和 -Xmx 设置为相同的值，推荐设置为机器内存的一半左右，剩余一半留给操作系统缓存使用。JVM 内存建议不要低于 2G，否则有可能因为内存不足导致 ES 无法正常启动或内存溢出，JVM 建议不要超过 32G，否则 JVM 会禁用内存对象指针压缩技术，造成内存浪费。机器内存大于 64G 内存时，推荐配置 -Xms30g -Xmx30g。JVM 堆内存较大时，内存垃圾回收暂停时间比较长，建议配置 ZGC 或 G1 垃圾回收算法。</p><h2 id="3-规模较大的集群配置专有主节点，避免脑裂问题"><a href="#3-规模较大的集群配置专有主节点，避免脑裂问题" class="headerlink" title="3. 规模较大的集群配置专有主节点，避免脑裂问题"></a>3. 规模较大的集群配置专有主节点，避免脑裂问题</h2><p>Elasticsearch 主节点负责集群元信息管理、index 的增删操作、节点的加入剔除，定期将最新的集群状态广播至各个节点。在集群规模较大时，建议配置专有主节点只负责集群管理，不存储数据，不承担数据读写压力。<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"># 专有主节点配置(conf/elasticsearch.yml)：</span><br><span class="line">node.master:true</span><br><span class="line">node.data: false</span><br><span class="line">node.ingest:false</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 数据节点配置(conf/elasticsearch.yml)：</span><br><span class="line">node.master:false</span><br><span class="line">node.data:true</span><br><span class="line">node.ingest:true</span><br></pre></td></tr></table></figure><p></p><p>Elasticsearch 默认每个节点既是候选主节点，又是数据节点。最小主节点数量参数 minimum_master_nodes 推荐配置为候选主节点数量一半以上，该配置告诉 Elasticsearch 当没有足够的 master 候选节点的时候，不进行 master 节点选举，等 master 节点足够了才进行选举。<br>例如对于 3 节点集群，最小主节点数量从默认值 1 改为 2。<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># 最小主节点数量配置(conf/elasticsearch.yml)：</span><br><span class="line">discovery.zen.minimum_master_nodes: 2</span><br></pre></td></tr></table></figure><p></p><a id="more"></a><h2 id="4-Linux操作系统调优"><a href="#4-Linux操作系统调优" class="headerlink" title="4. Linux操作系统调优"></a>4. Linux操作系统调优</h2><p>关闭交换分区，防止内存置换降低性能。<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"># 将/etc/fstab 文件中包含swap的行注释掉</span><br><span class="line">sed -i &apos;/swap/s/^/#/&apos; /etc/fstab</span><br><span class="line">swapoff -a</span><br><span class="line"></span><br><span class="line"># 单用户可以打开的最大文件数量，可以设置为官方推荐的65536或更大些</span><br><span class="line">echo &quot;* - nofile 655360&quot; &gt;&gt; /etc/security/limits.conf</span><br><span class="line"></span><br><span class="line"># 单用户线程数调大</span><br><span class="line">echo &quot;* - nproc 131072&quot; &gt;&gt; /etc/security/limits.conf</span><br><span class="line"></span><br><span class="line"># 单进程可以使用的最大map内存区域数量</span><br><span class="line">echo &quot;vm.max_map_count = 655360&quot; &gt;&gt; /etc/sysctl.conf</span><br><span class="line"></span><br><span class="line"># 参数修改立即生效</span><br><span class="line">sysctl -p</span><br></pre></td></tr></table></figure><p></p><h1 id="索引性能调优建议"><a href="#索引性能调优建议" class="headerlink" title="索引性能调优建议"></a>索引性能调优建议</h1><h2 id="1-设置合理的索引分片数和副本数"><a href="#1-设置合理的索引分片数和副本数" class="headerlink" title="1. 设置合理的索引分片数和副本数"></a>1. 设置合理的索引分片数和副本数</h2><p>索引分片数建议设置为集群节点的整数倍，初始数据导入时副本数设置为 0，生产环境副本数建议设置为 1（设置 1 个副本，集群任意 1 个节点宕机数据不会丢失；设置更多副本会占用更多存储空间，操作系统缓存命中率会下降，检索性能不一定提升）。单节点索引分片数建议不要超过 3 个，每个索引分片推荐 10-40GB 大小，索引分片数设置后不可以修改，副本数设置后可以修改。<br>Elasticsearch6.X 及之前的版本默认索引分片数为 5、副本数为 1，从 Elasticsearch7.0 开始调整为默认索引分片数为 1、副本数为 1。<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"># 索引设置</span><br><span class="line">curl -XPUT http://localhost:9200/fulltext001?pretty -H &apos;Content-Type: application/json&apos;   </span><br><span class="line">-d &apos;&#123;</span><br><span class="line">    &quot;settings&quot;: &#123;</span><br><span class="line">        &quot;refresh_interval&quot;: &quot;30s&quot;,</span><br><span class="line">        &quot;merge.policy.max_merged_segment&quot;: &quot;1000mb&quot;,</span><br><span class="line">        &quot;translog.durability&quot;: &quot;async&quot;,</span><br><span class="line">        &quot;translog.flush_threshold_size&quot;: &quot;2gb&quot;,</span><br><span class="line">        &quot;translog.sync_interval&quot;: &quot;100s&quot;,</span><br><span class="line">        &quot;index&quot;: &#123;</span><br><span class="line">            &quot;number_of_shards&quot;: &quot;21&quot;,</span><br><span class="line">            &quot;number_of_replicas&quot;: &quot;0&quot;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;&apos;</span><br><span class="line"></span><br><span class="line"># mapping 设置</span><br><span class="line">curl -XPOST http://localhost:9200/fulltext001/doc/_mapping?pretty  -H &apos;Content-Type: application/json&apos; </span><br><span class="line">-d &apos;&#123;</span><br><span class="line">    &quot;doc&quot;: &#123;</span><br><span class="line">        &quot;_all&quot;: &#123;</span><br><span class="line">            &quot;enabled&quot;: false</span><br><span class="line">        &#125;,</span><br><span class="line">        &quot;properties&quot;: &#123;</span><br><span class="line">            &quot;content&quot;: &#123;</span><br><span class="line">                &quot;type&quot;: &quot;text&quot;,</span><br><span class="line">                &quot;analyzer&quot;: &quot;ik_max_word&quot;</span><br><span class="line">            &#125;,</span><br><span class="line">            &quot;id&quot;: &#123;</span><br><span class="line">                &quot;type&quot;: &quot;keyword&quot;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;&apos;</span><br><span class="line"></span><br><span class="line"># 写入数据示例</span><br><span class="line">curl -XPUT &apos;http://localhost:9200/fulltext001/doc/1?pretty&apos; -H &apos;Content-Type: application/json&apos; </span><br><span class="line">-d &apos;&#123;</span><br><span class="line">    &quot;id&quot;: &quot;https://www.huxiu.com/article/215169.html&quot;,</span><br><span class="line">    &quot;content&quot;: &quot;“娃娃机，迷你KTV，VR体验馆，堪称商场三大标配‘神器’。”一家地处商业中心的大型综合体负责人告诉懂懂笔记，在过去的这几个月里，几乎所有的综合体都“标配”了这三种“设备”…&quot;</span><br><span class="line">&#125;&apos;</span><br><span class="line"></span><br><span class="line"># 修改副本数示例</span><br><span class="line">curl -XPUT &quot;http://localhost:9200/fulltext001/_settings&quot; -H &apos;Content-Type: application/json&apos; </span><br><span class="line">-d &apos;&#123;</span><br><span class="line">    &quot;number_of_replicas&quot;: 1</span><br><span class="line">&#125;&apos;</span><br></pre></td></tr></table></figure><p></p><h2 id="2-使用批量请求"><a href="#2-使用批量请求" class="headerlink" title="2. 使用批量请求"></a>2. 使用批量请求</h2><p>使用批量请求将产生比单文档索引请求好得多的性能。写入数据时调用批量提交接口，推荐每批量提交 5~15MB 数据。例如单条记录 1KB 大小，每批次提交 10000 条左右记录写入性能较优；单条记录 5KB 大小，每批次提交 2000 条左右记录写入性能较优。<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># 批量请求接口API</span><br><span class="line">curl -XPOST &quot;http://localhost:9200/_bulk&quot; -H &apos;Content-Type: application/json&apos; </span><br><span class="line">-d&apos;</span><br><span class="line">&#123; &quot;index&quot; : &#123; &quot;_index&quot; : &quot;test&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;1&quot; &#125; &#125;&#123; &quot;field1&quot; : &quot;value1&quot; &#125;</span><br><span class="line">&#123; &quot;delete&quot; : &#123; &quot;_index&quot; : &quot;test&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;2&quot; &#125; &#125;</span><br><span class="line">&#123; &quot;create&quot; : &#123; &quot;_index&quot; : &quot;test&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_id&quot; : &quot;3&quot; &#125; &#125;&#123; &quot;field1&quot; : &quot;value3&quot; &#125;</span><br><span class="line">&#123; &quot;update&quot; : &#123;&quot;_id&quot; : &quot;1&quot;, &quot;_type&quot; : &quot;_doc&quot;, &quot;_index&quot; : &quot;test&quot;&#125; &#125;&#123; &quot;doc&quot; : &#123;&quot;field2&quot; : &quot;value2&quot;&#125; &#125;&apos;</span><br></pre></td></tr></table></figure><p></p><h2 id="3-通过多进程-线程发送数据"><a href="#3-通过多进程-线程发送数据" class="headerlink" title="3. 通过多进程/线程发送数据"></a>3. 通过多进程/线程发送数据</h2><p>单线程批量写入数据往往不能充分利用服务器 CPU 资源，可以尝试调整写入线程数或者在多个客户端上同时向 Elasticsearch 服务器提交写入请求。与批量调整大小请求类似，只有测试才能确定最佳的 worker 数量。可以通过逐渐增加工作任务数量来测试，直到集群上的 I/O 或 CPU 饱和。</p><h2 id="4-调大refresh-interval"><a href="#4-调大refresh-interval" class="headerlink" title="4. 调大refresh interval"></a>4. 调大refresh interval</h2><p>在 Elasticsearch 中，写入和打开一个新段的轻量的过程叫做 refresh 。 默认情况下每个分片会每秒自动刷新一次。这就是为什么我们说 Elasticsearch 是近实时搜索: 文档的变化并不是立即对搜索可见，但会在一秒之内变为可见。<br>并不是所有的情况都需要每秒刷新。可能你正在使用 Elasticsearch 索引大量的日志文件，你可能想优化索引速度而不是近实时搜索，可以通过设置 refresh_interval，降低每个索引的刷新频率。<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># 设置 refresh interval API</span><br><span class="line">curl -XPUT &quot;http://localhost:9200/index&quot; -H &apos;Content-Type: application/json&apos; </span><br><span class="line">-d&apos;&#123;</span><br><span class="line">    &quot;settings&quot;: &#123;</span><br><span class="line">        &quot;refresh_interval&quot;: &quot;30s&quot;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;&apos;</span><br></pre></td></tr></table></figure><p></p><p>refresh_interval 可以在已经存在的索引上进行动态更新，在生产环境中，当你正在建立一个大的新索引时，可以先关闭自动刷新，待开始使用该索引时，再把它们调回来。<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">curl -XPUT &quot;http://localhost:9200/index/_settings&quot; -H &apos;Content-Type: application/json&apos; </span><br><span class="line">-d&apos;&#123; &quot;refresh_interval&quot;: -1 &#125;&apos;</span><br><span class="line"></span><br><span class="line">curl -XPUT &quot;http://localhost:9200/index/_settings&quot; -H &apos;Content-Type: application/json&apos; </span><br><span class="line">-d&apos;&#123; &quot;refresh_interval&quot;: &quot;1s&quot; &#125;&apos;</span><br></pre></td></tr></table></figure><p></p><h2 id="5-配置事务日志参数"><a href="#5-配置事务日志参数" class="headerlink" title="5. 配置事务日志参数"></a>5. 配置事务日志参数</h2><p>事务日志 translog 用于防止节点失败时的数据丢失。它的设计目的是帮助 shard 恢复操作，否则数据可能会从内存 flush 到磁盘时发生意外而丢失。事务日志 translog 的落盘(fsync)是 ES 在后台自动执行的，默认每 5 秒钟提交到磁盘上，或者当 translog 文件大小大于 512MB 提交，或者在每个成功的索引、删除、更新或批量请求时提交。<br>索引创建时，可以调整默认日志刷新间隔 5 秒，例如改为 60 秒，index.translog.sync_interval: “60s”。创建索引后，可以动态调整 translog 参数，”index.translog.durability”:”async” 相当于关闭了 index、bulk 等操作的同步 flush translog 操作，仅使用默认的定时刷新、文件大小阈值刷新的机制。<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># 动态设置 translog API</span><br><span class="line">curl -XPUT &quot;http://localhost:9200/index&quot; -H &apos;Content-Type: application/json&apos; </span><br><span class="line">-d&apos;&#123;</span><br><span class="line">    &quot;settings&quot;: &#123;</span><br><span class="line">        &quot;index.translog.durability&quot;: &quot;async&quot;,</span><br><span class="line">        &quot;translog.flush_threshold_size&quot;: &quot;2gb&quot;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;&apos;</span><br></pre></td></tr></table></figure><p></p><h2 id="6-设计mapping配置合适的字段类型"><a href="#6-设计mapping配置合适的字段类型" class="headerlink" title="6. 设计mapping配置合适的字段类型"></a>6. 设计mapping配置合适的字段类型</h2><p>Elasticsearch 在写入文档时，如果请求中指定的索引名不存在，会自动创建新索引，并根据文档内容猜测可能的字段类型。但这往往不是最高效的，我们可以根据应用场景来设计合理的字段类型。<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># 例如写入一条记录</span><br><span class="line">curl -XPUT &quot;http://localhost:9200/twitter/doc/1?pretty&quot; -H &apos;Content-Type: application/json&apos; </span><br><span class="line">-d&apos;&#123;</span><br><span class="line">    &quot;user&quot;: &quot;kimchy&quot;,</span><br><span class="line">    &quot;post_date&quot;: &quot;2009-11-15T13:12:00&quot;,</span><br><span class="line">    &quot;message&quot;: &quot;Trying out Elasticsearch, so far so good?&quot;</span><br><span class="line">&#125;&apos;</span><br></pre></td></tr></table></figure><p></p><p>查询 Elasticsearch 自动创建的索引 mapping，会发现将 post_date 字段自动识别为 date 类型，但是 message 和 user 字段被设置为 text、keyword 冗余字段，造成写入速度降低、占用更多磁盘空间。<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    &quot;twitter&quot;: &#123;</span><br><span class="line">        &quot;mappings&quot;: &#123;</span><br><span class="line">            &quot;doc&quot;: &#123;</span><br><span class="line">                &quot;properties&quot;: &#123;</span><br><span class="line">                    &quot;message&quot;: &#123;</span><br><span class="line">                        &quot;type&quot;: &quot;text&quot;,</span><br><span class="line">                        &quot;fields&quot;: &#123;</span><br><span class="line">                            &quot;keyword&quot;: &#123;</span><br><span class="line">                                &quot;type&quot;: &quot;keyword&quot;,</span><br><span class="line">                                &quot;ignore_above&quot;: 256</span><br><span class="line">                            &#125;</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;,</span><br><span class="line">                    &quot;post_date&quot;: &#123;</span><br><span class="line">                        &quot;type&quot;: &quot;date&quot;</span><br><span class="line">                    &#125;,</span><br><span class="line">                    &quot;user&quot;: &#123;</span><br><span class="line">                        &quot;type&quot;: &quot;text&quot;,</span><br><span class="line">                        &quot;fields&quot;: &#123;</span><br><span class="line">                            &quot;keyword&quot;: &#123;</span><br><span class="line">                                &quot;type&quot;: &quot;keyword&quot;,</span><br><span class="line">                                &quot;ignore_above&quot;: 256</span><br><span class="line">                            &#125;</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;,</span><br><span class="line">        &quot;settings&quot;: &#123;</span><br><span class="line">            &quot;index&quot;: &#123;</span><br><span class="line">                &quot;number_of_shards&quot;: &quot;5&quot;,</span><br><span class="line">                &quot;number_of_replicas&quot;: &quot;1&quot;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><p>根据业务场景设计索引配置合理的分片数、副本数，设置字段类型、分词器。如果不需要合并全部字段，禁用 _all 字段，通过 copy_to 来合并字段。<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">curl -XPUT &quot;http://localhost:9200/twitter?pretty&quot; -H &apos;Content-Type: application/json&apos; </span><br><span class="line">-d&apos;&#123;</span><br><span class="line">    &quot;settings&quot;: &#123;</span><br><span class="line">        &quot;index&quot;: &#123;</span><br><span class="line">            &quot;number_of_shards&quot;: &quot;20&quot;,</span><br><span class="line">            &quot;number_of_replicas&quot;: &quot;0&quot;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;&apos;</span><br><span class="line"></span><br><span class="line">curl -XPOST &quot;http://localhost:9200/twitter/doc/_mapping?pretty&quot; -H &apos;Content-Type: application/json&apos; </span><br><span class="line">-d&apos;&#123;</span><br><span class="line">    &quot;doc&quot;: &#123;</span><br><span class="line">        &quot;_all&quot;: &#123;</span><br><span class="line">            &quot;enabled&quot;: false</span><br><span class="line">        &#125;,</span><br><span class="line">        &quot;properties&quot;: &#123;</span><br><span class="line">            &quot;user&quot;: &#123;</span><br><span class="line">                &quot;type&quot;: &quot;keyword&quot;</span><br><span class="line">            &#125;,</span><br><span class="line">            &quot;post_date&quot;: &#123;</span><br><span class="line">                &quot;type&quot;: &quot;date&quot;</span><br><span class="line">            &#125;,</span><br><span class="line">            &quot;message&quot;: &#123;</span><br><span class="line">                &quot;type&quot;: &quot;text&quot;,</span><br><span class="line">                &quot;analyzer&quot;: &quot;cjk&quot;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;&apos;</span><br></pre></td></tr></table></figure><p></p><h1 id="查询性能调优建议"><a href="#查询性能调优建议" class="headerlink" title="查询性能调优建议"></a>查询性能调优建议</h1><h2 id="1-使用过滤器缓存和分片查询缓存"><a href="#1-使用过滤器缓存和分片查询缓存" class="headerlink" title="1. 使用过滤器缓存和分片查询缓存"></a>1. 使用过滤器缓存和分片查询缓存</h2><p>默认情况下，Elasticsearch 的查询会计算返回的每条数据与查询语句的相关度，但对于非全文索引的使用场景，用户并不关心查询结果与查询条件的相关度，只是想精确地查找目标数据。此时，可以通过 filter 来让 Elasticsearch 不计算评分，并且尽可能地缓存 filter 的结果集，供后续包含相同 filter 的查询使用，提高查询效率。<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"># 普通查询</span><br><span class="line">curl -XGET &quot;http://localhost:9200/twitter/_search&quot; -H &apos;Content-Type: application/json&apos; </span><br><span class="line">-d&apos;&#123;</span><br><span class="line">    &quot;query&quot;: &#123;</span><br><span class="line">        &quot;match&quot;: &#123;</span><br><span class="line">            &quot;user&quot;: &quot;kimchy&quot;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;&apos;</span><br><span class="line"></span><br><span class="line"># 过滤器(filter)查询</span><br><span class="line">curl -XGET &quot;http://localhost:9200/twitter/_search&quot; -H &apos;Content-Type: application/json&apos; </span><br><span class="line">-d&apos;&#123;</span><br><span class="line">    &quot;query&quot;: &#123;</span><br><span class="line">        &quot;bool&quot;: &#123;</span><br><span class="line">            &quot;filter&quot;: &#123;</span><br><span class="line">                &quot;match&quot;: &#123;</span><br><span class="line">                    &quot;user&quot;: &quot;kimchy&quot;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;&apos;</span><br></pre></td></tr></table></figure><p></p><p>分片查询缓存的目的是缓存聚合、提示词结果和命中数（它不会缓存返回的文档，因此，它只在 search_type=count 时起作用）。<br>通过下面的参数我们可以设置分片缓存的大小，默认情况下是 JVM 堆的 1% 大小，当然我们也可以手动设置在 config/elasticsearch.yml 文件里。<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">indices.requests.cache.size: 1%</span><br></pre></td></tr></table></figure><p></p><p>查看缓存占用内存情况(name 表示节点名, query_cache 表示过滤器缓存，request_cache 表示分片缓存，fielddata 表示字段数据缓存，segments 表示索引段)。<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl -XGET &quot;http://localhost:9200/_cat/nodes?h=name,query_cache.memory_size,request_cache.memory_size,fielddata.memory_size,segments.memory&amp;v&quot;</span><br></pre></td></tr></table></figure><p></p><h2 id="2-使用路由-routing"><a href="#2-使用路由-routing" class="headerlink" title="2. 使用路由 routing"></a>2. 使用路由 routing</h2><p>Elasticsearch写入文档时，文档会通过一个公式路由到一个索引中的一个分片上。默认的公式如下：<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">shard_num = hash(_routing) % num_primary_shards</span><br></pre></td></tr></table></figure><p></p><p><code>_routing</code> 字段的取值，默认是 <code>_id</code> 字段，可以根据业务场景设置经常查询的字段作为路由字段。例如可以考虑将用户 id、地区作为路由字段，查询时可以过滤不必要的分片，加快查询速度。<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"># 写入时指定路由</span><br><span class="line">curl -XPUT &quot;http://localhost:9200/my_index/my_type/1?routing=user1&quot; -H &apos;Content-Type: application/json&apos; </span><br><span class="line">-d&apos;&#123;</span><br><span class="line">    &quot;title&quot;: &quot;This is a document&quot;,</span><br><span class="line">    &quot;author&quot;: &quot;user1&quot;</span><br><span class="line">&#125;&apos;</span><br><span class="line"></span><br><span class="line"># 查询时不指定路由，需要查询所有分片</span><br><span class="line">curl -XGET &quot;http://localhost:9200/my_index/_search&quot; -H &apos;Content-Type: application/json&apos; </span><br><span class="line">-d&apos;&#123;</span><br><span class="line">    &quot;query&quot;: &#123;</span><br><span class="line">        &quot;match&quot;: &#123;</span><br><span class="line">            &quot;title&quot;: &quot;document&quot;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;&apos;</span><br><span class="line"></span><br><span class="line"># 返回结果</span><br><span class="line">&#123;</span><br><span class="line">    &quot;took&quot;: 2,</span><br><span class="line">    &quot;timed_out&quot;: false,</span><br><span class="line">    &quot;_shards&quot;: &#123;</span><br><span class="line">        &quot;total&quot;: 5,</span><br><span class="line">        &quot;successful&quot;: 5,</span><br><span class="line">        &quot;skipped&quot;: 0,</span><br><span class="line">        &quot;failed&quot;: 0</span><br><span class="line">    &#125;</span><br><span class="line">    ... ...</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"># 查询时指定路由，只需要查询1个分片</span><br><span class="line">curl -XGET &quot;http://localhost:9200/my_index/_search?routing=user1&quot; -H &apos;Content-Type: application/json&apos; </span><br><span class="line">-d&apos;&#123;</span><br><span class="line">    &quot;query&quot;: &#123;</span><br><span class="line">        &quot;match&quot;: &#123;</span><br><span class="line">            &quot;title&quot;: &quot;document&quot;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;&apos;</span><br><span class="line"></span><br><span class="line"># 返回结果</span><br><span class="line">&#123;</span><br><span class="line">    &quot;took&quot;: 1,</span><br><span class="line">    &quot;timed_out&quot;: false,</span><br><span class="line">    &quot;_shards&quot;: &#123;</span><br><span class="line">        &quot;total&quot;: 1,</span><br><span class="line">        &quot;successful&quot;: 1,</span><br><span class="line">        &quot;skipped&quot;: 0,</span><br><span class="line">        &quot;failed&quot;: 0</span><br><span class="line">    &#125;</span><br><span class="line">    ... ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><h2 id="3-强制合并只读索引，关闭历史数据索引"><a href="#3-强制合并只读索引，关闭历史数据索引" class="headerlink" title="3. 强制合并只读索引，关闭历史数据索引"></a>3. 强制合并只读索引，关闭历史数据索引</h2><p>只读索引可以从合并成一个单独的大 segment 中收益，减少索引碎片，减少 JVM 堆常驻内存。强制合并索引操作会耗费大量磁盘 IO，尽量配置在业务低峰期(例如凌晨)执行。历史数据索引如果业务上不再支持查询请求，可以考虑关闭索引，减少 JVM 内存占用。<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># 索引forcemerge API</span><br><span class="line">curl -XPOST &quot;http://localhost:9200/abc20180923/_forcemerge?max_num_segments=1&quot;</span><br><span class="line"></span><br><span class="line"># 索引关闭API</span><br><span class="line">curl -XPOST &quot;http://localhost:9200/abc2017*/_close&quot;</span><br></pre></td></tr></table></figure><p></p><h2 id="4-配置合适的分词器"><a href="#4-配置合适的分词器" class="headerlink" title="4. 配置合适的分词器"></a>4. 配置合适的分词器</h2><p>Elasticsearch 内置了很多分词器，包括 standard、cjk、nGram 等，也可以安装自研/开源分词器。根据业务场景选择合适的分词器，避免全部采用默认 standard 分词器。</p><p>常用分词器：</p><ul><li>standard：默认分词，英文按空格切分，中文按照单个汉字切分。</li><li>cjk：根据二元索引对中日韩文分词，可以保证查全率。</li><li>nGram：可以将英文按照字母切分，结合ES的短语搜索(match_phrase)使用。</li><li>IK：比较热门的中文分词，能按照中文语义切分，可以自定义词典。</li><li>pinyin：可以让用户输入拼音，就能查找到相关的关键词。</li><li>aliws：阿里巴巴自研分词，支持多种模型和分词算法，词库丰富，分词结果准确，适用于电商等对查准要求高的场景。</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># 分词效果测试API</span><br><span class="line">curl -XPOST &quot;http://localhost:9200/_analyze&quot; -H &apos;Content-Type: application/json&apos; </span><br><span class="line">-d&apos;&#123;</span><br><span class="line">    &quot;analyzer&quot;: &quot;ik_max_word&quot;,</span><br><span class="line">    &quot;text&quot;: &quot;南京市长江大桥&quot;</span><br><span class="line">&#125;&apos;</span><br></pre></td></tr></table></figure><h2 id="5-配置查询聚合节点"><a href="#5-配置查询聚合节点" class="headerlink" title="5. 配置查询聚合节点"></a>5. 配置查询聚合节点</h2><p>查询聚合节点可以发送粒子查询请求到其他节点，收集和合并结果，以及响应发出查询的客户端。通过给查询聚合节点配置更高规格的 CPU 和内存，可以加快查询运算速度、提升缓存命中率。<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># 查询聚合节点配置(conf/elasticsearch.yml)：</span><br><span class="line">node.master:false</span><br><span class="line">node.data:false</span><br><span class="line">node.ingest:false</span><br></pre></td></tr></table></figure><p></p><h2 id="6-设置查询读取记录条数和字段"><a href="#6-设置查询读取记录条数和字段" class="headerlink" title="6. 设置查询读取记录条数和字段"></a>6. 设置查询读取记录条数和字段</h2><p>默认的查询请求通常返回排序后的前 10 条记录，最多一次读取 10000 条记录，通过 from 和 size 参数控制读取记录范围，避免一次读取过多的记录。通过 _source 参数可以控制返回字段信息，尽量避免读取大字段。<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"># 查询请求示例</span><br><span class="line">curl -XGET http://localhost:9200/fulltext001/_search?pretty  -H &apos;Content-Type: application/json&apos; </span><br><span class="line">-d &apos;&#123;</span><br><span class="line">    &quot;from&quot;: 0,</span><br><span class="line">    &quot;size&quot;: 10,</span><br><span class="line">    &quot;_source&quot;: &quot;id&quot;,</span><br><span class="line">    &quot;query&quot;: &#123;</span><br><span class="line">        &quot;bool&quot;: &#123;</span><br><span class="line">            &quot;must&quot;: [</span><br><span class="line">                &#123;</span><br><span class="line">                    &quot;match&quot;: &#123;</span><br><span class="line">                        &quot;content&quot;: &quot;虎嗅&quot;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            ]</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;,</span><br><span class="line">    &quot;sort&quot;: [</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;id&quot;: &#123;</span><br><span class="line">                &quot;order&quot;: &quot;asc&quot;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    ]</span><br><span class="line">&#125;&apos;</span><br></pre></td></tr></table></figure><p></p><h2 id="7-设置-teminate-after-查询快速返回"><a href="#7-设置-teminate-after-查询快速返回" class="headerlink" title="7. 设置 teminate_after 查询快速返回"></a>7. 设置 teminate_after 查询快速返回</h2><p>如果不需要精确统计查询命中记录条数，可以配 teminate_after 指定每个 shard 最多匹配 N 条记录后返回，设置查询超时时间 timeout。在查询结果中可以通过 “terminated_early” 字段标识是否提前结束查询请求。<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"># teminate_after 查询语法示例</span><br><span class="line">curl -XGET &quot;http://localhost:9200/twitter/_search&quot; -H &apos;Content-Type: application/json&apos; </span><br><span class="line">-d&apos;&#123;</span><br><span class="line">    &quot;from&quot;: 0,</span><br><span class="line">    &quot;size&quot;: 10,</span><br><span class="line">    &quot;timeout&quot;: &quot;10s&quot;,</span><br><span class="line">    &quot;terminate_after&quot;: 1000,</span><br><span class="line">    &quot;query&quot;: &#123;</span><br><span class="line">        &quot;bool&quot;: &#123;</span><br><span class="line">            &quot;filter&quot;: &#123;</span><br><span class="line">                &quot;term&quot;: &#123;</span><br><span class="line">                    &quot;user&quot;: &quot;elastic&quot;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;&apos;</span><br></pre></td></tr></table></figure><p></p><h2 id="8-避免查询深度翻页"><a href="#8-避免查询深度翻页" class="headerlink" title="8. 避免查询深度翻页"></a>8. 避免查询深度翻页</h2><p>Elasticsearch 默认只允许查看排序前 10000 条的结果，当翻页查看排序靠后的记录时，响应耗时一般较长。使用 search_after 方式查询会更轻量级，如果每次只需要返回 10 条结果，则每个 shard 只需要返回 search_after 之后的 10 个结果即可，返回的总数据量只是和 shard 个数以及本次需要的个数有关，和历史已读取的个数无关。<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"># search_after查询语法示例</span><br><span class="line">curl -XGET &quot;http://localhost:9200/twitter/_search&quot; -H &apos;Content-Type: application/json&apos; </span><br><span class="line">-d&apos;&#123;</span><br><span class="line">    &quot;size&quot;: 10,</span><br><span class="line">    &quot;query&quot;: &#123;</span><br><span class="line">        &quot;match&quot;: &#123;</span><br><span class="line">            &quot;message&quot;: &quot;Elasticsearch&quot;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;,</span><br><span class="line">    &quot;sort&quot;: [</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;_score&quot;: &#123;</span><br><span class="line">                &quot;order&quot;: &quot;desc&quot;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;_id&quot;: &#123;</span><br><span class="line">                &quot;order&quot;: &quot;asc&quot;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    ],</span><br><span class="line">    &quot;search_after&quot;: [</span><br><span class="line">        0.84290016,     //上一次response中某个doc的score</span><br><span class="line">        &quot;1024&quot;          //上一次response中某个doc的id</span><br><span class="line">    ]</span><br><span class="line">&#125;&apos;</span><br></pre></td></tr></table></figure><p></p><h2 id="9-避免前缀模糊匹配"><a href="#9-避免前缀模糊匹配" class="headerlink" title="9. 避免前缀模糊匹配"></a>9. 避免前缀模糊匹配</h2><p>Elasticsearch 默认支持通过 <em>? 正则表达式来做模糊匹配，如果在一个数据量较大规模的索引上执行模糊匹配，尤其是前缀模糊匹配，通常耗时会比较长，甚至可能导致内存溢出。尽量避免在高并发查询请求的生产环境执行这类操作。<br>某客户需要对车牌号进行模糊查询，通过查询请求 “车牌号:</em>A8848*” 查询时，往往导致整个集群负载较高。通过对数据预处理，增加冗余字段 “车牌号.keyword”，并事先将所有车牌号按照1元、2元、3元…7元分词后存储至该字段，字段存储内容示例：沪,A,8,4,沪A,A8,88,84,48,沪A8…沪A88488。通过查询”车牌号.keyword:A8848”即可解决原来的性能问题。</p><h2 id="10-避免索引稀疏"><a href="#10-避免索引稀疏" class="headerlink" title="10. 避免索引稀疏"></a>10. 避免索引稀疏</h2><p>Elasticsearch6.X 之前的版本默认允许在一个 index 下面创建多个 type，Elasticsearch6.X 版本只允许创建一个 type，Elasticsearch7.X 版本只允许 type 值为 “_doc”。在一个索引下面创建多个字段不一样的 type，或者将几百个字段不一样的索引合并到一个索引中，会导致索引稀疏问题。<br>建议每个索引下只创建一个 type，字段不一样的数据分别独立创建 index，不要合并成一个大索引。每个查询请求根据需要去读取相应的索引，避免查询大索引扫描全部记录，加快查询速度。</p><h2 id="11-扩容集群节点个数，升级节点规格"><a href="#11-扩容集群节点个数，升级节点规格" class="headerlink" title="11. 扩容集群节点个数，升级节点规格"></a>11. 扩容集群节点个数，升级节点规格</h2><p>通常服务器节点数越多，服务器硬件配置规格越高，Elasticsearch 集群的处理能力越强。</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Tue Apr 07 2020 18:03:33 GMT+0800 (GMT+08:00) --&gt;&lt;h1 id=&quot;Elasticsearch部署建议&quot;&gt;&lt;a href=&quot;#Elasticsearch部署建议&quot; class=&quot;headerlink&quot; title=&quot;Elasticsearch部署建议&quot;&gt;&lt;/a&gt;Elasticsearch部署建议&lt;/h1&gt;&lt;h2 id=&quot;1-选择合理的硬件配置：尽可能使用-SSD&quot;&gt;&lt;a href=&quot;#1-选择合理的硬件配置：尽可能使用-SSD&quot; class=&quot;headerlink&quot; title=&quot;1. 选择合理的硬件配置：尽可能使用 SSD&quot;&gt;&lt;/a&gt;1. 选择合理的硬件配置：尽可能使用 SSD&lt;/h2&gt;&lt;p&gt;Elasticsearch 最大的瓶颈往往是磁盘读写性能，尤其是随机读取性能。使用SSD（PCI-E接口SSD卡/SATA接口SSD盘）通常比机械硬盘（SATA盘/SAS盘）查询速度快5~10倍，写入性能提升不明显。&lt;br&gt;对于文档检索类查询性能要求较高的场景，建议考虑 SSD 作为存储，同时按照 1:10 的比例配置内存和硬盘。对于日志分析类查询并发要求较低的场景，可以考虑采用机械硬盘作为存储，同时按照 1:50 的比例配置内存和硬盘。单节点存储数据建议在2TB以内，不要超过5TB，避免查询速度慢、系统不稳定。&lt;/p&gt;&lt;h2 id=&quot;2-给JVM配置机器一半的内存，但是不建议超过32G&quot;&gt;&lt;a href=&quot;#2-给JVM配置机器一半的内存，但是不建议超过32G&quot; class=&quot;headerlink&quot; title=&quot;2. 给JVM配置机器一半的内存，但是不建议超过32G&quot;&gt;&lt;/a&gt;2. 给JVM配置机器一半的内存，但是不建议超过32G&lt;/h2&gt;&lt;p&gt;修改 conf/jvm.options 配置，-Xms 和 -Xmx 设置为相同的值，推荐设置为机器内存的一半左右，剩余一半留给操作系统缓存使用。JVM 内存建议不要低于 2G，否则有可能因为内存不足导致 ES 无法正常启动或内存溢出，JVM 建议不要超过 32G，否则 JVM 会禁用内存对象指针压缩技术，造成内存浪费。机器内存大于 64G 内存时，推荐配置 -Xms30g -Xmx30g。JVM 堆内存较大时，内存垃圾回收暂停时间比较长，建议配置 ZGC 或 G1 垃圾回收算法。&lt;/p&gt;&lt;h2 id=&quot;3-规模较大的集群配置专有主节点，避免脑裂问题&quot;&gt;&lt;a href=&quot;#3-规模较大的集群配置专有主节点，避免脑裂问题&quot; class=&quot;headerlink&quot; title=&quot;3. 规模较大的集群配置专有主节点，避免脑裂问题&quot;&gt;&lt;/a&gt;3. 规模较大的集群配置专有主节点，避免脑裂问题&lt;/h2&gt;&lt;p&gt;Elasticsearch 主节点负责集群元信息管理、index 的增删操作、节点的加入剔除，定期将最新的集群状态广播至各个节点。在集群规模较大时，建议配置专有主节点只负责集群管理，不存储数据，不承担数据读写压力。&lt;br&gt;&lt;/p&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;# 专有主节点配置(conf/elasticsearch.yml)：&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;node.master:true&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;node.data: false&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;node.ingest:false&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;# 数据节点配置(conf/elasticsearch.yml)：&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;node.master:false&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;node.data:true&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;node.ingest:true&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;Elasticsearch 默认每个节点既是候选主节点，又是数据节点。最小主节点数量参数 minimum_master_nodes 推荐配置为候选主节点数量一半以上，该配置告诉 Elasticsearch 当没有足够的 master 候选节点的时候，不进行 master 节点选举，等 master 节点足够了才进行选举。&lt;br&gt;例如对于 3 节点集群，最小主节点数量从默认值 1 改为 2。&lt;br&gt;&lt;/p&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;# 最小主节点数量配置(conf/elasticsearch.yml)：&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;discovery.zen.minimum_master_nodes: 2&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;p&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="elk" scheme="https://yongnights.github.io/categories/elk/"/>
    
      <category term="Elasticsearch" scheme="https://yongnights.github.io/categories/elk/Elasticsearch/"/>
    
    
      <category term="elk" scheme="https://yongnights.github.io/tags/elk/"/>
    
      <category term="Elasticsearch" scheme="https://yongnights.github.io/tags/Elasticsearch/"/>
    
  </entry>
  
  <entry>
    <title>Logstash集成GaussDB(高斯DB)数据到Elasticsearch</title>
    <link href="https://yongnights.github.io/2020/04/03/Logstash%E9%9B%86%E6%88%90GaussDB(%E9%AB%98%E6%96%AFDB)%E6%95%B0%E6%8D%AE%E5%88%B0Elasticsearch/"/>
    <id>https://yongnights.github.io/2020/04/03/Logstash集成GaussDB(高斯DB)数据到Elasticsearch/</id>
    <published>2020-04-03T01:17:06.856Z</published>
    <updated>2020-04-03T01:30:57.339Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Tue Apr 07 2020 18:03:33 GMT+0800 (GMT+08:00) --><h1 id="GaussDB-简介"><a href="#GaussDB-简介" class="headerlink" title="GaussDB 简介"></a>GaussDB 简介</h1><p>GaussDB 数据库分为 GaussDB T 和 GaussDB A，分别面向 OLTP 和 OLAP 的业务用户。<br>GaussDB T 数据库是华为公司全自研的分布式数据库，支持x86和华为鲲鹏硬件架构。基于创新性数据库内核，提供高并发事务实时处理能力、两地三中心金融级高可用能力和分布式高扩展能力。<br>GaussDB A 是一款具备分析及混合负载能力的分布式数据库，支持x86和华为鲲鹏硬件架构，支持行存储与列存储，提供PB级数据分析能力、多模分析能力和实时处理能力，用于数据仓库、数据集市、实时分析、实时决策和混合负载等场景，广泛应用于金融、政府、电信等行业核心系统。</p><h1 id="Logstash-的-jdbc-input-plugin"><a href="#Logstash-的-jdbc-input-plugin" class="headerlink" title="Logstash 的 jdbc input plugin"></a>Logstash 的 jdbc input plugin</h1><p>参考 Logstash的 Jdbc input plugin 的官方文档，该插件可以通过JDBC接口将任何数据库中的数据导入 Logstash。周期性加载或一次加载，每一行是一个 event，列转成 filed。我们先解读下文档里提到的重要配置项。<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">jdbc_driver_library：JDBC驱动包路径。</span><br><span class="line">jdbc_driver_class：JDBC驱动程序类。</span><br><span class="line">jdbc_connection_string：JDBC连接串。</span><br><span class="line">jdbc_user：数据库用户名。</span><br><span class="line">jdbc_password：数据库用户口令。</span><br><span class="line">statement_filepath：SQL语句所在文件路径。</span><br><span class="line">scheduler：调度计划。</span><br></pre></td></tr></table></figure><p></p><a id="more"></a><p>以上参数已经支持了周期性加载或一次性加载。如果想按字段的自增列或时间戳来集成数据，还需要以下参数：<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">sql_last_value：这个参数内置在sql语句里。作为条件的变量值。</span><br><span class="line">last_run_metadata_path：sql_last_value 上次运行值所在的文件路径。</span><br><span class="line">use_column_value：设置为时true时，将定义的 tracking_column 值用作 :sql_last_value。默认false。</span><br><span class="line">tracking_column：值设置为将被跟踪的列。</span><br><span class="line">tracking_column_type：跟踪列的类型。目前仅支持数字和时间戳。</span><br><span class="line">record_last_run：上次运行 sql_last_value 值是否保存到 last_run_metadata_path。默认true。</span><br><span class="line">clean_run：是否应保留先前的运行状态。默认false。</span><br></pre></td></tr></table></figure><p></p><p>另外如果想使用预编译语句，语句里用？作为占位符，再增加以下参数：<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">use_prepared_statements：设置为 true 时，启用预编译语句。</span><br><span class="line">prepared_statement_name：预编译语句名称。</span><br><span class="line">prepared_statement_bind_values：数组类型，存放绑定值。:sql_last_value 可以作为预定义参数。</span><br></pre></td></tr></table></figure><p></p><p>参考：<a href="https://www.elastic.co/guide/en/logstash/7.5/plugins-inputs-jdbc.html" target="_blank" rel="noopener">https://www.elastic.co/guide/en/logstash/7.5/plugins-inputs-jdbc.html</a></p><h1 id="对接-GaussDB-T"><a href="#对接-GaussDB-T" class="headerlink" title="对接 GaussDB T"></a>对接 GaussDB T</h1><p>按每分钟一次频率的周期性来加载 GaussDB T 的会话信息到 Elasticsearch 中，input 区域的配置如下：<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">input &#123;</span><br><span class="line">    jdbc &#123;</span><br><span class="line">      jdbc_connection_string =&gt; &quot;jdbc:zenith:@vip:40000&quot;</span><br><span class="line">      jdbc_user =&gt; &quot;omm&quot;</span><br><span class="line">      jdbc_password =&gt; &quot;omm_password&quot;</span><br><span class="line">      jdbc_driver_library =&gt; &quot;/opt/gs/com.huawei.gauss.jdbc.ZenithDriver-GaussDB_100_1.0.1.SPC2.B003.jar&quot;</span><br><span class="line">      jdbc_driver_class =&gt; &quot;com.huawei.gauss.jdbc.ZenithDriver&quot;</span><br><span class="line">      statement_filepath =&gt; &quot;/opt/statement_filepath/gs_100_session.sql&quot;</span><br><span class="line">      schedule =&gt; &quot;*/1 * * * *&quot;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><p>statement_filepath 路径文件里配置的sql如下：<br></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> dv_sessions</span><br></pre></td></tr></table></figure><p></p><p>启动 logstash，可以看到logstash 日志中显示有<code>select * from dv_sessions</code>的信息</p><h1 id="对接-GaussDB-A"><a href="#对接-GaussDB-A" class="headerlink" title="对接 GaussDB A"></a>对接 GaussDB A</h1><p>按字段的时间戳来增量加载数据，注意 GaussDB A 的驱动和 GaussDB T 是不同的。input 区域的配置如下：<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">input &#123;</span><br><span class="line">    jdbc &#123;</span><br><span class="line">      jdbc_connection_string =&gt; &quot;jdbc:postgresql://vip:25308/postgres&quot;</span><br><span class="line">      jdbc_user =&gt; &quot;monitor&quot;</span><br><span class="line">      jdbc_password =&gt; &quot;monitor_password&quot;</span><br><span class="line">      jdbc_driver_library =&gt; &quot;/opt/gsdriver/gsjdbc4.jar&quot;</span><br><span class="line">      jdbc_driver_class =&gt; &quot;org.postgresql.Driver&quot;</span><br><span class="line">      statement_filepath =&gt; &quot;/opt/statement_filepath/gauss_active_session.sql&quot;</span><br><span class="line">      schedule =&gt; &quot;*/1 * * * *&quot;</span><br><span class="line">      record_last_run =&gt; &quot;true&quot;</span><br><span class="line">      use_column_value =&gt; &quot;true&quot;</span><br><span class="line">      tracking_column =&gt; &quot;sample_time&quot;</span><br><span class="line">      tracking_column_type =&gt; &quot;timestamp&quot;</span><br><span class="line">      clean_run =&gt; &quot;false&quot;</span><br><span class="line">      last_run_metadata_path =&gt; &quot;/opt/last_run_metadata_path/gauss_last_sample_time&quot;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><p>statement_filepath 路径文件里配置的sql如下，注意里面的预定义变量 :sql_last_value。<br></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> clustername,coorname,sample_time,datid,datname,pid,usesysid,usename,application_name,abbrev(client_addr) <span class="keyword">AS</span> client_addr,client_hostname,client_port,backend_start,xact_start,query_start,state_change,waiting,<span class="keyword">enqueue</span>,state,resource_pool,query_id,<span class="keyword">query</span> <span class="keyword">from</span> monitor.ash_pg_stat_activity_r <span class="keyword">where</span> sample_time &gt; :sql_last_value</span><br></pre></td></tr></table></figure><p></p><p>last_run_metadata_path 路径下的文件内容：<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">--- 2020-02-05 12:10:00.000000000 +08:00</span><br></pre></td></tr></table></figure><p></p><p>启动 logstash，可以看到 logstash 日志，注意 :sql_last_value的地方</p><h1 id="数据-output-到-Elasticsearch"><a href="#数据-output-到-Elasticsearch" class="headerlink" title="数据 output 到 Elasticsearch"></a>数据 output 到 Elasticsearch</h1><p>logstash 的 output 区域的配置如下：<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">output &#123;       </span><br><span class="line">    elasticsearch &#123;</span><br><span class="line">        hosts =&gt; [&quot;https://vip:9200&quot;] </span><br><span class="line">        index =&gt; &quot;gauss_active_session-%&#123;+YYYY.MM.dd&#125;&quot;</span><br><span class="line">        document_type =&gt; &quot;gauss_active_session&quot;</span><br><span class="line">        user =&gt; &quot;elastic&quot;</span><br><span class="line">        password =&gt; &quot;elastic_password&quot;</span><br><span class="line">        ssl =&gt; true</span><br><span class="line">        cacert =&gt; &quot;../es_client-ca.cer&quot;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><p>登入 kibana 查看，按每分钟增量加载的会话表数据已经集成到了 elasticsearch，后续就可以开始做数据分析和可视化了。</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Tue Apr 07 2020 18:03:33 GMT+0800 (GMT+08:00) --&gt;&lt;h1 id=&quot;GaussDB-简介&quot;&gt;&lt;a href=&quot;#GaussDB-简介&quot; class=&quot;headerlink&quot; title=&quot;GaussDB 简介&quot;&gt;&lt;/a&gt;GaussDB 简介&lt;/h1&gt;&lt;p&gt;GaussDB 数据库分为 GaussDB T 和 GaussDB A，分别面向 OLTP 和 OLAP 的业务用户。&lt;br&gt;GaussDB T 数据库是华为公司全自研的分布式数据库，支持x86和华为鲲鹏硬件架构。基于创新性数据库内核，提供高并发事务实时处理能力、两地三中心金融级高可用能力和分布式高扩展能力。&lt;br&gt;GaussDB A 是一款具备分析及混合负载能力的分布式数据库，支持x86和华为鲲鹏硬件架构，支持行存储与列存储，提供PB级数据分析能力、多模分析能力和实时处理能力，用于数据仓库、数据集市、实时分析、实时决策和混合负载等场景，广泛应用于金融、政府、电信等行业核心系统。&lt;/p&gt;&lt;h1 id=&quot;Logstash-的-jdbc-input-plugin&quot;&gt;&lt;a href=&quot;#Logstash-的-jdbc-input-plugin&quot; class=&quot;headerlink&quot; title=&quot;Logstash 的 jdbc input plugin&quot;&gt;&lt;/a&gt;Logstash 的 jdbc input plugin&lt;/h1&gt;&lt;p&gt;参考 Logstash的 Jdbc input plugin 的官方文档，该插件可以通过JDBC接口将任何数据库中的数据导入 Logstash。周期性加载或一次加载，每一行是一个 event，列转成 filed。我们先解读下文档里提到的重要配置项。&lt;br&gt;&lt;/p&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;jdbc_driver_library：JDBC驱动包路径。&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;jdbc_driver_class：JDBC驱动程序类。&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;jdbc_connection_string：JDBC连接串。&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;jdbc_user：数据库用户名。&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;jdbc_password：数据库用户口令。&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;statement_filepath：SQL语句所在文件路径。&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;scheduler：调度计划。&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;p&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="elk" scheme="https://yongnights.github.io/categories/elk/"/>
    
      <category term="Logstash" scheme="https://yongnights.github.io/categories/elk/Logstash/"/>
    
      <category term="GaussDB" scheme="https://yongnights.github.io/categories/elk/Logstash/GaussDB/"/>
    
      <category term="Elasticsearch" scheme="https://yongnights.github.io/categories/elk/Logstash/GaussDB/Elasticsearch/"/>
    
    
      <category term="elk" scheme="https://yongnights.github.io/tags/elk/"/>
    
      <category term="Elasticsearch" scheme="https://yongnights.github.io/tags/Elasticsearch/"/>
    
      <category term="Logstash" scheme="https://yongnights.github.io/tags/Logstash/"/>
    
      <category term="GaussDB" scheme="https://yongnights.github.io/tags/GaussDB/"/>
    
  </entry>
  
  <entry>
    <title>详细说明-CentOS7部署FastDFS+nginx模块</title>
    <link href="https://yongnights.github.io/2020/04/02/%E8%AF%A6%E7%BB%86%E8%AF%B4%E6%98%8E-CentOS7%E9%83%A8%E7%BD%B2FastDFS+nginx%E6%A8%A1%E5%9D%97/"/>
    <id>https://yongnights.github.io/2020/04/02/详细说明-CentOS7部署FastDFS+nginx模块/</id>
    <published>2020-04-02T03:57:08.447Z</published>
    <updated>2020-04-02T06:47:32.648Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Tue Apr 07 2020 18:03:33 GMT+0800 (GMT+08:00) --><h1 id="软件下载"><a href="#软件下载" class="headerlink" title="软件下载"></a>软件下载</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># 已经事先把所需软件下载好并上传到/usr/local/src目录了</span><br><span class="line">https://github.com/happyfish100/libfastcommon/archive/V1.0.43.tar.gz</span><br><span class="line">https://github.com/happyfish100/fastdfs-nginx-module/archive/V1.22.tar.gz</span><br><span class="line">https://github.com/happyfish100/fastdfs/archive/V6.06.tar.gz</span><br><span class="line">https://github.com/happyfish100/fastdfs-client-java/archive/V1.28.tar.gz</span><br><span class="line">https://openresty.org/download/openresty-1.15.8.3.tar.gz</span><br></pre></td></tr></table></figure><h1 id="基础环境设置"><a href="#基础环境设置" class="headerlink" title="基础环境设置"></a>基础环境设置</h1><h2 id="安装依赖组件"><a href="#安装依赖组件" class="headerlink" title="安装依赖组件"></a>安装依赖组件</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">yum -y install  gcc gcc-c++ libevent</span><br><span class="line">yum -y groupinstall &apos;Development Tools&apos;</span><br></pre></td></tr></table></figure><p><code><a id="more"></a></code></p><h2 id="安装libfastcommon"><a href="#安装libfastcommon" class="headerlink" title="安装libfastcommon"></a>安装libfastcommon</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">cd /usr/local/src</span><br><span class="line">tar -zxvf libfastcommon-1.0.43.tar.gz</span><br><span class="line">cd libfastcommon-1.0.43</span><br><span class="line">./make.sh</span><br><span class="line">./make.sh install</span><br><span class="line"></span><br><span class="line"># 检查文件是否存在，确保在/usr/lib路径下有libfastcommon.so和libfdfsclient.so</span><br><span class="line">ll /usr/lib | grep &quot;libf&quot;</span><br><span class="line">lrwxrwxrwx   1 root root     27 Apr  2 10:07 libfastcommon.so -&gt; /usr/lib64/libfastcommon.so</span><br><span class="line">-rwxr-xr-x   1 root root 356664 Apr  2 10:15 libfdfsclient.so</span><br></pre></td></tr></table></figure><h2 id="安装fastdfs"><a href="#安装fastdfs" class="headerlink" title="安装fastdfs"></a>安装fastdfs</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">cd /usr/local/src</span><br><span class="line">tar -zxvf fastdfs-6.06.tar.gz</span><br><span class="line">cd fastdfs-6.06</span><br><span class="line">./make.sh</span><br><span class="line">./make.sh install</span><br><span class="line"></span><br><span class="line"># FastDFS的配置文件默认安装到/etc/fdfs目录下</span><br><span class="line"></span><br><span class="line"># 安装成功后将fastdfs-6.06/conf下的俩文件拷贝到/etc/fdfs/下</span><br><span class="line">cd conf</span><br><span class="line">cp http.conf mime.types /etc/fdfs/</span><br><span class="line">cd /etc/fdfs/</span><br><span class="line">[root@bogon fdfs]# ll</span><br><span class="line">total 68</span><br><span class="line">-rw-r--r-- 1 root root  1909 Apr  2 10:15 client.conf.sample</span><br><span class="line">-rw-r--r-- 1 root root   965 Apr  2 10:16 http.conf</span><br><span class="line">-rw-r--r-- 1 root root 31172 Apr  2 10:16 mime.types</span><br><span class="line">-rw-r--r-- 1 root root 10246 Apr  2 10:15 storage.conf.sample</span><br><span class="line">-rw-r--r-- 1 root root   620 Apr  2 10:15 storage_ids.conf.sample</span><br><span class="line">-rw-r--r-- 1 root root  9138 Apr  2 10:15 tracker.conf.sample</span><br></pre></td></tr></table></figure><h3 id="fdfs-trackerd配置并启动"><a href="#fdfs-trackerd配置并启动" class="headerlink" title="fdfs_trackerd配置并启动"></a>fdfs_trackerd配置并启动</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"># 创建tracker工作目录,storage存储目录(选择大磁盘空间)等</span><br><span class="line">mkdir -p /opt/&#123;fdfs_tracker,fdfs_storage,fdfs_storage_data&#125;</span><br><span class="line"></span><br><span class="line">cd /etc/fdfs/</span><br><span class="line">cp tracker.conf.sample tracker.conf</span><br><span class="line">vim tracker.conf</span><br><span class="line">    disabled = false # 配置tracker.conf这个配置文件是否生效，因为在启动fastdfs服务端进程时需要指定配置文件，所以需要使次配置文件生效。false是生效，true是屏蔽。</span><br><span class="line">    bind_addr = # 程序的监听地址，如果不设定则监听所有地址，可以设置本地ip地址</span><br><span class="line">    port = 22122 #tracker监听的端口</span><br><span class="line">    base_path = /opt/fdfs_tracker # tracker保存data和logs的路径</span><br><span class="line">    http.server_port=8080 # http服务端口，保持默认</span><br><span class="line"></span><br><span class="line"># 启动fdfs_trackerd</span><br><span class="line">/usr/bin/fdfs_trackerd /etc/fdfs/tracker.conf start</span><br><span class="line"></span><br><span class="line"># 查看/opt/fdfs_tracker目录，发现目录下多了data和logs两个目录</span><br><span class="line"></span><br><span class="line"># 查看端口号，验证启动情况</span><br><span class="line">[root@bogon fdfs]# ps -ef | grep fdfs</span><br><span class="line">root       2119      1  0 10:22 ?        00:00:00 /usr/bin/fdfs_trackerd /etc/fdfs/tracker.conf start</span><br><span class="line">[root@bogon fdfs]# ss -tulnp | grep 22122</span><br><span class="line">tcp    LISTEN     0      128       *:22122      *:*    users:((&quot;fdfs_trackerd&quot;,pid=2119,fd=5))</span><br><span class="line"></span><br><span class="line"># 命令行选项</span><br><span class="line">Usage: /usr/bin/fdfs_trackerd &lt;config_file&gt; [start|stop|restart]</span><br><span class="line"></span><br><span class="line"># 设置开机自启动</span><br><span class="line">echo &quot;/usr/bin/fdfs_trackerd /etc/fdfs/tracker.conf restart&quot; | tee -a /etc/rc.d/rc.local</span><br></pre></td></tr></table></figure><h3 id="fdfs-storage配置并启动"><a href="#fdfs-storage配置并启动" class="headerlink" title="fdfs_storage配置并启动"></a>fdfs_storage配置并启动</h3><p>与tracker不同的是，storage还需要一个目录用来存储数据，所以在上面步骤中另外多建了两个目录fdfs_storage_data,fdfs_storage<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">cd /etc/fdfs/</span><br><span class="line">cp storage.conf.sample storage.conf</span><br><span class="line">vim storage.conf</span><br><span class="line">    disabled=false # 启用这个配置文件</span><br><span class="line">    group_name=group1 #组名，根据实际情况修改，文件链接中会用到</span><br><span class="line">    port=23000 #设置storage的端口号，默认是23000，同一个组的storage端口号必须一致</span><br><span class="line">    base_path = /opt/fdfs_storage # #设置storage数据文件和日志目录，注意,这个目录最好有大于50G的磁盘空间</span><br><span class="line">    store_path_count=1 #存储路径个数，需要和store_path个数匹配 </span><br><span class="line">    store_path0 = /opt/fdfs_storage_data # 实际保存文件的路径，注意,这个目录最好有大于50G的磁盘空间</span><br><span class="line">    tracker_server = 192.168.75.5:22122 # tracker监听地址和端口号，要与tracker.conf文件中设置的保持一致</span><br><span class="line">    http.server_port=8888 #设置 http 端口号</span><br><span class="line">    </span><br><span class="line"># 启动fdfs_storaged</span><br><span class="line">/usr/bin/fdfs_storaged /etc/fdfs/storage.conf start</span><br><span class="line"></span><br><span class="line"># 查看端口号，验证启动情况</span><br><span class="line">[root@bogon fdfs]# ps -ef | grep &quot;fdfs_storaged&quot;</span><br><span class="line">root       2194      1  7 10:36 ?        00:00:01 /usr/bin/fdfs_storaged /etc/fdfs/storage.conf start</span><br><span class="line">[root@bogon fdfs]# ss -tulnp | grep &quot;fdfs&quot;</span><br><span class="line">tcp    LISTEN     0      128       *:23000      *:*     users:((&quot;fdfs_storaged&quot;,pid=2194,fd=5))</span><br><span class="line">tcp    LISTEN     0      128       *:22122      *:*     users:((&quot;fdfs_trackerd&quot;,pid=2119,fd=5))</span><br><span class="line"></span><br><span class="line"># 命令行选项</span><br><span class="line">Usage: /usr/bin/fdfs_trackerd &lt;config_file&gt; [start|stop|restart]</span><br><span class="line"></span><br><span class="line"># 设置开机自启动</span><br><span class="line">echo &quot;/usr/bin/fdfs_storaged /etc/fdfs/storage.conf restart&quot; | tee -a /etc/rc.d/rc.local</span><br></pre></td></tr></table></figure><p></p><h3 id="校验整合"><a href="#校验整合" class="headerlink" title="校验整合"></a>校验整合</h3><p>要确定一下，storage是否注册到了tracker中去<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/usr/bin/fdfs_monitor /etc/fdfs/storage.conf</span><br></pre></td></tr></table></figure><p></p><p>成功后可以看到：ip_addr = 192.168.75.5 ACTIVE</p><h3 id="使用FastDFS自带工具测试"><a href="#使用FastDFS自带工具测试" class="headerlink" title="使用FastDFS自带工具测试"></a>使用FastDFS自带工具测试</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">cd /etc/fdfs/</span><br><span class="line">cp client.conf.sample client.conf</span><br><span class="line">vim client.conf</span><br><span class="line">    base_path = /opt/fdfs_tracker # tracker服务器文件路径</span><br><span class="line">    tracker_server = 192.168.75.5:22122 #tracker服务器IP地址和端口号</span><br><span class="line">    http.tracker_server_port = 8080 # tracker服务器的http端口号,必须和tracker的设置对应起来</span><br></pre></td></tr></table></figure><p>上传一张图片1.jpg到Centos服务器上的 /tmp 目录下，进行测试，命令如下：<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">/usr/bin/fdfs_test /etc/fdfs/client.conf upload /tmp/1.jpg</span><br><span class="line">This is FastDFS client test program v6.06</span><br><span class="line"></span><br><span class="line">Copyright (C) 2008, Happy Fish / YuQing</span><br><span class="line"></span><br><span class="line">FastDFS may be copied only under the terms of the GNU General</span><br><span class="line">Public License V3, which may be found in the FastDFS source kit.</span><br><span class="line">Please visit the FastDFS Home Page http://www.fastken.com/ </span><br><span class="line">for more detail.</span><br><span class="line"></span><br><span class="line">[2020-04-02 10:47:57] DEBUG - base_path=/opt/fdfs_tracker, connect_timeout=5, network_timeout=60, tracker_server_count=1, anti_steal_token=0, anti_steal_secret_key length=0, use_connection_pool=0, g_connection_pool_max_idle_time=3600s, use_storage_id=0, storage server id count: 0</span><br><span class="line"></span><br><span class="line">tracker_query_storage_store_list_without_group: </span><br><span class="line">        server 1. group_name=, ip_addr=192.168.75.5, port=23000</span><br><span class="line"></span><br><span class="line">group_name=group1, ip_addr=192.168.75.5, port=23000</span><br><span class="line">storage_upload_by_filename</span><br><span class="line">group_name=group1, remote_filename=M00/00/00/wKhLBV6FUl6AA0eTACSnIi51C7k037.jpg</span><br><span class="line">source ip address: 192.168.75.5</span><br><span class="line">file timestamp=2020-04-02 10:47:58</span><br><span class="line">file size=2402082</span><br><span class="line">file crc32=779422649</span><br><span class="line">example file url: http://192.168.75.5:8080/group1/M00/00/00/wKhLBV6FUl6AA0eTACSnIi51C7k037.jpg</span><br><span class="line">storage_upload_slave_by_filename</span><br><span class="line">group_name=group1, remote_filename=M00/00/00/wKhLBV6FUl6AA0eTACSnIi51C7k037_big.jpg</span><br><span class="line">source ip address: 192.168.75.5</span><br><span class="line">file timestamp=2020-04-02 10:47:58</span><br><span class="line">file size=2402082</span><br><span class="line">file crc32=779422649</span><br><span class="line">example file url: http://192.168.75.5:8080/group1/M00/00/00/wKhLBV6FUl6AA0eTACSnIi51C7k037_big.jpg</span><br></pre></td></tr></table></figure><p></p><p>以上图中的文件地址：<a href="http://192.168.75.5:8080/group1/M00/00/00/wKhLBV6FUl6AA0eTACSnIi51C7k037.jpg对应storage服务器上的/opt/fdfs_storage_data/data/00/00/wKhLBV6FUl6AA0eTACSnIi51C7k037.jpg文件" target="_blank" rel="noopener">http://192.168.75.5:8080/group1/M00/00/00/wKhLBV6FUl6AA0eTACSnIi51C7k037.jpg对应storage服务器上的/opt/fdfs_storage_data/data/00/00/wKhLBV6FUl6AA0eTACSnIi51C7k037.jpg文件</a>;</p><blockquote><p>组名：group1<br>磁盘：M00<br>目录：00/00<br>文件名称：wKhLBV6FUl6AA0eTACSnIi51C7k037.jpg<br>注意图片路径中的8080端口,这个是tracker的端口</p></blockquote><p>上传的图片会被上传到我们创建的fdfs_storage_data目录下，会有四个图片文件:<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[root@bogon 00]# pwd</span><br><span class="line">/opt/fdfs_storage_data/data/00/00</span><br><span class="line">[root@bogon 00]# ll</span><br><span class="line">total 4704</span><br><span class="line">-rw-r--r-- 1 root root 2402082 Apr  2 10:47 wKhLBV6FUl6AA0eTACSnIi51C7k037_big.jpg</span><br><span class="line">-rw-r--r-- 1 root root      49 Apr  2 10:47 wKhLBV6FUl6AA0eTACSnIi51C7k037_big.jpg-m</span><br><span class="line">-rw-r--r-- 1 root root 2402082 Apr  2 10:47 wKhLBV6FUl6AA0eTACSnIi51C7k037.jpg</span><br><span class="line">-rw-r--r-- 1 root root      49 Apr  2 10:47 wKhLBV6FUl6AA0eTACSnIi51C7k037.jpg-m</span><br></pre></td></tr></table></figure><p></p><p>data下有256个1级目录，每级目录下又有256个2级子目录，总共65536个文件，新写的文件会以hash的方式被路由到其中某个子目录下，然后将文件数据直接作为一个本地文件存储到该目录中。</p><h2 id="FastDFS和nginx结合使用"><a href="#FastDFS和nginx结合使用" class="headerlink" title="FastDFS和nginx结合使用"></a>FastDFS和nginx结合使用</h2><p>FastDFS通过Tracker服务器,将文件放在Storage服务器存储,但是同组之间的服务器需要复制文件,有延迟的问题.<br>假设Tracker服务器将文件上传到了172.20.132.57,文件ID已经返回客户端,这时,后台会将这个文件复制到172.20.132.57,如果复制没有完成,客户端就用这个ID在172.20.132.57取文件,肯定会出现错误。<br>这个fastdfs-nginx-module可以重定向连接到源服务器取文件,避免客户端由于复制延迟的问题,出现错误。<br>正是这样，FastDFS需要结合nginx，所以取消原来对HTTP的直接支持。</p><h3 id="在tracker上安装-nginx"><a href="#在tracker上安装-nginx" class="headerlink" title="在tracker上安装 nginx"></a>在tracker上安装 nginx</h3><p>在每个tracker上安装nginx的主要目的是做负载均衡及实现高可用。如果只有一台tracker服务器可以不配置nginx.<br>一个tracker对应多个storage，通过nginx对storage负载均衡;</p><h3 id="在storage上安装nginx-openresty"><a href="#在storage上安装nginx-openresty" class="headerlink" title="在storage上安装nginx(openresty)"></a>在storage上安装nginx(openresty)</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">cd /usr/local/src/</span><br><span class="line">tar -zxvf fastdfs-nginx-module-1.22.tar.gz</span><br><span class="line">cd fastdfs-nginx-module-1.22/src</span><br><span class="line">cp mod_fastdfs.conf /etc/fdfs/</span><br><span class="line">vim /etc/fdfs/mod_fastdfs.conf</span><br><span class="line">    base_path=/opt/fdfs_storage # 与storage.conf配置中的保持一致</span><br><span class="line">    tracker_server=192.168.75.5:22122 #tracker服务器的IP地址以及端口号</span><br><span class="line">    url_have_group_name = true # url中包含group名称</span><br><span class="line">    store_path0=/opt/fdfs_storage_data #与storage.conf中的路径保持一致</span><br><span class="line">    group_count = 1 #设置组的个数</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line">yum -y install pcre pcre-devel openssl openssl-devel zlib zlib-devel </span><br><span class="line">cd /usr/local/src</span><br><span class="line">tar -zxvf openresty-1.15.8.3.tar.gz</span><br><span class="line">cd openresty-1.15.8.3</span><br><span class="line">./configure \</span><br><span class="line">    --with-luajit \</span><br><span class="line">    --with-http_stub_status_module \</span><br><span class="line">    --with-http_ssl_module \</span><br><span class="line">    --with-http_realip_module \</span><br><span class="line">    --with-http_gzip_static_module \</span><br><span class="line">    --add-module=/usr/local/src/fastdfs-nginx-module-1.22/src</span><br><span class="line">gmake</span><br><span class="line">gmake install</span><br><span class="line"></span><br><span class="line"># 修改配置文件</span><br><span class="line">vim /usr/local/openresty/nginx/conf/nginx.conf</span><br><span class="line">    error_log  logs/error.log;</span><br><span class="line">    pid      logs/nginx.pid;</span><br><span class="line">    server&#123;</span><br><span class="line">        server_name  192.168.75.5; </span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"># 启动</span><br><span class="line">/usr/local/openresty/nginx/sbin/nginx -c /usr/local/openresty/nginx/conf/nginx.conf</span><br><span class="line"></span><br><span class="line"># 浏览器访问，出现openresty欢迎页面</span><br><span class="line"></span><br><span class="line"># 设置nginx开机启动</span><br><span class="line">echo &quot;/usr/local/openresty/nginx/sbin/nginx -c /usr/local/openresty/nginx/conf/nginx.conf&quot; | tee -a /etc/rc.d/rc.local</span><br><span class="line"></span><br><span class="line"># 再次修改配置文件，加载fastdfs模块</span><br><span class="line">vim /usr/local/openresty/nginx/conf/nginx.conf</span><br><span class="line">    server&#123;</span><br><span class="line">        location /group1/M00/ &#123;</span><br><span class="line">            root /opt/fdfs_storage/data;</span><br><span class="line">            ngx_fastdfs_module;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"># 重载nginx</span><br><span class="line">/usr/local/openresty/nginx/sbin/nginx -s reload</span><br><span class="line"></span><br><span class="line"># 参考上面测试的那一步图片url地址：http://192.168.75.5:8080/group1/M00/00/00/wKhLBV6FUl6AA0eTACSnIi51C7k037.jpg</span><br><span class="line">使用nginxf访问的话，实际地址是：http://192.168.75.5/group1/M00/00/00/wKhLBV6FUl6AA0eTACSnIi51C7k037.jpg</span><br><span class="line">需要把tracker使用的8080端口去掉，否则无法访问</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"># 进一步完善nginx配置文件</span><br><span class="line">    # 这个server设置的是storage nginx</span><br><span class="line">    server &#123;</span><br><span class="line">        listen       9991;</span><br><span class="line">        server_name  localhost;</span><br><span class="line"></span><br><span class="line">        location / &#123;</span><br><span class="line">            root   html;</span><br><span class="line">            index  index.html index.htm;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        location ~/group1/M00 &#123;</span><br><span class="line">            root /opt/fastdfs_storage/data;</span><br><span class="line">            ngx_fastdfs_module;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        location = /50x.html &#123;</span><br><span class="line">            root   html;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    # 若访问不到图片需要配置这个软连接</span><br><span class="line">    # ln -s /opt/fastdfs_storage_data/data/ /opt/fastdfs_storage_data/data/M00</span><br><span class="line">    </span><br><span class="line">    # 这个server设置的是tracker nginx</span><br><span class="line">    upstream fdfs_group1 &#123;</span><br><span class="line">        server 127.0.0.1:9991;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    server &#123;</span><br><span class="line">        listen       80;</span><br><span class="line">        server_name  localhost;</span><br><span class="line">        </span><br><span class="line">        location /group1/M00 &#123;</span><br><span class="line">            proxy_pass http://fdfs_group1;</span><br><span class="line">        &#125;</span><br><span class="line">        </span><br><span class="line">        error_page   500 502 503 504  /50x.html;</span><br><span class="line">        location = /50x.html &#123;</span><br><span class="line">            root   html;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><h1 id="搭建集群"><a href="#搭建集群" class="headerlink" title="搭建集群"></a>搭建集群</h1><h2 id="集群规划-单tracker-双storage"><a href="#集群规划-单tracker-双storage" class="headerlink" title="集群规划(单tracker,双storage)"></a>集群规划(单tracker,双storage)</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">虚拟机     IP                 说明</span><br><span class="line">tracker 192.168.75.5 tracker 服务器</span><br><span class="line">storage01 192.168.75.6 storage01服务器【group1】</span><br><span class="line">storage02 192.168.75.7 storage02服务器【group2】</span><br></pre></td></tr></table></figure><h2 id="软件清单"><a href="#软件清单" class="headerlink" title="软件清单"></a>软件清单</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">fastdfs-6.06.tar.gz</span><br><span class="line">fastdfs-client-java-1.28.tar.gz</span><br><span class="line">fastdfs-nginx-module-1.22.tar.gz</span><br><span class="line">libfastcommon-1.0.43.tar.gz</span><br><span class="line">openresty-1.15.8.3.tar.gz</span><br></pre></td></tr></table></figure><h2 id="安装步骤"><a href="#安装步骤" class="headerlink" title="安装步骤"></a>安装步骤</h2><h3 id="1-tracker服务器"><a href="#1-tracker服务器" class="headerlink" title="1.tracker服务器"></a>1.tracker服务器</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># 1. 安装libfastcommon 模块</span><br><span class="line"># 2. 编译安装 FastDFS</span><br><span class="line"># 3. 修改配置文件tarcker.conf和client.conf(测试上传)</span><br><span class="line"></span><br><span class="line"># vim /etc/fdfs/tracker.conf</span><br><span class="line">    store_lookup=0  #采用轮询策略进行存储，0：轮询 1：始终定向到某个group 2：选择存储空间最大的进行存储</span><br><span class="line"></span><br><span class="line"># 4. 开机启动</span><br></pre></td></tr></table></figure><h3 id="2-storage服务器"><a href="#2-storage服务器" class="headerlink" title="2.storage服务器"></a>2.storage服务器</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"># 1. 安装libfastcommon 模块</span><br><span class="line"># 2. 编译安装 FastDFS</span><br><span class="line"># 3. 修改配置文件storage.conf</span><br><span class="line"></span><br><span class="line"># storage01 配置</span><br><span class="line"># vim /etc/fdfs/storage.conf</span><br><span class="line">group_name=group1</span><br><span class="line">base_path=/home/fastdfs_storage</span><br><span class="line">store_path0=/home/fastdfs_storage</span><br><span class="line">tracker_server=192.168.75.6:22122</span><br><span class="line">http.server_port=8888</span><br><span class="line"></span><br><span class="line"># storage02 配置</span><br><span class="line"># vim /etc/fdfs/storage.conf</span><br><span class="line">group_name=group2</span><br><span class="line">base_path=/home/fastdfs_storage</span><br><span class="line">store_path0=/home/fastdfs_storage</span><br><span class="line">tracker_server=192.168.75.7:22122</span><br><span class="line">http.server_port=8888</span><br><span class="line"></span><br><span class="line"># 4. 开机启动</span><br><span class="line"># 5. 安装nginx和fastdfs-nginx-module模块</span><br><span class="line"></span><br><span class="line"># storage01 配置：</span><br><span class="line"># vim /etc/fdfs/mod_fastdfs.conf</span><br><span class="line">connect_timeout=10</span><br><span class="line">base_path=/home/fastdfs_storage</span><br><span class="line">url_have_group_name=true</span><br><span class="line">store_path0=/home/fastdfs_storage</span><br><span class="line">tracker_server=192.168.75.6:22122</span><br><span class="line">group_name=group1</span><br><span class="line"></span><br><span class="line"># storage02 配置：</span><br><span class="line"># vim /etc/fdfs/mod_fastdfs.conf</span><br><span class="line">connect_timeout=10</span><br><span class="line">base_path=/home/fastdfs_storage</span><br><span class="line">url_have_group_name=true</span><br><span class="line">store_path0=/home/fastdfs_storage</span><br><span class="line">tracker_server=192.168.75.7:22122</span><br><span class="line">group_name=group2</span><br><span class="line"></span><br><span class="line"># 6. 复制 FastDFS 安装目录的部分配置文件到 /etc/fdfs 目录</span><br><span class="line">cp http.conf mime.types /etc/fdfs/</span><br><span class="line"></span><br><span class="line"># 7. 配置nginx</span><br><span class="line">server &#123;</span><br><span class="line">    listen 8888;  </span><br><span class="line">    server_name localhost; </span><br><span class="line">     </span><br><span class="line">    location ~/group([0-9])/M00 &#123;</span><br><span class="line">        ngx_fastdfs_module;  </span><br><span class="line">    &#125;</span><br><span class="line">    error_page 500 502 503 504 /50x.html;  </span><br><span class="line">    location = /50x.html &#123;  </span><br><span class="line">        root html;  </span><br><span class="line">    &#125;  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="3-测试"><a href="#3-测试" class="headerlink" title="3.测试"></a>3.测试</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># vim /etc/fdfs/client.conf</span><br><span class="line">    base_path=/home/fastdfs_tracker</span><br><span class="line">    tracker_server=192.168.75.5:22122</span><br><span class="line"></span><br><span class="line">/usr/bin/fdfs_upload_file /etc/fdfs/client.conf test.jpg</span><br></pre></td></tr></table></figure><h3 id="4-tracker安装nginx"><a href="#4-tracker安装nginx" class="headerlink" title="4. tracker安装nginx"></a>4. tracker安装nginx</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">http &#123;  </span><br><span class="line">    include mime.types;  </span><br><span class="line">    default_type application/octet-stream;  </span><br><span class="line">    sendfile on;  </span><br><span class="line">    keepalive_timeout 65;</span><br><span class="line">    </span><br><span class="line">    #group1</span><br><span class="line">    upstream fdfs_group1 &#123;</span><br><span class="line">       server 192.168.75.6:8888;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    #group2</span><br><span class="line">    upstream fdfs_group2 &#123;</span><br><span class="line">       server 192.168.75.7:8888;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    server &#123;  </span><br><span class="line">        listen 8000;  </span><br><span class="line">        server_name localhost;</span><br><span class="line">        </span><br><span class="line">        location /group1/M00 &#123;</span><br><span class="line">           proxy_pass http://fdfs_group1;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        location /group2/M00 &#123;</span><br><span class="line">           proxy_pass http://fdfs_group2;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        error_page 500 502 503 504 /50x.html;  </span><br><span class="line">        location = /50x.html &#123;  </span><br><span class="line">            root html;  </span><br><span class="line">        &#125;  </span><br><span class="line">    &#125;  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Tue Apr 07 2020 18:03:33 GMT+0800 (GMT+08:00) --&gt;&lt;h1 id=&quot;软件下载&quot;&gt;&lt;a href=&quot;#软件下载&quot; class=&quot;headerlink&quot; title=&quot;软件下载&quot;&gt;&lt;/a&gt;软件下载&lt;/h1&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;# 已经事先把所需软件下载好并上传到/usr/local/src目录了&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;https://github.com/happyfish100/libfastcommon/archive/V1.0.43.tar.gz&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;https://github.com/happyfish100/fastdfs-nginx-module/archive/V1.22.tar.gz&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;https://github.com/happyfish100/fastdfs/archive/V6.06.tar.gz&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;https://github.com/happyfish100/fastdfs-client-java/archive/V1.28.tar.gz&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;https://openresty.org/download/openresty-1.15.8.3.tar.gz&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;h1 id=&quot;基础环境设置&quot;&gt;&lt;a href=&quot;#基础环境设置&quot; class=&quot;headerlink&quot; title=&quot;基础环境设置&quot;&gt;&lt;/a&gt;基础环境设置&lt;/h1&gt;&lt;h2 id=&quot;安装依赖组件&quot;&gt;&lt;a href=&quot;#安装依赖组件&quot; class=&quot;headerlink&quot; title=&quot;安装依赖组件&quot;&gt;&lt;/a&gt;安装依赖组件&lt;/h2&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;yum -y install  gcc gcc-c++ libevent&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;yum -y groupinstall &amp;apos;Development Tools&amp;apos;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;p&gt;&lt;code&gt;
    
    </summary>
    
      <category term="nginx" scheme="https://yongnights.github.io/categories/nginx/"/>
    
      <category term="FastDFS" scheme="https://yongnights.github.io/categories/nginx/FastDFS/"/>
    
    
      <category term="nginx" scheme="https://yongnights.github.io/tags/nginx/"/>
    
      <category term="FastDFS" scheme="https://yongnights.github.io/tags/FastDFS/"/>
    
  </entry>
  
  <entry>
    <title>CentOS7部署FastDFS+nginx模块</title>
    <link href="https://yongnights.github.io/2020/04/02/CentOS7%E9%83%A8%E7%BD%B2FastDFS+nginx%E6%A8%A1%E5%9D%97/"/>
    <id>https://yongnights.github.io/2020/04/02/CentOS7部署FastDFS+nginx模块/</id>
    <published>2020-04-02T01:31:47.162Z</published>
    <updated>2020-04-08T10:04:48.822Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Wed Apr 08 2020 18:05:48 GMT+0800 (GMT+08:00) --><h1 id="软件下载"><a href="#软件下载" class="headerlink" title="软件下载"></a>软件下载</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># 已经事先把所需软件下载好并上传到/usr/local/src目录了</span><br><span class="line">https://github.com/happyfish100/libfastcommon/archive/V1.0.43.tar.gz</span><br><span class="line">https://github.com/happyfish100/fastdfs-nginx-module/archive/V1.22.tar.gz</span><br><span class="line">https://github.com/happyfish100/fastdfs/archive/V6.06.tar.gz</span><br><span class="line">https://github.com/happyfish100/fastdfs-client-java/archive/V1.28.tar.gz</span><br><span class="line">https://openresty.org/download/openresty-1.15.8.3.tar.gz</span><br></pre></td></tr></table></figure><h1 id="基础环境设置"><a href="#基础环境设置" class="headerlink" title="基础环境设置"></a>基础环境设置</h1><h2 id="安装依赖组件"><a href="#安装依赖组件" class="headerlink" title="安装依赖组件"></a>安装依赖组件</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">yum -y install  gcc gcc-c++ libevent</span><br><span class="line">yum -y groupinstall &apos;Development Tools&apos;</span><br></pre></td></tr></table></figure><p><code><a id="more"></a></code></p><h2 id="安装libfastcommon"><a href="#安装libfastcommon" class="headerlink" title="安装libfastcommon"></a>安装libfastcommon</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">cd /usr/local/src</span><br><span class="line">tar -zxvf libfastcommon-1.0.43.tar.gz</span><br><span class="line">cd libfastcommon-1.0.43</span><br><span class="line">./make.sh</span><br><span class="line">./make.sh install</span><br><span class="line"></span><br><span class="line"># 检查文件是否存在</span><br><span class="line">[root@bogon libfastcommon-1.0.43]# ll /usr/lib64 | grep &quot;libfastcommon.so&quot; </span><br><span class="line">-rwxr-xr-x   1 root root  1035264 Apr  2 10:07 libfastcommon.so</span><br><span class="line">[root@bogon libfastcommon-1.0.43]# ll /usr/lib | grep &quot;libfastcommon.so&quot;  </span><br><span class="line">lrwxrwxrwx   1 root root    27 Apr  2 10:07 libfastcommon.so -&gt; /usr/lib64/libfastcommon.so</span><br></pre></td></tr></table></figure><h2 id="安装fastdfs"><a href="#安装fastdfs" class="headerlink" title="安装fastdfs"></a>安装fastdfs</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">cd /usr/local/src</span><br><span class="line">tar -zxvf fastdfs-6.06.tar.gz</span><br><span class="line">cd fastdfs-6.06</span><br><span class="line">./make.sh</span><br><span class="line">./make.sh install</span><br><span class="line"></span><br><span class="line"># 安装成功后将解压目录下的conf下的俩文件拷贝到/etc/fdfs/下</span><br><span class="line">cd conf</span><br><span class="line">cp http.conf mime.types /etc/fdfs/</span><br><span class="line">cd /etc/fdfs/</span><br><span class="line">[root@bogon fdfs]# ll</span><br><span class="line">total 68</span><br><span class="line">-rw-r--r-- 1 root root  1909 Apr  2 10:15 client.conf.sample</span><br><span class="line">-rw-r--r-- 1 root root   965 Apr  2 10:16 http.conf</span><br><span class="line">-rw-r--r-- 1 root root 31172 Apr  2 10:16 mime.types</span><br><span class="line">-rw-r--r-- 1 root root 10246 Apr  2 10:15 storage.conf.sample</span><br><span class="line">-rw-r--r-- 1 root root   620 Apr  2 10:15 storage_ids.conf.sample</span><br><span class="line">-rw-r--r-- 1 root root  9138 Apr  2 10:15 tracker.conf.sample</span><br></pre></td></tr></table></figure><h3 id="fdfs-trackerd配置并启动"><a href="#fdfs-trackerd配置并启动" class="headerlink" title="fdfs_trackerd配置并启动"></a>fdfs_trackerd配置并启动</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">cd /etc/fdfs/</span><br><span class="line">cp tracker.conf.sample tracker.conf</span><br><span class="line">mkdir -p /opt/&#123;fdfs_tracker,fdfs_storage,fdfs_storage_data&#125;</span><br><span class="line">vim tracker.conf</span><br><span class="line">    base_path = /opt/fdfs_tracker</span><br><span class="line"></span><br><span class="line"># 启动fdfs_trackerd</span><br><span class="line">/usr/bin/fdfs_trackerd /etc/fdfs/tracker.conf start</span><br><span class="line"># 查看端口号，验证启动情况</span><br><span class="line">[root@bogon fdfs]# ps -ef | grep fdfs</span><br><span class="line">root       2119      1  0 10:22 ?        00:00:00 /usr/bin/fdfs_trackerd /etc/fdfs/tracker.conf start</span><br><span class="line">[root@bogon fdfs]# ss -tulnp | grep 22122</span><br><span class="line">tcp    LISTEN     0      128       *:22122                 *:*                   users:((&quot;fdfs_trackerd&quot;,pid=2119,fd=5))</span><br><span class="line"></span><br><span class="line"># 命令行选项</span><br><span class="line">Usage: /usr/bin/fdfs_trackerd &lt;config_file&gt; [start|stop|restart]</span><br><span class="line"></span><br><span class="line"># 注意：在/opt/fdfs_data目录下生成两个目录,一个是数据,一个是日志.</span><br><span class="line"># 设置开机自启动</span><br><span class="line">echo &quot;/usr/bin/fdfs_trackerd /etc/fdfs/tracker.conf restart&quot; | tee -a /etc/rc.d/rc.local</span><br></pre></td></tr></table></figure><h3 id="fdfs-storaged配置并启动"><a href="#fdfs-storaged配置并启动" class="headerlink" title="fdfs_storaged配置并启动"></a>fdfs_storaged配置并启动</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">cd /etc/fdfs/</span><br><span class="line">cp storage.conf.sample storage.conf</span><br><span class="line">vim storage.conf</span><br><span class="line">    base_path = /opt/fdfs_storage # 注意,这个目录最好有大于50G的磁盘空间</span><br><span class="line">    store_path0 = /opt/fdfs_storage_data # 若配置这个参数，则该目录为实际保存文件的路径</span><br><span class="line">    tracker_server = 192.168.75.5:22122 # 注意：这个参数不能设置127.0.0.1，否则storage注册时会报错：ERROR - file: storage_func.c, line: 1361, conf file &quot;/etc/fdfs/storage.conf&quot;, tracker: &quot;127.0.0.1:22122&quot; is invalid, tracker server ip can&apos;t be 127.0.0.1</span><br><span class="line"># 启动fdfs_storaged</span><br><span class="line">/usr/bin/fdfs_storaged /etc/fdfs/storage.conf start</span><br><span class="line">[root@bogon fdfs]# ps -ef | grep &quot;fdfs_storaged&quot;</span><br><span class="line">root       2194      1  7 10:36 ?        00:00:01 /usr/bin/fdfs_storaged /etc/fdfs/storage.conf start</span><br><span class="line">[root@bogon fdfs]# ss -tulnp | grep &quot;fdfs&quot;</span><br><span class="line">tcp    LISTEN     0      128       *:23000                 *:*                   users:((&quot;fdfs_storaged&quot;,pid=2194,fd=5))</span><br><span class="line">tcp    LISTEN     0      128       *:22122                 *:*                   users:((&quot;fdfs_trackerd&quot;,pid=2119,fd=5))</span><br><span class="line"></span><br><span class="line"># 命令行选项</span><br><span class="line">Usage: /usr/bin/fdfs_trackerd &lt;config_file&gt; [start|stop|restart]</span><br><span class="line"># 设置开机自启动</span><br><span class="line">echo &quot;/usr/bin/fdfs_storaged /etc/fdfs/storage.conf restart&quot; | tee -a /etc/rc.d/rc.local</span><br></pre></td></tr></table></figure><h3 id="校验整合"><a href="#校验整合" class="headerlink" title="校验整合"></a>校验整合</h3><p>要确定一下，storage是否注册到了tracker中去<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/usr/bin/fdfs_monitor /etc/fdfs/storage.conf</span><br></pre></td></tr></table></figure><p></p><p>成功后可以看到：ip_addr = 192.168.75.5 ACTIVE</p><h3 id="使用FastDFS自带工具测试"><a href="#使用FastDFS自带工具测试" class="headerlink" title="使用FastDFS自带工具测试"></a>使用FastDFS自带工具测试</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">cd /etc/fdfs/</span><br><span class="line">cp client.conf.sample client.conf</span><br><span class="line">vim client.conf</span><br><span class="line">    base_path = /opt/fdfs_tracker #tracker服务器文件路径</span><br><span class="line">    tracker_server = 192.168.75.5:22122 #tracker服务器IP地址和端口号</span><br><span class="line">    http.tracker_server_port = 8080 # tracker服务器的http端口号,必须和tracker的设置对应起来</span><br></pre></td></tr></table></figure><p>上传一张图片1.jpg到Centos服务器上的 /tmp 目录下，进行测试，命令如下：<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">/usr/bin/fdfs_test /etc/fdfs/client.conf upload /tmp/1.jpg</span><br><span class="line">This is FastDFS client test program v6.06</span><br><span class="line"></span><br><span class="line">Copyright (C) 2008, Happy Fish / YuQing</span><br><span class="line"></span><br><span class="line">FastDFS may be copied only under the terms of the GNU General</span><br><span class="line">Public License V3, which may be found in the FastDFS source kit.</span><br><span class="line">Please visit the FastDFS Home Page http://www.fastken.com/ </span><br><span class="line">for more detail.</span><br><span class="line"></span><br><span class="line">[2020-04-02 10:47:57] DEBUG - base_path=/opt/fdfs_tracker, connect_timeout=5, network_timeout=60, tracker_server_count=1, anti_steal_token=0, anti_steal_secret_key length=0, use_connection_pool=0, g_connection_pool_max_idle_time=3600s, use_storage_id=0, storage server id count: 0</span><br><span class="line"></span><br><span class="line">tracker_query_storage_store_list_without_group: </span><br><span class="line">        server 1. group_name=, ip_addr=192.168.75.5, port=23000</span><br><span class="line"></span><br><span class="line">group_name=group1, ip_addr=192.168.75.5, port=23000</span><br><span class="line">storage_upload_by_filename</span><br><span class="line">group_name=group1, remote_filename=M00/00/00/wKhLBV6FUl6AA0eTACSnIi51C7k037.jpg</span><br><span class="line">source ip address: 192.168.75.5</span><br><span class="line">file timestamp=2020-04-02 10:47:58</span><br><span class="line">file size=2402082</span><br><span class="line">file crc32=779422649</span><br><span class="line">example file url: http://192.168.75.5:8080/group1/M00/00/00/wKhLBV6FUl6AA0eTACSnIi51C7k037.jpg</span><br><span class="line">storage_upload_slave_by_filename</span><br><span class="line">group_name=group1, remote_filename=M00/00/00/wKhLBV6FUl6AA0eTACSnIi51C7k037_big.jpg</span><br><span class="line">source ip address: 192.168.75.5</span><br><span class="line">file timestamp=2020-04-02 10:47:58</span><br><span class="line">file size=2402082</span><br><span class="line">file crc32=779422649</span><br><span class="line">example file url: http://192.168.75.5:8080/group1/M00/00/00/wKhLBV6FUl6AA0eTACSnIi51C7k037_big.jpg</span><br></pre></td></tr></table></figure><p></p><p>以上图中的文件地址：<a href="http://192.168.75.5:8080/group1/M00/00/00/wKhLBV6FUl6AA0eTACSnIi51C7k037.jpg对应storage服务器上的/opt/fdfs_storage_data/data/00/00/wKhLBV6FUl6AA0eTACSnIi51C7k037.jpg文件" target="_blank" rel="noopener">http://192.168.75.5:8080/group1/M00/00/00/wKhLBV6FUl6AA0eTACSnIi51C7k037.jpg对应storage服务器上的/opt/fdfs_storage_data/data/00/00/wKhLBV6FUl6AA0eTACSnIi51C7k037.jpg文件</a>;</p><blockquote><p>注意图片路径中的8080端口,这个是tracker的端口，</p></blockquote><p>但是查看该目录，会有四个图片文件:<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[root@bogon 00]# pwd</span><br><span class="line">/opt/fdfs_storage_data/data/00/00</span><br><span class="line">[root@bogon 00]# ll</span><br><span class="line">total 4704</span><br><span class="line">-rw-r--r-- 1 root root 2402082 Apr  2 10:47 wKhLBV6FUl6AA0eTACSnIi51C7k037_big.jpg</span><br><span class="line">-rw-r--r-- 1 root root      49 Apr  2 10:47 wKhLBV6FUl6AA0eTACSnIi51C7k037_big.jpg-m</span><br><span class="line">-rw-r--r-- 1 root root 2402082 Apr  2 10:47 wKhLBV6FUl6AA0eTACSnIi51C7k037.jpg</span><br><span class="line">-rw-r--r-- 1 root root      49 Apr  2 10:47 wKhLBV6FUl6AA0eTACSnIi51C7k037.jpg-m</span><br></pre></td></tr></table></figure><p></p><h2 id="FastDFS和nginx结合使用"><a href="#FastDFS和nginx结合使用" class="headerlink" title="FastDFS和nginx结合使用"></a>FastDFS和nginx结合使用</h2><h3 id="在tracker上安装-nginx"><a href="#在tracker上安装-nginx" class="headerlink" title="在tracker上安装 nginx"></a>在tracker上安装 nginx</h3><p>在每个tracker上安装nginx的主要目的是做负载均衡及实现高可用。如果只有一台tracker服务器可以不配置nginx.<br>一个tracker对应多个storage，通过nginx对storage负载均衡;</p><h3 id="在storage上安装nginx-openresty"><a href="#在storage上安装nginx-openresty" class="headerlink" title="在storage上安装nginx(openresty)"></a>在storage上安装nginx(openresty)</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">cd /usr/local/src/</span><br><span class="line">tar -zxvf fastdfs-nginx-module-1.22.tar.gz</span><br><span class="line">cd fastdfs-nginx-module-1.22/src</span><br><span class="line">cp mod_fastdfs.conf /etc/fdfs/</span><br><span class="line">vim /etc/fdfs/mod_fastdfs.conf</span><br><span class="line">    base_path=/opt/fdfs_storage</span><br><span class="line">    tracker_server=192.168.75.5:22122</span><br><span class="line">    url_have_group_name = true #url中包含group名称</span><br><span class="line">    store_path0=/opt/fdfs_storage_data #与storage.conf中的路径保持一致</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line">yum -y install pcre pcre-devel openssl openssl-devel</span><br><span class="line">cd /usr/local/src</span><br><span class="line">tar -zxvf openresty-1.15.8.3.tar.gz</span><br><span class="line">cd openresty-1.15.8.3</span><br><span class="line">./configure \</span><br><span class="line">    --with-luajit \</span><br><span class="line">    --with-http_stub_status_module \</span><br><span class="line">    --with-http_ssl_module \</span><br><span class="line">    --with-http_realip_module \</span><br><span class="line">    --with-http_gzip_static_module \</span><br><span class="line">    --add-module=/usr/local/src/fastdfs-nginx-module-1.22/src</span><br><span class="line">gmake</span><br><span class="line">gmake install</span><br><span class="line"></span><br><span class="line"># 启动</span><br><span class="line">vim /usr/local/openresty/nginx/conf/nginx.conf</span><br><span class="line">    error_log  logs/error.log;</span><br><span class="line">    pid      logs/nginx.pid;</span><br><span class="line">    server&#123;</span><br><span class="line">        server_name  192.168.75.5; </span><br><span class="line">    &#125;</span><br><span class="line">/usr/local/openresty/nginx/sbin/nginx -c /usr/local/openresty/nginx/conf/nginx.conf</span><br><span class="line"># 浏览器访问，出现openresty欢迎页面</span><br><span class="line"></span><br><span class="line"># 设置nginx开机启动</span><br><span class="line">echo &quot;/usr/local/openresty/nginx/sbin/nginx -c /usr/local/openresty/nginx/conf/nginx.conf&quot; | tee -a /etc/rc.d/rc.local</span><br><span class="line"></span><br><span class="line">vim /usr/local/openresty/nginx/conf/nginx.conf</span><br><span class="line">    server&#123;</span><br><span class="line">        location /group1/M00/ &#123;</span><br><span class="line">            root /opt/fdfs_storage/data;</span><br><span class="line">            ngx_fastdfs_module;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">/usr/local/openresty/nginx/sbin/nginx -s reload</span><br><span class="line"></span><br><span class="line"># 参考上面测试的那一步图片url地址：http://192.168.75.5:8080/group1/M00/00/00/wKhLBV6FUl6AA0eTACSnIi51C7k037.jpg</span><br><span class="line">使用nginxf访问的话，实际地址是：http://192.168.75.5/group1/M00/00/00/wKhLBV6FUl6AA0eTACSnIi51C7k037.jpg</span><br><span class="line">需要把tracker使用的8080端口去掉，否则无法访问</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"># 进一步完善nginx配置文件</span><br><span class="line">    # 这个server设置的是storage nginx</span><br><span class="line">    server &#123;</span><br><span class="line">        listen       9991;</span><br><span class="line">        server_name  localhost;</span><br><span class="line"></span><br><span class="line">        location / &#123;</span><br><span class="line">            root   html;</span><br><span class="line">            index  index.html index.htm;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        location ~/group1/M00 &#123;</span><br><span class="line">            root /opt/fastdfs_storage/data;</span><br><span class="line">            ngx_fastdfs_module;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        location = /50x.html &#123;</span><br><span class="line">            root   html;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    # 若访问不到图片需要配置这个软连接</span><br><span class="line">    # ln -s /opt/fastdfs_storage_data/data/ /opt/fastdfs_storage_data/data/M00</span><br><span class="line">    </span><br><span class="line">    # 这个server设置的是tracker nginx</span><br><span class="line">    upstream fdfs_group1 &#123;</span><br><span class="line">        server 127.0.0.1:9991;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    server &#123;</span><br><span class="line">        listen       80;</span><br><span class="line">        server_name  localhost;</span><br><span class="line">        </span><br><span class="line">        location /group1/M00 &#123;</span><br><span class="line">            proxy_pass http://fdfs_group1;</span><br><span class="line">        &#125;</span><br><span class="line">        </span><br><span class="line">        error_page   500 502 503 504  /50x.html;</span><br><span class="line">        location = /50x.html &#123;</span><br><span class="line">            root   html;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Wed Apr 08 2020 18:05:48 GMT+0800 (GMT+08:00) --&gt;&lt;h1 id=&quot;软件下载&quot;&gt;&lt;a href=&quot;#软件下载&quot; class=&quot;headerlink&quot; title=&quot;软件下载&quot;&gt;&lt;/a&gt;软件下载&lt;/h1&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;# 已经事先把所需软件下载好并上传到/usr/local/src目录了&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;https://github.com/happyfish100/libfastcommon/archive/V1.0.43.tar.gz&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;https://github.com/happyfish100/fastdfs-nginx-module/archive/V1.22.tar.gz&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;https://github.com/happyfish100/fastdfs/archive/V6.06.tar.gz&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;https://github.com/happyfish100/fastdfs-client-java/archive/V1.28.tar.gz&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;https://openresty.org/download/openresty-1.15.8.3.tar.gz&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;h1 id=&quot;基础环境设置&quot;&gt;&lt;a href=&quot;#基础环境设置&quot; class=&quot;headerlink&quot; title=&quot;基础环境设置&quot;&gt;&lt;/a&gt;基础环境设置&lt;/h1&gt;&lt;h2 id=&quot;安装依赖组件&quot;&gt;&lt;a href=&quot;#安装依赖组件&quot; class=&quot;headerlink&quot; title=&quot;安装依赖组件&quot;&gt;&lt;/a&gt;安装依赖组件&lt;/h2&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;yum -y install  gcc gcc-c++ libevent&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;yum -y groupinstall &amp;apos;Development Tools&amp;apos;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;p&gt;&lt;code&gt;
    
    </summary>
    
      <category term="nginx" scheme="https://yongnights.github.io/categories/nginx/"/>
    
      <category term="FastDFS" scheme="https://yongnights.github.io/categories/nginx/FastDFS/"/>
    
    
      <category term="nginx" scheme="https://yongnights.github.io/tags/nginx/"/>
    
      <category term="FastDFS" scheme="https://yongnights.github.io/tags/FastDFS/"/>
    
  </entry>
  
  <entry>
    <title>FastAPI快速入门</title>
    <link href="https://yongnights.github.io/2020/01/15/fastapi%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/"/>
    <id>https://yongnights.github.io/2020/01/15/fastapi快速入门/</id>
    <published>2020-01-15T02:30:32.629Z</published>
    <updated>2020-01-15T02:45:12.585Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Wed Jan 15 2020 18:06:02 GMT+0800 (GMT+08:00) --><p>fastapi是高性能的web框架。他的主要特点是：</p><ul><li>快速编码</li><li>减少人为bug</li><li>直观</li><li>简易</li><li>具有交互式文档</li><li>基于API的开放标准（并与之完全兼容）：OpenAPI（以前称为Swagger）和JSON Schema。</li></ul><p>技术背景：python3.6+、Starlette、Pydantic</p><p>官方文档地址：<a href="https://fastapi.tiangolo.com/" target="_blank" rel="noopener">https://fastapi.tiangolo.com/</a></p><h1 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pip install fastapi</span><br><span class="line">pip install uvicorn</span><br></pre></td></tr></table></figure><a id="more"></a><h1 id="quick-start"><a href="#quick-start" class="headerlink" title="quick start"></a>quick start</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"># main.py</span><br><span class="line"></span><br><span class="line">from fastapi import FastAPI</span><br><span class="line"></span><br><span class="line">app = FastAPI()</span><br><span class="line"></span><br><span class="line">@app.get(&quot;/&quot;)</span><br><span class="line">def read_root():</span><br><span class="line">    return &#123;&quot;Hello&quot;: &quot;World&quot;&#125;</span><br><span class="line"></span><br><span class="line">@app.get(&quot;/items/&#123;item_id&#125;&quot;)</span><br><span class="line">def read_item(item_id: int, q: str = None):</span><br><span class="line">    return &#123;&quot;item_id&quot;: item_id, &quot;q&quot;: q&#125;</span><br></pre></td></tr></table></figure><p>或者<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"># If your code uses async / await, use async def:</span><br><span class="line">from fastapi import FastAPI</span><br><span class="line"></span><br><span class="line">app = FastAPI()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">@app.get(&quot;/&quot;)</span><br><span class="line">async def read_root():</span><br><span class="line">    return &#123;&quot;Hello&quot;: &quot;World&quot;&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">@app.get(&quot;/items/&#123;item_id&#125;&quot;)</span><br><span class="line">async def read_item(item_id: int, q: str = None):</span><br><span class="line">    return &#123;&quot;item_id&quot;: item_id, &quot;q&quot;: q&#125;</span><br></pre></td></tr></table></figure><p></p><h1 id="运行"><a href="#运行" class="headerlink" title="运行"></a>运行</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">uvicorn main:app --reload</span><br></pre></td></tr></table></figure><p>看到如下提示，证明运行成功</p><p><img src="/fastapi快速入门.assets/1.png" alt></p><pre><code>main: 表示app所在文件名, the file main.py (the Python &quot;module&quot;).app：FastAPI实例, the object created inside of main.py with the line app = FastAPI().reload：debug模式，可以自动重启,make the server restart after code changes. Only do this for development.</code></pre><p>试着请求<a href="http://127.0.0.1:8000/items/5?q=somequery，会看到如下返回" target="_blank" rel="noopener">http://127.0.0.1:8000/items/5?q=somequery，会看到如下返回</a><br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;&quot;item_id&quot;: 5, &quot;q&quot;: &quot;somequery&quot;&#125;</span><br></pre></td></tr></table></figure><p></p><h1 id="交互文档"><a href="#交互文档" class="headerlink" title="交互文档"></a>交互文档</h1><p>试着打开<a href="http://127.0.0.1:8000/docs" target="_blank" rel="noopener">http://127.0.0.1:8000/docs</a><br><img src="/fastapi快速入门.assets/2.png" alt></p><h1 id="API文档"><a href="#API文档" class="headerlink" title="API文档"></a>API文档</h1><p>试着打开<a href="http://127.0.0.1:8000/redoc" target="_blank" rel="noopener">http://127.0.0.1:8000/redoc</a><br><img src="/fastapi快速入门.assets/3.png" alt></p><h1 id="update"><a href="#update" class="headerlink" title="update"></a>update</h1><p>通过上面的例子，我们已经用fastapi完成了第一个web服务，现在我们再添加一个接口</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">from fastapi import FastAPI</span><br><span class="line">from pydantic import BaseModel</span><br><span class="line"></span><br><span class="line">app = FastAPI()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class Item(BaseModel):</span><br><span class="line">    name: str</span><br><span class="line">    price: float</span><br><span class="line">    is_offer: bool = None</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">@app.get(&quot;/&quot;)</span><br><span class="line">def read_root():</span><br><span class="line">    return &#123;&quot;Hello&quot;: &quot;World&quot;&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">@app.get(&quot;/items/&#123;item_id&#125;&quot;)</span><br><span class="line">def read_item(item_id: int, q: str = None):</span><br><span class="line">    return &#123;&quot;item_id&quot;: item_id, &quot;q&quot;: q&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">@app.put(&quot;/items/&#123;item_id&#125;&quot;)</span><br><span class="line">def update_item(item_id: int, item: Item):</span><br><span class="line">    return &#123;&quot;item_name&quot;: item.name, &quot;item_id&quot;: item_id&#125;</span><br></pre></td></tr></table></figure><p>此时会发现，服务自动重启了，这是因为我们在启动命令后添加了–reload。再次查看文档，发现同样发生了改变。<br>到此，你已经可以快速的用fastapi搭建起服务了～</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Wed Jan 15 2020 18:06:02 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;fastapi是高性能的web框架。他的主要特点是：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;快速编码&lt;/li&gt;&lt;li&gt;减少人为bug&lt;/li&gt;&lt;li&gt;直观&lt;/li&gt;&lt;li&gt;简易&lt;/li&gt;&lt;li&gt;具有交互式文档&lt;/li&gt;&lt;li&gt;基于API的开放标准（并与之完全兼容）：OpenAPI（以前称为Swagger）和JSON Schema。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;技术背景：python3.6+、Starlette、Pydantic&lt;/p&gt;&lt;p&gt;官方文档地址：&lt;a href=&quot;https://fastapi.tiangolo.com/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://fastapi.tiangolo.com/&lt;/a&gt;&lt;/p&gt;&lt;h1 id=&quot;安装&quot;&gt;&lt;a href=&quot;#安装&quot; class=&quot;headerlink&quot; title=&quot;安装&quot;&gt;&lt;/a&gt;安装&lt;/h1&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;pip install fastapi&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;pip install uvicorn&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
      <category term="Python" scheme="https://yongnights.github.io/categories/Python/"/>
    
      <category term="FastAPI" scheme="https://yongnights.github.io/categories/Python/FastAPI/"/>
    
    
      <category term="Python" scheme="https://yongnights.github.io/tags/Python/"/>
    
      <category term="FastAPI" scheme="https://yongnights.github.io/tags/FastAPI/"/>
    
  </entry>
  
  <entry>
    <title>FastAPI教程进阶(一)</title>
    <link href="https://yongnights.github.io/2020/01/15/fastapi%E6%95%99%E7%A8%8B%E8%BF%9B%E9%98%B6%EF%BC%88%E4%B8%80%EF%BC%89/"/>
    <id>https://yongnights.github.io/2020/01/15/fastapi教程进阶（一）/</id>
    <published>2020-01-15T02:30:32.627Z</published>
    <updated>2020-01-15T02:33:29.752Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Wed Jan 15 2020 10:34:46 GMT+0800 (GMT+08:00) --><h1 id="一个简单的栗子"><a href="#一个简单的栗子" class="headerlink" title="一个简单的栗子"></a>一个简单的栗子</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">from fastapi import FastAPI</span><br><span class="line"></span><br><span class="line">app = FastAPI()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">@app.get(&quot;/&quot;)</span><br><span class="line">async def root():</span><br><span class="line">    return &#123;&quot;message&quot;: &quot;Hello World&quot;&#125;</span><br></pre></td></tr></table></figure><blockquote><p>FASTAPI继承Starlette，因此在Starlette中的所有可调用的对象在FASTAPI中可以直接引用</p></blockquote><a id="more"></a><h1 id="编写步骤"><a href="#编写步骤" class="headerlink" title="编写步骤"></a>编写步骤</h1><h2 id="步骤一：导入FastAPI"><a href="#步骤一：导入FastAPI" class="headerlink" title="步骤一：导入FastAPI"></a>步骤一：导入FastAPI</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">from fastapi import FastAPI</span><br></pre></td></tr></table></figure><h2 id="步骤二：创建FastAPI实例"><a href="#步骤二：创建FastAPI实例" class="headerlink" title="步骤二：创建FastAPI实例"></a>步骤二：创建FastAPI实例</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">app = FastAPI()</span><br></pre></td></tr></table></figure><h2 id="步骤三：创建访问路径"><a href="#步骤三：创建访问路径" class="headerlink" title="步骤三：创建访问路径"></a>步骤三：创建访问路径</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">@app.get(&quot;/&quot;)</span><br></pre></td></tr></table></figure><p>这个路径告诉FastAPI，该装饰器下的方法是用来处理路径是“/”的GET请求</p><h2 id="步骤四：定义方法，处理请求"><a href="#步骤四：定义方法，处理请求" class="headerlink" title="步骤四：定义方法，处理请求"></a>步骤四：定义方法，处理请求</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">async def root():</span><br></pre></td></tr></table></figure><h2 id="步骤五：返回响应信息"><a href="#步骤五：返回响应信息" class="headerlink" title="步骤五：返回响应信息"></a>步骤五：返回响应信息</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">return &#123;&quot;message&quot;: &quot;Hello World&quot;&#125;</span><br></pre></td></tr></table></figure><h2 id="步骤六：运行"><a href="#步骤六：运行" class="headerlink" title="步骤六：运行"></a>步骤六：运行</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">uvicorn main:app --reload</span><br></pre></td></tr></table></figure><h1 id="获取路径参数"><a href="#获取路径参数" class="headerlink" title="获取路径参数"></a>获取路径参数</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">from fastapi import FastAPI</span><br><span class="line"></span><br><span class="line">app = FastAPI()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">@app.get(&quot;/items/&#123;item_id&#125;&quot;)</span><br><span class="line">async def read_item(item_id):</span><br><span class="line">    return &#123;&quot;item_id&quot;: item_id&#125;</span><br></pre></td></tr></table></figure><p>路径中的item_id将会被解析，传递给方法中的item_id。请求<a href="http://127.0.0.1:8000/items/foo会返回如下结果：" target="_blank" rel="noopener">http://127.0.0.1:8000/items/foo会返回如下结果：</a><br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;&quot;item_id&quot;:&quot;foo&quot;&#125;</span><br></pre></td></tr></table></figure><p></p><p>也可以在方法中定义参数类型：<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">from fastapi import FastAPI</span><br><span class="line"></span><br><span class="line">app = FastAPI()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">@app.get(&quot;/items/&#123;item_id&#125;&quot;)</span><br><span class="line">async def read_item(item_id: int):</span><br><span class="line">    return &#123;&quot;item_id&quot;: item_id&#125;</span><br></pre></td></tr></table></figure><p></p><p>继续请求<a href="http://127.0.0.1:8000/items/3，会返回" target="_blank" rel="noopener">http://127.0.0.1:8000/items/3，会返回</a><br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;&quot;item_id&quot;:3&#125;</span><br></pre></td></tr></table></figure><p></p><p>此时的item_id是int类型的3，而不是string类型，这是因为FastAPI在解析请求时，自动根据声明的类型进行了解析<br>如果请求<a href="http://127.0.0.1:8000/items/foo，此时会返回：" target="_blank" rel="noopener">http://127.0.0.1:8000/items/foo，此时会返回：</a><br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    &quot;detail&quot;: [</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;loc&quot;: [</span><br><span class="line">                &quot;path&quot;,</span><br><span class="line">                &quot;item_id&quot;</span><br><span class="line">            ],</span><br><span class="line">            &quot;msg&quot;: &quot;value is not a valid integer&quot;,</span><br><span class="line">            &quot;type&quot;: &quot;type_error.integer&quot;</span><br><span class="line">        &#125;</span><br><span class="line">    ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><p>这是因为foo并不能转换成int类型。请求<a href="http://127.0.0.1:8000/items/4.2也会出现上述错误" target="_blank" rel="noopener">http://127.0.0.1:8000/items/4.2也会出现上述错误</a></p><blockquote><p>所有的数据类型验证，都是通过Pydantic完成的</p></blockquote><p>如果想对路径参数做一个预定义，可以使用Enum：<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">from enum import Enum</span><br><span class="line"></span><br><span class="line">from fastapi import FastAPI</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class ModelName(str, Enum):</span><br><span class="line">    alexnet = &quot;alexnet&quot;</span><br><span class="line">    resnet = &quot;resnet&quot;</span><br><span class="line">    lenet = &quot;lenet&quot;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">app = FastAPI()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">@app.get(&quot;/model/&#123;model_name&#125;&quot;)</span><br><span class="line">async def get_model(model_name: ModelName):</span><br><span class="line">    if model_name == ModelName.alexnet:</span><br><span class="line">        return &#123;&quot;model_name&quot;: model_name, &quot;message&quot;: &quot;Deep Learning FTW!&quot;&#125;</span><br><span class="line">    if model_name.value == &quot;lenet&quot;:</span><br><span class="line">        return &#123;&quot;model_name&quot;: model_name, &quot;message&quot;: &quot;LeCNN all the images&quot;&#125;</span><br><span class="line">    return &#123;&quot;model_name&quot;: model_name, &quot;message&quot;: &quot;Have some residuals&quot;&#125;</span><br></pre></td></tr></table></figure><p></p><p>打开<a href="http://127.0.0.1:8000/docs" target="_blank" rel="noopener">http://127.0.0.1:8000/docs</a>:<br><img src="/fastapi教程进阶（一）.assets/1.png" alt><br>除此之外，假如想接收一个路径参数，它本身就是一个路径，就像/files/{file_path}，而这个file_path是home/johndoe/myfile.txt时，可以写成/files/{file_path:path}：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">from fastapi import FastAPI</span><br><span class="line"></span><br><span class="line">app = FastAPI()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">@app.get(&quot;/files/&#123;file_path:path&#125;&quot;)</span><br><span class="line">async def read_user_me(file_path: str):</span><br><span class="line">    return &#123;&quot;file_path&quot;: file_path&#125;</span><br></pre></td></tr></table></figure><pre><code>OpenAPI本身不支持在路径参数包含路径，但是可以当作Starlette内部的一个使用方法</code></pre><p>此时访问<a href="http://127.0.0.1:8000/files/home/johndoe/myfile.txt，返回：" target="_blank" rel="noopener">http://127.0.0.1:8000/files/home/johndoe/myfile.txt，返回：</a><br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;&quot;file_path&quot;:&quot;home/johndoe/myfile.txt&quot;&#125;</span><br></pre></td></tr></table></figure><p></p><p>如果将路径改为/files/{file_path}，会返回：<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;&quot;detail&quot;:&quot;Not Found&quot;&#125;</span><br></pre></td></tr></table></figure><p></p><h1 id="获取查询参数"><a href="#获取查询参数" class="headerlink" title="获取查询参数"></a>获取查询参数</h1><p>这里依旧是一个例子：<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">from fastapi import FastAPI</span><br><span class="line"></span><br><span class="line">app = FastAPI()</span><br><span class="line"></span><br><span class="line">fake_items_db = [&#123;&quot;item_name&quot;: &quot;Foo&quot;&#125;, &#123;&quot;item_name&quot;: &quot;Bar&quot;&#125;, &#123;&quot;item_name&quot;: &quot;Baz&quot;&#125;]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">@app.get(&quot;/items/&quot;)</span><br><span class="line">async def read_item(skip: int = 0, limit: int = 10):</span><br><span class="line">    return fake_items_db[skip : skip + limit]</span><br></pre></td></tr></table></figure><p></p><p>尝试访问<a href="http://127.0.0.1:8000/items/?skip=0&amp;limit=2，返回：" target="_blank" rel="noopener">http://127.0.0.1:8000/items/?skip=0&amp;limit=2，返回：</a><br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[&#123;&quot;item_name&quot;:&quot;Foo&quot;&#125;,&#123;&quot;item_name&quot;:&quot;Bar&quot;&#125;]</span><br></pre></td></tr></table></figure><p></p><p>尝试访问<a href="http://127.0.0.1:8000/items/，返回：" target="_blank" rel="noopener">http://127.0.0.1:8000/items/，返回：</a><br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[&#123;&quot;item_name&quot;:&quot;Foo&quot;&#125;,&#123;&quot;item_name&quot;:&quot;Bar&quot;&#125;,&#123;&quot;item_name&quot;:&quot;Baz&quot;&#125;]</span><br></pre></td></tr></table></figure><p></p><p>由于我们在定义方法的时候，分别赋予skip和limit默认值，当不添加querystring时，会使用默认值。当然，我们也可以将默认值赋值为None：<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">from fastapi import FastAPI</span><br><span class="line"></span><br><span class="line">app = FastAPI()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">@app.get(&quot;/items/&#123;item_id&#125;&quot;)</span><br><span class="line">async def read_item(item_id: str, q: str = None):</span><br><span class="line">    if q:</span><br><span class="line">        return &#123;&quot;item_id&quot;: item_id, &quot;q&quot;: q&#125;</span><br><span class="line">    return &#123;&quot;item_id&quot;: item_id&#125;</span><br></pre></td></tr></table></figure><p></p><p>此时，我们请求<a href="http://127.0.0.1:8000/items/1?q=qqq" target="_blank" rel="noopener">http://127.0.0.1:8000/items/1?q=qqq</a>:<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;&quot;item_id&quot;:&quot;1&quot;,&quot;q&quot;:&quot;qqq&quot;&#125;</span><br></pre></td></tr></table></figure><p></p><blockquote><p>值得放心的一点是，FastAPI很聪明，他知道参数来自哪里～</p></blockquote><p>假如，我们不给参数默认值会发生什么情况呢？这里还是一个例子：<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">from fastapi import FastAPI</span><br><span class="line"></span><br><span class="line">app = FastAPI()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">@app.get(&quot;/items/&#123;item_id&#125;&quot;)</span><br><span class="line">async def read_user_item(item_id: str, needy: str):</span><br><span class="line">    item = &#123;&quot;item_id&quot;: item_id, &quot;needy&quot;: needy&#125;</span><br><span class="line">    return item</span><br></pre></td></tr></table></figure><p></p><p>继续请求<a href="http://127.0.0.1:8000/items/1，会发现，返回报错：" target="_blank" rel="noopener">http://127.0.0.1:8000/items/1，会发现，返回报错：</a><br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  &quot;detail&quot;: [</span><br><span class="line">    &#123;</span><br><span class="line">      &quot;loc&quot;: [</span><br><span class="line">        &quot;query&quot;,</span><br><span class="line">        &quot;needy&quot;</span><br><span class="line">      ],</span><br><span class="line">      &quot;msg&quot;: &quot;field required&quot;,</span><br><span class="line">      &quot;type&quot;: &quot;value_error.missing&quot;</span><br><span class="line">    &#125;</span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Wed Jan 15 2020 10:34:46 GMT+0800 (GMT+08:00) --&gt;&lt;h1 id=&quot;一个简单的栗子&quot;&gt;&lt;a href=&quot;#一个简单的栗子&quot; class=&quot;headerlink&quot; title=&quot;一个简单的栗子&quot;&gt;&lt;/a&gt;一个简单的栗子&lt;/h1&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;from fastapi import FastAPI&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;app = FastAPI()&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;@app.get(&amp;quot;/&amp;quot;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;async def root():&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    return &amp;#123;&amp;quot;message&amp;quot;: &amp;quot;Hello World&amp;quot;&amp;#125;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;blockquote&gt;&lt;p&gt;FASTAPI继承Starlette，因此在Starlette中的所有可调用的对象在FASTAPI中可以直接引用&lt;/p&gt;&lt;/blockquote&gt;
    
    </summary>
    
      <category term="Python" scheme="https://yongnights.github.io/categories/Python/"/>
    
      <category term="FastAPI" scheme="https://yongnights.github.io/categories/Python/FastAPI/"/>
    
    
      <category term="Python" scheme="https://yongnights.github.io/tags/Python/"/>
    
      <category term="FastAPI" scheme="https://yongnights.github.io/tags/FastAPI/"/>
    
  </entry>
  
  <entry>
    <title>Jenkins 使用 SonarQube 扫描 Coding</title>
    <link href="https://yongnights.github.io/2020/01/14/Jenkins%20%E4%BD%BF%E7%94%A8%20SonarQube%20%E6%89%AB%E6%8F%8F%20Coding/"/>
    <id>https://yongnights.github.io/2020/01/14/Jenkins 使用 SonarQube 扫描 Coding/</id>
    <published>2020-01-14T08:25:14.197Z</published>
    <updated>2020-01-14T08:53:49.242Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Tue Jan 14 2020 16:55:04 GMT+0800 (GMT+08:00) --><p>系统环境：</p><ul><li>Jenkins 版本：2.176</li><li>SonarQube 版本：7.4.0</li></ul><h1 id="一、SonarQube-介绍"><a href="#一、SonarQube-介绍" class="headerlink" title="一、SonarQube 介绍"></a>一、SonarQube 介绍</h1><h2 id="1、SonarQube-简介"><a href="#1、SonarQube-简介" class="headerlink" title="1、SonarQube 简介"></a>1、SonarQube 简介</h2><p>SonarQube 是一个用于代码质量管理的开源平台，用于管理源代码的质量。同时 SonarQube 还对大量的持续集成工具提供了接口支持，可以很方便地在持续集成中使用 SonarQube。此外， SonarQube 的插件还可以对 Java 以外的其他编程语言提供支持，对国际化以及报告文档化也有良好的支持。</p><h2 id="2、SonarQube工作原理"><a href="#2、SonarQube工作原理" class="headerlink" title="2、SonarQube工作原理"></a>2、SonarQube工作原理</h2><p>SonarQube 并不是简单地将各种质量检测工具的结果直接展现给客户，而是通过不同的插件算法来对这些结果进行再加工，最终以量化的方式来衡量代码质量，从而方便地对不同规模和种类的工程进行相应的代码质量管理。</p><h2 id="3、SonarQube-特性"><a href="#3、SonarQube-特性" class="headerlink" title="3、SonarQube 特性"></a>3、SonarQube 特性</h2><pre><code>多语言的平台： 支持超过20种编程语言，包括Java、Python、C#、C/C++、JavaScript等常用语言。自定义规则： 用户可根据不同项目自定义Quality Profile以及Quality Gates。丰富的插件： SonarQube 拥有丰富的插件，从而拥有强大的可扩展性。持续集成： 通过对某项目的持续扫描，可以对该项目的代码质量做长期的把控，并且预防新增代码中的不严谨和冗余。质量门： 在扫描代码后可以通过对“质量门”的比对判定此次“构建”的结果是否通过，质量门可以由用户定义，由多维度判定是否通过。</code></pre><h2 id="4、需要注意的代码质量问题"><a href="#4、需要注意的代码质量问题" class="headerlink" title="4、需要注意的代码质量问题"></a>4、需要注意的代码质量问题</h2><pre><code>(1)、不遵循代码标准： SonarQube可以通过PMD,CheckStyle,Findbugs等等代码规则检测工具规 范代码编写。(2)、糟糕的复杂度分布： 文件、类、方法等，如果复杂度过高将难以改变，这会使得开发人员难以理解它们且如果没有自动化的单元测试，对于程序中的任何组件的改变都将可能导致需要全面的回归测试。(3)、注释不足或者过多： 没有注释将使代码可读性变差，特别是当不可避免地出现人员变动 时，程序的可读性将大幅下降而过多的注释又会使得开发人员将精力过多地花费在阅读注释上，亦违背初衷。(4)、缺乏单元测试： SonarQube 可以很方便地统计并展示单元测试覆盖率。(5)、潜在的缺陷： –SonarQube 可以通过PMD,CheckStyle,Findbugs等等代码规则检测工具检 测出潜在的缺陷。(6)、重复： 显然程序中包含大量复制粘贴的代码是质量低下的，SonarQube 源码中重复严重的地方。(7)、糟糕的设计</code></pre><a id="more"></a><h1 id="二、一般执行流程"><a href="#二、一般执行流程" class="headerlink" title="二、一般执行流程"></a>二、一般执行流程</h1><p>在项目中一般流程为：</p><pre><code>(1)、项目人员开发代码。(2)、将代码推送到持久化仓库，如 Git。(3)、Jenkins 进行代码拉取，然后利用 SonarQube 扫描器进行扫描分析代码信息。(4)、将分析结果等信息上传至 SonarQube Server 服务器进行分类处理。(5)、SonarQube 将分析结果等信息持久化到数据库，如 Mysql。(6)、开发人员访问 SonarQube UI 界面访问，查看扫描出的结果信息进行项目优化。</code></pre><p>这里只描述 Jenkins 如何与 SonarQube 集成</p><pre><code>执行过程流程图</code></pre><p><img src="/Jenkins_SonarQube/jenkins-sona-1002.jpg" alt></p><h1 id="三、SonarQuke-配置"><a href="#三、SonarQuke-配置" class="headerlink" title="三、SonarQuke 配置"></a>三、SonarQuke 配置</h1><h2 id="1、禁用SCM传感器"><a href="#1、禁用SCM传感器" class="headerlink" title="1、禁用SCM传感器"></a>1、禁用SCM传感器</h2><pre><code>点击 配置—SCM—Disable the SCM Sensor 将其关闭。</code></pre><p><img src="/Jenkins_SonarQube/jenkins-sona-1003.jpg" alt></p><h2 id="2、安装-JAVA-分析插件"><a href="#2、安装-JAVA-分析插件" class="headerlink" title="2、安装 JAVA 分析插件"></a>2、安装 JAVA 分析插件</h2><p>由于这里要分析的项目是 JAVA 项目，所以需要确保安装 Java 语言分析插件，如果是别的类型的项目，可以类似安装相关分析插件即可。</p><ul><li>点击 配置—应用市场—插件 搜索 SonarJava 插件安装</li></ul><blockquote><p>如果忘记安装，可能会导致 Jenkins 编译过程中提示没有语言插件的异常错误信息,确保一定要安装。<br><img src="/Jenkins_SonarQube/jenkins-sona-1004.jpg" alt></p></blockquote><h2 id="3、生成-Token"><a href="#3、生成-Token" class="headerlink" title="3、生成 Token"></a>3、生成 Token</h2><p>这里生成验证用的 Token 字符串，用于 Jenkins 在执行流水线时候将待检测信息发送到 SonarQube 时候用于的安全验证。</p><ul><li>点击 头像—我的账号—安全—生成令牌 生成验证的 Token。</li></ul><blockquote><p>因为此 Token 不会显示第二次，所以这里记住此 Token。<br><img src="/Jenkins_SonarQube/jenkins-sona-1005.jpg" alt></p></blockquote><h1 id="四、Jenkins-安装插件"><a href="#四、Jenkins-安装插件" class="headerlink" title="四、Jenkins 安装插件"></a>四、Jenkins 安装插件</h1><h2 id="1、需要安装的插件介绍"><a href="#1、需要安装的插件介绍" class="headerlink" title="1、需要安装的插件介绍"></a>1、需要安装的插件介绍</h2><p>Jenkins 先提前安装好可能需要用到的插件，这里需要用到一下插件：</p><ul><li>Maven Integration</li></ul><p>Maven 插件，用于编译 Maven 项目和安装 Maven 工具到任务中。</p><ul><li>Pipeline Utility Steps</li></ul><blockquote><p>参考：<a href="https://jenkins.io/doc/pipeline/steps/pipeline-utility-steps/" target="_blank" rel="noopener">https://jenkins.io/doc/pipeline/steps/pipeline-utility-steps/</a></p></blockquote><p>用于在 Pipeline 执行过程中操作文件“读/写”的插件，这里用其创建 Sonar properties 配置文件。</p><ul><li>SonarQube Scanner</li></ul><blockquote><p>参考：<a href="https://docs.sonarqube.org/display/SCAN/Analyzing+with+SonarQube+Scanner+for+Jenkins" target="_blank" rel="noopener">https://docs.sonarqube.org/display/SCAN/Analyzing+with+SonarQube+Scanner+for+Jenkins</a></p></blockquote><p>SonarQube 是一种用于连续检查代码质量的开源平台，该插件可轻松与 SonarQube 集成。</p><h2 id="2、安装-SonarQube-Scanner-插件"><a href="#2、安装-SonarQube-Scanner-插件" class="headerlink" title="2、安装 SonarQube Scanner 插件"></a>2、安装 SonarQube Scanner 插件</h2><p>打开 系统管理—插件管理—可选插件 输入 sonarqube 进行插件筛选，如下如方式进行安装。<br><img src="/Jenkins_SonarQube/jenkins-sona-1006.jpg" alt><br>关于安装 Pipeline Utility Steps 与 Maven Integration 插件和上面类似，请自行安装即可，这里不过多描述。</p><h1 id="五、Jenkins-配置插件"><a href="#五、Jenkins-配置插件" class="headerlink" title="五、Jenkins 配置插件"></a>五、Jenkins 配置插件</h1><h2 id="1、连接-SonarQube-配置"><a href="#1、连接-SonarQube-配置" class="headerlink" title="1、连接 SonarQube 配置"></a>1、连接 SonarQube 配置</h2><p>打开 系统管理—系统设置—SonarQube servers 配置下面属性<br><img src="/Jenkins_SonarQube/jenkins-sona-1007.jpg" alt><br>参数说明：</p><ul><li>Name： 用于 Jenklins Pipeline 中构建环境指定的名称，在 Pipeline 脚本中会用到，自定义即可。</li><li>Server URL： SonarQube 地址。</li><li>Server authentication token： 用于连接 SonarQube 的 Token，将上面 SonarQube 中生成的 Token 输入即可。</li></ul><h2 id="2、配置-SonarQube-Scanner-插件"><a href="#2、配置-SonarQube-Scanner-插件" class="headerlink" title="2、配置 SonarQube Scanner 插件"></a>2、配置 SonarQube Scanner 插件</h2><p>打开 系统管理—全局工具配置—SonarQube Scanner 输入 Name，选择最新版本点击自动安装即可<br><img src="/Jenkins_SonarQube/jenkins-sona-1008.jpg" alt></p><h2 id="3、配置-Maven-插件"><a href="#3、配置-Maven-插件" class="headerlink" title="3、配置 Maven 插件"></a>3、配置 Maven 插件</h2><p>打开 系统管理—全局工具配置—Maven 输入 Name，选择最新版本点击自动安装即可<br><img src="/Jenkins_SonarQube/jenkins-sona-1009.jpg" alt></p><h1 id="六、创建流水线项目写-Pipeline-脚本"><a href="#六、创建流水线项目写-Pipeline-脚本" class="headerlink" title="六、创建流水线项目写 Pipeline 脚本"></a>六、创建流水线项目写 Pipeline 脚本</h1><h2 id="1、创建流水线任务"><a href="#1、创建流水线任务" class="headerlink" title="1、创建流水线任务"></a>1、创建流水线任务</h2><p><img src="/Jenkins_SonarQube/jenkins-sona-1010.jpg" alt></p><h2 id="2、设置-SonarQube-配置文件"><a href="#2、设置-SonarQube-配置文件" class="headerlink" title="2、设置 SonarQube 配置文件"></a>2、设置 SonarQube 配置文件</h2><p>(1)、Sonar 配置文件说明</p><p>在使用 SonarQube 来进行代码扫描时候需要一个名称为 sonar-project.properties 的配置文件。该文件设置了项目的一些属性用于 SonarQube 扫描的属性。</p><p>例如，设置项目在 Sonar 面板中的唯一标识 Key，项目名称及其版本，要扫描项目的语言类型等等。</p><blockquote><p>sonar-project.properties<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">sonar.projectKey=key:value</span><br><span class="line">sonar.projectName=ProjectName</span><br><span class="line">sonar.projectVersion=1.0.0</span><br><span class="line">sonar.sources=src</span><br><span class="line">sonar.language=java</span><br><span class="line">sonar.sourceEncoding=UTF-8</span><br><span class="line">sonar.java.binaries=target/classes</span><br><span class="line">sonar.java.source=1.8</span><br><span class="line">sonar.java.target=1.8</span><br></pre></td></tr></table></figure><p></p></blockquote><p>配置参数：</p><ul><li>sonar.projectKey： 项目在 SonarQube 的唯一标识，不能重复</li><li>sonar.projectName=ProjectName： 项目名称</li><li>sonar.projectVersion： 项目版本</li><li>sonar.language： 项目语言，例如 Java、C#、PHP 等</li><li>sonar.sourceEncoding： 编码方式</li><li>sonar.sources： 项目源代码目录</li><li>sonar.java.binaries： 编译后 class 文件目录</li></ul><p>(2)、Sonar 配置文件存放位置</p><p>这个文件可以放在源代码根目录中，也可是设置到 Jenkins 变量。</p><p>① ————————方式一：放置到源代码———————————————–</p><p>直接在源代码中放置文件 sonar-project.properties，然后在此配置文件中设置这些配置参数。<br><img src="/Jenkins_SonarQube/jenkins-sona-1011.jpg" alt><br>② ————————方式二：设置到变量并在 Jenkins 编译时候创建————————</p><p>可以设置文本到环境变量中，在变量文本中设置哪些配置参数，之后在执行 Pipeline 脚本时候利用 “Pipeline Utility Steps” 插件的创建文件方法创建 sonar-project.properties 文件。</p><p>在 Jenkins sonar-qube-coding 任务—&gt;配置—&gt;参数化构建过程—&gt;添加参数—&gt;文本参数 输入 Sonar 配置。</p><ul><li>变量名称：sonar_project_properties</li><li>变量内容：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">sonar.sources=src</span><br><span class="line">sonar.language=java</span><br><span class="line">sonar.sourceEncoding=UTF-8</span><br><span class="line">sonar.java.binaries=target/classes</span><br><span class="line">sonar.java.source=1.8</span><br><span class="line">sonar.java.target=1.8</span><br></pre></td></tr></table></figure></li></ul><blockquote><p>注意:这里不设置 sonar.projectKey、sonar.projectName、sonar.projectVersion 这三个参数，将三个参数在执行 Pipeline 脚本的时候设置。<br><img src="/Jenkins_SonarQube/jenkins-sona-1012.jpg" alt><br>这里为了配置更灵活方便，所以采用将 SonarQube 配置设置到环境变量</p></blockquote><h2 id="3、创建-Pipeline-脚本"><a href="#3、创建-Pipeline-脚本" class="headerlink" title="3、创建 Pipeline 脚本"></a>3、创建 Pipeline 脚本</h2><p>配置 Jenkins 任务，创建脚本并加入到 “流水线” 配置项中<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">// 设置超时时间为10分钟，如果未成功则结束任务</span><br><span class="line">timeout(time: 600, unit: &apos;SECONDS&apos;) &#123;</span><br><span class="line">    node () &#123;</span><br><span class="line">        stage(&apos;Git 拉取阶段&apos;)&#123;</span><br><span class="line">            // Git 拉取代码</span><br><span class="line">            git branch: &quot;master&quot; ,changelog: true , url: &quot;https://github.com/a324670547/springboot-helloworld&quot;</span><br><span class="line">        &#125;</span><br><span class="line">        stage(&apos;Maven 编译阶段&apos;) &#123;</span><br><span class="line">            // 设置 Maven 工具,引用先前全局工具配置中设置工具的名称</span><br><span class="line">            def m3 = tool name: &apos;maven&apos;</span><br><span class="line">            // 执行 Maven 命令</span><br><span class="line">            sh &quot;$&#123;m3&#125;/bin/mvn -B -e clean install -Dmaven.test.skip=true&quot;</span><br><span class="line">        &#125;</span><br><span class="line">        stage(&apos;SonarQube 扫描阶段&apos;)&#123;</span><br><span class="line">            // 读取maven变量</span><br><span class="line">            pom = readMavenPom file: &quot;./pom.xml&quot;</span><br><span class="line">            // 创建SonarQube配置文件</span><br><span class="line">            writeFile file: &apos;sonar-project.properties&apos;, </span><br><span class="line">                      text: &quot;&quot;&quot;sonar.projectKey=$&#123;pom.artifactId&#125;:$&#123;pom.version&#125;\n&quot;&quot;&quot;+</span><br><span class="line">                            &quot;&quot;&quot;sonar.projectName=$&#123;pom.artifactId&#125;\n&quot;&quot;&quot;+</span><br><span class="line">                            &quot;&quot;&quot;sonar.projectVersion=$&#123;pom.version&#125;\n&quot;&quot;&quot;+</span><br><span class="line">                            &quot;&quot;&quot;$&#123;sonar_project_properties&#125;&quot;&quot;&quot;</span><br><span class="line">            // 设置 SonarQube 代码扫描工具,引用先前全局工具配置中设置工具的名称</span><br><span class="line">            def sonarqubeScanner = tool name: &apos;sonar-scanner&apos;</span><br><span class="line">            // 设置 SonarQube 环境,其中参数设置为之前系统设置中SonarQuke服务器配置的 Name</span><br><span class="line">            withSonarQubeEnv(&apos;jenkins&apos;) &#123;</span><br><span class="line">                // 执行代码扫描</span><br><span class="line">                sh &quot;$&#123;sonarqubeScanner&#125;/bin/sonar-scanner&quot;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><p><img src="/Jenkins_SonarQube/jenkins-sona-1013.jpg" alt></p><h1 id="七、执行-Jenkins-任务"><a href="#七、执行-Jenkins-任务" class="headerlink" title="七、执行 Jenkins 任务"></a>七、执行 Jenkins 任务</h1><h2 id="1、执行-Jenkins-Pipeline-任务"><a href="#1、执行-Jenkins-Pipeline-任务" class="headerlink" title="1、执行 Jenkins Pipeline 任务"></a>1、执行 Jenkins Pipeline 任务</h2><p>点击 Build with Parameters 执行 Jenkins 任务<br><img src="/Jenkins_SonarQube/jenkins-sona-1014.jpg" alt></p><h2 id="2、查看任务执行日志"><a href="#2、查看任务执行日志" class="headerlink" title="2、查看任务执行日志"></a>2、查看任务执行日志</h2><p>查看日志信息为：<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br></pre></td><td class="code"><pre><span class="line">......</span><br><span class="line">[Pipeline] &#123; (Git 拉取阶段)</span><br><span class="line">[Pipeline] echo</span><br><span class="line">Git 阶段</span><br><span class="line"> &gt; git rev-parse --is-inside-work-tree # timeout=10</span><br><span class="line">Fetching changes from the remote Git repository</span><br><span class="line"> &gt; git config remote.origin.url https://github.com/a324670547/springboot-helloworld # timeout=10</span><br><span class="line">Fetching upstream changes from https://github.com/a324670547/springboot-helloworld</span><br><span class="line"> &gt; git --version # timeout=10</span><br><span class="line"> &gt; git fetch --tags --progress https://github.com/a324670547/springboot-helloworld </span><br><span class="line">Commit message: &quot;修改jenkinsfile&quot;</span><br><span class="line"> &gt; git rev-list --no-walk a34691106075d58bc99d9dcc06f5eadcc03ca759 # timeout=10</span><br><span class="line">[Pipeline] &#123; (Maven 编译阶段)</span><br><span class="line">[Pipeline] tool</span><br><span class="line">+ /var/jenkins_home/tools/hudson.tasks.Maven_MavenInstallation/maven/bin/mvn -B -e clean install -Dmaven.test.skip=true</span><br><span class="line">[INFO] Error stacktraces are turned on.</span><br><span class="line">[INFO] Scanning for projects...</span><br><span class="line">[INFO] </span><br><span class="line">[INFO] ------------------&lt; club.mydlq:springboot-helloworld &gt;------------------</span><br><span class="line">[INFO] Building springboot-helloworld 0.0.1</span><br><span class="line">[INFO] --------------------------------[ jar ]---------------------------------</span><br><span class="line">[INFO] </span><br><span class="line">[INFO] --- maven-clean-plugin:3.1.0:clean (default-clean) @ springboot-helloworld ---</span><br><span class="line">[INFO] Deleting /var/jenkins_home/workspace/sonar-qube-coding/target</span><br><span class="line">[INFO] </span><br><span class="line">[INFO] --- maven-resources-plugin:3.1.0:resources (default-resources) @ springboot-helloworld ---</span><br><span class="line">[INFO] Using &apos;UTF-8&apos; encoding to copy filtered resources.</span><br><span class="line">[INFO] Copying 1 resource</span><br><span class="line">[INFO] Copying 0 resource</span><br><span class="line">[INFO] </span><br><span class="line">[INFO] --- maven-compiler-plugin:3.8.0:compile (default-compile) @ springboot-helloworld ---</span><br><span class="line">[INFO] Changes detected - recompiling the module!</span><br><span class="line">[INFO] Compiling 2 source files to /var/jenkins_home/workspace/sonar-qube-coding/target/classes</span><br><span class="line">[INFO] </span><br><span class="line">[INFO] --- maven-resources-plugin:3.1.0:testResources (default-testResources) @ springboot-helloworld ---</span><br><span class="line">[INFO] Not copying test resources</span><br><span class="line">[INFO] </span><br><span class="line">[INFO] --- maven-compiler-plugin:3.8.0:testCompile (default-testCompile) @ springboot-helloworld ---</span><br><span class="line">[INFO] Not compiling test sources</span><br><span class="line">[INFO] </span><br><span class="line">[INFO] --- maven-surefire-plugin:2.22.1:test (default-test) @ springboot-helloworld ---</span><br><span class="line">[INFO] Tests are skipped.</span><br><span class="line">[INFO] </span><br><span class="line">[INFO] --- maven-jar-plugin:3.1.1:jar (default-jar) @ springboot-helloworld ---</span><br><span class="line">[INFO] Building jar: /var/jenkins_home/workspace/sonar-qube-coding/target/springboot-helloworld-0.0.1.jar</span><br><span class="line">[INFO] </span><br><span class="line">[INFO] --- spring-boot-maven-plugin:2.1.4.RELEASE:repackage (repackage) @ springboot-helloworld ---</span><br><span class="line">[INFO] Replacing main artifact with repackaged archive</span><br><span class="line">[INFO] </span><br><span class="line">[INFO] --- maven-install-plugin:2.5.2:install (default-install) @ springboot-helloworld ---</span><br><span class="line">[INFO] Installing /var/jenkins_home/workspace/sonar-qube-coding/target/springboot-helloworld-0.0.1.jar to /root/.m2/repository/club/mydlq/springboot-helloworld/0.0.1/springboot-helloworld-0.0.1.jar</span><br><span class="line">[INFO] Installing /var/jenkins_home/workspace/sonar-qube-coding/pom.xml to /root/.m2/repository/club/mydlq/springboot-helloworld/0.0.1/springboot-helloworld-0.0.1.pom</span><br><span class="line">[INFO] ------------------------------------------------------------------------</span><br><span class="line">[INFO] BUILD SUCCESS</span><br><span class="line">[INFO] ------------------------------------------------------------------------</span><br><span class="line">[INFO] Total time:  3.695 s</span><br><span class="line">[INFO] Finished at: 2019-05-09T17:59:45Z</span><br><span class="line">[INFO] ------------------------------------------------------------------------</span><br><span class="line">[Pipeline] &#125;</span><br><span class="line">[Pipeline] &#123; (SonarQube 扫描阶段)</span><br><span class="line">[Pipeline] readMavenPom</span><br><span class="line">[Pipeline] writeFile</span><br><span class="line">[Pipeline] tool</span><br><span class="line">[Pipeline] withSonarQubeEnv</span><br><span class="line">Injecting SonarQube environment variables using the configuration: jenkins</span><br><span class="line">[Pipeline] &#123;</span><br><span class="line">[Pipeline] sh</span><br><span class="line">+ /var/jenkins_home/tools/hudson.plugins.sonar.SonarRunnerInstallation/sonar-scanner/bin/sonar-scanner</span><br><span class="line">INFO: Scanner configuration file: /var/jenkins_home/tools/hudson.plugins.sonar.SonarRunnerInstallation/sonar-scanner/conf/sonar-scanner.properties</span><br><span class="line">INFO: Project root configuration file: /var/jenkins_home/workspace/sonar-qube-coding/sonar-project.properties</span><br><span class="line">INFO: SonarQube Scanner 3.3.0.1492</span><br><span class="line">INFO: Java 1.8.0_212 Oracle Corporation (64-bit)</span><br><span class="line">INFO: Linux 3.10.0-957.1.3.el7.x86_64 amd64</span><br><span class="line">INFO: User cache: /root/.sonar/cache</span><br><span class="line">INFO: SonarQube server 7.4.0</span><br><span class="line">INFO: Default locale: &quot;en&quot;, source code encoding: &quot;UTF-8&quot;</span><br><span class="line">INFO: Publish mode</span><br><span class="line">INFO: Load global settings</span><br><span class="line">INFO: Load global settings (done) | time=89ms</span><br><span class="line">INFO: Server id: D549D2A8-AWpYoogtP1ytl0VN9Fsr</span><br><span class="line">INFO: User cache: /root/.sonar/cache</span><br><span class="line">INFO: Load/download plugins</span><br><span class="line">INFO: Load plugins index</span><br><span class="line">INFO: Load plugins index (done) | time=31ms</span><br><span class="line">INFO: Plugin [l10nzh] defines &apos;l10nen&apos; as base plugin. This metadata can be removed from manifest of l10n plugins since version 5.2.</span><br><span class="line">INFO: Load/download plugins (done) | time=38ms</span><br><span class="line">INFO: Loaded core extensions: </span><br><span class="line">INFO: Process project properties</span><br><span class="line">INFO: Load project repositories</span><br><span class="line">INFO: Load project repositories (done) | time=11ms</span><br><span class="line">INFO: Load quality profiles</span><br><span class="line">INFO: Load quality profiles (done) | time=32ms</span><br><span class="line">INFO: Load active rules</span><br><span class="line">INFO: Load active rules (done) | time=201ms</span><br><span class="line">INFO: Load metrics repository</span><br><span class="line">INFO: Load metrics repository (done) | time=24ms</span><br><span class="line">INFO: Project key: springboot-helloworld:0.0.1</span><br><span class="line">INFO: Project base dir: /var/jenkins_home/workspace/sonar-qube-coding</span><br><span class="line">INFO: -------------  Scan springboot-helloworld</span><br><span class="line">INFO: Base dir: /var/jenkins_home/workspace/sonar-qube-coding</span><br><span class="line">INFO: Working dir: /var/jenkins_home/workspace/sonar-qube-coding/.scannerwork</span><br><span class="line">INFO: Source paths: src</span><br><span class="line">INFO: Source encoding: UTF-8, default locale: en</span><br><span class="line">INFO: Load server rules</span><br><span class="line">INFO: Load server rules (done) | time=109ms</span><br><span class="line">INFO: Language is forced to java</span><br><span class="line">INFO: Index files</span><br><span class="line">WARN: File &apos;/var/jenkins_home/workspace/sonar-qube-coding/src/main/resources/application.yaml&apos; is ignored because it doesn&apos;t belong to the forced language &apos;java&apos;</span><br><span class="line">INFO: 2 files indexed</span><br><span class="line">INFO: Quality profile for java: Sonar way</span><br><span class="line">INFO: Sensor JavaSquidSensor [java]</span><br><span class="line">INFO: Configured Java source version (sonar.java.source): 8</span><br><span class="line">INFO: JavaClasspath initialization</span><br><span class="line">WARN: Bytecode of dependencies was not provided for analysis of source files, you might end up with less precise results. Bytecode can be provided using sonar.java.libraries property.</span><br><span class="line">INFO: JavaClasspath initialization (done) | time=8ms</span><br><span class="line">INFO: JavaTestClasspath initialization</span><br><span class="line">INFO: JavaTestClasspath initialization (done) | time=0ms</span><br><span class="line">INFO: Java Main Files AST scan</span><br><span class="line">INFO: 2 source files to be analyzed</span><br><span class="line">INFO: 2/2 source files have been analyzed</span><br><span class="line">INFO: Java Main Files AST scan (done) | time=609ms</span><br><span class="line">INFO: Java Test Files AST scan</span><br><span class="line">INFO: 0 source files to be analyzed</span><br><span class="line">INFO: Java Test Files AST scan (done) | time=1ms</span><br><span class="line">INFO: Sensor JavaSquidSensor [java] (done) | time=1334ms</span><br><span class="line">INFO: Sensor SurefireSensor [java]</span><br><span class="line">INFO: 0/0 source files have been analyzed</span><br><span class="line">INFO: parsing [/var/jenkins_home/workspace/sonar-qube-coding/target/surefire-reports]</span><br><span class="line">INFO: Sensor SurefireSensor [java] (done) | time=55ms</span><br><span class="line">INFO: Sensor JaCoCoSensor [java]</span><br><span class="line">INFO: Sensor JaCoCoSensor [java] (done) | time=2ms</span><br><span class="line">INFO: Sensor JavaXmlSensor [java]</span><br><span class="line">INFO: Sensor JavaXmlSensor [java] (done) | time=0ms</span><br><span class="line">INFO: Sensor Zero Coverage Sensor</span><br><span class="line">INFO: Sensor Zero Coverage Sensor (done) | time=8ms</span><br><span class="line">INFO: Sensor Java CPD Block Indexer</span><br><span class="line">INFO: Sensor Java CPD Block Indexer (done) | time=10ms</span><br><span class="line">INFO: SCM Publisher is disabled</span><br><span class="line">INFO: 2 files had no CPD blocks</span><br><span class="line">INFO: Calculating CPD for 0 files</span><br><span class="line">INFO: CPD calculation finished</span><br><span class="line">INFO: Analysis report generated in 114ms, dir size=13 KB</span><br><span class="line">INFO: Analysis reports compressed in 39ms, zip size=6 KB</span><br><span class="line">INFO: Analysis report uploaded in 671ms</span><br><span class="line">INFO: ANALYSIS SUCCESSFUL, you can browse http://10.2.5.143:9000/dashboard?id=springboot-helloworld%3A0.0.1</span><br><span class="line">INFO: Note that you will be able to access the updated dashboard once the server has processed the submitted analysis report</span><br><span class="line">INFO: More about the report processing at http://10.2.5.143:9000/api/ce/task?id=AWqdwF0moB4s4osu4wHu</span><br><span class="line">INFO: Task total time: 3.412 s</span><br><span class="line">INFO: ------------------------------------------------------------------------</span><br><span class="line">INFO: EXECUTION SUCCESS</span><br><span class="line">INFO: ------------------------------------------------------------------------</span><br><span class="line">INFO: Total time: 4.469s</span><br><span class="line">INFO: Final Memory: 10M/158M</span><br><span class="line">INFO: ------------------------------------------------------------------------</span><br><span class="line">......</span><br><span class="line">Finished: SUCCESS</span><br></pre></td></tr></table></figure><p></p><p>八、SonarQube 查看代码扫描结果</p><p>登录 SonarQube 平台，查看代码扫描结果<br><img src="/Jenkins_SonarQube/jenkins-sona-1015.jpg" alt></p><p>转载地址：<a href="http://www.mydlq.club/article/11/" target="_blank" rel="noopener">http://www.mydlq.club/article/11/</a></p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Tue Jan 14 2020 16:55:04 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;系统环境：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Jenkins 版本：2.176&lt;/li&gt;&lt;li&gt;SonarQube 版本：7.4.0&lt;/li&gt;&lt;/ul&gt;&lt;h1 id=&quot;一、SonarQube-介绍&quot;&gt;&lt;a href=&quot;#一、SonarQube-介绍&quot; class=&quot;headerlink&quot; title=&quot;一、SonarQube 介绍&quot;&gt;&lt;/a&gt;一、SonarQube 介绍&lt;/h1&gt;&lt;h2 id=&quot;1、SonarQube-简介&quot;&gt;&lt;a href=&quot;#1、SonarQube-简介&quot; class=&quot;headerlink&quot; title=&quot;1、SonarQube 简介&quot;&gt;&lt;/a&gt;1、SonarQube 简介&lt;/h2&gt;&lt;p&gt;SonarQube 是一个用于代码质量管理的开源平台，用于管理源代码的质量。同时 SonarQube 还对大量的持续集成工具提供了接口支持，可以很方便地在持续集成中使用 SonarQube。此外， SonarQube 的插件还可以对 Java 以外的其他编程语言提供支持，对国际化以及报告文档化也有良好的支持。&lt;/p&gt;&lt;h2 id=&quot;2、SonarQube工作原理&quot;&gt;&lt;a href=&quot;#2、SonarQube工作原理&quot; class=&quot;headerlink&quot; title=&quot;2、SonarQube工作原理&quot;&gt;&lt;/a&gt;2、SonarQube工作原理&lt;/h2&gt;&lt;p&gt;SonarQube 并不是简单地将各种质量检测工具的结果直接展现给客户，而是通过不同的插件算法来对这些结果进行再加工，最终以量化的方式来衡量代码质量，从而方便地对不同规模和种类的工程进行相应的代码质量管理。&lt;/p&gt;&lt;h2 id=&quot;3、SonarQube-特性&quot;&gt;&lt;a href=&quot;#3、SonarQube-特性&quot; class=&quot;headerlink&quot; title=&quot;3、SonarQube 特性&quot;&gt;&lt;/a&gt;3、SonarQube 特性&lt;/h2&gt;&lt;pre&gt;&lt;code&gt;多语言的平台： 支持超过20种编程语言，包括Java、Python、C#、C/C++、JavaScript等常用语言。
自定义规则： 用户可根据不同项目自定义Quality Profile以及Quality Gates。
丰富的插件： SonarQube 拥有丰富的插件，从而拥有强大的可扩展性。
持续集成： 通过对某项目的持续扫描，可以对该项目的代码质量做长期的把控，并且预防新增代码中的不严谨和冗余。
质量门： 在扫描代码后可以通过对“质量门”的比对判定此次“构建”的结果是否通过，质量门可以由用户定义，由多维度判定是否通过。
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&quot;4、需要注意的代码质量问题&quot;&gt;&lt;a href=&quot;#4、需要注意的代码质量问题&quot; class=&quot;headerlink&quot; title=&quot;4、需要注意的代码质量问题&quot;&gt;&lt;/a&gt;4、需要注意的代码质量问题&lt;/h2&gt;&lt;pre&gt;&lt;code&gt;(1)、不遵循代码标准： SonarQube可以通过PMD,CheckStyle,Findbugs等等代码规则检测工具规 范代码编写。
(2)、糟糕的复杂度分布： 文件、类、方法等，如果复杂度过高将难以改变，这会使得开发人员难以理解它们且如果没有自动化的单元测试，对于程序中的任何组件的改变都将可能导致需要全面的回归测试。
(3)、注释不足或者过多： 没有注释将使代码可读性变差，特别是当不可避免地出现人员变动 时，程序的可读性将大幅下降而过多的注释又会使得开发人员将精力过多地花费在阅读注释上，亦违背初衷。
(4)、缺乏单元测试： SonarQube 可以很方便地统计并展示单元测试覆盖率。
(5)、潜在的缺陷： –SonarQube 可以通过PMD,CheckStyle,Findbugs等等代码规则检测工具检 测出潜在的缺陷。
(6)、重复： 显然程序中包含大量复制粘贴的代码是质量低下的，SonarQube 源码中重复严重的地方。
(7)、糟糕的设计
&lt;/code&gt;&lt;/pre&gt;
    
    </summary>
    
      <category term="Jenkins" scheme="https://yongnights.github.io/categories/Jenkins/"/>
    
      <category term="SonarQube" scheme="https://yongnights.github.io/categories/Jenkins/SonarQube/"/>
    
    
      <category term="Jenkins" scheme="https://yongnights.github.io/tags/Jenkins/"/>
    
      <category term="SonarQube" scheme="https://yongnights.github.io/tags/SonarQube/"/>
    
  </entry>
  
  <entry>
    <title>Linux yum安装PostgreSQL9.6</title>
    <link href="https://yongnights.github.io/2020/01/13/Linux%20yum%E5%AE%89%E8%A3%85PostgreSQL9.6/"/>
    <id>https://yongnights.github.io/2020/01/13/Linux yum安装PostgreSQL9.6/</id>
    <published>2020-01-13T06:49:57.590Z</published>
    <updated>2020-01-13T06:50:29.540Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Mon Jan 13 2020 14:59:26 GMT+0800 (GMT+08:00) --><p>PostgreSQL10版本的主从安装配置在 <a href="https://www.cnblogs.com/virtulreal/p/11675841.html" target="_blank" rel="noopener">https://www.cnblogs.com/virtulreal/p/11675841.html</a></p><h2 id="一、下载安装"><a href="#一、下载安装" class="headerlink" title="一、下载安装"></a>一、下载安装</h2><h3 id="1、创建PostgreSQL9-6的yum源文件"><a href="#1、创建PostgreSQL9-6的yum源文件" class="headerlink" title="1、创建PostgreSQL9.6的yum源文件"></a>1、创建PostgreSQL9.6的yum源文件</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ yum install https://download.postgresql.org/pub/repos/yum/9.6/redhat/rhel-7-x86_64/pgdg-centos96-9.6-3.noarch.rpm</span><br></pre></td></tr></table></figure><h3 id="2、安装PostgreSQL客户端"><a href="#2、安装PostgreSQL客户端" class="headerlink" title="2、安装PostgreSQL客户端"></a>2、安装PostgreSQL客户端</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ yum install postgresql96</span><br></pre></td></tr></table></figure><a id="more"></a><h3 id="3、安装PostgreSQL服务端"><a href="#3、安装PostgreSQL服务端" class="headerlink" title="3、安装PostgreSQL服务端"></a>3、安装PostgreSQL服务端</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ yum install postgresql96-server</span><br></pre></td></tr></table></figure><h3 id="4、安装PostgreSQL拓展包-可选"><a href="#4、安装PostgreSQL拓展包-可选" class="headerlink" title="4、安装PostgreSQL拓展包(可选)"></a>4、安装PostgreSQL拓展包(可选)</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ yum install postgresql96-devel.x86_64</span><br></pre></td></tr></table></figure><h3 id="5、安装PostgreSQL的附加模块（可选）"><a href="#5、安装PostgreSQL的附加模块（可选）" class="headerlink" title="5、安装PostgreSQL的附加模块（可选）"></a>5、安装PostgreSQL的附加模块（可选）</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ yum install postgresql96-contrib.x86_64</span><br></pre></td></tr></table></figure><h2 id="二、配置初始化"><a href="#二、配置初始化" class="headerlink" title="二、配置初始化"></a>二、配置初始化</h2><h3 id="初始化数据库"><a href="#初始化数据库" class="headerlink" title="初始化数据库"></a>初始化数据库</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ /usr/pgsql-9.6/bin/postgresql96-setup initdb</span><br></pre></td></tr></table></figure><h3 id="启动postgresql服务，并设置为开机自动启动"><a href="#启动postgresql服务，并设置为开机自动启动" class="headerlink" title="启动postgresql服务，并设置为开机自动启动"></a>启动postgresql服务，并设置为开机自动启动</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ systemctl enable postgresql-9.6</span><br><span class="line">$ systemctl start postgresql-9.6</span><br></pre></td></tr></table></figure><h2 id="postgres用户初始配置"><a href="#postgres用户初始配置" class="headerlink" title="postgres用户初始配置"></a>postgres用户初始配置</h2><h3 id="安装完成后，操作系统会自动创建一个postgres用户用来管理数据库，为其初始化密码-输入命令后连输2次密码-："><a href="#安装完成后，操作系统会自动创建一个postgres用户用来管理数据库，为其初始化密码-输入命令后连输2次密码-：" class="headerlink" title="安装完成后，操作系统会自动创建一个postgres用户用来管理数据库，为其初始化密码(输入命令后连输2次密码)："></a>安装完成后，操作系统会自动创建一个postgres用户用来管理数据库，为其初始化密码(输入命令后连输2次密码)：</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ passwd postgres</span><br></pre></td></tr></table></figure><h2 id="数据库初始配置"><a href="#数据库初始配置" class="headerlink" title="数据库初始配置"></a>数据库初始配置</h2><h3 id="使用数据库自带的postgres用户登录数据库-并为其赋予密码"><a href="#使用数据库自带的postgres用户登录数据库-并为其赋予密码" class="headerlink" title="使用数据库自带的postgres用户登录数据库,并为其赋予密码"></a>使用数据库自带的postgres用户登录数据库,并为其赋予密码</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ su - postgres</span><br><span class="line">$ psql -U postgres</span><br><span class="line">alter user postgres with password &apos;你的密码&apos;;</span><br></pre></td></tr></table></figure><h2 id="配置远程连接"><a href="#配置远程连接" class="headerlink" title="配置远程连接"></a>配置远程连接</h2><blockquote><p>可能在/var/lib/pgsql/9.6/data下，可以</p></blockquote><h3 id="1、使用find-name-‘pg-hba-conf’查找到pg-hba-conf，修改pg-hba-conf"><a href="#1、使用find-name-‘pg-hba-conf’查找到pg-hba-conf，修改pg-hba-conf" class="headerlink" title="1、使用find / -name ‘pg_hba.conf’查找到pg_hba.conf，修改pg_hba.conf"></a>1、使用find / -name ‘pg_hba.conf’查找到pg_hba.conf，修改pg_hba.conf</h3><blockquote><p>在最后添加允许访问IP段（全网段可访问）<br>host all all 0.0.0.0/0 md5</p></blockquote><h3 id="2、使用find-name-‘postgresql-conf’找到-postgresql-conf"><a href="#2、使用find-name-‘postgresql-conf’找到-postgresql-conf" class="headerlink" title="2、使用find / -name ‘postgresql.conf’找到 postgresql.conf"></a>2、使用find / -name ‘postgresql.conf’找到 postgresql.conf</h3><blockquote><p>找到用户参数listen_address(取消掉注释),改成下面样式:</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">listen_address = &apos;*&apos;</span><br></pre></td></tr></table></figure><blockquote><p>启用密码验证</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">#password_encryption = on 修改为 password_encryption = on</span><br></pre></td></tr></table></figure><h3 id="3、重启数据库"><a href="#3、重启数据库" class="headerlink" title="3、重启数据库"></a>3、重启数据库</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ systemctl restart postgresql-9.6</span><br></pre></td></tr></table></figure><blockquote><p>备注:使用Navicat For PostgreSql来连接</p></blockquote><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Mon Jan 13 2020 14:59:26 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;PostgreSQL10版本的主从安装配置在 &lt;a href=&quot;https://www.cnblogs.com/virtulreal/p/11675841.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://www.cnblogs.com/virtulreal/p/11675841.html&lt;/a&gt;&lt;/p&gt;&lt;h2 id=&quot;一、下载安装&quot;&gt;&lt;a href=&quot;#一、下载安装&quot; class=&quot;headerlink&quot; title=&quot;一、下载安装&quot;&gt;&lt;/a&gt;一、下载安装&lt;/h2&gt;&lt;h3 id=&quot;1、创建PostgreSQL9-6的yum源文件&quot;&gt;&lt;a href=&quot;#1、创建PostgreSQL9-6的yum源文件&quot; class=&quot;headerlink&quot; title=&quot;1、创建PostgreSQL9.6的yum源文件&quot;&gt;&lt;/a&gt;1、创建PostgreSQL9.6的yum源文件&lt;/h3&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;$ yum install https://download.postgresql.org/pub/repos/yum/9.6/redhat/rhel-7-x86_64/pgdg-centos96-9.6-3.noarch.rpm&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;h3 id=&quot;2、安装PostgreSQL客户端&quot;&gt;&lt;a href=&quot;#2、安装PostgreSQL客户端&quot; class=&quot;headerlink&quot; title=&quot;2、安装PostgreSQL客户端&quot;&gt;&lt;/a&gt;2、安装PostgreSQL客户端&lt;/h3&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;$ yum install postgresql96&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
      <category term="PostgreSQL" scheme="https://yongnights.github.io/categories/PostgreSQL/"/>
    
    
      <category term="PostgreSQL" scheme="https://yongnights.github.io/tags/PostgreSQL/"/>
    
  </entry>
  
  <entry>
    <title>配置 Nginx 反向代理 WebSocket</title>
    <link href="https://yongnights.github.io/2020/01/13/%E9%85%8D%E7%BD%AE%20Nginx%20%E5%8F%8D%E5%90%91%E4%BB%A3%E7%90%86%20WebSocket/"/>
    <id>https://yongnights.github.io/2020/01/13/配置 Nginx 反向代理 WebSocket/</id>
    <published>2020-01-13T06:46:58.717Z</published>
    <updated>2020-01-13T06:47:34.484Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Mon Jan 13 2020 14:59:26 GMT+0800 (GMT+08:00) --><p>用Nginx给网站做反向代理和负载均衡是广泛使用的一种Web服务器部署技术。不仅能够保证后端服务器的隐蔽性，还可以提高网站部署灵活性。</p><p>今天我们来讲一下，如何用Nginx给WebSocket服务器实现反向代理和负载均衡。</p><h3 id="什么是反向代理和负载均衡"><a href="#什么是反向代理和负载均衡" class="headerlink" title="什么是反向代理和负载均衡"></a>什么是反向代理和负载均衡</h3><ul><li>反向代理(Reverse Proxy)方式是指以代理服务器来接受Internet上的连接请求，然后将请求转发给内部网络上的服务器。并将内部服务器上得到的结果返回给Internet上请求连接的客户端，此时代理服务器对外就表现为一个服务器。</li><li>负载均衡(Load Balancing)建立在现有网络结构之上，它提供了一种廉价有效透明的方法扩展网络设备和服务器的带宽、增加吞吐量、加强网络数据处理能力、提高网络的灵活性和可用性。</li></ul><a id="more"></a><h3 id="什么是WebSocket"><a href="#什么是WebSocket" class="headerlink" title="什么是WebSocket"></a>什么是WebSocket</h3><p>WebSocket协议相比较于HTTP协议成功握手后可以多次进行通讯，直到连接被关闭。但是WebSocket中的握手和HTTP中的握手兼容，它使用HTTP中的Upgrade协议头将连接从HTTP升级到WebSocket。这使得WebSocket程序可以更容易的使用现已存在的基础设施。</p><p>WebSocket工作在HTTP的80和443端口并使用前缀<code>ws://</code>或者<code>wss://</code>进行协议标注，在建立连接时使用HTTP/1.1的101状态码进行协议切换，当前标准不支持两个客户端之间不借助HTTP直接建立Websocket连接。</p><p>更多Websocket的介绍可参考「<a href="http://t.cn/RaT8tNb" target="_blank" rel="noopener">WebSocket教程</a>」一文。</p><h3 id="创建基于Node的WebSocket服务"><a href="#创建基于Node的WebSocket服务" class="headerlink" title="创建基于Node的WebSocket服务"></a>创建基于Node的WebSocket服务</h3><p>Nginx在官方博客上给出了一个实践样例「<a href="https://www.nginx.com/blog/websocket-nginx/" target="_blank" rel="noopener">Using Nginx as a Websocket Proxy</a>」，我们以这个例子来演示WebSocket的交互过程。</p><p>这个例子中将会使用到nodejs的一个WebSocket的ws模块。</p><h4 id="安装node-js和npm"><a href="#安装node-js和npm" class="headerlink" title="安装node.js和npm"></a>安装node.js和npm</h4><ul><li>Debian/Ubuntu</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ apt-get install nodejs npm</span><br></pre></td></tr></table></figure><ul><li>RHEL/CentOS</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ yum install nodejs npm</span><br></pre></td></tr></table></figure><h4 id="创建nodejs软链"><a href="#创建nodejs软链" class="headerlink" title="创建nodejs软链"></a>创建nodejs软链</h4><p>在Ubuntu上创建一个名叫node软链。Centos默认为node，不用在单独创建了。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># 如果不创建，后面运行wscat时Ubuntu环境中会报错。</span><br><span class="line">$ ln -s /usr/bin/nodejs /usr/bin/node</span><br></pre></td></tr></table></figure><h4 id="安装ws和wscat模块"><a href="#安装ws和wscat模块" class="headerlink" title="安装ws和wscat模块"></a>安装ws和wscat模块</h4><p><code>ws</code>是nodejs的WebSocket实现，我们借助它来搭建简单的WebSocket Echo Server。<code>wscat</code>是一个可执行的WebSocket客户端，用来调试WebSocket服务是否正常。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ npm install ws wscat</span><br></pre></td></tr></table></figure><p>如果访问官方仓库比较慢的话，可用淘宝提供的镜像服务。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ npm --registry=https://registry.npm.taobao.org install ws wscat</span><br></pre></td></tr></table></figure><h4 id="创建一个简单的服务端"><a href="#创建一个简单的服务端" class="headerlink" title="创建一个简单的服务端"></a>创建一个简单的服务端</h4><p>这个简单的服务端实现的是向客户端返回客户端发送的消息。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">$ vim server.js</span><br><span class="line"></span><br><span class="line">console.log(&quot;Server started&quot;);</span><br><span class="line">var Msg = &apos;&apos;;</span><br><span class="line">var WebSocketServer = require(&apos;ws&apos;).Server</span><br><span class="line">    , wss = new WebSocketServer(&#123;port: 8010&#125;);</span><br><span class="line">    wss.on(&apos;connection&apos;, function(ws) &#123;</span><br><span class="line">        ws.on(&apos;message&apos;, function(message) &#123;</span><br><span class="line">        console.log(&apos;Received from client: %s&apos;, message);</span><br><span class="line">        ws.send(&apos;Server received from client: &apos; + message);</span><br><span class="line">    &#125;);</span><br><span class="line"> &#125;);</span><br></pre></td></tr></table></figure><p>运行这个简单的<code>echo</code>服务</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ node server.js</span><br><span class="line">Server started</span><br></pre></td></tr></table></figure><p>验证服务端是否正常启动</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ netstat  -tlunp|grep 8010</span><br><span class="line">tcp6       0      0 :::8010                 :::*                    LISTEN      23864/nodejs</span><br></pre></td></tr></table></figure><h4 id="使用wscat做为客户端测试"><a href="#使用wscat做为客户端测试" class="headerlink" title="使用wscat做为客户端测试"></a>使用wscat做为客户端测试</h4><p><code>wscat</code>命令默认安装当前用户目录<code>node_modules/wscat/</code>目录，我这里的位置是<code>/root/node_modules/wscat/bin/wscat</code>。</p><p>输入任意内容进行测试，得到相同返回则说明运行正常。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">$ cd /root/node_modules/wscat/bin/</span><br><span class="line">$ ./wscat --connect ws://127.0.0.1:8010</span><br><span class="line"></span><br><span class="line">connected (press CTRL+C to quit)</span><br><span class="line">&gt; Hello</span><br><span class="line">&lt; Server received from client: Hello</span><br><span class="line"></span><br><span class="line">&gt; Welcome to www.hi-linux.com</span><br><span class="line">&lt; Server received from client: Welcome to www.hi-linux.com</span><br></pre></td></tr></table></figure><h3 id="使用Nginx对WebSocket进行反向代理"><a href="#使用Nginx对WebSocket进行反向代理" class="headerlink" title="使用Nginx对WebSocket进行反向代理"></a>使用Nginx对WebSocket进行反向代理</h3><h4 id="安装Nginx"><a href="#安装Nginx" class="headerlink" title="安装Nginx"></a>安装Nginx</h4><ul><li>下载对应软件包</li></ul><p>Nginx从1.3.13版本就开始支持WebSocket了，并且可以为WebSocket应用程序做反向代理和负载均衡。这里Nginx选用1.9.2版本。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ cd /root</span><br><span class="line">$ wget &apos;http://nginx.org/download/nginx-1.9.2.tar.gz&apos;</span><br></pre></td></tr></table></figure><ul><li>编译安装Nginx</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ apt-get install libreadline-dev libncurses5-dev libpcre3-dev libssl-dev perl make build-essential</span><br><span class="line">$ tar xzvf nginx-1.9.2.tar.gz</span><br><span class="line">$ cd nginx-1.9.2</span><br><span class="line">$ ./configure</span><br><span class="line">$ make &amp;&amp; make install</span><br></pre></td></tr></table></figure><h4 id="配置Nginx"><a href="#配置Nginx" class="headerlink" title="配置Nginx"></a>配置Nginx</h4><ul><li>修改Nginx主配置文件</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">$ vim /usr/local/nginx/conf/nginx.conf</span><br><span class="line"></span><br><span class="line"># 在http上下文中增加如下配置，确保Nginx能处理正常http请求。</span><br><span class="line"></span><br><span class="line">http &#123;</span><br><span class="line"></span><br><span class="line">  map $http_upgrade $connection_upgrade &#123;</span><br><span class="line">    default upgrade;</span><br><span class="line">    &apos;&apos;   close;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  upstream websocket &#123;</span><br><span class="line">    #ip_hash;</span><br><span class="line">    server localhost:8010;  </span><br><span class="line">    server localhost:8011;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"># 以下配置是在server上下文中添加，location指用于websocket连接的path。</span><br><span class="line"></span><br><span class="line">  server &#123;</span><br><span class="line">    listen       80;</span><br><span class="line">    server_name localhost;</span><br><span class="line">    access_log /var/log/nginx/yourdomain.log;</span><br><span class="line"></span><br><span class="line">    location / &#123;</span><br><span class="line">      proxy_pass http://websocket;</span><br><span class="line">      proxy_read_timeout 300s;</span><br><span class="line"></span><br><span class="line">      proxy_set_header Host $host;</span><br><span class="line">      proxy_set_header X-Real-IP $remote_addr;</span><br><span class="line">      proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;</span><br><span class="line"></span><br><span class="line">      proxy_http_version 1.1;</span><br><span class="line">      proxy_set_header Upgrade $http_upgrade;</span><br><span class="line">      proxy_set_header Connection $connection_upgrade;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>最重要的就是在反向代理的配置中增加了如下两行，其它的部分和普通的HTTP反向代理没有任何差别。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">proxy_set_header Upgrade $http_upgrade;</span><br><span class="line">proxy_set_header Connection $connection_upgrade;</span><br></pre></td></tr></table></figure><p>这里面的关键部分在于HTTP的请求中多了如下头部：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Upgrade: websocket</span><br><span class="line">Connection: Upgrade</span><br></pre></td></tr></table></figure><p>这两个字段表示请求服务器升级协议为WebSocket。服务器处理完请求后，响应如下报文：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># 状态码为101</span><br><span class="line">HTTP/1.1 101 Switching Protocols</span><br><span class="line">Upgrade: websocket</span><br><span class="line">Connection: upgrade</span><br></pre></td></tr></table></figure><p>告诉客户端已成功切换协议，升级为Websocket协议。握手成功之后，服务器端和客户端便角色对等，就像普通的Socket一样，能够双向通信。不再进行HTTP的交互，而是开始WebSocket的数据帧协议实现数据交换。</p><p>这里使用<code>map</code>指令可以将变量组合成为新的变量，会根据客户端传来的连接中是否带有Upgrade头来决定是否给源站传递Connection头，这样做的方法比直接全部传递upgrade更加优雅。</p><p>默认情况下，连接将会在无数据传输60秒后关闭，<code>proxy_read_timeout</code>参数可以延长这个时间或者源站通过定期发送ping帧以保持连接并确认连接是否还在使用。</p><ul><li>启动Nginx</li></ul><p>Nginx会默认安装到<code>/usr/local/nginx</code>目录下。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ cd /usr/local/nginx/sbin</span><br><span class="line">$ ./nginx -c /usr/local/nginx/conf/nginx.conf</span><br></pre></td></tr></table></figure><p>如果你想以Systemd服务的方式更方便的管理Nginx，可参考「<a href="https://www.hi-linux.com/posts/1084.html" target="_blank" rel="noopener">基于Upsync模块实现Nginx动态配置</a>」 一文。</p><ul><li>测试通过Nginx访问WebSocket服务</li></ul><p>上面的配置会使NGINX监听80端口，并把接收到的任何请求传递给后端的WebSocket服务器。我们可以使用<code>wscat</code>作为客户端来测试一下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ cd /root/node_modules/wscat/bin/</span><br><span class="line">$ ./wscat --connect ws://192.168.2.210</span><br><span class="line">connected (press CTRL+C to quit)</span><br><span class="line">&gt; Hello Nginx</span><br><span class="line">&lt; Server received from client: Hello Nginx</span><br><span class="line">&gt; Welcome to www.hi-linux.com</span><br><span class="line">&lt; Server received from client: Welcome to www.hi-linux.com</span><br></pre></td></tr></table></figure><ul><li>反向代理服务器在支持WebSocket时面临的挑战</li></ul><p>WebSocket是端对端的，所以当一个代理服务器从客户端拦截一个Upgrade请求，它需要去发送它自己的Upgrade请求到后端服务器，也包括合适的头。</p><p>因为WebSocket是一个长连接，不像HTTP那样是典型的短连接，所以反向代理服务器需要允许连接保持着打开，而不是在它们看起来空闲时就将它们关闭。</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Mon Jan 13 2020 14:59:26 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;用Nginx给网站做反向代理和负载均衡是广泛使用的一种Web服务器部署技术。不仅能够保证后端服务器的隐蔽性，还可以提高网站部署灵活性。&lt;/p&gt;&lt;p&gt;今天我们来讲一下，如何用Nginx给WebSocket服务器实现反向代理和负载均衡。&lt;/p&gt;&lt;h3 id=&quot;什么是反向代理和负载均衡&quot;&gt;&lt;a href=&quot;#什么是反向代理和负载均衡&quot; class=&quot;headerlink&quot; title=&quot;什么是反向代理和负载均衡&quot;&gt;&lt;/a&gt;什么是反向代理和负载均衡&lt;/h3&gt;&lt;ul&gt;&lt;li&gt;反向代理(Reverse Proxy)方式是指以代理服务器来接受Internet上的连接请求，然后将请求转发给内部网络上的服务器。并将内部服务器上得到的结果返回给Internet上请求连接的客户端，此时代理服务器对外就表现为一个服务器。&lt;/li&gt;&lt;li&gt;负载均衡(Load Balancing)建立在现有网络结构之上，它提供了一种廉价有效透明的方法扩展网络设备和服务器的带宽、增加吞吐量、加强网络数据处理能力、提高网络的灵活性和可用性。&lt;/li&gt;&lt;/ul&gt;
    
    </summary>
    
      <category term="Nginx" scheme="https://yongnights.github.io/categories/Nginx/"/>
    
      <category term="WebSocket" scheme="https://yongnights.github.io/categories/Nginx/WebSocket/"/>
    
    
      <category term="Nginx" scheme="https://yongnights.github.io/tags/Nginx/"/>
    
      <category term="WebSocket" scheme="https://yongnights.github.io/tags/WebSocket/"/>
    
  </entry>
  
  <entry>
    <title>harbor helm仓库使用</title>
    <link href="https://yongnights.github.io/2020/01/13/harbor%20helm%E4%BB%93%E5%BA%93%E4%BD%BF%E7%94%A8/"/>
    <id>https://yongnights.github.io/2020/01/13/harbor helm仓库使用/</id>
    <published>2020-01-13T06:41:13.429Z</published>
    <updated>2020-01-13T06:41:56.756Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Mon Jan 13 2020 14:59:26 GMT+0800 (GMT+08:00) --><p>harbor helm仓库使用</p><p>官方文档地址：<a href="https://github.com/goharbor/harbor" target="_blank" rel="noopener">https://github.com/goharbor/harbor</a></p><p>Monocular 从1.0 开始专注于helm 的UI展示，对于部署以及维护已经去掉了，官方也提供了相关的说明以及推荐了几个可选的部署工具，从使用以及架构上来说kubeapps 就是Monocular + helm 操作的集合，比Monocular早期版本有好多提升</p><h1 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h1><ul><li>下载离线安装包<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wget https://github.com/goharbor/harbor/releases/download/v1.9.3/harbor-offline-installer-v1.9.3.tgz</span><br></pre></td></tr></table></figure></li></ul><a id="more"></a><ul><li>配置harbor</li></ul><blockquote><p>主要是harbor.cfg文件<br>目前主要配置hostname和port ,使用自己服务器的ip，修改默认端口号<br></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">hostname: 192.168.75.100</span><br><span class="line">http:</span><br><span class="line">  <span class="comment"># port for http, default is 80. If https enabled, this port will redirect to https port</span></span><br><span class="line">  port: 10000</span><br></pre></td></tr></table></figure><p></p></blockquote><ul><li><p>生成docker-compose file</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 先安装docker-compose，地址：https://github.com/docker/compose/releases</span></span><br><span class="line"><span class="comment"># 需要docker-compose(1.18.0+)版本</span></span><br><span class="line">curl -L https://github.com/docker/compose/releases/download/1.25.0/docker-compose-`uname -s`-`uname -m` -o /usr/<span class="built_in">local</span>/bin/docker-compose</span><br><span class="line">chmod +x /usr/<span class="built_in">local</span>/bin/docker-compose</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看docker-compose版本</span></span><br><span class="line">[root@ks-allinone harbor]<span class="comment"># docker-compose version</span></span><br><span class="line">docker-compose version 1.25.0, build 0a186604</span><br><span class="line">docker-py version: 4.1.0</span><br><span class="line">CPython version: 3.7.4</span><br><span class="line">OpenSSL version: OpenSSL 1.1.0l  10 Sep 2019</span><br><span class="line"></span><br><span class="line">./install.sh   --with-clair --with-chartmuseum</span><br></pre></td></tr></table></figure></li><li><p>使用<br>地址：<a href="http://192.168.75.100:10000" target="_blank" rel="noopener">http://192.168.75.100:10000</a><br>账号：admin<br>默认密码：Harbor12345</p></li><li><p>其他操作</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 安装helm</span></span><br><span class="line">curl https://raw.githubusercontent.com/kubernetes/helm/master/scripts/get | bash</span><br><span class="line"></span><br><span class="line"><span class="comment"># 安装push 插件</span></span><br><span class="line">helm init </span><br><span class="line">helm plugin install https://github.com/chartmuseum/helm-push</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看安装的插件</span></span><br><span class="line">helm plugin list</span><br><span class="line">NAME    VERSION DESCRIPTION                      </span><br><span class="line">push    0.7.1   Push chart package to ChartMuseum</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 添加harbor helm 私服</span></span><br><span class="line"><span class="comment"># 首先需要创建项目myrepo(当前设计的模式为public)</span></span><br><span class="line"><span class="comment"># chartrepo是必备的,不可缺少，不然就会推送到默认的library上面去了</span></span><br><span class="line"></span><br><span class="line">helm repo add --username=admin --password=Harbor12345 myrepo http://192.168.75.100:10000/chartrepo/myrepo</span><br><span class="line"><span class="string">"myrepo"</span> has been added to your repositories</span><br><span class="line"></span><br><span class="line"><span class="comment"># or 添加特定仓库</span></span><br><span class="line">helm repo add --username=admin --password=Harbor12345 myrepo https://xx.xx.xx.xx/chartrepo/myproject</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建demo</span></span><br><span class="line">helm create app</span><br><span class="line"></span><br><span class="line">Creating app</span><br><span class="line"></span><br><span class="line"><span class="comment"># 推送到harbor,push</span></span><br><span class="line">helm push --username=admin --password=Harbor12345 app myrepo</span><br><span class="line">Pushing app-0.1.0.tgz to myrepo...</span><br><span class="line">Done.</span><br></pre></td></tr></table></figure></li></ul><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Mon Jan 13 2020 14:59:26 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;harbor helm仓库使用&lt;/p&gt;&lt;p&gt;官方文档地址：&lt;a href=&quot;https://github.com/goharbor/harbor&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://github.com/goharbor/harbor&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Monocular 从1.0 开始专注于helm 的UI展示，对于部署以及维护已经去掉了，官方也提供了相关的说明以及推荐了几个可选的部署工具，从使用以及架构上来说kubeapps 就是Monocular + helm 操作的集合，比Monocular早期版本有好多提升&lt;/p&gt;&lt;h1 id=&quot;安装&quot;&gt;&lt;a href=&quot;#安装&quot; class=&quot;headerlink&quot; title=&quot;安装&quot;&gt;&lt;/a&gt;安装&lt;/h1&gt;&lt;ul&gt;&lt;li&gt;下载离线安装包&lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;wget https://github.com/goharbor/harbor/releases/download/v1.9.3/harbor-offline-installer-v1.9.3.tgz&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;/li&gt;&lt;/ul&gt;
    
    </summary>
    
      <category term="Harbor" scheme="https://yongnights.github.io/categories/Harbor/"/>
    
      <category term="Helm" scheme="https://yongnights.github.io/categories/Harbor/Helm/"/>
    
    
      <category term="Harbor" scheme="https://yongnights.github.io/tags/Harbor/"/>
    
      <category term="Helm" scheme="https://yongnights.github.io/tags/Helm/"/>
    
  </entry>
  
  <entry>
    <title>Dockfile文件解析</title>
    <link href="https://yongnights.github.io/2020/01/13/Dockfile%E6%96%87%E4%BB%B6%E8%A7%A3%E6%9E%90/"/>
    <id>https://yongnights.github.io/2020/01/13/Dockfile文件解析/</id>
    <published>2020-01-13T06:32:51.240Z</published>
    <updated>2020-01-13T06:33:34.412Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Mon Jan 13 2020 14:59:26 GMT+0800 (GMT+08:00) --><h1 id="1-Dockerfile内容基础知识"><a href="#1-Dockerfile内容基础知识" class="headerlink" title="1. Dockerfile内容基础知识"></a>1. Dockerfile内容基础知识</h1><ul><li>每条保留字指令都必须为大写字母且后面要跟随至少一个参数</li><li>指令按照从上到下，顺序执行</li><li>#表示注释</li><li>每条指令都会创建一个新的镜像层，并对镜像进行提交</li></ul><h1 id="2-Docker执行Dockerfile的大致流程"><a href="#2-Docker执行Dockerfile的大致流程" class="headerlink" title="2. Docker执行Dockerfile的大致流程"></a>2. Docker执行Dockerfile的大致流程</h1><ul><li>docker从基础镜像运行一个容器</li><li>执行一条指令并对容器作出修改</li><li>执行类似docker commit的操作提交一个新的镜像层</li><li>docker再基于刚提交的镜像运行一个新容器</li><li>执行dockerfile中的下一条指令直到所有指令都执行完成</li></ul><a id="more"></a><h1 id="3-DockerFile体系结构-保留字指令"><a href="#3-DockerFile体系结构-保留字指令" class="headerlink" title="3. DockerFile体系结构(保留字指令)"></a>3. DockerFile体系结构(保留字指令)</h1><ul><li>FROM：基础镜像，当前新镜像是基于哪个镜像的</li><li>MAINTAINER：镜像维护者的姓名和邮箱地址</li><li>RUN：容器构建时需要运行的命令</li><li>EXPOSE：当前容器对外暴露出的端口</li><li>WORKDIR：指定在创建容器后，终端默认登陆的进来工作目录，一个落脚点</li><li>ENV：用来在构建镜像过程中设置环境变量 (ENV MY_PATH /usr/mytest)</li><li>ADD：将宿主机目录下的文件拷贝进镜像且ADD命令会自动处理URL和解压tar压缩包</li><li>COPY：类似ADD，拷贝文件和目录到镜像中。将从构建上下文目录中 &lt;源路径&gt; 的文件/目录复制到新的一层的镜像内的 &lt;目标路径&gt; 位置 (COPY src dest)(COPY [“src”, “dest”])</li><li>VOLUME：容器数据卷，用于数据保存和持久化工作</li><li>CMD：指定一个容器启动时要运行的命令。可以有多个 CMD 指令，但只有最后一个生效，CMD 会被 docker run 之后的参数替换</li><li>ENTRYPOINT：指定一个容器启动时要运行的命令，ENTRYPOINT 的目的和 CMD 一样，都是在指定容器启动程序及参数</li><li>ONBUILD：当构建一个被继承的Dockerfile时运行命令，父镜像在被子继承后父镜像的onbuild被触发</li></ul><h1 id="4-示例内容"><a href="#4-示例内容" class="headerlink" title="4. 示例内容"></a>4. 示例内容</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">FROM         centos</span><br><span class="line">MAINTAINER    zzyy&lt;zzyybs@126.com&gt;</span><br><span class="line">#把宿主机当前上下文的c.txt拷贝到容器/usr/local/路径下</span><br><span class="line">COPY c.txt /usr/local/cincontainer.txt</span><br><span class="line">#把java与tomcat添加到容器中</span><br><span class="line">ADD jdk-8u171-linux-x64.tar.gz /usr/local/</span><br><span class="line">ADD apache-tomcat-9.0.8.tar.gz /usr/local/</span><br><span class="line">#安装vim编辑器</span><br><span class="line">RUN yum -y install vim</span><br><span class="line">#设置工作访问时候的WORKDIR路径，登录落脚点</span><br><span class="line">ENV MYPATH /usr/local</span><br><span class="line">WORKDIR $MYPATH</span><br><span class="line">#配置java与tomcat环境变量</span><br><span class="line">ENV JAVA_HOME /usr/local/jdk1.8.0_171</span><br><span class="line">ENV CLASSPATH $JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar</span><br><span class="line">ENV CATALINA_HOME /usr/local/apache-tomcat-9.0.8</span><br><span class="line">ENV CATALINA_BASE /usr/local/apache-tomcat-9.0.8</span><br><span class="line">ENV PATH $PATH:$JAVA_HOME/bin:$CATALINA_HOME/lib:$CATALINA_HOME/bin</span><br><span class="line">#容器运行时监听的端口</span><br><span class="line">EXPOSE  8080</span><br><span class="line">#启动时运行tomcat</span><br><span class="line"># ENTRYPOINT [&quot;/usr/local/apache-tomcat-9.0.8/bin/startup.sh&quot; ]</span><br><span class="line"># CMD [&quot;/usr/local/apache-tomcat-9.0.8/bin/catalina.sh&quot;,&quot;run&quot;]</span><br><span class="line">CMD /usr/local/apache-tomcat-9.0.8/bin/startup.sh &amp;&amp; tail -F /usr/local/apache-tomcat-9.0.8/bin/logs/catalina.out</span><br></pre></td></tr></table></figure><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Mon Jan 13 2020 14:59:26 GMT+0800 (GMT+08:00) --&gt;&lt;h1 id=&quot;1-Dockerfile内容基础知识&quot;&gt;&lt;a href=&quot;#1-Dockerfile内容基础知识&quot; class=&quot;headerlink&quot; title=&quot;1. Dockerfile内容基础知识&quot;&gt;&lt;/a&gt;1. Dockerfile内容基础知识&lt;/h1&gt;&lt;ul&gt;&lt;li&gt;每条保留字指令都必须为大写字母且后面要跟随至少一个参数&lt;/li&gt;&lt;li&gt;指令按照从上到下，顺序执行&lt;/li&gt;&lt;li&gt;#表示注释&lt;/li&gt;&lt;li&gt;每条指令都会创建一个新的镜像层，并对镜像进行提交&lt;/li&gt;&lt;/ul&gt;&lt;h1 id=&quot;2-Docker执行Dockerfile的大致流程&quot;&gt;&lt;a href=&quot;#2-Docker执行Dockerfile的大致流程&quot; class=&quot;headerlink&quot; title=&quot;2. Docker执行Dockerfile的大致流程&quot;&gt;&lt;/a&gt;2. Docker执行Dockerfile的大致流程&lt;/h1&gt;&lt;ul&gt;&lt;li&gt;docker从基础镜像运行一个容器&lt;/li&gt;&lt;li&gt;执行一条指令并对容器作出修改&lt;/li&gt;&lt;li&gt;执行类似docker commit的操作提交一个新的镜像层&lt;/li&gt;&lt;li&gt;docker再基于刚提交的镜像运行一个新容器&lt;/li&gt;&lt;li&gt;执行dockerfile中的下一条指令直到所有指令都执行完成&lt;/li&gt;&lt;/ul&gt;
    
    </summary>
    
      <category term="Docker" scheme="https://yongnights.github.io/categories/Docker/"/>
    
    
      <category term="Docker" scheme="https://yongnights.github.io/tags/Docker/"/>
    
  </entry>
  
  <entry>
    <title>Beats：如何使用Filebeat将MySQL日志发送到Elasticsearch</title>
    <link href="https://yongnights.github.io/2020/01/13/Beats%EF%BC%9A%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8Filebeat%E5%B0%86MySQL%E6%97%A5%E5%BF%97%E5%8F%91%E9%80%81%E5%88%B0Elasticsearch/"/>
    <id>https://yongnights.github.io/2020/01/13/Beats：如何使用Filebeat将MySQL日志发送到Elasticsearch/</id>
    <published>2020-01-13T05:48:57.482Z</published>
    <updated>2020-01-13T05:49:29.207Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Mon Jan 13 2020 13:50:27 GMT+0800 (GMT+08:00) --><p>在今天的文章中，我们来详细地描述如果使用Filebeat把MySQL的日志信息传输到Elasticsearch中。为了说明问题的方便，我们的测试系统的配置是这样的：</p><p><img src="https://img-blog.csdnimg.cn/20200113112001771.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9lbGFzdGljc3RhY2suYmxvZy5jc2RuLm5ldA==,size_16,color_FFFFFF,t_70" alt></p><p>我有一台MacOS机器。在上面我安装了Elasticsearch及Kibana。在这个机器里，我同时安装了一个Ubuntu 18.04的虚拟机。在这个Ubunutu机器上，我安装了MySQL及Filebeat。它们的IP地址分别显示如上。针对你们自己的测试环境，你们的IP地址可能和我的不太一样。</p><h1 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h1><h2 id="安装Elasticsearch"><a href="#安装Elasticsearch" class="headerlink" title="安装Elasticsearch"></a>安装Elasticsearch</h2><p>如果大家还没安装好自己的Elastic Stack的话，那么请按照我之前的教程“如何在Linux，MacOS及Windows上进行安装Elasticsearch” 安装好自己的Elasticsearch。由于我们的Elastic Stack需要被另外一个Ubuntu VM来访问，我们需要对我们的Elasticsearch进行配置。首先使用一个编辑器打开在config目录下的elasticsearch.yml配置文件。我们需要修改network.host的IP地址。在你的Mac及Linux机器上，我们可以使用:</p><p><code>$ ifconfig</code><br>来查看到我们的机器的IP地址。针对我的情况，我的机器的IP地址是：192.168.0.100。</p><p><img src="https://img-blog.csdnimg.cn/2020011108244570.png" alt></p><p>我们也必须在elasticsearch.yml的最后加上discovery.type: single-node，表明我们是单个node。</p><p>等修改完我们的IP地址后，我们保存elasticsearch.yml文件。然后重新运行我们的elasticsearch。我们可以在一个浏览器中输入刚才输入的IP地址并加上端口号9200。这样可以查看一下我们的elasticsearch是否已经正常运行了。</p><p><img src="https://img-blog.csdnimg.cn/20200111082557138.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9lbGFzdGljc3RhY2suYmxvZy5jc2RuLm5ldA==,size_16,color_FFFFFF,t_70" alt></p><a id="more"></a><h2 id="安装Kibana"><a href="#安装Kibana" class="headerlink" title="安装Kibana"></a>安装Kibana</h2><p>我们可以按照“如何在Linux，MacOS及Windows上安装Elastic栈中的Kibana”中介绍的那样来安装我们的Kibana。由于我们的Elasticsearch的IP地址已经改变，所以我们必须修改我们的Kibana的配置文件。我们使用自己喜欢的编辑器打开在config目录下的kibana.yml文件，并找到server.host。把它的值修改为自己的电脑的IP地址。针对我的情况是：</p><p><img src="https://img-blog.csdnimg.cn/20200111082858750.png" alt></p><p>同时找到elasticsearch.hosts，并把自己的IP地址输入进去：</p><p><img src="https://img-blog.csdnimg.cn/20200111082940393.png" alt></p><p>保存我们的kibana.yml文件，并运行我们的Kibana。同时在浏览器的地址中输入自己的IP地址及5601端口：</p><p><img src="https://img-blog.csdnimg.cn/20200111083033160.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9lbGFzdGljc3RhY2suYmxvZy5jc2RuLm5ldA==,size_16,color_FFFFFF,t_70" alt></p><p>如果配置成功的话，我们就可以看到上面的画面。</p><h2 id="安装Ubuntu虚拟机"><a href="#安装Ubuntu虚拟机" class="headerlink" title="安装Ubuntu虚拟机"></a>安装Ubuntu虚拟机</h2><p>这个不在我的这个教程之内。在网上我们可以找到许多的教程教我们如何安装Ubuntu虚拟机。</p><h2 id="在Ubuntu上安装MySQL"><a href="#在Ubuntu上安装MySQL" class="headerlink" title="在Ubuntu上安装MySQL"></a>在Ubuntu上安装MySQL</h2><p>我们可以按照链接<a href="https://vitux.com/how-to-install-and-configure-mysql-in-ubuntu-18-04-lts/来安装我们的MySQL。简单地说，安装步骤如下" target="_blank" rel="noopener">https://vitux.com/how-to-install-and-configure-mysql-in-ubuntu-18-04-lts/来安装我们的MySQL。简单地说，安装步骤如下</a>:</p><p>如果尚未安装MySQL，则可以使用以下步骤安装和配置它。 您需要做的第一件事就是更新系统。<br><code>sudo apt-get update</code></p><p>然后像这样安装MySQL：<code>sudo apt-get install mysql-server</code><br>在安装过程中，系统将提示您设置root密码。 记下它，因为管理MySQL数据库将需要它。或者，你通过如下的方法来设置MySQL的密码：<br><code>sudo mysql</code></p><p>等进入到MySQL后，打入如下的指令来创建你的root用户的密码：<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; ALTER USER &apos;root&apos;@&apos;localhost&apos; IDENTIFIED WITH mysql_native_password BY &apos;password&apos;;</span><br></pre></td></tr></table></figure><p></p><p>在上面的句子里，使用自己喜欢的密码来代替password。</p><p>下一步是配置MySQL以写入常规查询日志文件和慢速查询日志文件，因为默认情况下会禁用这些配置。 要更改配置，您将需要编辑包含用户数据库设置的my.cnf文件。<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo vi /etc/mysql/my.cnf</span><br></pre></td></tr></table></figure><p></p><p>常规查询和慢速查询的有效配置应如下所示：<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[mysqld]</span><br><span class="line">general_log = 1</span><br><span class="line">general_log_file = /var/log/mysql/mysql.log</span><br><span class="line">slow_query_log = 1</span><br><span class="line">slow_query_log_file = /var/log/mysql/mysql-slow.log</span><br><span class="line">long_query_time = 1</span><br><span class="line">log_queries_not_using_indexes = 1</span><br></pre></td></tr></table></figure><p></p><p>我们可以把上面的配置添加到我们的my.cnf文件当中去：</p><p><img src="https://img-blog.csdnimg.cn/20200113113204124.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9lbGFzdGljc3RhY2suYmxvZy5jc2RuLm5ldA==,size_16,color_FFFFFF,t_70" alt></p><p>请注意，在低于5.1.29的MySQL版本中，使用了变量log_slow_queries而不是slow_query_log。</p><p>进行以下更改后，请确保重新启动MySQL：<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo service mysql restart</span><br></pre></td></tr></table></figure><p></p><p>现在，您的MySQL已准备好编写慢速查询，这些查询将通过Filebeat传送到您的Elasticsearch集群中。我们可以检查一下我们的MySQL是否已经成功运行：<code>systemctl status mysql.service</code></p><p>等我们成功配置后我们的MySQL，我们可以开始对我们的MySQL进行一些操作，然后你可以在如下的目录中查看到相应的log文件：<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ls /var/log/mysql/</span><br></pre></td></tr></table></figure><p></p><p><img src="https://img-blog.csdnimg.cn/20200113113637953.png" alt></p><h2 id="安装Filebeat"><a href="#安装Filebeat" class="headerlink" title="安装Filebeat"></a>安装Filebeat</h2><p>在Ubuntu上安装Filebeat也是非常直接的。我们可以先打开我们的Kibana。</p><p><img src="https://img-blog.csdnimg.cn/20200113113931296.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9lbGFzdGljc3RhY2suYmxvZy5jc2RuLm5ldA==,size_16,color_FFFFFF,t_70" alt></p><p>点击上面的“Add log data”按钮:</p><p><img src="https://img-blog.csdnimg.cn/20200113114043534.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9lbGFzdGljc3RhY2suYmxvZy5jc2RuLm5ldA==,size_16,color_FFFFFF,t_70" alt></p><p>点击上面的“MySQL logs”按钮：</p><p><img src="https://img-blog.csdnimg.cn/20200113114151618.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9lbGFzdGljc3RhY2suYmxvZy5jc2RuLm5ldA==,size_16,color_FFFFFF,t_70" alt></p><p>选择上面的操作系统。针对我们的Ubuntu系统，它是一个DEB格式的安装文件。我们按照上面的要求一步一步地进行安装和修改。在修改filebeat.yml文件时，我们需要注意的三点：</p><p>1）修改MySQL的log路径：</p><p><img src="https://img-blog.csdnimg.cn/20200113114441487.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9lbGFzdGljc3RhY2suYmxvZy5jc2RuLm5ldA==,size_16,color_FFFFFF,t_70" alt></p><p>2）填入Kibana的地址：</p><p><img src="https://img-blog.csdnimg.cn/20200113114538152.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9lbGFzdGljc3RhY2suYmxvZy5jc2RuLm5ldA==,size_16,color_FFFFFF,t_70" alt></p><p>3）填入Elasticsearch的地址及端口号：</p><p><img src="https://img-blog.csdnimg.cn/20200113114716773.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9lbGFzdGljc3RhY2suYmxvZy5jc2RuLm5ldA==,size_16,color_FFFFFF,t_70" alt></p><p>我们需要运行如下的命令来把相应的dashboard，pipeline及template信息上传到Elasticsearch和Kibana中。<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sudo filebeat modules enable mysql</span><br><span class="line">sudo filebeat setup</span><br><span class="line">sudo service filebeat start</span><br></pre></td></tr></table></figure><p></p><p>等我们启动我们的filebeat后，我们可以通过如下的命令来检查filebeat服务是否运行正常：<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo systemctl status filebeat</span><br></pre></td></tr></table></figure><p></p><h1 id="Kibana查看"><a href="#Kibana查看" class="headerlink" title="Kibana查看"></a>Kibana查看</h1><p>我们可以打开Kibana，并在Kibana中查看由filebeat发送过来的MySQL的数据：</p><p><img src="https://img-blog.csdnimg.cn/20200113115325246.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9lbGFzdGljc3RhY2suYmxvZy5jc2RuLm5ldA==,size_16,color_FFFFFF,t_70" alt></p><p>在上面，我们可以看到MySQL的dashboard：</p><p><img src="https://img-blog.csdnimg.cn/2020011311545031.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9lbGFzdGljc3RhY2suYmxvZy5jc2RuLm5ldA==,size_16,color_FFFFFF,t_70" alt></p><p>至此，我们可以看到所有的关于MySQL的信息，这里包括以下queries及error logs等。</p><p>上面我们显示了如何直接把MySQL的信息发送到Elasticsearch，并对数据进行分析。当然，我们也可以把数据发送到logstash来对数据进行处理，然后再发送到Elasticsearch中。我们的filebeat.yml文件的配置文件可以这么写：<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">filebeat.prospectors:</span><br><span class="line">- input_type: log</span><br><span class="line"> paths:</span><br><span class="line"> - /var/log/mysql/*.log</span><br><span class="line"> document_type: syslog</span><br><span class="line"> registry: /var/lib/filebeat/registry</span><br><span class="line">output.logstash:</span><br><span class="line"> hosts: [&quot;mylogstashurl.example.com:5044&quot;]</span><br></pre></td></tr></table></figure><p></p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><h2 id="如本教程所示，Filebeat是用于MySQL数据库和Elasticsearch集群的出色日志传送解决方案。-与以前的版本相比，它非常轻巧，可以有效地发送日志事件。-Filebeat支持压缩，并且可以通过单个yaml文件轻松配置。-使用Filebeat，您可以轻松地管理日志文件，跟踪日志注册表，创建自定义字段以在日志中启用细化过滤和发现，以及使用Kibana可视化功能立即为日志数据供电。"><a href="#如本教程所示，Filebeat是用于MySQL数据库和Elasticsearch集群的出色日志传送解决方案。-与以前的版本相比，它非常轻巧，可以有效地发送日志事件。-Filebeat支持压缩，并且可以通过单个yaml文件轻松配置。-使用Filebeat，您可以轻松地管理日志文件，跟踪日志注册表，创建自定义字段以在日志中启用细化过滤和发现，以及使用Kibana可视化功能立即为日志数据供电。" class="headerlink" title="如本教程所示，Filebeat是用于MySQL数据库和Elasticsearch集群的出色日志传送解决方案。 与以前的版本相比，它非常轻巧，可以有效地发送日志事件。 Filebeat支持压缩，并且可以通过单个yaml文件轻松配置。 使用Filebeat，您可以轻松地管理日志文件，跟踪日志注册表，创建自定义字段以在日志中启用细化过滤和发现，以及使用Kibana可视化功能立即为日志数据供电。"></a>如本教程所示，Filebeat是用于MySQL数据库和Elasticsearch集群的出色日志传送解决方案。 与以前的版本相比，它非常轻巧，可以有效地发送日志事件。 Filebeat支持压缩，并且可以通过单个yaml文件轻松配置。 使用Filebeat，您可以轻松地管理日志文件，跟踪日志注册表，创建自定义字段以在日志中启用细化过滤和发现，以及使用Kibana可视化功能立即为日志数据供电。</h2><p>版权声明：本文为CSDN博主「Elastic 中国社区官方博客」的原创文章，遵循 CC 4.0 BY-SA 版权协议，转载请附上原文出处链接及本声明。<br>原文链接：<a href="https://blog.csdn.net/UbuntuTouch/article/details/103954935" target="_blank" rel="noopener">https://blog.csdn.net/UbuntuTouch/article/details/103954935</a></p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Mon Jan 13 2020 13:50:27 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;在今天的文章中，我们来详细地描述如果使用Filebeat把MySQL的日志信息传输到Elasticsearch中。为了说明问题的方便，我们的测试系统的配置是这样的：&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://img-blog.csdnimg.cn/20200113112001771.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9lbGFzdGljc3RhY2suYmxvZy5jc2RuLm5ldA==,size_16,color_FFFFFF,t_70&quot; alt&gt;&lt;/p&gt;&lt;p&gt;我有一台MacOS机器。在上面我安装了Elasticsearch及Kibana。在这个机器里，我同时安装了一个Ubuntu 18.04的虚拟机。在这个Ubunutu机器上，我安装了MySQL及Filebeat。它们的IP地址分别显示如上。针对你们自己的测试环境，你们的IP地址可能和我的不太一样。&lt;/p&gt;&lt;h1 id=&quot;准备工作&quot;&gt;&lt;a href=&quot;#准备工作&quot; class=&quot;headerlink&quot; title=&quot;准备工作&quot;&gt;&lt;/a&gt;准备工作&lt;/h1&gt;&lt;h2 id=&quot;安装Elasticsearch&quot;&gt;&lt;a href=&quot;#安装Elasticsearch&quot; class=&quot;headerlink&quot; title=&quot;安装Elasticsearch&quot;&gt;&lt;/a&gt;安装Elasticsearch&lt;/h2&gt;&lt;p&gt;如果大家还没安装好自己的Elastic Stack的话，那么请按照我之前的教程“如何在Linux，MacOS及Windows上进行安装Elasticsearch” 安装好自己的Elasticsearch。由于我们的Elastic Stack需要被另外一个Ubuntu VM来访问，我们需要对我们的Elasticsearch进行配置。首先使用一个编辑器打开在config目录下的elasticsearch.yml配置文件。我们需要修改network.host的IP地址。在你的Mac及Linux机器上，我们可以使用:&lt;/p&gt;&lt;p&gt;&lt;code&gt;$ ifconfig&lt;/code&gt;&lt;br&gt;来查看到我们的机器的IP地址。针对我的情况，我的机器的IP地址是：192.168.0.100。&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://img-blog.csdnimg.cn/2020011108244570.png&quot; alt&gt;&lt;/p&gt;&lt;p&gt;我们也必须在elasticsearch.yml的最后加上discovery.type: single-node，表明我们是单个node。&lt;/p&gt;&lt;p&gt;等修改完我们的IP地址后，我们保存elasticsearch.yml文件。然后重新运行我们的elasticsearch。我们可以在一个浏览器中输入刚才输入的IP地址并加上端口号9200。这样可以查看一下我们的elasticsearch是否已经正常运行了。&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;https://img-blog.csdnimg.cn/20200111082557138.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9lbGFzdGljc3RhY2suYmxvZy5jc2RuLm5ldA==,size_16,color_FFFFFF,t_70&quot; alt&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="elk" scheme="https://yongnights.github.io/categories/elk/"/>
    
    
      <category term="elk" scheme="https://yongnights.github.io/tags/elk/"/>
    
  </entry>
  
  <entry>
    <title>Elasticsearch中text与keyword的区别</title>
    <link href="https://yongnights.github.io/2020/01/10/Elasticsearch%E4%B8%ADtext%E4%B8%8Ekeyword%E7%9A%84%E5%8C%BA%E5%88%AB/"/>
    <id>https://yongnights.github.io/2020/01/10/Elasticsearch中text与keyword的区别/</id>
    <published>2020-01-10T09:42:22.793Z</published>
    <updated>2020-01-10T09:45:11.112Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Fri Jan 10 2020 17:45:51 GMT+0800 (GMT+08:00) --><p>ES更新到5版本后，取消了 string 数据类型，代替它的是 keyword 和 text 数据类型.那么 text 和keyword有什么区别呢？</p><h1 id="添加数据"><a href="#添加数据" class="headerlink" title="添加数据"></a>添加数据</h1><p>使用bulk往es数据库中批量添加一些document<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">POST /book/novel/_bulk</span><br><span class="line">&#123;&quot;index&quot;: &#123;&quot;_id&quot;: 1&#125;&#125;</span><br><span class="line">&#123;&quot;name&quot;: &quot;Gone with the Wind&quot;, &quot;author&quot;: &quot;Margaret Mitchell&quot;, &quot;date&quot;: &quot;2018-01-01&quot;&#125;</span><br><span class="line">&#123;&quot;index&quot;: &#123;&quot;_id&quot;: 2&#125;&#125;</span><br><span class="line">&#123;&quot;name&quot;: &quot;Robinson Crusoe&quot;, &quot;author&quot;: &quot;Daniel Defoe&quot;, &quot;date&quot;: &quot;2018-01-02&quot;&#125;</span><br><span class="line">&#123;&quot;index&quot;: &#123;&quot;_id&quot;: 3&#125;&#125;</span><br><span class="line">&#123;&quot;name&quot;: &quot;Pride and Prejudice&quot;, &quot;author&quot;: &quot;Jane Austen&quot;, &quot;date&quot;: &quot;2018-01-01&quot;&#125;</span><br><span class="line">&#123;&quot;index&quot;: &#123;&quot;_id&quot;: 4&#125;&#125;</span><br><span class="line">&#123;&quot;name&quot;: &quot;Jane Eyre&quot;, &quot;author&quot;: &quot;Charlotte Bronte&quot;, &quot;date&quot;: &quot;2018-01-02&quot;&#125;</span><br></pre></td></tr></table></figure><p></p><h1 id="查看mapping"><a href="#查看mapping" class="headerlink" title="查看mapping"></a>查看mapping</h1><p>发现name、author的type是text，<br>还有个field是keyword，keyword的type是keyword：<br><img src="/Elasticsearch中text与keyword的区别/1.png" alt></p><a id="more"></a><h1 id="查询"><a href="#查询" class="headerlink" title="查询"></a>查询</h1><p>使用term查询某个小说：<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">GET book/novel/_search</span><br><span class="line">&#123;</span><br><span class="line">  &quot;query&quot;: &#123;</span><br><span class="line">    &quot;constant_score&quot;: &#123;</span><br><span class="line">      &quot;filter&quot;: &#123;</span><br><span class="line">        &quot;term&quot;: &#123;</span><br><span class="line">          &quot;name&quot;: &quot;Gone with the Wind&quot;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;,</span><br><span class="line">      &quot;boost&quot;: 1.2</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><p>结果是什么也没有查到：<br><img src="/Elasticsearch中text与keyword的区别/2.png" alt></p><p>然后使用name的keyword查询：<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">GET book/novel/_search</span><br><span class="line">&#123;</span><br><span class="line">  &quot;query&quot;: &#123;</span><br><span class="line">    &quot;constant_score&quot;: &#123;</span><br><span class="line">      &quot;filter&quot;: &#123;</span><br><span class="line">        &quot;term&quot;: &#123;</span><br><span class="line">          &quot;name.keyword&quot;: &quot;Gone with the Wind&quot;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;,</span><br><span class="line">      &quot;boost&quot;: 1.2</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><p>可以查询到一条数据：<br><img src="/Elasticsearch中text与keyword的区别/3.png" alt></p><h1 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h1><p>使用name不能查到，而使用name.keyword可以查到，我们可以通过下面的实验来判断：</p><p>使用name进行分词的时候，结果会有4个词出来：<br><img src="/Elasticsearch中text与keyword的区别/4.png" alt></p><p>使用name.keyword进行分词的时候，结果只有一个词出来：<br><img src="/Elasticsearch中text与keyword的区别/5.png" alt></p><h1 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h1><p>text类型：会分词，先把对象进行分词处理，然后再再存入到es中。<br>当使用多个单词进行查询的时候，当然查不到已经分词过的内容！</p><p>keyword：不分词，没有把es中的对象进行分词处理，而是存入了整个对象！<br>这时候当然可以进行完整地查询！默认是256个字符！</p><p>作者：香山上的麻雀<br>链接：<a href="https://www.jianshu.com/p/1189ff372c38" target="_blank" rel="noopener">https://www.jianshu.com/p/1189ff372c38</a><br>来源：简书<br>著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Fri Jan 10 2020 17:45:51 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;ES更新到5版本后，取消了 string 数据类型，代替它的是 keyword 和 text 数据类型.那么 text 和keyword有什么区别呢？&lt;/p&gt;&lt;h1 id=&quot;添加数据&quot;&gt;&lt;a href=&quot;#添加数据&quot; class=&quot;headerlink&quot; title=&quot;添加数据&quot;&gt;&lt;/a&gt;添加数据&lt;/h1&gt;&lt;p&gt;使用bulk往es数据库中批量添加一些document&lt;br&gt;&lt;/p&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;POST /book/novel/_bulk&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&amp;#123;&amp;quot;index&amp;quot;: &amp;#123;&amp;quot;_id&amp;quot;: 1&amp;#125;&amp;#125;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&amp;#123;&amp;quot;name&amp;quot;: &amp;quot;Gone with the Wind&amp;quot;, &amp;quot;author&amp;quot;: &amp;quot;Margaret Mitchell&amp;quot;, &amp;quot;date&amp;quot;: &amp;quot;2018-01-01&amp;quot;&amp;#125;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&amp;#123;&amp;quot;index&amp;quot;: &amp;#123;&amp;quot;_id&amp;quot;: 2&amp;#125;&amp;#125;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&amp;#123;&amp;quot;name&amp;quot;: &amp;quot;Robinson Crusoe&amp;quot;, &amp;quot;author&amp;quot;: &amp;quot;Daniel Defoe&amp;quot;, &amp;quot;date&amp;quot;: &amp;quot;2018-01-02&amp;quot;&amp;#125;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&amp;#123;&amp;quot;index&amp;quot;: &amp;#123;&amp;quot;_id&amp;quot;: 3&amp;#125;&amp;#125;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&amp;#123;&amp;quot;name&amp;quot;: &amp;quot;Pride and Prejudice&amp;quot;, &amp;quot;author&amp;quot;: &amp;quot;Jane Austen&amp;quot;, &amp;quot;date&amp;quot;: &amp;quot;2018-01-01&amp;quot;&amp;#125;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&amp;#123;&amp;quot;index&amp;quot;: &amp;#123;&amp;quot;_id&amp;quot;: 4&amp;#125;&amp;#125;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&amp;#123;&amp;quot;name&amp;quot;: &amp;quot;Jane Eyre&amp;quot;, &amp;quot;author&amp;quot;: &amp;quot;Charlotte Bronte&amp;quot;, &amp;quot;date&amp;quot;: &amp;quot;2018-01-02&amp;quot;&amp;#125;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;p&gt;&lt;/p&gt;&lt;h1 id=&quot;查看mapping&quot;&gt;&lt;a href=&quot;#查看mapping&quot; class=&quot;headerlink&quot; title=&quot;查看mapping&quot;&gt;&lt;/a&gt;查看mapping&lt;/h1&gt;&lt;p&gt;发现name、author的type是text，&lt;br&gt;还有个field是keyword，keyword的type是keyword：&lt;br&gt;&lt;img src=&quot;/Elasticsearch中text与keyword的区别/1.png&quot; alt&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="elk" scheme="https://yongnights.github.io/categories/elk/"/>
    
    
      <category term="elk" scheme="https://yongnights.github.io/tags/elk/"/>
    
  </entry>
  
  <entry>
    <title>使用 Logstash 和 JDBC 确保 Elasticsearch 与关系型数据库保持同步</title>
    <link href="https://yongnights.github.io/2020/01/09/%E4%BD%BF%E7%94%A8%20Logstash%20%E5%92%8C%20JDBC%20%E7%A1%AE%E4%BF%9D%20Elasticsearch%20%E4%B8%8E%E5%85%B3%E7%B3%BB%E5%9E%8B%E6%95%B0%E6%8D%AE%E5%BA%93%E4%BF%9D%E6%8C%81%E5%90%8C%E6%AD%A5/"/>
    <id>https://yongnights.github.io/2020/01/09/使用 Logstash 和 JDBC 确保 Elasticsearch 与关系型数据库保持同步/</id>
    <published>2020-01-09T06:51:49.472Z</published>
    <updated>2020-01-09T06:58:55.563Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Thu Jan 09 2020 14:59:07 GMT+0800 (GMT+08:00) --><p>为了充分利用 Elasticsearch 提供的强大搜索功能，很多公司都会在既有关系型数据库的基础上再部署 Elasticsearch。在这种情况下，很可能需要确保 Elasticsearch 与所关联关系型数据库中的数据保持同步。因此，在本篇博文中，我会演示如何使用 Logstash 来高效地复制数据并将关系型数据库中的更新同步到 Elasticsearch 中。本文中所列出的代码和方法已使用 MySQL 进行过测试，但理论上应该适用于任何关系数据库管理系统 (RDBMS)。</p><h2 id="系统配置"><a href="#系统配置" class="headerlink" title="系统配置"></a>系统配置</h2><p>在本篇文章中，我使用下列产品进行测试：</p><ul><li><a href="https://dev.mysql.com/" target="_blank" rel="noopener">MySQL</a>：8.0.16</li><li><a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.1/index.html" target="_blank" rel="noopener">Elasticsearch</a>：7.1.1</li><li><a href="https://www.elastic.co/guide/en/logstash/7.1/introduction.html" target="_blank" rel="noopener">Logstash</a>：7.1.1</li><li><a href="https://www.java.com/en/" target="_blank" rel="noopener">Java</a>：1.8.0_162-b12</li><li><a href="https://www.elastic.co/guide/en/logstash/7.1/plugins-inputs-jdbc.html" target="_blank" rel="noopener">JDBC 输入插件</a>：v4.3.13</li><li><a href="https://dev.mysql.com/downloads/connector/j/" target="_blank" rel="noopener">JDBC 连接器</a>：Connector/J 8.0.16</li></ul><h2 id="同步步骤整体概览"><a href="#同步步骤整体概览" class="headerlink" title="同步步骤整体概览"></a>同步步骤整体概览</h2><p>在本篇博文中，我们使用 Logstash 和 JDBC 输入插件来让 Elasticsearch 与 MySQL 保持同步。从概念上讲，Logstash 的 JDBC 输入插件会运行一个循环来定期对 MySQL 进行轮询，从而找出在此次循环的上次迭代后插入或更改的记录。如要让其正确运行，必须满足下列条件：</p><ol><li>在将 MySQL 中的文档写入 Elasticsearch 时，Elasticsearch 中的 “_id” 字段必须设置为 MySQL 中的 “id” 字段。这可在 MySQL 记录与 Elasticsearch 文档之间建立一个直接映射关系。如果在 MySQL 中更新了某条记录，那么将会在 Elasticsearch 中覆盖整条相关记录。请注意，在 Elasticsearch 中覆盖文档的效率与<a href="https://www.elastic.co/guide/en/elasticsearch/reference/6.7/getting-started-update-documents.html" target="_blank" rel="noopener">更新操作</a>的效率一样高，因为从内部原理上来讲，更新便包括删除旧文档以及随后对全新文档进行索引。</li><li>当在 MySQL 中插入或更新数据时，该条记录必须有一个包含更新或插入时间的字段。通过此字段，便可允许 Logstash 仅请求获得在轮询循环的上次迭代后编辑或插入的文档。Logstash 每次对 MySQL 进行轮询时，都会保存其从 MySQL 所读取最后一条记录的更新或插入时间。在下一次迭代时，Logstash 便知道其仅需请求获得符合下列条件的记录：更新或插入时间晚于在轮询循环中的上一次迭代中所收到的最后一条记录。</li></ol><p>如果满足上述条件，我们便可配置 Logstash，以定期请求从 MySQL 获得新增或已编辑的全部记录，然后将它们写入 Elasticsearch 中。完成这些操作的 Logstash 代码在本篇博文的后面会列出。</p><a id="more"></a><h2 id="MySQL-设置"><a href="#MySQL-设置" class="headerlink" title="MySQL 设置"></a>MySQL 设置</h2><p>可以使用下列代码配置 MySQL 数据库和数据表：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">CREATE DATABASE es_db;</span><br><span class="line">USE es_db;</span><br><span class="line">DROP TABLE IF EXISTS es_table;</span><br><span class="line">CREATE TABLE es_table (</span><br><span class="line">  id BIGINT(20) UNSIGNED NOT NULL,</span><br><span class="line">  PRIMARY KEY (id),</span><br><span class="line">  UNIQUE KEY unique_id (id),</span><br><span class="line">  client_name VARCHAR(32) NOT NULL,</span><br><span class="line">  modification_time TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,</span><br><span class="line">  insertion_time TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP</span><br><span class="line">);</span><br></pre></td></tr></table></figure><p>在上面的 MySQL 配置中，有几个参数需要特别注意：</p><ul><li><code>es_table</code>：这是 MySQL 数据表的名称，数据会从这里读取出来并同步到 Elasticsearch。</li><li><code>id</code>：这是该条记录的唯一标识符。请注意 “id” 已被定义为 PRIMARY KEY（主键）和 UNIQUE KEY（唯一键）。这能确保每个 “id” 仅在当前表格中出现一次。其将会转换为 “_id”，以用于更新 Elasticsearch 中的文档及向 Elasticsearch 中插入文档。</li><li><code>client_name</code>：此字段表示在每条记录中所存储的用户定义数据。在本篇博文中，为简单起见，我们只有一个包含用户定义数据的字段，但您可以轻松添加更多字段。我们要更改的就是这个字段，从而向大家演示不仅新插入的 MySQL 记录被复制到了 Elasticsearch 中，而且更新的记录也被正确传播到了 Elasticsearch 中。</li><li><code>modification_time</code>：在 MySQL 中插入或更改任何记录时，都会将这个所定义字段的值设置为编辑时间。有了这个编辑时间，我们便能提取自从上次 Logstash 请求从 MySQL 获取记录后被编辑的任何记录。</li><li><code>insertion_time</code>：此字段主要用于演示目的，并非正确进行同步需满足的严格必要条件。我们用其来跟踪记录最初插入到 MySQL 中的时间。</li></ul><h2 id="MySQL-操作"><a href="#MySQL-操作" class="headerlink" title="MySQL 操作"></a>MySQL 操作</h2><p>完成上述配置后，可以通过下列语句向 MySQL 中写入记录：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">INSERT INTO es_table (id, client_name) VALUES (&lt;id&gt;, &lt;client name&gt;);</span><br></pre></td></tr></table></figure><p>可以通过下列命令更新 MySQL 中的记录：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">UPDATE es_table SET client_name = &lt;new client name&gt; WHERE id=&lt;id&gt;;</span><br></pre></td></tr></table></figure><p>可以通过下列语句完成 MySQL 更新/插入操作 (upsert)：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">INSERT INTO es_table (id, client_name) VALUES (&lt;id&gt;, &lt;client name when created&gt; ON DUPLICATE KEY UPDATE client_name=&lt;client name when updated&gt;;</span><br></pre></td></tr></table></figure><h2 id="同步代码"><a href="#同步代码" class="headerlink" title="同步代码"></a>同步代码</h2><p>下列 Logstash 管道会实施在前一部分中所描述的同步代码：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">input &#123;</span><br><span class="line">  jdbc &#123;</span><br><span class="line">    jdbc_driver_library =&gt; &quot;&lt;path&gt;/mysql-connector-java-8.0.16.jar&quot;</span><br><span class="line">    jdbc_driver_class =&gt; &quot;com.mysql.jdbc.Driver&quot;</span><br><span class="line">    jdbc_connection_string =&gt; &quot;jdbc:mysql://&lt;MySQL host&gt;:3306/es_db&quot;</span><br><span class="line">    jdbc_user =&gt; &lt;my username&gt;</span><br><span class="line">    jdbc_password =&gt; &lt;my password&gt;</span><br><span class="line">    jdbc_paging_enabled =&gt; true</span><br><span class="line">    tracking_column =&gt; &quot;unix_ts_in_secs&quot;</span><br><span class="line">    use_column_value =&gt; true</span><br><span class="line">    tracking_column_type =&gt; &quot;numeric&quot;</span><br><span class="line">    schedule =&gt; &quot;*/5 * * * * *&quot;</span><br><span class="line">    statement =&gt; &quot;SELECT *, UNIX_TIMESTAMP(modification_time) AS unix_ts_in_secs FROM es_table WHERE (UNIX_TIMESTAMP(modification_time) &gt; :sql_last_value AND modification_time &lt; NOW()) ORDER BY modification_time ASC&quot;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">filter &#123;</span><br><span class="line">  mutate &#123;</span><br><span class="line">    copy =&gt; &#123; &quot;id&quot; =&gt; &quot;[@metadata][_id]&quot;&#125;</span><br><span class="line">    remove_field =&gt; [&quot;id&quot;, &quot;@version&quot;, &quot;unix_ts_in_secs&quot;]</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">output &#123;</span><br><span class="line">  # stdout &#123; codec =&gt;  &quot;rubydebug&quot;&#125;</span><br><span class="line">  elasticsearch &#123;</span><br><span class="line">      index =&gt; &quot;rdbms_sync_idx&quot;</span><br><span class="line">      document_id =&gt; &quot;%&#123;[@metadata][_id]&#125;&quot;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">Read Less</span><br></pre></td></tr></table></figure><p>在上述管道中，应该重点强调几个区域：</p><ul><li><code>tracking_column</code>：此字段会指定 “unix_ts_in_secs” 字段（用于跟踪 Logstash 从 MySQL 读取的最后一个文档，下面会进行描述），其存储在 <a href="https://www.elastic.co/guide/en/logstash/7.1/plugins-inputs-jdbc.html#plugins-inputs-jdbc-last_run_metadata_path" target="_blank" rel="noopener">.logstash_jdbc_last_run</a> 中的磁盘上。该值将会用来确定 Logstash 在其轮询循环的下一次迭代中所请求文档的起始值。在 .logstash_jdbc_last_run 中所存储的值可以作为 “:sql_last_value” 通过 SELECT 语句进行访问。</li><li><code>unix_ts_in_secs</code>：这是一个由上述 SELECT 语句生成的字段，包含可作为标准 <a href="https://en.wikipedia.org/wiki/Unix_time" target="_blank" rel="noopener">Unix 时间戳</a>（自 Epoch 起秒数）的 “modification_time”。我们刚讨论的 “tracking column” 会引用该字段。Unix 时间戳用于跟踪进度，而非作为简单的时间戳；如将其作为简单时间戳，可能会导致错误，因为在 UMT 和本地时区之间正确地来回转换是一个十分复杂的过程。</li><li><code>sql_last_value</code>：这是一个<a href="https://www.elastic.co/guide/en/logstash/7.1/plugins-inputs-jdbc.html#_predefined_parameters" target="_blank" rel="noopener">内置参数</a>，包括 Logstash 轮询循环中当前迭代的起始点，上面 JDBC 输入配置中的 SELECT 语句便会引用这一参数。该字段会设置为 “unix_ts_in_secs”（读取自 .logstash_jdbc_last_run）的最新值。在 Logstash 轮询循环内所执行的 MySQL 查询中，其会用作所返回文档的起点。通过在查询中加入这一变量，能够确保不会将之前传播到 Elasticsearch 的插入或更新内容重新发送到 Elasticsearch。</li><li><code>schedule</code>：其会使用 cron 语法来指定 Logstash 应当以什么频率对 MySQL 进行轮询以查找变更。这里所指定的 <code>&quot;*/5 * * * * *&quot;</code> 会告诉 Logstash 每 5 秒钟联系一次 MySQL。</li><li><code>modification_time &lt; NOW()</code>：SELECT 中的这一部分是一个较难解释的概念，我们会在下一部分详加解释。</li><li><code>filter</code>：在这一部分，我们只需简单地将 MySQL 记录中的 “id” 值复制到名为 “_id” 的元数据字段，因为我们之后输出时会引用这一字段，以确保写入 Elasticsearch 的每个文档都有正确的 “_id” 值。通过使用元数据字段，可以确保这一临时值不会导致创建新的字段。我们还从文档中删除了 “id”、“@version” 和 “unix_ts_in_secs” 字段，因为我们不希望将这些字段写入到 Elasticsearch 中。</li><li><code>output</code>：在这一部分，我们指定每个文档都应当写入 Elasticsearch，还需为其分配一个 “_id”（需从我们在筛选部分所创建的元数据字段提取出来）。还会有一个包含被注释掉代码的 rubydebug 输出，启用此输出后能够帮助您进行故障排查。</li></ul><h2 id="SELECT-语句正确性分析"><a href="#SELECT-语句正确性分析" class="headerlink" title="SELECT 语句正确性分析"></a>SELECT 语句正确性分析</h2><p>在这一部分，我们会详加解释为什么在 SELECT 语句中添加 <code>modification_time &lt; NOW()</code> 至关重要。为帮助解释这一概念，我们首先给出几个反面例子，向您演示为什么两种最直观的方法行不通。然后会解释为什么添加 <code>modification_time &lt; NOW()</code> 能够克服那两种直观方法所导致的问题。</p><h3 id="直观方法应用情况：一"><a href="#直观方法应用情况：一" class="headerlink" title="直观方法应用情况：一"></a>直观方法应用情况：一</h3><p>在这一部分，我们会演示如果 WHERE 子句中不包括 <code>modification_time &lt; NOW()</code>，而仅仅指定 <code>UNIX_TIMESTAMP(modification_time) &gt; :sql_last_value</code> 的话，会发生什么情况。在这种情况下，SELECT 语句如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">statement =&gt; &quot;SELECT *, UNIX_TIMESTAMP(modification_time) AS unix_ts_in_secs FROM es_table WHERE (UNIX_TIMESTAMP(modification_time) &gt; :sql_last_value) ORDER BY modification_time ASC&quot;</span><br></pre></td></tr></table></figure><p>乍看起来，上面的方法好像应可以正常运行，但是对于一些边缘情况，其可能会错过一些文档。举例说明，我们假设 MySQL 现在每秒插入两个文档，Logstash 每 5 秒执行一次 SELECT 语句。具体如下图所示，T0 到 T10 分别代表每一秒，MySQL 中的数据则以 R1 到 R22 表示。我们假定 Logstash 轮询循环的第一个迭代发生在 T5，其会读取文档 R1 到 R11，如蓝绿色的方框所示。在 <code>sql_last_value</code> 中存储的值现在是 T5，因为这是所读取最后一条记录 (R11) 的时间戳。我们还假设在 Logstash 从 MySQL 读取完文件后，另一个时间戳为 T5 的文档 R12 立即插入到了 MySQL 中。</p><p><img src="/如何使用 Logstash 和 JDBC 确保 Elasticsearch 与关系型数据库保持同步.assets/how-to-sync-es-image1.jpg" alt="图表显示读取记录时会错开一条"></p><p>在上述 SELECT 语句的下一个迭代中，我们仅会提取时间晚于 T5 的文档（因为 <code>WHERE (UNIX_TIMESTAMP(modification_time) &gt; :sql_last_value)</code> 就是如此规定的），这也就意味着将会跳过记录 R12。您可以参看下面的图表，其中蓝绿色方框表示 Logstash 在当前迭代中读取的记录，灰色方框表示 Logstash 之前读取的记录。</p><p><img src="/如何使用 Logstash 和 JDBC 确保 Elasticsearch 与关系型数据库保持同步.assets/how-to-sync-es-image2.jpg" alt="图表显示肯定不会写入 R12"></p><p>请注意，如果使用这种情况中的 SELECT 语句，记录 R12 永远不会写到 Elasticsearch 中。</p><h3 id="直观方法应用情况：二"><a href="#直观方法应用情况：二" class="headerlink" title="直观方法应用情况：二"></a>直观方法应用情况：二</h3><p>为了解决上面的问题，您可能决定更改 WHERE 子句为 greater than or equals（晚于或等于），具体如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">statement =&gt; &quot;SELECT *, UNIX_TIMESTAMP(modification_time) AS unix_ts_in_secs FROM es_table WHERE (UNIX_TIMESTAMP(modification_time) &gt;= :sql_last_value) ORDER BY modification_time ASC&quot;</span><br></pre></td></tr></table></figure><p>然而，这种实施策略也并不理想。这种情况下的问题是：在最近一个时间间隔内从 MySQL 读取的最近文档会重复发送到 Elasticsearch。尽管这不会对结果的正确性造成任何影响，但的确做了无用功。和前一部分类似，在最初的 Logstash 轮询迭代后，下图显示了已经从 MySQL 读取了哪些文档。</p><p><img src="/如何使用 Logstash 和 JDBC 确保 Elasticsearch 与关系型数据库保持同步.assets/how-to-sync-es-image3.jpg" alt="图表仍显示读取记录时会错开一条"></p><p>当执行后续的 Logstash 轮询迭代时，我们会将时间晚于或等于 T5 的文档全部提取出来。可以参见下面的图表。请注意：记录 11（紫色显示）会再次发送到 Elasticsearch。</p><p><img src="https://images.contentstack.io/v3/assets/bltefdd0b53724fa2ce/blt3271df7272a7a495/5d098547616162aa5a857b42/how-to-sync-es-image4.jpg" alt="图表显示紫色记录 (/如何使用 Logstash 和 JDBC 确保 Elasticsearch 与关系型数据库保持同步.assets/how-to-sync-es-image4.jpg) 会重复发送"></p><p>前面两种情况都不甚理想。在第一种情况中，会丢失数据，而在第二种情况中，会从 MySQL 读取冗余数据并将这些数据发送到 Elasticsearch。</p><h3 id="如何解决直观方法所带来的的问题"><a href="#如何解决直观方法所带来的的问题" class="headerlink" title="如何解决直观方法所带来的的问题"></a>如何解决直观方法所带来的的问题</h3><p>鉴于前面两种情况都不太理想，应该采用另一种办法。通过指定 <code>(UNIX_TIMESTAMP(modification_time) &gt; :sql_last_value AND modification_time &lt; NOW())</code>，我们会将每个文档都发送到 Elasticsearch，而且只发送一次。</p><p>请参见下面的图表，其中当前的 Logstash 轮询会在 T5 执行。请注意，由于必须满足 <code>modification_time &lt; NOW()</code>，所以只会从 MySQL 中读取截至（但不包括）时间段 T5 的文档。由于我们已经提取了 T4 的全部文档，而未读取 T5 的任何文档，所以我们知道对于下一次的Logstash 轮询迭代，<code>sql_last_value</code> 将会被设置为 T4。</p><p><img src="/如何使用 Logstash 和 JDBC 确保 Elasticsearch 与关系型数据库保持同步.assets/how-to-sync-es-image5.jpg" alt="图表显示所读取记录的正确数量"></p><p>下图演示了在 Logstash 轮询的下一次迭代中将会发生什么情况。由于 <code>UNIX_TIMESTAMP(modification_time) &gt; :sql_last_value</code>，并且 <code>sql_last_value</code> 设置为 T4，我们知道仅会从 T5 开始提取文档。此外，由于只会提取满足 <code>modification_time &lt; NOW()</code> 的文档，所以仅会提取到截至（含）T9 的文档。再说一遍，这意味着 T9 中的所有文档都已提取出来，而且对于下一次迭代 <code>sql_last_value</code> 将会设置为 T9。所以这一方法消除了对于任何给定时间间隔仅检索到 MySQL 文档的一个子集的风险。</p><p><img src="/如何使用 Logstash 和 JDBC 确保 Elasticsearch 与关系型数据库保持同步.assets/how-to-sync-es-image6.jpg" alt="图表显示正确读取的第二个记录集合"></p><h2 id="测试系统"><a href="#测试系统" class="headerlink" title="测试系统"></a>测试系统</h2><p>可以通过一些简单测试来展示我们的实施方案能够实现预期效果。我们可以使用下列命令向 MySQL 中写入记录：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">INSERT INTO es_table (id, client_name) VALUES (1, &apos;Jim Carrey&apos;);</span><br><span class="line">INSERT INTO es_table (id, client_name) VALUES (2, &apos;Mike Myers&apos;);</span><br><span class="line">INSERT INTO es_table (id, client_name) VALUES (3, &apos;Bryan Adams&apos;);</span><br></pre></td></tr></table></figure><p>JDBC 输入计划触发了从 MySQL 读取记录的操作并将记录写入 Elasticsearch 后，我们即可运行下列 Elasticsearch 查询来查看 Elasticsearch 中的文档：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">GET rdbms_sync_idx/_search</span><br></pre></td></tr></table></figure><p>其会返回类似下面回复的内容：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">&quot;hits&quot; : &#123;</span><br><span class="line">    &quot;total&quot; : &#123;</span><br><span class="line">      &quot;value&quot; :3,</span><br><span class="line">      &quot;relation&quot; : &quot;eq&quot;</span><br><span class="line">    &#125;,</span><br><span class="line">    &quot;max_score&quot; :1.0,</span><br><span class="line">    &quot;hits&quot; : [</span><br><span class="line">      &#123;</span><br><span class="line">        &quot;_index&quot; : &quot;rdbms_sync_idx&quot;,</span><br><span class="line">        &quot;_type&quot; : &quot;_doc&quot;,</span><br><span class="line">        &quot;_id&quot; :&quot;1&quot;,</span><br><span class="line">        &quot;_score&quot; :1.0,</span><br><span class="line">        &quot;_source&quot; : &#123;</span><br><span class="line">          &quot;insertion_time&quot; :&quot;2019-06-18T12:58:56.000Z&quot;,</span><br><span class="line">          &quot;@timestamp&quot; :&quot;2019-06-18T13:04:27.436Z&quot;,</span><br><span class="line">          &quot;modification_time&quot; :&quot;2019-06-18T12:58:56.000Z&quot;,</span><br><span class="line">          &quot;client_name&quot; :&quot;Jim Carrey&quot;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;,</span><br><span class="line">Etc …</span><br></pre></td></tr></table></figure><p>然后我们可以使用下列命令更新在 MySQL 中对应至 <code>_id=1</code> 的文档：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">UPDATE es_table SET client_name = &apos;Jimbo Kerry&apos; WHERE id=1;</span><br></pre></td></tr></table></figure><p>其会正确更新 _id 被识别为 1 的文档。我们可以通过运行下列命令直接查看 Elasticsearch 中的文档：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">GET rdbms_sync_idx/_doc/1</span><br></pre></td></tr></table></figure><p>其会返回一个类似下面的文档：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  &quot;_index&quot; : &quot;rdbms_sync_idx&quot;,</span><br><span class="line">  &quot;_type&quot; : &quot;_doc&quot;,</span><br><span class="line">  &quot;_id&quot; :&quot;1&quot;,</span><br><span class="line">  &quot;_version&quot; :2,</span><br><span class="line">  &quot;_seq_no&quot; :3,</span><br><span class="line">  &quot;_primary_term&quot; :1,</span><br><span class="line">  &quot;found&quot; : true,</span><br><span class="line">  &quot;_source&quot; : &#123;</span><br><span class="line">    &quot;insertion_time&quot; :&quot;2019-06-18T12:58:56.000Z&quot;,</span><br><span class="line">    &quot;@timestamp&quot; :&quot;2019-06-18T13:09:30.300Z&quot;,</span><br><span class="line">    &quot;modification_time&quot; :&quot;2019-06-18T13:09:28.000Z&quot;,</span><br><span class="line">    &quot;client_name&quot; :&quot;Jimbo Kerry&quot;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>请注意 <code>_version</code> 现已设置为 2，<code>modification_time</code> 现在已不同于 <code>insertion_time</code>，并且 <code>client_name</code> 字段已正确更新至新值。在本例中，<code>@timestamp</code> 字段的用处并不大，由 Logstash 默认添加。</p><p>MySQL 中的更新/插入 (upsert) 可通过下列命令完成，您可以验证正确信息是否会反映在 Elasticsearch 中：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">INSERT INTO es_table (id, client_name) VALUES (4, &apos;Bob is new&apos;) ON DUPLICATE KEY UPDATE client_name=&apos;Bob exists already&apos;;</span><br></pre></td></tr></table></figure><h2 id="那么删除文档呢？"><a href="#那么删除文档呢？" class="headerlink" title="那么删除文档呢？"></a>那么删除文档呢？</h2><p>聪明的读者可能已经发现，如果从 MySQL 中删除一个文档，那么这一删除操作并不会传播到 Elasticsearch。可以考虑通过下列方法来解决这一问题：</p><ol><li>MySQL 记录可以包含一个 “is_deleted” 字段，用来显示该条记录是否仍有效。这一方法被称为“软删除”。正如对 MySQL 中的记录进行其他更新一样，”is_deleted” 字段将会通过 Logstash 传播至 Elasticsearch。如果实施这一方法，则需要编写 Elasticsearch 和 MySQL 查询，从而将 “is_deleted” 为 “true”（正）的记录/文档排除在外。 最后，可以通过后台作业来从 MySQL 和 Elastic 中移除此类文档。</li><li>另一种方法是确保负责从 MySQL 中删除记录的任何系统随后也会执行一条命令，从而直接从 Elasticsearch 中删除相应文档。</li></ol><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>在本篇博文中，我演示了如何使用 Logstash 来将 Elasticsearch 与关系型数据库保持同步。在这里所列出的代码和方法已使用 MySQL 进行测试，但理论上应该适用于任何关系数据库管理系统 (RDBMS)。</p><p>如果对 Logstash 或任何其他 Elasticsearch 相关主题有疑问，请在<a href="https://discuss.elastic.co/" target="_blank" rel="noopener">讨论论坛</a>中查看各种宝贵的讨论、见解和信息。而且，不要忘记试用 <a href="https://www.elastic.co/cn/cloud/elasticsearch-service" target="_blank" rel="noopener">Elasticsearch Service</a>，这是由 Elasticsearch 开发公司提供支持的唯一一款托管型 Elasticsearch 和 Kibana 产品。</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Thu Jan 09 2020 14:59:07 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;为了充分利用 Elasticsearch 提供的强大搜索功能，很多公司都会在既有关系型数据库的基础上再部署 Elasticsearch。在这种情况下，很可能需要确保 Elasticsearch 与所关联关系型数据库中的数据保持同步。因此，在本篇博文中，我会演示如何使用 Logstash 来高效地复制数据并将关系型数据库中的更新同步到 Elasticsearch 中。本文中所列出的代码和方法已使用 MySQL 进行过测试，但理论上应该适用于任何关系数据库管理系统 (RDBMS)。&lt;/p&gt;&lt;h2 id=&quot;系统配置&quot;&gt;&lt;a href=&quot;#系统配置&quot; class=&quot;headerlink&quot; title=&quot;系统配置&quot;&gt;&lt;/a&gt;系统配置&lt;/h2&gt;&lt;p&gt;在本篇文章中，我使用下列产品进行测试：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;a href=&quot;https://dev.mysql.com/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;MySQL&lt;/a&gt;：8.0.16&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://www.elastic.co/guide/en/elasticsearch/reference/7.1/index.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Elasticsearch&lt;/a&gt;：7.1.1&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://www.elastic.co/guide/en/logstash/7.1/introduction.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Logstash&lt;/a&gt;：7.1.1&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://www.java.com/en/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Java&lt;/a&gt;：1.8.0_162-b12&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://www.elastic.co/guide/en/logstash/7.1/plugins-inputs-jdbc.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;JDBC 输入插件&lt;/a&gt;：v4.3.13&lt;/li&gt;&lt;li&gt;&lt;a href=&quot;https://dev.mysql.com/downloads/connector/j/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;JDBC 连接器&lt;/a&gt;：Connector/J 8.0.16&lt;/li&gt;&lt;/ul&gt;&lt;h2 id=&quot;同步步骤整体概览&quot;&gt;&lt;a href=&quot;#同步步骤整体概览&quot; class=&quot;headerlink&quot; title=&quot;同步步骤整体概览&quot;&gt;&lt;/a&gt;同步步骤整体概览&lt;/h2&gt;&lt;p&gt;在本篇博文中，我们使用 Logstash 和 JDBC 输入插件来让 Elasticsearch 与 MySQL 保持同步。从概念上讲，Logstash 的 JDBC 输入插件会运行一个循环来定期对 MySQL 进行轮询，从而找出在此次循环的上次迭代后插入或更改的记录。如要让其正确运行，必须满足下列条件：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;在将 MySQL 中的文档写入 Elasticsearch 时，Elasticsearch 中的 “_id” 字段必须设置为 MySQL 中的 “id” 字段。这可在 MySQL 记录与 Elasticsearch 文档之间建立一个直接映射关系。如果在 MySQL 中更新了某条记录，那么将会在 Elasticsearch 中覆盖整条相关记录。请注意，在 Elasticsearch 中覆盖文档的效率与&lt;a href=&quot;https://www.elastic.co/guide/en/elasticsearch/reference/6.7/getting-started-update-documents.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;更新操作&lt;/a&gt;的效率一样高，因为从内部原理上来讲，更新便包括删除旧文档以及随后对全新文档进行索引。&lt;/li&gt;&lt;li&gt;当在 MySQL 中插入或更新数据时，该条记录必须有一个包含更新或插入时间的字段。通过此字段，便可允许 Logstash 仅请求获得在轮询循环的上次迭代后编辑或插入的文档。Logstash 每次对 MySQL 进行轮询时，都会保存其从 MySQL 所读取最后一条记录的更新或插入时间。在下一次迭代时，Logstash 便知道其仅需请求获得符合下列条件的记录：更新或插入时间晚于在轮询循环中的上一次迭代中所收到的最后一条记录。&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;如果满足上述条件，我们便可配置 Logstash，以定期请求从 MySQL 获得新增或已编辑的全部记录，然后将它们写入 Elasticsearch 中。完成这些操作的 Logstash 代码在本篇博文的后面会列出。&lt;/p&gt;
    
    </summary>
    
      <category term="elk" scheme="https://yongnights.github.io/categories/elk/"/>
    
    
      <category term="elk" scheme="https://yongnights.github.io/tags/elk/"/>
    
  </entry>
  
  <entry>
    <title>logstash知识点</title>
    <link href="https://yongnights.github.io/2020/01/09/logstash%E7%9F%A5%E8%AF%86%E7%82%B9/"/>
    <id>https://yongnights.github.io/2020/01/09/logstash知识点/</id>
    <published>2020-01-09T03:03:56.696Z</published>
    <updated>2020-01-10T08:33:01.411Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Fri Jan 10 2020 17:45:51 GMT+0800 (GMT+08:00) --><ol start="0"><li>Logstash是位于Data和Elasticsearch之间的一个中间件。Logstash是一个功能强大的工具，可与各种部署集成。 它提供了大量插件。 它从数据源实时地把数据进行采集，可帮助您解析，丰富，转换和缓冲来自各种来源的数据，并最终把数据传入到Elasticsearch之中。 如果您的数据需要Beats中没有的其他处理，则需要将Logstash添加到部署中。Logstash部署于ingest node之中。<br>0.1 默认情况下，Logstash在管道（pipeline）阶段之间使用内存中有界队列（输入到过滤器和过滤器到输出）来缓冲事件。 如果Logstash不安全地终止，则存储在内存中的所有事件都将丢失。 为防止数据丢失，您可以使Logstash通过使用持久队列将正在进行的事件持久化到磁盘上。可以通过在logstash.yml文件中设置queue.type：persisted属性来启用持久队列，该文件位于LOGSTASH_HOME/config文件夹下。 logstash.yml是一个配置文件，其中包含与Logstash相关的设置。 默认情况下，文件存储在LOGSTASH_HOME/data/queue中。 您可以通过在logstash.yml中设置path.queue属性来覆盖它。</li><li>在使用logstash之前,必须要先安装JAVA</li><li>下载地址:<a href="https://artifacts.elastic.co/downloads/logstash/logstash-7.3.0.tar.gz" target="_blank" rel="noopener">https://artifacts.elastic.co/downloads/logstash/logstash-7.3.0.tar.gz</a> (里面的版本号可以根据实际情况进行修改)</li><li><p>运行最基本的Logstash管道</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd logstash-7.3.0</span><br><span class="line">bin/logstash -e &apos;input &#123; stdin &#123; &#125; &#125; output &#123; stdout &#123;&#125; &#125;&apos;</span><br></pre></td></tr></table></figure></li><li><p>创建logstash.conf文件来运行管道</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"># logstash.conf文件内容</span><br><span class="line">input &#123; </span><br><span class="line">    stdin&#123; &#125;</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line">output &#123;</span><br><span class="line">    stdout &#123;</span><br><span class="line">       codec =&gt; rubydebug</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"># 运行</span><br><span class="line">./bin/logstash -f logstash.conf (path_to_logstash_conf_file)</span><br></pre></td></tr></table></figure></li></ol><blockquote><p>提示：在运行Logstash时使用<code>-r</code>标志可让您在更改和保存配置后自动重新加载配置。 在测试新配置时，这将很有用，因为您可以对其进行修改，这样就不必在每次更改配置时都手动启动Logstash。</p></blockquote><a id="more"></a><ol start="5"><li><p>获得所有的plugins</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/logstash-plugin list</span><br></pre></td></tr></table></figure></li><li><p>input读取csv文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">input &#123;</span><br><span class="line">file &#123;</span><br><span class="line">path =&gt; &quot;/Users/liuxg/data/cars.csv&quot;</span><br><span class="line">start_position =&gt; &quot;beginning&quot;</span><br><span class="line">sincedb_path =&gt; &quot;null&quot;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ol><p>在input中，定义了一个文件，它的path指向csv文件的位置。start_position指向beginning。如果对于一个实时的数据源来说，它通常是ending，这样表示它每次都是从最后拿到那个数据。sincedb_path通常指向一个文件。这个文件保存上次操作的位置。设置为/dev/null，表明不存储这个数据</p><ol start="7"><li><p>在Logstash中，按照顺序执行的处理方式被叫做一个pipeline。一个pipeline含有一个按照顺序执行的逻辑数据流。pipeline从input里获取数据，并传送给一个队列，并接着传入到一些worker去处理</p></li><li><p>官方提供的lostash关于apache,nginx应用的日志处理样本，网站: <a href="https://github.com/elastic/examples/tree/master/Common%20Data%20Formats" target="_blank" rel="noopener">https://github.com/elastic/examples/tree/master/Common%20Data%20Formats</a></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"># apache_logstash.conf</span><br><span class="line">input &#123;  </span><br><span class="line">  stdin &#123; &#125; </span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">filter &#123;</span><br><span class="line">  grok &#123;</span><br><span class="line">    match =&gt; &#123;</span><br><span class="line">      &quot;message&quot; =&gt; &apos;%&#123;IPORHOST:clientip&#125; %&#123;USER:ident&#125; %&#123;USER:auth&#125; \[%&#123;HTTPDATE:timestamp&#125;\] &quot;%&#123;WORD:verb&#125; %&#123;DATA:request&#125; HTTP/%&#123;NUMBER:httpversion&#125;&quot; %&#123;NUMBER:response:int&#125; (?:-|%&#123;NUMBER:bytes:int&#125;) %&#123;QS:referrer&#125; %&#123;QS:agent&#125;&apos;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  date &#123;</span><br><span class="line">    match =&gt; [ &quot;timestamp&quot;, &quot;dd/MMM/YYYY:HH:mm:ss Z&quot; ]</span><br><span class="line">    locale =&gt; en</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  geoip &#123;</span><br><span class="line">    source =&gt; &quot;clientip&quot;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  useragent &#123;</span><br><span class="line">    source =&gt; &quot;agent&quot;</span><br><span class="line">    target =&gt; &quot;useragent&quot;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">output &#123;</span><br><span class="line">  stdout &#123;</span><br><span class="line">    codec =&gt; dots &#123;&#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  elasticsearch &#123;</span><br><span class="line">    index =&gt; &quot;apache_elastic_example&quot;</span><br><span class="line">    template =&gt; &quot;./apache_template.json&quot;</span><br><span class="line">    template_name =&gt; &quot;apache_elastic_example&quot;</span><br><span class="line">    template_overwrite =&gt; true</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line"># apache_template.json</span><br><span class="line">&#123;</span><br><span class="line"></span><br><span class="line">  &quot;template&quot;: &quot;apache_elastic_example&quot;,</span><br><span class="line">  &quot;settings&quot;: &#123;</span><br><span class="line">     &quot;index.refresh_interval&quot;: &quot;5s&quot;</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;mappings&quot;: &#123;</span><br><span class="line">     &quot;_default_&quot;: &#123;</span><br><span class="line">        &quot;dynamic_templates&quot;: [</span><br><span class="line">           &#123;</span><br><span class="line">              &quot;message_field&quot;: &#123;</span><br><span class="line">                 &quot;mapping&quot;: &#123;</span><br><span class="line">                    &quot;norms&quot;: false,</span><br><span class="line">                    &quot;type&quot;: &quot;text&quot;</span><br><span class="line">                 &#125;,</span><br><span class="line">                 &quot;match_mapping_type&quot;: &quot;string&quot;,</span><br><span class="line">                 &quot;match&quot;: &quot;message&quot;</span><br><span class="line">              &#125;</span><br><span class="line">           &#125;,</span><br><span class="line">           &#123;</span><br><span class="line">              &quot;string_fields&quot;: &#123;</span><br><span class="line">                 &quot;mapping&quot;: &#123;</span><br><span class="line">                    &quot;norms&quot;: false,</span><br><span class="line">                    &quot;type&quot;: &quot;text&quot;,</span><br><span class="line">                    &quot;fields&quot;: &#123;</span><br><span class="line">                       &quot;keyword&quot;: &#123;</span><br><span class="line">                          &quot;ignore_above&quot;: 256,</span><br><span class="line">                          &quot;type&quot;: &quot;keyword&quot;</span><br><span class="line">                       &#125;</span><br><span class="line">                    &#125;</span><br><span class="line">                 &#125;,</span><br><span class="line">                 &quot;match_mapping_type&quot;: &quot;string&quot;,</span><br><span class="line">                 &quot;match&quot;: &quot;*&quot;</span><br><span class="line">              &#125;</span><br><span class="line">           &#125;</span><br><span class="line">        ],</span><br><span class="line">        &quot;properties&quot;: &#123;</span><br><span class="line">           &quot;geoip&quot;: &#123;</span><br><span class="line">              &quot;dynamic&quot;: true,</span><br><span class="line">              &quot;properties&quot;: &#123;</span><br><span class="line">                 &quot;location&quot;: &#123;</span><br><span class="line">                    &quot;type&quot;: &quot;geo_point&quot;</span><br><span class="line">                 &#125;,</span><br><span class="line">                 &quot;ip&quot;: &#123;</span><br><span class="line">                   &quot;type&quot;: &quot;ip&quot;</span><br><span class="line">                 &#125;,</span><br><span class="line">                 &quot;continent_code&quot;: &#123;</span><br><span class="line">                  &quot;type&quot;: &quot;keyword&quot;</span><br><span class="line">                 &#125;,</span><br><span class="line">                 &quot;country_name&quot;: &#123;</span><br><span class="line">                   &quot;type&quot;: &quot;keyword&quot;</span><br><span class="line">                 &#125;</span><br><span class="line">              &#125;,</span><br><span class="line">              &quot;type&quot;: &quot;object&quot;</span><br><span class="line">           &#125;,</span><br><span class="line">           &quot;@version&quot;: &#123;</span><br><span class="line">              &quot;type&quot;: &quot;keyword&quot;</span><br><span class="line">           &#125;</span><br><span class="line">        &#125;</span><br><span class="line">     &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"># nginx_logstash.conf</span><br><span class="line">input &#123;</span><br><span class="line">  stdin &#123; &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">filter &#123;</span><br><span class="line">  grok &#123;</span><br><span class="line">    match =&gt; &#123;</span><br><span class="line">      &quot;message&quot; =&gt; &apos;%&#123;IPORHOST:remote_ip&#125; - %&#123;DATA:user_name&#125; \[%&#123;HTTPDATE:time&#125;\] &quot;%&#123;WORD:request_action&#125; %&#123;DATA:request&#125; HTTP/%&#123;NUMBER:http_version&#125;&quot; %&#123;NUMBER:response&#125; %&#123;NUMBER:bytes&#125; &quot;%&#123;DATA:referrer&#125;&quot; &quot;%&#123;DATA:agent&#125;&quot;&apos;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  date &#123;</span><br><span class="line">    match =&gt; [ &quot;time&quot;, &quot;dd/MMM/YYYY:HH:mm:ss Z&quot; ]</span><br><span class="line">    locale =&gt; en</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  geoip &#123;</span><br><span class="line">    source =&gt; &quot;remote_ip&quot;</span><br><span class="line">    target =&gt; &quot;geoip&quot;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  useragent &#123;</span><br><span class="line">    source =&gt; &quot;agent&quot;</span><br><span class="line">    target =&gt; &quot;user_agent&quot;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">output &#123;</span><br><span class="line">stdout &#123;</span><br><span class="line"> codec =&gt; dots &#123;&#125;</span><br><span class="line"> &#125;</span><br><span class="line">  elasticsearch &#123;</span><br><span class="line">    index =&gt; &quot;nginx_elastic_stack_example&quot;</span><br><span class="line">    document_type =&gt; &quot;logs&quot;</span><br><span class="line">    template =&gt; &quot;./nginx_template.json&quot;</span><br><span class="line">    template_name =&gt; &quot;nginx_elastic_stack_example&quot;</span><br><span class="line">    template_overwrite =&gt; true</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line"># nginx_template.json</span><br><span class="line">&#123;</span><br><span class="line"></span><br><span class="line">  &quot;template&quot;: &quot;nginx_elastic_stack_example&quot;,</span><br><span class="line">  &quot;settings&quot;: &#123;</span><br><span class="line">     &quot;index.refresh_interval&quot;: &quot;5s&quot;</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;mappings&quot;: &#123;</span><br><span class="line">     &quot;_default_&quot;: &#123;</span><br><span class="line">        &quot;dynamic_templates&quot;: [</span><br><span class="line">           &#123;</span><br><span class="line">              &quot;message_field&quot;: &#123;</span><br><span class="line">                 &quot;mapping&quot;: &#123;</span><br><span class="line">                    &quot;index&quot;: &quot;analyzed&quot;,</span><br><span class="line">                    &quot;norms&quot;: false,</span><br><span class="line">                    &quot;type&quot;: &quot;string&quot;</span><br><span class="line">                 &#125;,</span><br><span class="line">                 &quot;match_mapping_type&quot;: &quot;string&quot;,</span><br><span class="line">                 &quot;match&quot;: &quot;message&quot;</span><br><span class="line">              &#125;</span><br><span class="line">           &#125;,</span><br><span class="line">           &#123;</span><br><span class="line">              &quot;string_fields&quot;: &#123;</span><br><span class="line">                 &quot;mapping&quot;: &#123;</span><br><span class="line">                    &quot;norms&quot;: false,</span><br><span class="line">                    &quot;type&quot;: &quot;text&quot;,</span><br><span class="line">                    &quot;fields&quot;: &#123;</span><br><span class="line">                       &quot;raw&quot;: &#123;</span><br><span class="line">                          &quot;type&quot;: &quot;keyword&quot;</span><br><span class="line">                       &#125;</span><br><span class="line">                    &#125;</span><br><span class="line">                 &#125;,</span><br><span class="line">                 &quot;match_mapping_type&quot;: &quot;string&quot;,</span><br><span class="line">                 &quot;match&quot;: &quot;*&quot;</span><br><span class="line">              &#125;</span><br><span class="line">           &#125;</span><br><span class="line">        ],</span><br><span class="line">        &quot;properties&quot;: &#123;</span><br><span class="line">           &quot;geoip&quot;: &#123;</span><br><span class="line">              &quot;dynamic&quot;: true,</span><br><span class="line">              &quot;properties&quot;: &#123;</span><br><span class="line">                 &quot;location&quot;: &#123;</span><br><span class="line">                    &quot;type&quot;: &quot;geo_point&quot;</span><br><span class="line">                 &#125;</span><br><span class="line">              &#125;,</span><br><span class="line">              &quot;type&quot;: &quot;object&quot;</span><br><span class="line">           &#125;,</span><br><span class="line">           &quot;bytes&quot;: &#123;</span><br><span class="line">              &quot;type&quot;: &quot;float&quot;</span><br><span class="line">           &#125;,</span><br><span class="line">           &quot;request&quot;: &#123;</span><br><span class="line">              &quot;type&quot;: &quot;keyword&quot;</span><br><span class="line">           &#125;</span><br><span class="line">        &#125;,</span><br><span class="line">        &quot;_all&quot;: &#123;</span><br><span class="line">           &quot;enabled&quot;: true</span><br><span class="line">        &#125;</span><br><span class="line">     &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"># nginx_json_logstash.conf</span><br><span class="line">input &#123;</span><br><span class="line">  stdin &#123;</span><br><span class="line">    codec =&gt; json</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">filter &#123;</span><br><span class="line"></span><br><span class="line">  date &#123;</span><br><span class="line">    match =&gt; [&quot;time&quot;, &quot;dd/MMM/YYYY:HH:mm:ss Z&quot; ]</span><br><span class="line">    locale =&gt; en</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  geoip &#123;</span><br><span class="line">    source =&gt; &quot;remote_ip&quot;</span><br><span class="line">    target =&gt; &quot;geoip&quot;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  useragent &#123;</span><br><span class="line">    source =&gt; &quot;agent&quot;</span><br><span class="line">    target =&gt; &quot;user_agent&quot;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  grok &#123;</span><br><span class="line">    match =&gt; [ &quot;request&quot; , &quot;%&#123;WORD:request_action&#125; %&#123;DATA:request1&#125; HTTP/%&#123;NUMBER:http_version&#125;&quot; ]</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">output &#123;</span><br><span class="line">  stdout  &#123;</span><br><span class="line">    codec =&gt; dots &#123;&#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  elasticsearch &#123;</span><br><span class="line">    index =&gt; &quot;nginx_json_elastic_stack_example&quot;</span><br><span class="line">    document_type =&gt; &quot;logs&quot;</span><br><span class="line">    template =&gt; &quot;./nginx_json_template.json&quot;</span><br><span class="line">    template_name =&gt; &quot;nginx_json_elastic_stack_example&quot;</span><br><span class="line">    template_overwrite =&gt; true</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"># nginx_json_template.json</span><br><span class="line">&#123;</span><br><span class="line">  &quot;index_patterns&quot;: &quot;nginx_json_elastic&quot;,</span><br><span class="line">  &quot;settings&quot;: &#123;</span><br><span class="line">    &quot;index.refresh_interval&quot;: &quot;5s&quot;</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;mappings&quot;: &#123;</span><br><span class="line">    &quot;doc&quot;: &#123;</span><br><span class="line">      &quot;dynamic_templates&quot;: [</span><br><span class="line">        &#123;</span><br><span class="line">          &quot;message_field&quot;: &#123;</span><br><span class="line">            &quot;mapping&quot;: &#123;</span><br><span class="line">              &quot;norms&quot;: false,</span><br><span class="line">              &quot;type&quot;: &quot;text&quot;</span><br><span class="line">            &#125;,</span><br><span class="line">            &quot;match_mapping_type&quot;: &quot;string&quot;,</span><br><span class="line">            &quot;match&quot;: &quot;message&quot;</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">          &quot;string_fields&quot;: &#123;</span><br><span class="line">            &quot;mapping&quot;: &#123;</span><br><span class="line">              &quot;type&quot;: &quot;text&quot;,</span><br><span class="line">              &quot;norms&quot;: false,</span><br><span class="line">              &quot;fields&quot;: &#123;</span><br><span class="line">                &quot;keyword&quot;: &#123;</span><br><span class="line">                  &quot;type&quot;: &quot;keyword&quot;,</span><br><span class="line">                  &quot;ignore_above&quot;: 256</span><br><span class="line">                &#125;</span><br><span class="line">              &#125;</span><br><span class="line">            &#125;,</span><br><span class="line">            &quot;match_mapping_type&quot;: &quot;string&quot;,</span><br><span class="line">            &quot;match&quot;: &quot;*&quot;</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">      ],</span><br><span class="line">      &quot;properties&quot;: &#123;</span><br><span class="line">        &quot;geoip&quot;: &#123;</span><br><span class="line">          &quot;dynamic&quot;: true,</span><br><span class="line">          &quot;properties&quot;: &#123;</span><br><span class="line">            &quot;location&quot;: &#123;</span><br><span class="line">              &quot;type&quot;: &quot;geo_point&quot;</span><br><span class="line">            &#125;</span><br><span class="line">          &#125;,</span><br><span class="line">          &quot;type&quot;: &quot;object&quot;</span><br><span class="line">        &#125;,</span><br><span class="line">        &quot;request&quot;: &#123;</span><br><span class="line">          &quot;type&quot;: &quot;keyword&quot;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ol start="9"><li>处理多个input<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line"># multi-input.conf</span><br><span class="line">input &#123;</span><br><span class="line">  file &#123;</span><br><span class="line">    path =&gt; &quot;/data/multi-input/apache.log&quot;</span><br><span class="line">  start_position =&gt; &quot;beginning&quot;</span><br><span class="line">    sincedb_path =&gt; &quot;/dev/null&quot;</span><br><span class="line">    # ignore_older =&gt; 100000</span><br><span class="line">    type =&gt; &quot;apache&quot;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line">input &#123;</span><br><span class="line">  file &#123;</span><br><span class="line">    path =&gt; &quot;/data/multi-input/apache-daily-access.log&quot;</span><br><span class="line">  start_position =&gt; &quot;beginning&quot;</span><br><span class="line">    sincedb_path =&gt; &quot;/dev/null&quot;</span><br><span class="line">    type =&gt; &quot;daily&quot;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line">filter &#123;</span><br><span class="line">  grok &#123;</span><br><span class="line">    match =&gt; &#123;</span><br><span class="line">      &quot;message&quot; =&gt; &apos;%&#123;IPORHOST:clientip&#125; %&#123;USER:ident&#125; %&#123;USER:auth&#125; \[%&#123;HTTPDATE:timestamp&#125;\] &quot;%&#123;WORD:verb&#125; %&#123;DATA:request&#125; HTTP/%&#123;NUMBER:httpversion&#125;&quot; %&#123;NUMBER:response:int&#125; (?:-|%&#123;NUMBER:bytes:int&#125;) %&#123;QS:referrer&#125; %&#123;QS:agent&#125;&apos;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"> </span><br><span class="line">if [type] == &quot;apache&quot; &#123;</span><br><span class="line">mutate &#123;</span><br><span class="line">  add_tag =&gt; [&quot;apache&quot;]</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line">if [type] == &quot;daily&quot; &#123;</span><br><span class="line">mutate &#123;</span><br><span class="line">add_tag =&gt; [&quot;daily&quot;]</span><br><span class="line">&#125;</span><br><span class="line">&#125; </span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line">output &#123;</span><br><span class="line">stdout &#123;</span><br><span class="line">codec =&gt; rubydebug</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line">if &quot;apache&quot; in [tags] &#123;</span><br><span class="line">  elasticsearch &#123;</span><br><span class="line">    index =&gt; &quot;apache_log&quot;</span><br><span class="line">    template =&gt; &quot;/data/apache_template.json&quot;</span><br><span class="line">    template_name =&gt; &quot;apache_elastic_example&quot;</span><br><span class="line">    template_overwrite =&gt; true</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line">if &quot;daily&quot; in [tags] &#123;</span><br><span class="line">  elasticsearch &#123;</span><br><span class="line">    index =&gt; &quot;apache_daily&quot;</span><br><span class="line">    template =&gt; &quot;/data/apache_template.json&quot;</span><br><span class="line">    template_name =&gt; &quot;apache_elastic_example&quot;</span><br><span class="line">    template_overwrite =&gt; true</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"># 运行</span><br><span class="line">./bin/logstash -f multi-input.conf</span><br></pre></td></tr></table></figure></li></ol><p>使用了两个input。它们分别对应不同的log文件。对于这两个input，使用了不同的type来表示：apache和daily。尽管它们的格式是一样的，它们共同使用同样的一个grok filter，但是还是想分别对它们进行处理。为此，添加了一个tag。也可以添加一个field来进行区别。在output的部分，根据在filter部分设置的tag来对它们输出到不同的index里。<br>daily的事件最早被处理及输出,接着apache的数据才开始处理.</p><ol start="10"><li>处理多个配置文件(conf)<br>一个pipeline含有一个逻辑的数据流，它从input接收数据，并把它们传入到队列里，经过worker的处理，最后输出到output。这个output可以是Elasticsearch或其它</li></ol><ul><li>多个pipeline</li></ul><p>两个不同的conf配置文件<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line"># apache.conf</span><br><span class="line">input &#123;</span><br><span class="line">    file &#123;</span><br><span class="line">        path =&gt; &quot;/data/multi-input/apache.log&quot;</span><br><span class="line">        start_position =&gt; &quot;beginning&quot;</span><br><span class="line">        sincedb_path =&gt; &quot;/dev/null&quot;</span><br><span class="line">        # ignore_older =&gt; 100000</span><br><span class="line">        type =&gt; &quot;apache&quot;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line">filter &#123;</span><br><span class="line">  grok &#123;</span><br><span class="line">    match =&gt; &#123;</span><br><span class="line">      &quot;message&quot; =&gt; &apos;%&#123;IPORHOST:clientip&#125; %&#123;USER:ident&#125; %&#123;USER:auth&#125; \[%&#123;HTTPDATE:timestamp&#125;\] &quot;%&#123;WORD:verb&#125; %&#123;DATA:request&#125; HTTP/%&#123;NUMBER:httpversion&#125;&quot; %&#123;NUMBER:response:int&#125; (?:-|%&#123;NUMBER:bytes:int&#125;) %&#123;QS:referrer&#125; %&#123;QS:agent&#125;&apos;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line">output &#123;</span><br><span class="line">stdout &#123;</span><br><span class="line">codec =&gt; rubydebug</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line">  elasticsearch &#123;</span><br><span class="line">    index =&gt; &quot;apache_log&quot;</span><br><span class="line">    template =&gt; &quot;/data/apache_template.json&quot;</span><br><span class="line">    template_name =&gt; &quot;apache_elastic_example&quot;</span><br><span class="line">    template_overwrite =&gt; true</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"># daily.conf</span><br><span class="line">input &#123;</span><br><span class="line">    file &#123;</span><br><span class="line">        path =&gt; &quot;/data/multi-pipeline/apache-daily-access.log&quot;</span><br><span class="line">        start_position =&gt; &quot;beginning&quot;</span><br><span class="line">        sincedb_path =&gt; &quot;/dev/null&quot;</span><br><span class="line">        type =&gt; &quot;daily&quot;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line">filter &#123;</span><br><span class="line">  grok &#123;</span><br><span class="line">    match =&gt; &#123;</span><br><span class="line">      &quot;message&quot; =&gt; &apos;%&#123;IPORHOST:clientip&#125; %&#123;USER:ident&#125; %&#123;USER:auth&#125; \[%&#123;HTTPDATE:timestamp&#125;\] &quot;%&#123;WORD:verb&#125; %&#123;DATA:request&#125; HTTP/%&#123;NUMBER:httpversion&#125;&quot; %&#123;NUMBER:response:int&#125; (?:-|%&#123;NUMBER:bytes:int&#125;) %&#123;QS:referrer&#125; %&#123;QS:agent&#125;&apos;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line">output &#123;</span><br><span class="line">stdout &#123;</span><br><span class="line">codec =&gt; rubydebug</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line">  elasticsearch &#123;</span><br><span class="line">    index =&gt; &quot;apache_daily&quot;</span><br><span class="line">    template =&gt; &quot;/data/multi-pipeline/apache_template.json&quot;</span><br><span class="line">    template_name =&gt; &quot;apache_elastic_example&quot;</span><br><span class="line">    template_overwrite =&gt; true</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><p>在logstash的安装目录下的config文件目录下,修改pipelines.yml文件.<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"># pipelines.yml</span><br><span class="line">- pipeline.id: daily</span><br><span class="line">  pipeline.workers: 1</span><br><span class="line">  pipeline.batch.size: 1</span><br><span class="line">  path.config: &quot;/data/multi-pipeline/daily.conf&quot;</span><br><span class="line">  </span><br><span class="line">- pipeline.id: apache</span><br><span class="line">  queue.type: persisted</span><br><span class="line">  path.config: &quot;/data/multi-pipeline/apache.conf&quot;</span><br></pre></td></tr></table></figure><p></p><p>启动,注意：不使用<code>-f</code>参数指定配置文件<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/bin/logstash</span><br></pre></td></tr></table></figure><p></p><p>在终端中可以看到有两个piple在同时运行。</p><ul><li>一个pipeline<br>修改位于Logstash安装目录下的config子目录里的pipleline.yml文件<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># pipelines.yml</span><br><span class="line">- pipeline.id: my_logs</span><br><span class="line">  queue.type: persisted</span><br><span class="line">  path.config: &quot;/data/multi-pipeline/*.conf&quot;</span><br></pre></td></tr></table></figure></li></ul><p>这里把所有位于/data/multi-pipeline/下的所有的conf文件都放于一个pipeline里。<br>启动,注意：不使用<code>-f</code>参数指定配置文件<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/bin/logstash</span><br></pre></td></tr></table></figure><p></p><p>在终端中会看到两个同样的输出，这是因为把两个.conf文件放于一个pipleline里运行，那么有两个stdout的输出分别位于两个.conf文件了。<br>apache_log里有20条数据，它包括两个log文件里所有的事件，这是因为它们都是一个pipleline。同样可以在apache_daily看到同样的20条数据。</p><p>采用这种方式意味着会把两个不同的配置文件获取的日志输出到同一个索引中。合并数据的话可以使用这种方式。</p><ol start="11"><li>把MySQL数据导入到Elasticsearch中</li></ol><p>官方文档地址: <a href="https://www.elastic.co/guide/en/logstash/current/plugins-inputs-jdbc.html#plugins-inputs-jdbc-parameters" target="_blank" rel="noopener">https://www.elastic.co/guide/en/logstash/current/plugins-inputs-jdbc.html#plugins-inputs-jdbc-parameters</a></p><ul><li>MySQL安装,准备一些测试数据</li><li>Logstash安装<br>根据mysql的版本信息下载相应的JDBC connector驱动,下载网站: <a href="https://dev.mysql.com/downloads/connector/j/" target="_blank" rel="noopener">https://dev.mysql.com/downloads/connector/j/</a><br>下载完这个Connector后，把这个connector存入到Logstash安装目录下的logstash-core/lib/jars/子目录中。<br>最终地址是这样的：logstash-7.3.0/logstash-core/lib/jars/mysql-connector-java-8.0.17.jar</li><li>Logstash 配置<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"># sales.conf</span><br><span class="line">input &#123;</span><br><span class="line">jdbc &#123;</span><br><span class="line">       jdbc_connection_string =&gt; &quot;jdbc:mysql://localhost:3306/data&quot;</span><br><span class="line">       jdbc_user =&gt; &quot;root&quot;</span><br><span class="line">       jdbc_password =&gt; &quot;YourMyQLPassword&quot;</span><br><span class="line">       jdbc_validate_connection =&gt; true</span><br><span class="line">       jdbc_driver_library =&gt; &quot;&quot;</span><br><span class="line">       jdbc_driver_class =&gt; &quot;com.mysql.cj.jdbc.Driver&quot;</span><br><span class="line">       parameters =&gt; &#123; &quot;Product_id&quot; =&gt; &quot;Product1&quot; &#125;</span><br><span class="line">       statement =&gt; &quot;SELECT * FROM SalesJan2009 WHERE Product = :Product_id&quot;</span><br><span class="line">    &#125;    </span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">output &#123;</span><br><span class="line">stdout &#123;</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line">   elasticsearch &#123;</span><br><span class="line">     index =&gt; &quot;sales&quot;</span><br><span class="line">     hosts =&gt; &quot;localhost:9200&quot;</span><br><span class="line">     document_type =&gt; &quot;_doc&quot;</span><br><span class="line">&#125; </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul><p>替换jdbc_user和jdbc_password为自己的MySQL账号的用户名及密码。特别值得指出的是jdbc_driver_library按elastic的文档是可以放入JDBC驱动的路径及驱动名称。实践证明如果这个驱动不在JAVA的classpath里，是不能被正确地加载。<br>正因为这样的原因，在上一步里把驱动mysql-connector-java-8.0.17.jar放入到Logstash的jar目录里，所以这里就直接填入空字符串。</p><ul><li>运行Logstash加载数据<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./bin/logstash --debug -f sales.conf</span><br></pre></td></tr></table></figure></li></ul><p>注意：在MySQL中删除数据的话则不会自动同步删除es中的数据，需要另作处理</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Fri Jan 10 2020 17:45:51 GMT+0800 (GMT+08:00) --&gt;&lt;ol start=&quot;0&quot;&gt;&lt;li&gt;Logstash是位于Data和Elasticsearch之间的一个中间件。Logstash是一个功能强大的工具，可与各种部署集成。 它提供了大量插件。 它从数据源实时地把数据进行采集，可帮助您解析，丰富，转换和缓冲来自各种来源的数据，并最终把数据传入到Elasticsearch之中。 如果您的数据需要Beats中没有的其他处理，则需要将Logstash添加到部署中。Logstash部署于ingest node之中。&lt;br&gt;0.1 默认情况下，Logstash在管道（pipeline）阶段之间使用内存中有界队列（输入到过滤器和过滤器到输出）来缓冲事件。 如果Logstash不安全地终止，则存储在内存中的所有事件都将丢失。 为防止数据丢失，您可以使Logstash通过使用持久队列将正在进行的事件持久化到磁盘上。可以通过在logstash.yml文件中设置queue.type：persisted属性来启用持久队列，该文件位于LOGSTASH_HOME/config文件夹下。 logstash.yml是一个配置文件，其中包含与Logstash相关的设置。 默认情况下，文件存储在LOGSTASH_HOME/data/queue中。 您可以通过在logstash.yml中设置path.queue属性来覆盖它。&lt;/li&gt;&lt;li&gt;在使用logstash之前,必须要先安装JAVA&lt;/li&gt;&lt;li&gt;下载地址:&lt;a href=&quot;https://artifacts.elastic.co/downloads/logstash/logstash-7.3.0.tar.gz&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://artifacts.elastic.co/downloads/logstash/logstash-7.3.0.tar.gz&lt;/a&gt; (里面的版本号可以根据实际情况进行修改)&lt;/li&gt;&lt;li&gt;&lt;p&gt;运行最基本的Logstash管道&lt;/p&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;cd logstash-7.3.0&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;bin/logstash -e &amp;apos;input &amp;#123; stdin &amp;#123; &amp;#125; &amp;#125; output &amp;#123; stdout &amp;#123;&amp;#125; &amp;#125;&amp;apos;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;创建logstash.conf文件来运行管道&lt;/p&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;12&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;# logstash.conf文件内容&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;input &amp;#123; &lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    stdin&amp;#123; &amp;#125;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&amp;#125;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt; &lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;output &amp;#123;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    stdout &amp;#123;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;       codec =&amp;gt; rubydebug&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&amp;#125;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;# 运行&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;./bin/logstash -f logstash.conf (path_to_logstash_conf_file)&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;/li&gt;&lt;/ol&gt;&lt;blockquote&gt;&lt;p&gt;提示：在运行Logstash时使用&lt;code&gt;-r&lt;/code&gt;标志可让您在更改和保存配置后自动重新加载配置。 在测试新配置时，这将很有用，因为您可以对其进行修改，这样就不必在每次更改配置时都手动启动Logstash。&lt;/p&gt;&lt;/blockquote&gt;
    
    </summary>
    
      <category term="elk" scheme="https://yongnights.github.io/categories/elk/"/>
    
    
      <category term="elk" scheme="https://yongnights.github.io/tags/elk/"/>
    
  </entry>
  
  <entry>
    <title>filebeat知识点</title>
    <link href="https://yongnights.github.io/2020/01/09/filebeat%E7%9F%A5%E8%AF%86%E7%82%B9/"/>
    <id>https://yongnights.github.io/2020/01/09/filebeat知识点/</id>
    <published>2020-01-09T03:03:13.889Z</published>
    <updated>2020-01-09T05:52:21.863Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Thu Jan 09 2020 14:55:30 GMT+0800 (GMT+08:00) --><p>在Filebeat的根目录下，有一个叫做filebeat.yml的文件。<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">filebeat.inputs:</span><br><span class="line">- type: log</span><br><span class="line">  enabled: true</span><br><span class="line">  paths:</span><br><span class="line">    - ./sample.log</span><br><span class="line"> </span><br><span class="line">output.logstash:</span><br></pre></td></tr></table></figure><p></p><p>这里需要注意的是之前有的文章里第一行写的是filebeat.prospectors。经过测试在新的版本里不再适用。</p><p>通过如下的命令来运行filebeat:<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./filebeat</span><br></pre></td></tr></table></figure><p></p><p>在默认的情况下，filebeat会自动寻找定义在filebeat.yml文件里的配置。如果配置文件是另外的名字，可以通过如下的命令来执行filebeat:<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./filebeat -c YourYmlFile.yml</span><br></pre></td></tr></table></figure><p></p><a id="more"></a><p>Filebeat的registry文件存储Filebeat用于跟踪上次读取位置的状态和位置信息。</p><ul><li>data/registry 针对 .tar.gz and .tgz 归档文件安装</li><li>/var/lib/filebeat/registry 针对 DEB 及 RPM 安装包</li><li>c:\ProgramData\filebeat\registry 针对 Windows zip 文件</li></ul><p>如果想重新运行一遍数据，可以直接到相应的目录下删除那个叫做registry的目录即可。针对.tar.gz的安装包来说，可以直接删除这个文件。<br>那么重新运行上面的<code>./filebeat</code>命令即可。它将会重新把数据从头再进行处理一遍。这对于我调试来说是非常有用的。</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Thu Jan 09 2020 14:55:30 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;在Filebeat的根目录下，有一个叫做filebeat.yml的文件。&lt;br&gt;&lt;/p&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;filebeat.inputs:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;- type: log&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;  enabled: true&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;  paths:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    - ./sample.log&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt; &lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;output.logstash:&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;这里需要注意的是之前有的文章里第一行写的是filebeat.prospectors。经过测试在新的版本里不再适用。&lt;/p&gt;&lt;p&gt;通过如下的命令来运行filebeat:&lt;br&gt;&lt;/p&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;./filebeat&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;在默认的情况下，filebeat会自动寻找定义在filebeat.yml文件里的配置。如果配置文件是另外的名字，可以通过如下的命令来执行filebeat:&lt;br&gt;&lt;/p&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;./filebeat -c YourYmlFile.yml&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;p&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="elk" scheme="https://yongnights.github.io/categories/elk/"/>
    
    
      <category term="elk" scheme="https://yongnights.github.io/tags/elk/"/>
    
  </entry>
  
  <entry>
    <title>logstash中output{}的另类写法</title>
    <link href="https://yongnights.github.io/2020/01/09/logstash%E4%B8%ADoutput%7B%7D%E7%9A%84%E5%8F%A6%E7%B1%BB%E5%86%99%E6%B3%95/"/>
    <id>https://yongnights.github.io/2020/01/09/logstash中output{}的另类写法/</id>
    <published>2020-01-09T01:38:06.097Z</published>
    <updated>2020-01-09T01:38:33.222Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Thu Jan 09 2020 09:39:25 GMT+0800 (GMT+08:00) --><p>日志传输路径如下：<br>filebeat-&gt;redis-&gt;logstash-&gt;es</p><p>在filebeat配置文件中，收集日志的时候配置的有如下参数：<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">fields:</span><br><span class="line">  log_source: messages</span><br></pre></td></tr></table></figure><p></p><p>表示的是会把log_source作为fields的二级字段</p><p>若是配置如下，表示的是会把log_source作为顶级字段：<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">fields:</span><br><span class="line">  log_source: messages</span><br><span class="line">fields_under_root: true</span><br></pre></td></tr></table></figure><p></p><p>使用这个字段来作为区分不同应用日志的来源；</p><a id="more"></a><p>在logstash中从redis读取后，output给es的时候，根据上述不同的字段来创建不同的应用日志索引。<br>常见的写法是多使用if条件进行区分，如下所示：<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">if [fields][log_source] == &apos;test_custom&apos; &#123;</span><br><span class="line">  elasticsearch &#123;</span><br><span class="line">    hosts =&gt; [&quot;http://172.17.107.187:9203&quot;, &quot;http://172.17.107.187:9201&quot;,&quot;http://172.17.107.187:9202&quot;]</span><br><span class="line">    index =&gt; &quot;filebeat_test_custom-%&#123;+YYYY.MM.dd&#125;&quot;</span><br><span class="line">    user =&gt; &quot;elastic&quot;</span><br><span class="line">    password =&gt; &quot;escluter123456&quot;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">if [fields][log_source] == &quot;test_user&quot; &#123;</span><br><span class="line">  elasticsearch &#123;</span><br><span class="line">    hosts =&gt; [&quot;http://172.17.107.187:9203&quot;,&quot;http://172.17.107.187:9201&quot;,&quot;http://172.17.107.187:9202&quot;]</span><br><span class="line">    index =&gt; &quot;filebeat_test_user-%&#123;+YYYY.MM.dd&#125;&quot;</span><br><span class="line">    user =&gt; &quot;elastic&quot;</span><br><span class="line">    password =&gt; &quot;escluter123456&quot;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><p>这样写也能使用，但是考虑到假设这个区分字段比较多的话，那这得写多少个if条件呀，所以可以使用如下的用法：<br>在创建索引的时候使用上这个区分用的字段，具体如下：<br></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">elasticsearch &#123;</span><br><span class="line">    hosts =&gt; [&quot;http://172.17.107.187:9203&quot;,&quot;http://172.17.107.187:9201&quot;,&quot;http://172.17.107.187:9202&quot;]</span><br><span class="line">    index =&gt; &quot;filebeat_%&#123;[fields][log_source]&#125;-%&#123;+YYYY.MM.dd&#125;&quot;</span><br><span class="line">    user =&gt; &quot;elastic&quot;</span><br><span class="line">    password =&gt; &quot;escluter123456&quot;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p></p><p>说明：<code>%{[fields][log_source]}</code>表示的是获取区分字段的值</p><p>若是顶级字段则是这样的用法：<code>%{[log_source]}</code></p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Thu Jan 09 2020 09:39:25 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;日志传输路径如下：&lt;br&gt;filebeat-&amp;gt;redis-&amp;gt;logstash-&amp;gt;es&lt;/p&gt;&lt;p&gt;在filebeat配置文件中，收集日志的时候配置的有如下参数：&lt;br&gt;&lt;/p&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;fields:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;  log_source: messages&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;表示的是会把log_source作为fields的二级字段&lt;/p&gt;&lt;p&gt;若是配置如下，表示的是会把log_source作为顶级字段：&lt;br&gt;&lt;/p&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;fields:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;  log_source: messages&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;fields_under_root: true&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;使用这个字段来作为区分不同应用日志的来源；&lt;/p&gt;
    
    </summary>
    
      <category term="elk" scheme="https://yongnights.github.io/categories/elk/"/>
    
    
      <category term="elk" scheme="https://yongnights.github.io/tags/elk/"/>
    
  </entry>
  
</feed>
